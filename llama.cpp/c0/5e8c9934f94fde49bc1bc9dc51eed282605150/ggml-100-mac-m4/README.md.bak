### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.90 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.47 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.91 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.78 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.78 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.98 sec*proc (28 tests)

Total Test time (real) = 220.99 sec

real	3m41.048s
user	7m36.153s
sys	0m6.272s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.87 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.43 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.65 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.73 sec*proc (28 tests)

Total Test time (real) =  51.74 sec

real	0m51.752s
user	1m13.075s
sys	0m5.726s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.765 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.354 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.364 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.365 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.366 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.367 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.368 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.369 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.370 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.370 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.371 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.374 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.379 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.380 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.380 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.381 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.381 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.382 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.781 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.783 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.784 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.784 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.785 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.785 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.786 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.786 I llama_model_loader: - type  f32:  124 tensors
0.00.029.787 I llama_model_loader: - type  f16:   73 tensors
0.00.034.236 I llm_load_vocab: special tokens cache size = 5
0.00.036.509 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.514 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.514 I llm_load_print_meta: arch             = bert
0.00.036.515 I llm_load_print_meta: vocab type       = WPM
0.00.036.515 I llm_load_print_meta: n_vocab          = 30522
0.00.036.516 I llm_load_print_meta: n_merges         = 0
0.00.036.516 I llm_load_print_meta: vocab_only       = 0
0.00.036.518 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.518 I llm_load_print_meta: n_embd           = 384
0.00.036.518 I llm_load_print_meta: n_layer          = 12
0.00.036.522 I llm_load_print_meta: n_head           = 12
0.00.036.525 I llm_load_print_meta: n_head_kv        = 12
0.00.036.526 I llm_load_print_meta: n_rot            = 32
0.00.036.526 I llm_load_print_meta: n_swa            = 0
0.00.036.526 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.526 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.527 I llm_load_print_meta: n_gqa            = 1
0.00.036.528 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.529 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.530 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.531 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.531 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.532 I llm_load_print_meta: n_ff             = 1536
0.00.036.535 I llm_load_print_meta: n_expert         = 0
0.00.036.535 I llm_load_print_meta: n_expert_used    = 0
0.00.036.536 I llm_load_print_meta: causal attn      = 0
0.00.036.536 I llm_load_print_meta: pooling type     = 2
0.00.036.536 I llm_load_print_meta: rope type        = 2
0.00.036.536 I llm_load_print_meta: rope scaling     = linear
0.00.036.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.537 I llm_load_print_meta: freq_scale_train = 1
0.00.036.538 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.539 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.539 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.539 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.540 I llm_load_print_meta: model type       = 33M
0.00.036.564 I llm_load_print_meta: model ftype      = F16
0.00.036.567 I llm_load_print_meta: model params     = 33.21 M
0.00.036.568 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.568 I llm_load_print_meta: general.name     = Bge Small
0.00.036.569 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.569 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.569 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.569 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.572 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.572 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.572 I llm_load_print_meta: max token length = 21
0.00.038.606 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.038.606 I llm_load_tensors: offloading output layer to GPU
0.00.038.606 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.038.634 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.635 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.939 I llama_new_context_with_model: n_ctx         = 512
0.00.038.939 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.940 I llama_new_context_with_model: n_batch       = 2048
0.00.038.940 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.940 I llama_new_context_with_model: flash_attn    = 0
0.00.038.941 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.941 I llama_new_context_with_model: freq_scale    = 1
0.00.038.942 I ggml_metal_init: allocating
0.00.038.957 I ggml_metal_init: found device: Apple M4
0.00.038.963 I ggml_metal_init: picking default device: Apple M4
0.00.039.904 I ggml_metal_init: using embedded metal library
0.00.044.297 I ggml_metal_init: GPU name:   Apple M4
0.00.044.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.302 I ggml_metal_init: simdgroup reduction   = true
0.00.044.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.302 I ggml_metal_init: has bfloat            = true
0.00.044.302 I ggml_metal_init: use bfloat            = true
0.00.044.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.763 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.057.330 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.332 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.334 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.092 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.094 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.094 I llama_new_context_with_model: graph nodes  = 429
0.00.058.094 I llama_new_context_with_model: graph splits = 2
0.00.058.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.058.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.471 I 
0.00.065.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.183 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.379 I llama_perf_context_print:        load time =      47.70 ms
0.00.071.380 I llama_perf_context_print: prompt eval time =       5.04 ms /     9 tokens (    0.56 ms per token,  1785.01 tokens per second)
0.00.071.381 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.381 I llama_perf_context_print:       total time =       5.91 ms /    10 tokens
0.00.071.528 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.051s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.249 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.014 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.020 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.025 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.025 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.027 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.028 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.028 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.028 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.029 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.029 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.031 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.031 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.032 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.032 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.032 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.033 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.485 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.165 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.166 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.167 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.167 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.167 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.168 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.168 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.168 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.169 I llama_model_loader: - type  f32:  124 tensors
0.00.015.169 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.491 I llm_load_vocab: special tokens cache size = 5
0.00.018.823 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.827 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.828 I llm_load_print_meta: arch             = bert
0.00.018.828 I llm_load_print_meta: vocab type       = WPM
0.00.018.828 I llm_load_print_meta: n_vocab          = 30522
0.00.018.830 I llm_load_print_meta: n_merges         = 0
0.00.018.830 I llm_load_print_meta: vocab_only       = 0
0.00.018.830 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.830 I llm_load_print_meta: n_embd           = 384
0.00.018.831 I llm_load_print_meta: n_layer          = 12
0.00.018.834 I llm_load_print_meta: n_head           = 12
0.00.018.834 I llm_load_print_meta: n_head_kv        = 12
0.00.018.835 I llm_load_print_meta: n_rot            = 32
0.00.018.835 I llm_load_print_meta: n_swa            = 0
0.00.018.835 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.835 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.836 I llm_load_print_meta: n_gqa            = 1
0.00.018.837 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.837 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.838 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.838 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.838 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.840 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.841 I llm_load_print_meta: n_ff             = 1536
0.00.018.841 I llm_load_print_meta: n_expert         = 0
0.00.018.841 I llm_load_print_meta: n_expert_used    = 0
0.00.018.841 I llm_load_print_meta: causal attn      = 0
0.00.018.841 I llm_load_print_meta: pooling type     = 2
0.00.018.841 I llm_load_print_meta: rope type        = 2
0.00.018.841 I llm_load_print_meta: rope scaling     = linear
0.00.018.842 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.842 I llm_load_print_meta: freq_scale_train = 1
0.00.018.842 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.842 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.843 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.843 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.843 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.843 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.843 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.843 I llm_load_print_meta: model type       = 33M
0.00.018.850 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.850 I llm_load_print_meta: model params     = 33.21 M
0.00.018.852 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.852 I llm_load_print_meta: general.name     = Bge Small
0.00.018.853 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.853 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.853 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.853 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.853 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.854 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.854 I llm_load_print_meta: max token length = 21
0.00.020.073 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.074 I llm_load_tensors: offloading output layer to GPU
0.00.020.074 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.082 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.083 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.283 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.284 I llama_new_context_with_model: n_ctx         = 512
0.00.020.284 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.284 I llama_new_context_with_model: n_batch       = 2048
0.00.020.284 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.285 I llama_new_context_with_model: flash_attn    = 0
0.00.020.285 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.285 I llama_new_context_with_model: freq_scale    = 1
0.00.020.286 I ggml_metal_init: allocating
0.00.020.289 I ggml_metal_init: found device: Apple M4
0.00.020.290 I ggml_metal_init: picking default device: Apple M4
0.00.020.896 I ggml_metal_init: using embedded metal library
0.00.023.446 I ggml_metal_init: GPU name:   Apple M4
0.00.023.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.448 I ggml_metal_init: simdgroup reduction   = true
0.00.023.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.453 I ggml_metal_init: has bfloat            = true
0.00.023.453 I ggml_metal_init: use bfloat            = true
0.00.023.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.916 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.404 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.406 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.407 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.091 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.092 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.093 I llama_new_context_with_model: graph nodes  = 429
0.00.034.093 I llama_new_context_with_model: graph splits = 2
0.00.034.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.289 I 
0.00.039.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.845 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.308 I llama_perf_context_print:        load time =      30.04 ms
0.00.044.309 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2075.17 tokens per second)
0.00.044.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.309 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.044.462 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.206 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.806 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.100 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.109 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.039.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.113 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.039.114 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.039.114 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.039.116 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.039.117 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.039.118 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.039.118 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.039.119 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.039.122 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.039.122 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.039.123 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.039.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.801 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.053.597 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.053.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.053.598 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.053.598 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.053.599 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.053.599 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.053.599 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.053.600 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.053.600 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.053.601 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.053.601 I llama_model_loader: - type  f32:   40 tensors
0.00.053.608 I llama_model_loader: - type  f16:   30 tensors
0.00.071.682 W llm_load_vocab: empty token at index 5
0.00.076.213 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.077.491 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.077.522 I llm_load_vocab: special tokens cache size = 5
0.00.335.111 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.123 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.124 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.125 I llm_load_print_meta: vocab type       = BPE
0.00.335.125 I llm_load_print_meta: n_vocab          = 61056
0.00.335.125 I llm_load_print_meta: n_merges         = 39382
0.00.335.125 I llm_load_print_meta: vocab_only       = 0
0.00.335.126 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.126 I llm_load_print_meta: n_embd           = 384
0.00.335.126 I llm_load_print_meta: n_layer          = 4
0.00.335.134 I llm_load_print_meta: n_head           = 12
0.00.335.134 I llm_load_print_meta: n_head_kv        = 12
0.00.335.135 I llm_load_print_meta: n_rot            = 32
0.00.335.135 I llm_load_print_meta: n_swa            = 0
0.00.335.135 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.135 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.135 I llm_load_print_meta: n_gqa            = 1
0.00.335.136 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.136 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.137 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.137 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.137 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.138 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.138 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.138 I llm_load_print_meta: n_ff             = 1536
0.00.335.139 I llm_load_print_meta: n_expert         = 0
0.00.335.139 I llm_load_print_meta: n_expert_used    = 0
0.00.335.139 I llm_load_print_meta: causal attn      = 0
0.00.335.139 I llm_load_print_meta: pooling type     = -1
0.00.335.139 I llm_load_print_meta: rope type        = -1
0.00.335.139 I llm_load_print_meta: rope scaling     = linear
0.00.335.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.140 I llm_load_print_meta: freq_scale_train = 1
0.00.335.140 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.140 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.140 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.141 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.142 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.143 I llm_load_print_meta: model type       = 33M
0.00.335.174 I llm_load_print_meta: model ftype      = F16
0.00.335.174 I llm_load_print_meta: model params     = 32.90 M
0.00.335.175 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.175 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.175 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.177 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.177 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.178 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.178 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.178 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.178 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.178 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.178 I llm_load_print_meta: max token length = 45
0.00.336.647 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.648 I llm_load_tensors: offloading output layer to GPU
0.00.336.648 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.674 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.675 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.239 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.240 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.240 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.240 I llama_new_context_with_model: n_batch       = 2048
0.00.337.240 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.241 I llama_new_context_with_model: flash_attn    = 0
0.00.337.241 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.242 I llama_new_context_with_model: freq_scale    = 1
0.00.337.242 I ggml_metal_init: allocating
0.00.337.246 I ggml_metal_init: found device: Apple M4
0.00.337.249 I ggml_metal_init: picking default device: Apple M4
0.00.338.333 I ggml_metal_init: using embedded metal library
0.00.340.890 I ggml_metal_init: GPU name:   Apple M4
0.00.340.891 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.892 I ggml_metal_init: simdgroup reduction   = true
0.00.340.892 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.893 I ggml_metal_init: has bfloat            = true
0.00.340.893 I ggml_metal_init: use bfloat            = true
0.00.340.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.308 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.352.686 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.352.688 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.692 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.334 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.335 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.335 I llama_new_context_with_model: graph nodes  = 154
0.00.353.335 I llama_new_context_with_model: graph splits = 2
0.00.353.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.907 I 
0.00.365.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.193 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.194 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.202 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.202 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.205 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.205 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.740 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.409 I llama_perf_context_print:        load time =     341.09 ms
0.00.370.410 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16958.42 tokens per second)
0.00.370.411 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.411 I llama_perf_context_print:       total time =       4.50 ms /    63 tokens
0.00.370.674 I ggml_metal_free: deallocating

real	0m1.095s
user	0m0.340s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.121 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.217 I main: llama backend init
0.00.000.222 I main: load the model and apply lora adapter, if any
0.00.057.189 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.069.542 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.069.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.069.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.069.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.069.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.069.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.069.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.069.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.579 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.580 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.609 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.371 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.085.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.387 I llama_model_loader: - type  f32:  194 tensors
0.00.085.387 I llama_model_loader: - type  f16:   98 tensors
0.00.120.986 I llm_load_vocab: special tokens cache size = 25
0.00.128.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.128.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.128.895 I llm_load_print_meta: arch             = gptneox
0.00.128.895 I llm_load_print_meta: vocab type       = BPE
0.00.128.895 I llm_load_print_meta: n_vocab          = 50304
0.00.128.895 I llm_load_print_meta: n_merges         = 50009
0.00.128.896 I llm_load_print_meta: vocab_only       = 0
0.00.128.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.128.896 I llm_load_print_meta: n_embd           = 2048
0.00.128.896 I llm_load_print_meta: n_layer          = 24
0.00.128.900 I llm_load_print_meta: n_head           = 16
0.00.128.901 I llm_load_print_meta: n_head_kv        = 16
0.00.128.902 I llm_load_print_meta: n_rot            = 32
0.00.128.902 I llm_load_print_meta: n_swa            = 0
0.00.128.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.128.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.128.905 I llm_load_print_meta: n_gqa            = 1
0.00.128.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.128.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.128.907 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.128.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.128.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.128.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.128.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.128.909 I llm_load_print_meta: n_ff             = 8192
0.00.128.909 I llm_load_print_meta: n_expert         = 0
0.00.128.910 I llm_load_print_meta: n_expert_used    = 0
0.00.128.910 I llm_load_print_meta: causal attn      = 1
0.00.128.910 I llm_load_print_meta: pooling type     = 0
0.00.128.910 I llm_load_print_meta: rope type        = 2
0.00.128.910 I llm_load_print_meta: rope scaling     = linear
0.00.128.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.128.911 I llm_load_print_meta: freq_scale_train = 1
0.00.128.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.128.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.128.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.128.912 I llm_load_print_meta: ssm_d_inner      = 0
0.00.128.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.128.912 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.128.912 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.128.912 I llm_load_print_meta: model type       = 1.4B
0.00.128.932 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.128.932 I llm_load_print_meta: model params     = 1.41 B
0.00.128.933 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.128.933 I llm_load_print_meta: general.name     = 1.4B
0.00.128.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.128.933 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.128.933 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.128.933 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.128.934 I llm_load_print_meta: LF token         = 128 ''
0.00.128.934 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.128.934 I llm_load_print_meta: max token length = 1024
0.00.130.872 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.130.872 I llm_load_tensors: offloading output layer to GPU
0.00.130.872 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.130.891 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.130.892 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.131.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.131.324 I llama_new_context_with_model: n_ctx         = 2048
0.00.131.324 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.131.324 I llama_new_context_with_model: n_batch       = 2048
0.00.131.325 I llama_new_context_with_model: n_ubatch      = 512
0.00.131.325 I llama_new_context_with_model: flash_attn    = 0
0.00.131.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.131.326 I llama_new_context_with_model: freq_scale    = 1
0.00.131.326 I ggml_metal_init: allocating
0.00.131.337 I ggml_metal_init: found device: Apple M4
0.00.131.339 I ggml_metal_init: picking default device: Apple M4
0.00.132.078 I ggml_metal_init: using embedded metal library
0.00.143.300 I ggml_metal_init: GPU name:   Apple M4
0.00.143.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.143.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.143.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.143.305 I ggml_metal_init: simdgroup reduction   = true
0.00.143.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.143.305 I ggml_metal_init: has bfloat            = true
0.00.143.305 I ggml_metal_init: use bfloat            = true
0.00.143.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.143.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.168.250 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.189.287 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.189.294 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.189.313 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.190.345 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.190.348 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.190.348 I llama_new_context_with_model: graph nodes  = 967
0.00.190.348 I llama_new_context_with_model: graph splits = 2
0.00.190.351 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.190.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.190.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.271.753 I main: llama threadpool init, n_threads = 4
0.00.271.797 I 
0.00.271.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.271.829 I 
0.00.272.043 I sampler seed: 1234
0.00.272.047 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.272.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.272.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.272.074 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.116.485 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.116.486 I llama_perf_context_print:        load time =     214.55 ms
0.02.116.487 I llama_perf_context_print: prompt eval time =      54.28 ms /     7 tokens (    7.75 ms per token,   128.97 tokens per second)
0.02.116.488 I llama_perf_context_print:        eval time =    1787.19 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.116.489 I llama_perf_context_print:       total time =    1844.74 ms /    70 tokens
0.02.116.711 I ggml_metal_free: deallocating

real	0m2.412s
user	0m0.148s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.815 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.134 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.975 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.987 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.988 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.004 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.661 I llama_model_loader: - type  f32:  194 tensors
0.00.055.661 I llama_model_loader: - type  f16:   98 tensors
0.00.083.412 I llm_load_vocab: special tokens cache size = 25
0.00.089.960 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.963 I llm_load_print_meta: arch             = gptneox
0.00.089.963 I llm_load_print_meta: vocab type       = BPE
0.00.089.964 I llm_load_print_meta: n_vocab          = 50304
0.00.089.964 I llm_load_print_meta: n_merges         = 50009
0.00.089.964 I llm_load_print_meta: vocab_only       = 0
0.00.089.964 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.964 I llm_load_print_meta: n_embd           = 2048
0.00.089.964 I llm_load_print_meta: n_layer          = 24
0.00.089.967 I llm_load_print_meta: n_head           = 16
0.00.089.968 I llm_load_print_meta: n_head_kv        = 16
0.00.089.968 I llm_load_print_meta: n_rot            = 32
0.00.089.968 I llm_load_print_meta: n_swa            = 0
0.00.089.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.969 I llm_load_print_meta: n_gqa            = 1
0.00.089.970 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.971 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.973 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.974 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.974 I llm_load_print_meta: n_ff             = 8192
0.00.089.974 I llm_load_print_meta: n_expert         = 0
0.00.089.974 I llm_load_print_meta: n_expert_used    = 0
0.00.089.974 I llm_load_print_meta: causal attn      = 1
0.00.089.975 I llm_load_print_meta: pooling type     = 0
0.00.089.975 I llm_load_print_meta: rope type        = 2
0.00.089.975 I llm_load_print_meta: rope scaling     = linear
0.00.089.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.977 I llm_load_print_meta: freq_scale_train = 1
0.00.089.977 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.977 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.977 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.978 I llm_load_print_meta: model type       = 1.4B
0.00.089.990 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.990 I llm_load_print_meta: model params     = 1.41 B
0.00.089.991 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.991 I llm_load_print_meta: general.name     = 1.4B
0.00.089.992 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.992 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.993 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.993 I llm_load_print_meta: LF token         = 128 ''
0.00.089.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.993 I llm_load_print_meta: max token length = 1024
0.00.092.424 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.425 I llm_load_tensors: offloading output layer to GPU
0.00.092.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.435 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.437 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.797 I llama_new_context_with_model: n_ctx         = 128
0.00.092.798 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.798 I llama_new_context_with_model: n_batch       = 128
0.00.092.798 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.798 I llama_new_context_with_model: flash_attn    = 0
0.00.092.799 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.799 I llama_new_context_with_model: freq_scale    = 1
0.00.092.799 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.800 I ggml_metal_init: allocating
0.00.092.808 I ggml_metal_init: found device: Apple M4
0.00.092.811 I ggml_metal_init: picking default device: Apple M4
0.00.093.427 I ggml_metal_init: using embedded metal library
0.00.095.963 I ggml_metal_init: GPU name:   Apple M4
0.00.095.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.966 I ggml_metal_init: simdgroup reduction   = true
0.00.095.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.966 I ggml_metal_init: has bfloat            = true
0.00.095.966 I ggml_metal_init: use bfloat            = true
0.00.095.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.202 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.204 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.157 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.158 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.158 I llama_new_context_with_model: graph nodes  = 967
0.00.107.159 I llama_new_context_with_model: graph splits = 2
0.00.107.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.442.396 I 
0.01.442.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.442.497 I perplexity: tokenizing the input ..
0.01.456.145 I perplexity: tokenization took 13.644 ms
0.01.456.151 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.579.331 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.581.358 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.581.404 I llama_perf_context_print:        load time =    1418.25 ms
0.01.581.406 I llama_perf_context_print: prompt eval time =     122.22 ms /   128 tokens (    0.95 ms per token,  1047.32 tokens per second)
0.01.581.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.581.408 I llama_perf_context_print:       total time =     139.01 ms /   129 tokens
0.01.581.993 I ggml_metal_free: deallocating

real	0m1.781s
user	0m0.124s
sys	0m0.234s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.743 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.974 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.975 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.976 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.977 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.984 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.508 I llama_model_loader: - type  f32:  194 tensors
0.00.028.509 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.225 I llm_load_vocab: special tokens cache size = 25
0.00.055.346 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.352 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.352 I llm_load_print_meta: arch             = gptneox
0.00.055.352 I llm_load_print_meta: vocab type       = BPE
0.00.055.353 I llm_load_print_meta: n_vocab          = 50304
0.00.055.353 I llm_load_print_meta: n_merges         = 50009
0.00.055.353 I llm_load_print_meta: vocab_only       = 0
0.00.055.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.353 I llm_load_print_meta: n_embd           = 2048
0.00.055.354 I llm_load_print_meta: n_layer          = 24
0.00.055.360 I llm_load_print_meta: n_head           = 16
0.00.055.361 I llm_load_print_meta: n_head_kv        = 16
0.00.055.361 I llm_load_print_meta: n_rot            = 32
0.00.055.361 I llm_load_print_meta: n_swa            = 0
0.00.055.365 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.365 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.366 I llm_load_print_meta: n_gqa            = 1
0.00.055.367 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.367 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.368 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.369 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.369 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.369 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.369 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.370 I llm_load_print_meta: n_ff             = 8192
0.00.055.370 I llm_load_print_meta: n_expert         = 0
0.00.055.370 I llm_load_print_meta: n_expert_used    = 0
0.00.055.370 I llm_load_print_meta: causal attn      = 1
0.00.055.371 I llm_load_print_meta: pooling type     = 0
0.00.055.371 I llm_load_print_meta: rope type        = 2
0.00.055.371 I llm_load_print_meta: rope scaling     = linear
0.00.055.371 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.372 I llm_load_print_meta: freq_scale_train = 1
0.00.055.372 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.372 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.372 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.373 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.373 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.373 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.373 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.373 I llm_load_print_meta: model type       = 1.4B
0.00.055.387 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.388 I llm_load_print_meta: model params     = 1.41 B
0.00.055.388 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.388 I llm_load_print_meta: general.name     = 1.4B
0.00.055.389 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.389 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.390 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.390 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.391 I llm_load_print_meta: LF token         = 128 ''
0.00.055.391 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.391 I llm_load_print_meta: max token length = 1024
0.00.057.782 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.782 I llm_load_tensors: offloading output layer to GPU
0.00.057.782 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.794 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.795 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.201 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.202 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.202 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.202 I llama_new_context_with_model: n_batch       = 2048
0.00.058.203 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.203 I llama_new_context_with_model: flash_attn    = 0
0.00.058.203 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.203 I llama_new_context_with_model: freq_scale    = 1
0.00.058.204 I ggml_metal_init: allocating
0.00.058.207 I ggml_metal_init: found device: Apple M4
0.00.058.209 I ggml_metal_init: picking default device: Apple M4
0.00.058.964 I ggml_metal_init: using embedded metal library
0.00.061.507 I ggml_metal_init: GPU name:   Apple M4
0.00.061.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.510 I ggml_metal_init: simdgroup reduction   = true
0.00.061.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.510 I ggml_metal_init: has bfloat            = true
0.00.061.510 I ggml_metal_init: use bfloat            = true
0.00.061.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.919 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.251 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.253 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.253 I llama_new_context_with_model: graph nodes  = 967
0.00.096.254 I llama_new_context_with_model: graph splits = 2
0.00.096.258 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.331.111 I main: llama threadpool init, n_threads = 4
0.01.331.154 I 
0.01.331.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.331.176 I 
0.01.331.452 I sampler seed: 1234
0.01.331.456 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.331.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.331.500 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.331.500 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.422.938 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.02.422.939 I llama_perf_context_print:        load time =    1321.36 ms
0.02.422.939 I llama_perf_context_print: prompt eval time =      47.00 ms /     7 tokens (    6.71 ms per token,   148.94 tokens per second)
0.02.422.940 I llama_perf_context_print:        eval time =    1041.54 ms /    63 runs   (   16.53 ms per token,    60.49 tokens per second)
0.02.422.940 I llama_perf_context_print:       total time =    1091.83 ms /    70 tokens
0.02.423.167 I ggml_metal_free: deallocating

real	0m2.442s
user	0m0.113s
sys	0m0.243s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.894 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.968 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.969 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.970 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.971 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.971 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.976 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.452 I llama_model_loader: - type  f32:  194 tensors
0.00.033.452 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.794 I llm_load_vocab: special tokens cache size = 25
0.00.063.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.888 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.888 I llm_load_print_meta: arch             = gptneox
0.00.063.889 I llm_load_print_meta: vocab type       = BPE
0.00.063.889 I llm_load_print_meta: n_vocab          = 50304
0.00.063.889 I llm_load_print_meta: n_merges         = 50009
0.00.063.889 I llm_load_print_meta: vocab_only       = 0
0.00.063.889 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.890 I llm_load_print_meta: n_embd           = 2048
0.00.063.890 I llm_load_print_meta: n_layer          = 24
0.00.063.893 I llm_load_print_meta: n_head           = 16
0.00.063.894 I llm_load_print_meta: n_head_kv        = 16
0.00.063.894 I llm_load_print_meta: n_rot            = 32
0.00.063.894 I llm_load_print_meta: n_swa            = 0
0.00.063.895 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.895 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.896 I llm_load_print_meta: n_gqa            = 1
0.00.063.896 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.897 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.898 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.898 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.898 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.898 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.898 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.899 I llm_load_print_meta: n_ff             = 8192
0.00.063.899 I llm_load_print_meta: n_expert         = 0
0.00.063.905 I llm_load_print_meta: n_expert_used    = 0
0.00.063.905 I llm_load_print_meta: causal attn      = 1
0.00.063.905 I llm_load_print_meta: pooling type     = 0
0.00.063.905 I llm_load_print_meta: rope type        = 2
0.00.063.905 I llm_load_print_meta: rope scaling     = linear
0.00.063.906 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.906 I llm_load_print_meta: freq_scale_train = 1
0.00.063.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.907 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.907 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.908 I llm_load_print_meta: model type       = 1.4B
0.00.063.919 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.920 I llm_load_print_meta: model params     = 1.41 B
0.00.063.920 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.920 I llm_load_print_meta: general.name     = 1.4B
0.00.063.921 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.921 I llm_load_print_meta: LF token         = 128 ''
0.00.063.921 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.922 I llm_load_print_meta: max token length = 1024
0.00.066.014 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.014 I llm_load_tensors: offloading output layer to GPU
0.00.066.014 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.025 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.026 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.442 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.443 I llama_new_context_with_model: n_ctx         = 128
0.00.066.443 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.443 I llama_new_context_with_model: n_batch       = 128
0.00.066.443 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.443 I llama_new_context_with_model: flash_attn    = 0
0.00.066.444 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.444 I llama_new_context_with_model: freq_scale    = 1
0.00.066.444 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.445 I ggml_metal_init: allocating
0.00.066.451 I ggml_metal_init: found device: Apple M4
0.00.066.453 I ggml_metal_init: picking default device: Apple M4
0.00.067.005 I ggml_metal_init: using embedded metal library
0.00.069.330 I ggml_metal_init: GPU name:   Apple M4
0.00.069.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.332 I ggml_metal_init: simdgroup reduction   = true
0.00.069.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.333 I ggml_metal_init: has bfloat            = true
0.00.069.333 I ggml_metal_init: use bfloat            = true
0.00.069.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.813 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.034 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.048 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.924 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.925 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.925 I llama_new_context_with_model: graph nodes  = 967
0.00.079.926 I llama_new_context_with_model: graph splits = 2
0.00.079.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.916.632 I 
0.00.916.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.916.671 I perplexity: tokenizing the input ..
0.00.924.354 I perplexity: tokenization took 7.681 ms
0.00.924.362 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.048.626 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.049.757 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.049.779 I llama_perf_context_print:        load time =     905.73 ms
0.01.049.780 I llama_perf_context_print: prompt eval time =     124.04 ms /   128 tokens (    0.97 ms per token,  1031.94 tokens per second)
0.01.049.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.049.781 I llama_perf_context_print:       total time =     133.15 ms /   129 tokens
0.01.050.079 I ggml_metal_free: deallocating

real	0m1.068s
user	0m0.091s
sys	0m0.148s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.016.490 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.175 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.435 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.439 I llama_model_loader: - type  f32:  194 tensors
0.00.046.439 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.439 I llama_model_loader: - type q6_K:    1 tensors
0.00.072.453 I llm_load_vocab: special tokens cache size = 25
0.00.081.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.764 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.764 I llm_load_print_meta: arch             = gptneox
0.00.081.765 I llm_load_print_meta: vocab type       = BPE
0.00.081.765 I llm_load_print_meta: n_vocab          = 50304
0.00.081.765 I llm_load_print_meta: n_merges         = 50009
0.00.081.765 I llm_load_print_meta: vocab_only       = 0
0.00.081.772 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.772 I llm_load_print_meta: n_embd           = 2048
0.00.081.772 I llm_load_print_meta: n_layer          = 24
0.00.081.776 I llm_load_print_meta: n_head           = 16
0.00.081.777 I llm_load_print_meta: n_head_kv        = 16
0.00.081.778 I llm_load_print_meta: n_rot            = 32
0.00.081.778 I llm_load_print_meta: n_swa            = 0
0.00.081.778 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.778 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.781 I llm_load_print_meta: n_gqa            = 1
0.00.081.782 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.783 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.784 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.784 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.786 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.787 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.787 I llm_load_print_meta: n_ff             = 8192
0.00.081.788 I llm_load_print_meta: n_expert         = 0
0.00.081.788 I llm_load_print_meta: n_expert_used    = 0
0.00.081.788 I llm_load_print_meta: causal attn      = 1
0.00.081.790 I llm_load_print_meta: pooling type     = 0
0.00.081.790 I llm_load_print_meta: rope type        = 2
0.00.081.790 I llm_load_print_meta: rope scaling     = linear
0.00.081.791 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.791 I llm_load_print_meta: freq_scale_train = 1
0.00.081.793 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.793 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.793 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.794 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.795 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.796 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.796 I llm_load_print_meta: model type       = 1.4B
0.00.081.809 I llm_load_print_meta: model ftype      = Q4_0
0.00.081.810 I llm_load_print_meta: model params     = 1.41 B
0.00.081.811 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.081.811 I llm_load_print_meta: general.name     = 1.4B
0.00.081.811 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.811 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.812 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.812 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.812 I llm_load_print_meta: LF token         = 128 ''
0.00.081.812 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.813 I llm_load_print_meta: max token length = 1024
0.00.084.648 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.648 I llm_load_tensors: offloading output layer to GPU
0.00.084.648 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.659 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.084.661 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.085.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.154 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.155 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.155 I llama_new_context_with_model: n_batch       = 2048
0.00.085.155 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.156 I llama_new_context_with_model: flash_attn    = 0
0.00.085.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.156 I llama_new_context_with_model: freq_scale    = 1
0.00.085.157 I ggml_metal_init: allocating
0.00.085.166 I ggml_metal_init: found device: Apple M4
0.00.085.175 I ggml_metal_init: picking default device: Apple M4
0.00.086.158 I ggml_metal_init: using embedded metal library
0.00.089.808 I ggml_metal_init: GPU name:   Apple M4
0.00.089.810 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.811 I ggml_metal_init: simdgroup reduction   = true
0.00.089.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.812 I ggml_metal_init: has bfloat            = true
0.00.089.812 I ggml_metal_init: use bfloat            = true
0.00.089.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.772 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.126.337 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.345 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.615 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.617 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.618 I llama_new_context_with_model: graph nodes  = 967
0.00.127.618 I llama_new_context_with_model: graph splits = 2
0.00.127.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.880.808 I main: llama threadpool init, n_threads = 4
0.00.880.910 I 
0.00.880.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.880.964 I 
0.00.881.582 I sampler seed: 1234
0.00.881.595 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.881.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.881.631 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.881.631 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.583.661 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.583.662 I llama_perf_context_print:        load time =     864.30 ms
0.01.583.663 I llama_perf_context_print: prompt eval time =      50.34 ms /     7 tokens (    7.19 ms per token,   139.07 tokens per second)
0.01.583.664 I llama_perf_context_print:        eval time =     648.58 ms /    63 runs   (   10.29 ms per token,    97.13 tokens per second)
0.01.583.664 I llama_perf_context_print:       total time =     702.87 ms /    70 tokens
0.01.583.903 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.137s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.102 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.068 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.856 I llama_model_loader: - type  f32:  194 tensors
0.00.025.857 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.857 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.310 I llm_load_vocab: special tokens cache size = 25
0.00.052.356 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.359 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.359 I llm_load_print_meta: arch             = gptneox
0.00.052.359 I llm_load_print_meta: vocab type       = BPE
0.00.052.359 I llm_load_print_meta: n_vocab          = 50304
0.00.052.360 I llm_load_print_meta: n_merges         = 50009
0.00.052.360 I llm_load_print_meta: vocab_only       = 0
0.00.052.360 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.360 I llm_load_print_meta: n_embd           = 2048
0.00.052.360 I llm_load_print_meta: n_layer          = 24
0.00.052.363 I llm_load_print_meta: n_head           = 16
0.00.052.364 I llm_load_print_meta: n_head_kv        = 16
0.00.052.364 I llm_load_print_meta: n_rot            = 32
0.00.052.364 I llm_load_print_meta: n_swa            = 0
0.00.052.364 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.364 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.365 I llm_load_print_meta: n_gqa            = 1
0.00.052.366 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.367 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.367 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.368 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.368 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.368 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.368 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.369 I llm_load_print_meta: n_ff             = 8192
0.00.052.369 I llm_load_print_meta: n_expert         = 0
0.00.052.369 I llm_load_print_meta: n_expert_used    = 0
0.00.052.369 I llm_load_print_meta: causal attn      = 1
0.00.052.370 I llm_load_print_meta: pooling type     = 0
0.00.052.370 I llm_load_print_meta: rope type        = 2
0.00.052.370 I llm_load_print_meta: rope scaling     = linear
0.00.052.370 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.371 I llm_load_print_meta: freq_scale_train = 1
0.00.052.371 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.371 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.371 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.372 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.372 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.372 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.372 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.372 I llm_load_print_meta: model type       = 1.4B
0.00.052.384 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.384 I llm_load_print_meta: model params     = 1.41 B
0.00.052.385 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.385 I llm_load_print_meta: general.name     = 1.4B
0.00.052.385 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.387 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.387 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.389 I llm_load_print_meta: LF token         = 128 ''
0.00.052.389 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.389 I llm_load_print_meta: max token length = 1024
0.00.054.306 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.307 I llm_load_tensors: offloading output layer to GPU
0.00.054.307 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.318 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.319 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.642 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.643 I llama_new_context_with_model: n_ctx         = 128
0.00.054.643 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.643 I llama_new_context_with_model: n_batch       = 128
0.00.054.643 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.643 I llama_new_context_with_model: flash_attn    = 0
0.00.054.644 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.644 I llama_new_context_with_model: freq_scale    = 1
0.00.054.644 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.645 I ggml_metal_init: allocating
0.00.054.647 I ggml_metal_init: found device: Apple M4
0.00.054.649 I ggml_metal_init: picking default device: Apple M4
0.00.055.206 I ggml_metal_init: using embedded metal library
0.00.057.508 I ggml_metal_init: GPU name:   Apple M4
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.510 I ggml_metal_init: simdgroup reduction   = true
0.00.057.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.511 I ggml_metal_init: has bfloat            = true
0.00.057.511 I ggml_metal_init: use bfloat            = true
0.00.057.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.065 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.317 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.210 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.211 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.212 I llama_new_context_with_model: graph nodes  = 967
0.00.069.212 I llama_new_context_with_model: graph splits = 2
0.00.069.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.179 I 
0.00.618.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.221 I perplexity: tokenizing the input ..
0.00.626.244 I perplexity: tokenization took 8.02 ms
0.00.626.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.926 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.115 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.142 I llama_perf_context_print:        load time =     608.07 ms
0.00.750.143 I llama_perf_context_print: prompt eval time =     122.43 ms /   128 tokens (    0.96 ms per token,  1045.50 tokens per second)
0.00.750.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.144 I llama_perf_context_print:       total time =     131.97 ms /   129 tokens
0.00.750.686 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.079s
sys	0m0.100s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.680 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.772 I llama_model_loader: - type  f32:  194 tensors
0.00.026.772 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.772 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.598 I llm_load_vocab: special tokens cache size = 25
0.00.052.686 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.689 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.689 I llm_load_print_meta: arch             = gptneox
0.00.052.690 I llm_load_print_meta: vocab type       = BPE
0.00.052.690 I llm_load_print_meta: n_vocab          = 50304
0.00.052.690 I llm_load_print_meta: n_merges         = 50009
0.00.052.690 I llm_load_print_meta: vocab_only       = 0
0.00.052.690 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.691 I llm_load_print_meta: n_embd           = 2048
0.00.052.691 I llm_load_print_meta: n_layer          = 24
0.00.052.694 I llm_load_print_meta: n_head           = 16
0.00.052.695 I llm_load_print_meta: n_head_kv        = 16
0.00.052.695 I llm_load_print_meta: n_rot            = 32
0.00.052.695 I llm_load_print_meta: n_swa            = 0
0.00.052.695 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.695 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.696 I llm_load_print_meta: n_gqa            = 1
0.00.052.697 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.697 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.698 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.698 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.698 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.699 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.699 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.700 I llm_load_print_meta: n_ff             = 8192
0.00.052.700 I llm_load_print_meta: n_expert         = 0
0.00.052.701 I llm_load_print_meta: n_expert_used    = 0
0.00.052.702 I llm_load_print_meta: causal attn      = 1
0.00.052.704 I llm_load_print_meta: pooling type     = 0
0.00.052.704 I llm_load_print_meta: rope type        = 2
0.00.052.704 I llm_load_print_meta: rope scaling     = linear
0.00.052.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.705 I llm_load_print_meta: freq_scale_train = 1
0.00.052.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.705 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.705 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.706 I llm_load_print_meta: model type       = 1.4B
0.00.052.718 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.718 I llm_load_print_meta: model params     = 1.41 B
0.00.052.719 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.719 I llm_load_print_meta: general.name     = 1.4B
0.00.052.719 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.719 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.719 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.719 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.720 I llm_load_print_meta: LF token         = 128 ''
0.00.052.720 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.720 I llm_load_print_meta: max token length = 1024
0.00.054.621 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.621 I llm_load_tensors: offloading output layer to GPU
0.00.054.621 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.632 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.633 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.979 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.980 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.980 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.980 I llama_new_context_with_model: n_batch       = 2048
0.00.054.980 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.980 I llama_new_context_with_model: flash_attn    = 0
0.00.054.981 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.981 I llama_new_context_with_model: freq_scale    = 1
0.00.054.981 I ggml_metal_init: allocating
0.00.054.984 I ggml_metal_init: found device: Apple M4
0.00.054.986 I ggml_metal_init: picking default device: Apple M4
0.00.055.579 I ggml_metal_init: using embedded metal library
0.00.057.896 I ggml_metal_init: GPU name:   Apple M4
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.899 I ggml_metal_init: simdgroup reduction   = true
0.00.057.899 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.899 I ggml_metal_init: has bfloat            = true
0.00.057.900 I ggml_metal_init: use bfloat            = true
0.00.057.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.901 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.361 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.138 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.147 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.173 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.172 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.173 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.174 I llama_new_context_with_model: graph nodes  = 967
0.00.087.174 I llama_new_context_with_model: graph splits = 2
0.00.087.176 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.387 I main: llama threadpool init, n_threads = 4
0.00.758.423 I 
0.00.758.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.448 I 
0.00.758.603 I sampler seed: 1234
0.00.758.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.649 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.651 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.489.582 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.01.489.582 I llama_perf_context_print:        load time =     749.70 ms
0.01.489.583 I llama_perf_context_print: prompt eval time =      39.57 ms /     7 tokens (    5.65 ms per token,   176.92 tokens per second)
0.01.489.584 I llama_perf_context_print:        eval time =     688.85 ms /    63 runs   (   10.93 ms per token,    91.46 tokens per second)
0.01.489.584 I llama_perf_context_print:       total time =     731.20 ms /    70 tokens
0.01.489.857 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.109s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.712 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.234 I llama_model_loader: - type  f32:  194 tensors
0.00.024.235 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.235 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.862 I llm_load_vocab: special tokens cache size = 25
0.00.049.915 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.917 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.918 I llm_load_print_meta: arch             = gptneox
0.00.049.918 I llm_load_print_meta: vocab type       = BPE
0.00.049.918 I llm_load_print_meta: n_vocab          = 50304
0.00.049.918 I llm_load_print_meta: n_merges         = 50009
0.00.049.918 I llm_load_print_meta: vocab_only       = 0
0.00.049.919 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.919 I llm_load_print_meta: n_embd           = 2048
0.00.049.919 I llm_load_print_meta: n_layer          = 24
0.00.049.923 I llm_load_print_meta: n_head           = 16
0.00.049.924 I llm_load_print_meta: n_head_kv        = 16
0.00.049.926 I llm_load_print_meta: n_rot            = 32
0.00.049.926 I llm_load_print_meta: n_swa            = 0
0.00.049.926 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.926 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.927 I llm_load_print_meta: n_gqa            = 1
0.00.049.928 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.928 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.929 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.929 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.929 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.930 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.930 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.931 I llm_load_print_meta: n_ff             = 8192
0.00.049.931 I llm_load_print_meta: n_expert         = 0
0.00.049.931 I llm_load_print_meta: n_expert_used    = 0
0.00.049.931 I llm_load_print_meta: causal attn      = 1
0.00.049.931 I llm_load_print_meta: pooling type     = 0
0.00.049.931 I llm_load_print_meta: rope type        = 2
0.00.049.932 I llm_load_print_meta: rope scaling     = linear
0.00.049.933 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.934 I llm_load_print_meta: freq_scale_train = 1
0.00.049.934 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.934 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.934 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.934 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.934 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.935 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.935 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.935 I llm_load_print_meta: model type       = 1.4B
0.00.049.946 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.947 I llm_load_print_meta: model params     = 1.41 B
0.00.049.947 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.948 I llm_load_print_meta: general.name     = 1.4B
0.00.049.948 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: LF token         = 128 ''
0.00.049.950 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: max token length = 1024
0.00.051.861 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.861 I llm_load_tensors: offloading output layer to GPU
0.00.051.861 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.872 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.873 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.193 I llama_new_context_with_model: n_ctx         = 128
0.00.052.193 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.193 I llama_new_context_with_model: n_batch       = 128
0.00.052.193 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.193 I llama_new_context_with_model: flash_attn    = 0
0.00.052.194 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.194 I llama_new_context_with_model: freq_scale    = 1
0.00.052.194 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.195 I ggml_metal_init: allocating
0.00.052.197 I ggml_metal_init: found device: Apple M4
0.00.052.199 I ggml_metal_init: picking default device: Apple M4
0.00.052.761 I ggml_metal_init: using embedded metal library
0.00.055.056 I ggml_metal_init: GPU name:   Apple M4
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.058 I ggml_metal_init: simdgroup reduction   = true
0.00.055.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.058 I ggml_metal_init: has bfloat            = true
0.00.055.058 I ggml_metal_init: use bfloat            = true
0.00.055.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.612 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.977 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.980 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.996 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.849 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.850 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.850 I llama_new_context_with_model: graph nodes  = 967
0.00.066.850 I llama_new_context_with_model: graph splits = 2
0.00.066.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.264 I 
0.00.646.308 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.330 I perplexity: tokenizing the input ..
0.00.654.315 I perplexity: tokenization took 7.984 ms
0.00.654.319 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.144 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.778.290 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.778.318 I llama_perf_context_print:        load time =     637.45 ms
0.00.778.319 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.05 tokens per second)
0.00.778.320 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.320 I llama_perf_context_print:       total time =     132.06 ms /   129 tokens
0.00.778.809 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.078s
sys	0m0.093s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.018.622 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.943 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.044.847 I llama_model_loader: - type  f32:  194 tensors
0.00.044.847 I llama_model_loader: - type q5_0:   97 tensors
0.00.044.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.860 I llm_load_vocab: special tokens cache size = 25
0.00.071.879 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.884 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.884 I llm_load_print_meta: arch             = gptneox
0.00.071.884 I llm_load_print_meta: vocab type       = BPE
0.00.071.885 I llm_load_print_meta: n_vocab          = 50304
0.00.071.885 I llm_load_print_meta: n_merges         = 50009
0.00.071.885 I llm_load_print_meta: vocab_only       = 0
0.00.071.885 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.885 I llm_load_print_meta: n_embd           = 2048
0.00.071.886 I llm_load_print_meta: n_layer          = 24
0.00.071.890 I llm_load_print_meta: n_head           = 16
0.00.071.892 I llm_load_print_meta: n_head_kv        = 16
0.00.071.892 I llm_load_print_meta: n_rot            = 32
0.00.071.892 I llm_load_print_meta: n_swa            = 0
0.00.071.892 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.892 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.893 I llm_load_print_meta: n_gqa            = 1
0.00.071.894 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.894 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.895 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.895 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.897 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.897 I llm_load_print_meta: n_ff             = 8192
0.00.071.899 I llm_load_print_meta: n_expert         = 0
0.00.071.899 I llm_load_print_meta: n_expert_used    = 0
0.00.071.899 I llm_load_print_meta: causal attn      = 1
0.00.071.899 I llm_load_print_meta: pooling type     = 0
0.00.071.899 I llm_load_print_meta: rope type        = 2
0.00.071.900 I llm_load_print_meta: rope scaling     = linear
0.00.071.900 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.900 I llm_load_print_meta: freq_scale_train = 1
0.00.071.900 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.901 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.901 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.901 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.901 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.901 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.902 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.902 I llm_load_print_meta: model type       = 1.4B
0.00.071.916 I llm_load_print_meta: model ftype      = Q5_0
0.00.071.917 I llm_load_print_meta: model params     = 1.41 B
0.00.071.917 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.071.917 I llm_load_print_meta: general.name     = 1.4B
0.00.071.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.918 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.918 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.918 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.918 I llm_load_print_meta: LF token         = 128 ''
0.00.071.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.919 I llm_load_print_meta: max token length = 1024
0.00.073.947 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.947 I llm_load_tensors: offloading output layer to GPU
0.00.073.948 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.959 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.073.961 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.074.280 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.281 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.281 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.281 I llama_new_context_with_model: n_batch       = 2048
0.00.074.282 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.282 I llama_new_context_with_model: flash_attn    = 0
0.00.074.282 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.282 I llama_new_context_with_model: freq_scale    = 1
0.00.074.283 I ggml_metal_init: allocating
0.00.074.286 I ggml_metal_init: found device: Apple M4
0.00.074.288 I ggml_metal_init: picking default device: Apple M4
0.00.074.910 I ggml_metal_init: using embedded metal library
0.00.077.597 I ggml_metal_init: GPU name:   Apple M4
0.00.077.599 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.600 I ggml_metal_init: simdgroup reduction   = true
0.00.077.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.601 I ggml_metal_init: has bfloat            = true
0.00.077.601 I ggml_metal_init: use bfloat            = true
0.00.077.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.730 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.650 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.658 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.681 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.637 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.639 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.640 I llama_new_context_with_model: graph nodes  = 967
0.00.107.640 I llama_new_context_with_model: graph splits = 2
0.00.107.645 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.780 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.939.848 I main: llama threadpool init, n_threads = 4
0.00.939.893 I 
0.00.939.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.939.917 I 
0.00.940.177 I sampler seed: 1234
0.00.940.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.940.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.940.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.940.227 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.728.487 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.728.487 I llama_perf_context_print:        load time =     921.22 ms
0.01.728.488 I llama_perf_context_print: prompt eval time =      47.73 ms /     7 tokens (    6.82 ms per token,   146.66 tokens per second)
0.01.728.489 I llama_perf_context_print:        eval time =     737.85 ms /    63 runs   (   11.71 ms per token,    85.38 tokens per second)
0.01.728.489 I llama_perf_context_print:       total time =     788.64 ms /    70 tokens
0.01.728.741 I ggml_metal_free: deallocating

real	0m1.749s
user	0m0.113s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.334 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.335 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.925 I llama_model_loader: - type  f32:  194 tensors
0.00.023.925 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.925 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.257 I llm_load_vocab: special tokens cache size = 25
0.00.050.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.063 I llm_load_print_meta: arch             = gptneox
0.00.050.063 I llm_load_print_meta: vocab type       = BPE
0.00.050.063 I llm_load_print_meta: n_vocab          = 50304
0.00.050.063 I llm_load_print_meta: n_merges         = 50009
0.00.050.064 I llm_load_print_meta: vocab_only       = 0
0.00.050.064 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.064 I llm_load_print_meta: n_embd           = 2048
0.00.050.064 I llm_load_print_meta: n_layer          = 24
0.00.050.067 I llm_load_print_meta: n_head           = 16
0.00.050.068 I llm_load_print_meta: n_head_kv        = 16
0.00.050.068 I llm_load_print_meta: n_rot            = 32
0.00.050.069 I llm_load_print_meta: n_swa            = 0
0.00.050.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.071 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.072 I llm_load_print_meta: n_gqa            = 1
0.00.050.073 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.074 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.074 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.075 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.077 I llm_load_print_meta: n_ff             = 8192
0.00.050.077 I llm_load_print_meta: n_expert         = 0
0.00.050.077 I llm_load_print_meta: n_expert_used    = 0
0.00.050.078 I llm_load_print_meta: causal attn      = 1
0.00.050.078 I llm_load_print_meta: pooling type     = 0
0.00.050.078 I llm_load_print_meta: rope type        = 2
0.00.050.078 I llm_load_print_meta: rope scaling     = linear
0.00.050.078 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.079 I llm_load_print_meta: freq_scale_train = 1
0.00.050.079 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.079 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.079 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.080 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.084 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.084 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.084 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.085 I llm_load_print_meta: model type       = 1.4B
0.00.050.096 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.096 I llm_load_print_meta: model params     = 1.41 B
0.00.050.097 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.097 I llm_load_print_meta: general.name     = 1.4B
0.00.050.097 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.098 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.098 I llm_load_print_meta: LF token         = 128 ''
0.00.050.098 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.098 I llm_load_print_meta: max token length = 1024
0.00.052.078 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.079 I llm_load_tensors: offloading output layer to GPU
0.00.052.079 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.090 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.091 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.413 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.414 I llama_new_context_with_model: n_ctx         = 128
0.00.052.414 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.414 I llama_new_context_with_model: n_batch       = 128
0.00.052.414 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.414 I llama_new_context_with_model: flash_attn    = 0
0.00.052.415 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.415 I llama_new_context_with_model: freq_scale    = 1
0.00.052.415 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.416 I ggml_metal_init: allocating
0.00.052.419 I ggml_metal_init: found device: Apple M4
0.00.052.420 I ggml_metal_init: picking default device: Apple M4
0.00.052.972 I ggml_metal_init: using embedded metal library
0.00.055.306 I ggml_metal_init: GPU name:   Apple M4
0.00.055.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.309 I ggml_metal_init: simdgroup reduction   = true
0.00.055.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.309 I ggml_metal_init: has bfloat            = true
0.00.055.309 I ggml_metal_init: use bfloat            = true
0.00.055.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.816 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.123 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.128 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.094 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.095 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.095 I llama_new_context_with_model: graph nodes  = 967
0.00.067.096 I llama_new_context_with_model: graph splits = 2
0.00.067.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.891 I 
0.00.731.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.936 I perplexity: tokenizing the input ..
0.00.740.239 I perplexity: tokenization took 8.302 ms
0.00.740.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.048 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.875.469 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.875.492 I llama_perf_context_print:        load time =     723.06 ms
0.00.875.493 I llama_perf_context_print: prompt eval time =     133.56 ms /   128 tokens (    1.04 ms per token,   958.38 tokens per second)
0.00.875.493 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.494 I llama_perf_context_print:       total time =     143.61 ms /   129 tokens
0.00.875.850 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.079s
sys	0m0.114s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.684 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.868 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.884 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.758 I llama_model_loader: - type  f32:  194 tensors
0.00.036.759 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.315 I llm_load_vocab: special tokens cache size = 25
0.00.065.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.504 I llm_load_print_meta: arch             = gptneox
0.00.065.504 I llm_load_print_meta: vocab type       = BPE
0.00.065.505 I llm_load_print_meta: n_vocab          = 50304
0.00.065.505 I llm_load_print_meta: n_merges         = 50009
0.00.065.505 I llm_load_print_meta: vocab_only       = 0
0.00.065.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.505 I llm_load_print_meta: n_embd           = 2048
0.00.065.505 I llm_load_print_meta: n_layer          = 24
0.00.065.508 I llm_load_print_meta: n_head           = 16
0.00.065.508 I llm_load_print_meta: n_head_kv        = 16
0.00.065.509 I llm_load_print_meta: n_rot            = 32
0.00.065.509 I llm_load_print_meta: n_swa            = 0
0.00.065.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.510 I llm_load_print_meta: n_gqa            = 1
0.00.065.510 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.511 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.511 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.512 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.512 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.514 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.514 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.514 I llm_load_print_meta: n_ff             = 8192
0.00.065.514 I llm_load_print_meta: n_expert         = 0
0.00.065.515 I llm_load_print_meta: n_expert_used    = 0
0.00.065.515 I llm_load_print_meta: causal attn      = 1
0.00.065.517 I llm_load_print_meta: pooling type     = 0
0.00.065.517 I llm_load_print_meta: rope type        = 2
0.00.065.517 I llm_load_print_meta: rope scaling     = linear
0.00.065.517 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.518 I llm_load_print_meta: freq_scale_train = 1
0.00.065.518 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.518 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.518 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.518 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.519 I llm_load_print_meta: model type       = 1.4B
0.00.065.525 I llm_load_print_meta: model ftype      = Q5_1
0.00.065.525 I llm_load_print_meta: model params     = 1.41 B
0.00.065.527 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.065.527 I llm_load_print_meta: general.name     = 1.4B
0.00.065.527 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.528 I llm_load_print_meta: LF token         = 128 ''
0.00.065.529 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.529 I llm_load_print_meta: max token length = 1024
0.00.067.401 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.401 I llm_load_tensors: offloading output layer to GPU
0.00.067.402 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.407 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.067.408 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.067.736 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.737 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.737 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.737 I llama_new_context_with_model: n_batch       = 2048
0.00.067.737 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.738 I llama_new_context_with_model: flash_attn    = 0
0.00.067.738 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.738 I llama_new_context_with_model: freq_scale    = 1
0.00.067.739 I ggml_metal_init: allocating
0.00.067.742 I ggml_metal_init: found device: Apple M4
0.00.067.744 I ggml_metal_init: picking default device: Apple M4
0.00.068.369 I ggml_metal_init: using embedded metal library
0.00.070.776 I ggml_metal_init: GPU name:   Apple M4
0.00.070.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.778 I ggml_metal_init: simdgroup reduction   = true
0.00.070.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.778 I ggml_metal_init: has bfloat            = true
0.00.070.778 I ggml_metal_init: use bfloat            = true
0.00.070.779 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.779 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.159 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.199 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.321 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.322 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.322 I llama_new_context_with_model: graph nodes  = 967
0.00.101.322 I llama_new_context_with_model: graph splits = 2
0.00.101.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.365 I main: llama threadpool init, n_threads = 4
0.00.775.406 I 
0.00.775.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.432 I 
0.00.775.662 I sampler seed: 1234
0.00.775.667 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.687 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.687 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.687 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.612.365 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.612.366 I llama_perf_context_print:        load time =     766.68 ms
0.01.612.366 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.55 tokens per second)
0.01.612.367 I llama_perf_context_print:        eval time =     791.48 ms /    63 runs   (   12.56 ms per token,    79.60 tokens per second)
0.01.612.367 I llama_perf_context_print:       total time =     837.00 ms /    70 tokens
0.01.612.561 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.250 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.666 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.523 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.310 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.311 I llama_model_loader: - type  f32:  194 tensors
0.00.026.311 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.311 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.382 I llm_load_vocab: special tokens cache size = 25
0.00.052.381 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.385 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.385 I llm_load_print_meta: arch             = gptneox
0.00.052.386 I llm_load_print_meta: vocab type       = BPE
0.00.052.386 I llm_load_print_meta: n_vocab          = 50304
0.00.052.386 I llm_load_print_meta: n_merges         = 50009
0.00.052.389 I llm_load_print_meta: vocab_only       = 0
0.00.052.390 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.390 I llm_load_print_meta: n_embd           = 2048
0.00.052.390 I llm_load_print_meta: n_layer          = 24
0.00.052.394 I llm_load_print_meta: n_head           = 16
0.00.052.395 I llm_load_print_meta: n_head_kv        = 16
0.00.052.395 I llm_load_print_meta: n_rot            = 32
0.00.052.395 I llm_load_print_meta: n_swa            = 0
0.00.052.395 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.396 I llm_load_print_meta: n_gqa            = 1
0.00.052.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.397 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.398 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.398 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.398 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.399 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.399 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.399 I llm_load_print_meta: n_ff             = 8192
0.00.052.400 I llm_load_print_meta: n_expert         = 0
0.00.052.400 I llm_load_print_meta: n_expert_used    = 0
0.00.052.400 I llm_load_print_meta: causal attn      = 1
0.00.052.400 I llm_load_print_meta: pooling type     = 0
0.00.052.400 I llm_load_print_meta: rope type        = 2
0.00.052.400 I llm_load_print_meta: rope scaling     = linear
0.00.052.401 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.401 I llm_load_print_meta: freq_scale_train = 1
0.00.052.401 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.401 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.401 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.402 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.402 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.402 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.402 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.402 I llm_load_print_meta: model type       = 1.4B
0.00.052.417 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.417 I llm_load_print_meta: model params     = 1.41 B
0.00.052.417 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.418 I llm_load_print_meta: general.name     = 1.4B
0.00.052.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.418 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.418 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.418 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.420 I llm_load_print_meta: LF token         = 128 ''
0.00.052.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.420 I llm_load_print_meta: max token length = 1024
0.00.054.394 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.395 I llm_load_tensors: offloading output layer to GPU
0.00.054.395 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.406 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.407 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.744 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.745 I llama_new_context_with_model: n_ctx         = 128
0.00.054.745 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.745 I llama_new_context_with_model: n_batch       = 128
0.00.054.745 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.745 I llama_new_context_with_model: flash_attn    = 0
0.00.054.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.746 I llama_new_context_with_model: freq_scale    = 1
0.00.054.747 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.747 I ggml_metal_init: allocating
0.00.054.752 I ggml_metal_init: found device: Apple M4
0.00.054.754 I ggml_metal_init: picking default device: Apple M4
0.00.055.330 I ggml_metal_init: using embedded metal library
0.00.057.674 I ggml_metal_init: GPU name:   Apple M4
0.00.057.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.676 I ggml_metal_init: simdgroup reduction   = true
0.00.057.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.676 I ggml_metal_init: has bfloat            = true
0.00.057.677 I ggml_metal_init: use bfloat            = true
0.00.057.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.869 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.164 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.168 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.086 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.087 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.088 I llama_new_context_with_model: graph nodes  = 967
0.00.070.088 I llama_new_context_with_model: graph splits = 2
0.00.070.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.437 I 
0.00.639.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.488 I perplexity: tokenizing the input ..
0.00.646.914 I perplexity: tokenization took 7.424 ms
0.00.646.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.615 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.782.138 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.782.155 I llama_perf_context_print:        load time =     629.18 ms
0.00.782.156 I llama_perf_context_print: prompt eval time =     133.46 ms /   128 tokens (    1.04 ms per token,   959.06 tokens per second)
0.00.782.157 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.157 I llama_perf_context_print:       total time =     142.72 ms /   129 tokens
0.00.782.474 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.079s
sys	0m0.099s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.545 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.821 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.822 I llama_model_loader: - type  f32:  194 tensors
0.00.025.822 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.822 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.822 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.409 I llm_load_vocab: special tokens cache size = 25
0.00.052.288 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.291 I llm_load_print_meta: arch             = gptneox
0.00.052.291 I llm_load_print_meta: vocab type       = BPE
0.00.052.291 I llm_load_print_meta: n_vocab          = 50304
0.00.052.292 I llm_load_print_meta: n_merges         = 50009
0.00.052.292 I llm_load_print_meta: vocab_only       = 0
0.00.052.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.292 I llm_load_print_meta: n_embd           = 2048
0.00.052.292 I llm_load_print_meta: n_layer          = 24
0.00.052.295 I llm_load_print_meta: n_head           = 16
0.00.052.296 I llm_load_print_meta: n_head_kv        = 16
0.00.052.296 I llm_load_print_meta: n_rot            = 32
0.00.052.296 I llm_load_print_meta: n_swa            = 0
0.00.052.299 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.299 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.300 I llm_load_print_meta: n_gqa            = 1
0.00.052.300 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.301 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.301 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.302 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.302 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.302 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.302 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.303 I llm_load_print_meta: n_ff             = 8192
0.00.052.303 I llm_load_print_meta: n_expert         = 0
0.00.052.303 I llm_load_print_meta: n_expert_used    = 0
0.00.052.304 I llm_load_print_meta: causal attn      = 1
0.00.052.304 I llm_load_print_meta: pooling type     = 0
0.00.052.304 I llm_load_print_meta: rope type        = 2
0.00.052.304 I llm_load_print_meta: rope scaling     = linear
0.00.052.306 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.306 I llm_load_print_meta: freq_scale_train = 1
0.00.052.306 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.307 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.307 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.307 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.307 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.307 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.307 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.307 I llm_load_print_meta: model type       = 1.4B
0.00.052.314 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.314 I llm_load_print_meta: model params     = 1.41 B
0.00.052.315 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.315 I llm_load_print_meta: general.name     = 1.4B
0.00.052.315 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.316 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.316 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.316 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.316 I llm_load_print_meta: LF token         = 128 ''
0.00.052.317 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.317 I llm_load_print_meta: max token length = 1024
0.00.054.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.079 I llm_load_tensors: offloading output layer to GPU
0.00.054.079 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.085 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.086 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.398 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.398 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.398 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.398 I llama_new_context_with_model: n_batch       = 2048
0.00.054.399 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.399 I llama_new_context_with_model: flash_attn    = 0
0.00.054.399 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.399 I llama_new_context_with_model: freq_scale    = 1
0.00.054.400 I ggml_metal_init: allocating
0.00.054.403 I ggml_metal_init: found device: Apple M4
0.00.054.405 I ggml_metal_init: picking default device: Apple M4
0.00.054.986 I ggml_metal_init: using embedded metal library
0.00.057.309 I ggml_metal_init: GPU name:   Apple M4
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.311 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.311 I ggml_metal_init: simdgroup reduction   = true
0.00.057.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.311 I ggml_metal_init: has bfloat            = true
0.00.057.311 I ggml_metal_init: use bfloat            = true
0.00.057.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.957 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.201 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.211 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.234 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.249 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.251 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.251 I llama_new_context_with_model: graph nodes  = 967
0.00.086.251 I llama_new_context_with_model: graph splits = 2
0.00.086.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.395 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.412 I main: llama threadpool init, n_threads = 4
0.00.467.463 I 
0.00.467.489 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.489 I 
0.00.467.726 I sampler seed: 1234
0.00.467.732 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.467.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.467.787 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.467.788 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.147.491 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.147.491 I llama_perf_context_print:        load time =     456.86 ms
0.01.147.492 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.75 tokens per second)
0.01.147.493 I llama_perf_context_print:        eval time =     640.90 ms /    63 runs   (   10.17 ms per token,    98.30 tokens per second)
0.01.147.493 I llama_perf_context_print:       total time =     680.09 ms /    70 tokens
0.01.147.694 I ggml_metal_free: deallocating

real	0m1.166s
user	0m0.110s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.876 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.895 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.897 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.898 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.898 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.902 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.903 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.903 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.905 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.571 I llama_model_loader: - type  f32:  194 tensors
0.00.026.572 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.572 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.445 I llm_load_vocab: special tokens cache size = 25
0.00.052.514 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.517 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.517 I llm_load_print_meta: arch             = gptneox
0.00.052.517 I llm_load_print_meta: vocab type       = BPE
0.00.052.518 I llm_load_print_meta: n_vocab          = 50304
0.00.052.518 I llm_load_print_meta: n_merges         = 50009
0.00.052.518 I llm_load_print_meta: vocab_only       = 0
0.00.052.518 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.518 I llm_load_print_meta: n_embd           = 2048
0.00.052.519 I llm_load_print_meta: n_layer          = 24
0.00.052.522 I llm_load_print_meta: n_head           = 16
0.00.052.523 I llm_load_print_meta: n_head_kv        = 16
0.00.052.523 I llm_load_print_meta: n_rot            = 32
0.00.052.524 I llm_load_print_meta: n_swa            = 0
0.00.052.524 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.524 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.525 I llm_load_print_meta: n_gqa            = 1
0.00.052.525 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.526 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.526 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.527 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.527 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.527 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.527 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.529 I llm_load_print_meta: n_ff             = 8192
0.00.052.529 I llm_load_print_meta: n_expert         = 0
0.00.052.529 I llm_load_print_meta: n_expert_used    = 0
0.00.052.529 I llm_load_print_meta: causal attn      = 1
0.00.052.529 I llm_load_print_meta: pooling type     = 0
0.00.052.529 I llm_load_print_meta: rope type        = 2
0.00.052.529 I llm_load_print_meta: rope scaling     = linear
0.00.052.530 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.530 I llm_load_print_meta: freq_scale_train = 1
0.00.052.530 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.530 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.531 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.531 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.531 I llm_load_print_meta: model type       = 1.4B
0.00.052.538 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.538 I llm_load_print_meta: model params     = 1.41 B
0.00.052.539 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.541 I llm_load_print_meta: general.name     = 1.4B
0.00.052.541 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.541 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.541 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.542 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.542 I llm_load_print_meta: LF token         = 128 ''
0.00.052.542 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.542 I llm_load_print_meta: max token length = 1024
0.00.054.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.189 I llm_load_tensors: offloading output layer to GPU
0.00.054.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.195 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.196 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.518 I llama_new_context_with_model: n_ctx         = 128
0.00.054.519 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.519 I llama_new_context_with_model: n_batch       = 128
0.00.054.519 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.519 I llama_new_context_with_model: flash_attn    = 0
0.00.054.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.520 I llama_new_context_with_model: freq_scale    = 1
0.00.054.521 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.521 I ggml_metal_init: allocating
0.00.054.524 I ggml_metal_init: found device: Apple M4
0.00.054.526 I ggml_metal_init: picking default device: Apple M4
0.00.055.110 I ggml_metal_init: using embedded metal library
0.00.057.533 I ggml_metal_init: GPU name:   Apple M4
0.00.057.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.535 I ggml_metal_init: simdgroup reduction   = true
0.00.057.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.535 I ggml_metal_init: has bfloat            = true
0.00.057.536 I ggml_metal_init: use bfloat            = true
0.00.057.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.641 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.062 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.072 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.093 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.103 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.105 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.105 I llama_new_context_with_model: graph nodes  = 967
0.00.069.105 I llama_new_context_with_model: graph splits = 2
0.00.069.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.754 I 
0.00.588.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.822 I perplexity: tokenizing the input ..
0.00.602.448 I perplexity: tokenization took 13.621 ms
0.00.602.459 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.917 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.752.524 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.752.561 I llama_perf_context_print:        load time =     579.87 ms
0.00.752.563 I llama_perf_context_print: prompt eval time =     145.19 ms /   128 tokens (    1.13 ms per token,   881.59 tokens per second)
0.00.752.564 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.565 I llama_perf_context_print:       total time =     163.81 ms /   129 tokens
0.00.753.910 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.110s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.810 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.167 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.688 I llama_model_loader: - type  f32:  194 tensors
0.00.025.688 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.688 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.688 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.688 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.481 I llm_load_vocab: special tokens cache size = 25
0.00.051.438 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.442 I llm_load_print_meta: arch             = gptneox
0.00.051.442 I llm_load_print_meta: vocab type       = BPE
0.00.051.442 I llm_load_print_meta: n_vocab          = 50304
0.00.051.443 I llm_load_print_meta: n_merges         = 50009
0.00.051.443 I llm_load_print_meta: vocab_only       = 0
0.00.051.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.443 I llm_load_print_meta: n_embd           = 2048
0.00.051.443 I llm_load_print_meta: n_layer          = 24
0.00.051.446 I llm_load_print_meta: n_head           = 16
0.00.051.447 I llm_load_print_meta: n_head_kv        = 16
0.00.051.449 I llm_load_print_meta: n_rot            = 32
0.00.051.449 I llm_load_print_meta: n_swa            = 0
0.00.051.450 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.450 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.450 I llm_load_print_meta: n_gqa            = 1
0.00.051.451 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.452 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.452 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.453 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.453 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.453 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.453 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.454 I llm_load_print_meta: n_ff             = 8192
0.00.051.454 I llm_load_print_meta: n_expert         = 0
0.00.051.454 I llm_load_print_meta: n_expert_used    = 0
0.00.051.454 I llm_load_print_meta: causal attn      = 1
0.00.051.454 I llm_load_print_meta: pooling type     = 0
0.00.051.455 I llm_load_print_meta: rope type        = 2
0.00.051.459 I llm_load_print_meta: rope scaling     = linear
0.00.051.460 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.460 I llm_load_print_meta: freq_scale_train = 1
0.00.051.460 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.460 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.461 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.461 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.461 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.461 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.461 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.461 I llm_load_print_meta: model type       = 1.4B
0.00.051.473 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.474 I llm_load_print_meta: model params     = 1.41 B
0.00.051.476 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.476 I llm_load_print_meta: general.name     = 1.4B
0.00.051.476 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.477 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.477 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.477 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.477 I llm_load_print_meta: LF token         = 128 ''
0.00.051.477 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: max token length = 1024
0.00.053.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.396 I llm_load_tensors: offloading output layer to GPU
0.00.053.396 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.407 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.408 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.743 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.743 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.744 I llama_new_context_with_model: n_batch       = 2048
0.00.053.744 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.744 I llama_new_context_with_model: flash_attn    = 0
0.00.053.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.745 I llama_new_context_with_model: freq_scale    = 1
0.00.053.745 I ggml_metal_init: allocating
0.00.053.751 I ggml_metal_init: found device: Apple M4
0.00.053.753 I ggml_metal_init: picking default device: Apple M4
0.00.054.314 I ggml_metal_init: using embedded metal library
0.00.056.614 I ggml_metal_init: GPU name:   Apple M4
0.00.056.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.616 I ggml_metal_init: simdgroup reduction   = true
0.00.056.616 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.616 I ggml_metal_init: has bfloat            = true
0.00.056.616 I ggml_metal_init: use bfloat            = true
0.00.056.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.140 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.146 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.168 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.157 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.159 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.159 I llama_new_context_with_model: graph nodes  = 967
0.00.086.159 I llama_new_context_with_model: graph splits = 2
0.00.086.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.717 I main: llama threadpool init, n_threads = 4
0.00.524.761 I 
0.00.524.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.784 I 
0.00.525.014 I sampler seed: 1234
0.00.525.020 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.525.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.525.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.525.031 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.274.813 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.01.274.814 I llama_perf_context_print:        load time =     513.90 ms
0.01.274.814 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.66 tokens per second)
0.01.274.815 I llama_perf_context_print:        eval time =     702.45 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.274.818 I llama_perf_context_print:       total time =     750.10 ms /    70 tokens
0.01.275.037 I ggml_metal_free: deallocating

real	0m1.292s
user	0m0.110s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.186 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.435 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.023.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.573 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.036.927 I llama_model_loader: - type  f32:  194 tensors
0.00.036.927 I llama_model_loader: - type q3_K:   25 tensors
0.00.036.927 I llama_model_loader: - type q4_K:   71 tensors
0.00.036.928 I llama_model_loader: - type q5_K:    1 tensors
0.00.036.928 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.540 I llm_load_vocab: special tokens cache size = 25
0.00.070.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.763 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.763 I llm_load_print_meta: arch             = gptneox
0.00.070.763 I llm_load_print_meta: vocab type       = BPE
0.00.070.763 I llm_load_print_meta: n_vocab          = 50304
0.00.070.764 I llm_load_print_meta: n_merges         = 50009
0.00.070.764 I llm_load_print_meta: vocab_only       = 0
0.00.070.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.764 I llm_load_print_meta: n_embd           = 2048
0.00.070.764 I llm_load_print_meta: n_layer          = 24
0.00.070.767 I llm_load_print_meta: n_head           = 16
0.00.070.768 I llm_load_print_meta: n_head_kv        = 16
0.00.070.768 I llm_load_print_meta: n_rot            = 32
0.00.070.768 I llm_load_print_meta: n_swa            = 0
0.00.070.768 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.769 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.769 I llm_load_print_meta: n_gqa            = 1
0.00.070.770 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.771 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.772 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.774 I llm_load_print_meta: n_ff             = 8192
0.00.070.775 I llm_load_print_meta: n_expert         = 0
0.00.070.775 I llm_load_print_meta: n_expert_used    = 0
0.00.070.775 I llm_load_print_meta: causal attn      = 1
0.00.070.775 I llm_load_print_meta: pooling type     = 0
0.00.070.775 I llm_load_print_meta: rope type        = 2
0.00.070.775 I llm_load_print_meta: rope scaling     = linear
0.00.070.776 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.776 I llm_load_print_meta: freq_scale_train = 1
0.00.070.776 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.776 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.777 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.777 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.777 I llm_load_print_meta: model type       = 1.4B
0.00.070.788 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.070.789 I llm_load_print_meta: model params     = 1.41 B
0.00.070.789 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.070.789 I llm_load_print_meta: general.name     = 1.4B
0.00.070.790 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.790 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.790 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.790 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.790 I llm_load_print_meta: LF token         = 128 ''
0.00.070.791 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.791 I llm_load_print_meta: max token length = 1024
0.00.072.870 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.871 I llm_load_tensors: offloading output layer to GPU
0.00.072.871 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.882 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.072.883 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.073.237 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.238 I llama_new_context_with_model: n_ctx         = 128
0.00.073.238 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.073.238 I llama_new_context_with_model: n_batch       = 128
0.00.073.238 I llama_new_context_with_model: n_ubatch      = 128
0.00.073.238 I llama_new_context_with_model: flash_attn    = 0
0.00.073.239 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.239 I llama_new_context_with_model: freq_scale    = 1
0.00.073.239 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.073.240 I ggml_metal_init: allocating
0.00.073.244 I ggml_metal_init: found device: Apple M4
0.00.073.246 I ggml_metal_init: picking default device: Apple M4
0.00.073.827 I ggml_metal_init: using embedded metal library
0.00.076.421 I ggml_metal_init: GPU name:   Apple M4
0.00.076.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.423 I ggml_metal_init: simdgroup reduction   = true
0.00.076.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.424 I ggml_metal_init: has bfloat            = true
0.00.076.424 I ggml_metal_init: use bfloat            = true
0.00.076.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.645 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.919 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.921 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.939 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.808 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.809 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.809 I llama_new_context_with_model: graph nodes  = 967
0.00.088.810 I llama_new_context_with_model: graph splits = 2
0.00.088.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.428 I 
0.00.559.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.468 I perplexity: tokenizing the input ..
0.00.569.352 I perplexity: tokenization took 9.882 ms
0.00.569.360 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.783 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.703.086 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.703.113 I llama_perf_context_print:        load time =     548.99 ms
0.00.703.114 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.16 tokens per second)
0.00.703.115 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.115 I llama_perf_context_print:       total time =     143.69 ms /   129 tokens
0.00.703.556 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.100s
sys	0m0.078s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.471 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.024 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.026 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.026 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.634 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.637 I llama_model_loader: - type  f32:  194 tensors
0.00.025.637 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.637 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.638 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.372 I llm_load_vocab: special tokens cache size = 25
0.00.052.401 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.404 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.404 I llm_load_print_meta: arch             = gptneox
0.00.052.404 I llm_load_print_meta: vocab type       = BPE
0.00.052.405 I llm_load_print_meta: n_vocab          = 50304
0.00.052.405 I llm_load_print_meta: n_merges         = 50009
0.00.052.405 I llm_load_print_meta: vocab_only       = 0
0.00.052.405 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.405 I llm_load_print_meta: n_embd           = 2048
0.00.052.405 I llm_load_print_meta: n_layer          = 24
0.00.052.408 I llm_load_print_meta: n_head           = 16
0.00.052.411 I llm_load_print_meta: n_head_kv        = 16
0.00.052.412 I llm_load_print_meta: n_rot            = 32
0.00.052.412 I llm_load_print_meta: n_swa            = 0
0.00.052.412 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.413 I llm_load_print_meta: n_gqa            = 1
0.00.052.413 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.414 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.415 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.415 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.415 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.415 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.416 I llm_load_print_meta: n_ff             = 8192
0.00.052.416 I llm_load_print_meta: n_expert         = 0
0.00.052.421 I llm_load_print_meta: n_expert_used    = 0
0.00.052.421 I llm_load_print_meta: causal attn      = 1
0.00.052.422 I llm_load_print_meta: pooling type     = 0
0.00.052.422 I llm_load_print_meta: rope type        = 2
0.00.052.422 I llm_load_print_meta: rope scaling     = linear
0.00.052.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.423 I llm_load_print_meta: freq_scale_train = 1
0.00.052.423 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.423 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.423 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.423 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.424 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.424 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.424 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.424 I llm_load_print_meta: model type       = 1.4B
0.00.052.437 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.437 I llm_load_print_meta: model params     = 1.41 B
0.00.052.438 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.438 I llm_load_print_meta: general.name     = 1.4B
0.00.052.438 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.439 I llm_load_print_meta: LF token         = 128 ''
0.00.052.439 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.439 I llm_load_print_meta: max token length = 1024
0.00.054.418 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.418 I llm_load_tensors: offloading output layer to GPU
0.00.054.419 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.429 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.431 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.747 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.748 I llama_new_context_with_model: n_batch       = 2048
0.00.054.748 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.748 I llama_new_context_with_model: flash_attn    = 0
0.00.054.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.749 I llama_new_context_with_model: freq_scale    = 1
0.00.054.749 I ggml_metal_init: allocating
0.00.054.753 I ggml_metal_init: found device: Apple M4
0.00.054.762 I ggml_metal_init: picking default device: Apple M4
0.00.055.521 I ggml_metal_init: using embedded metal library
0.00.057.837 I ggml_metal_init: GPU name:   Apple M4
0.00.057.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.840 I ggml_metal_init: simdgroup reduction   = true
0.00.057.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.840 I ggml_metal_init: has bfloat            = true
0.00.057.840 I ggml_metal_init: use bfloat            = true
0.00.057.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.269 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.243 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.249 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.266 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.245 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.247 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.247 I llama_new_context_with_model: graph nodes  = 967
0.00.087.247 I llama_new_context_with_model: graph splits = 2
0.00.087.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.393 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.757 I main: llama threadpool init, n_threads = 4
0.00.645.802 I 
0.00.645.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.848 I 
0.00.646.084 I sampler seed: 1234
0.00.646.090 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.646.101 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.646.101 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.646.101 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.407.414 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.01.407.415 I llama_perf_context_print:        load time =     635.28 ms
0.01.407.416 I llama_perf_context_print: prompt eval time =      51.03 ms /     7 tokens (    7.29 ms per token,   137.17 tokens per second)
0.01.407.417 I llama_perf_context_print:        eval time =     707.39 ms /    63 runs   (   11.23 ms per token,    89.06 tokens per second)
0.01.407.420 I llama_perf_context_print:       total time =     761.66 ms /    70 tokens
0.01.407.657 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.109s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.218 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.221 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.221 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.222 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.223 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.898 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.620 I llama_model_loader: - type  f32:  194 tensors
0.00.024.620 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.620 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.620 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.209 I llm_load_vocab: special tokens cache size = 25
0.00.050.076 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.079 I llm_load_print_meta: arch             = gptneox
0.00.050.079 I llm_load_print_meta: vocab type       = BPE
0.00.050.080 I llm_load_print_meta: n_vocab          = 50304
0.00.050.080 I llm_load_print_meta: n_merges         = 50009
0.00.050.080 I llm_load_print_meta: vocab_only       = 0
0.00.050.080 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.080 I llm_load_print_meta: n_embd           = 2048
0.00.050.080 I llm_load_print_meta: n_layer          = 24
0.00.050.083 I llm_load_print_meta: n_head           = 16
0.00.050.084 I llm_load_print_meta: n_head_kv        = 16
0.00.050.084 I llm_load_print_meta: n_rot            = 32
0.00.050.084 I llm_load_print_meta: n_swa            = 0
0.00.050.085 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.085 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.085 I llm_load_print_meta: n_gqa            = 1
0.00.050.086 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.087 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.089 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.089 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.090 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.090 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.090 I llm_load_print_meta: n_ff             = 8192
0.00.050.091 I llm_load_print_meta: n_expert         = 0
0.00.050.091 I llm_load_print_meta: n_expert_used    = 0
0.00.050.091 I llm_load_print_meta: causal attn      = 1
0.00.050.091 I llm_load_print_meta: pooling type     = 0
0.00.050.091 I llm_load_print_meta: rope type        = 2
0.00.050.091 I llm_load_print_meta: rope scaling     = linear
0.00.050.094 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.094 I llm_load_print_meta: freq_scale_train = 1
0.00.050.094 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.095 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.095 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.095 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.095 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.095 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.096 I llm_load_print_meta: model type       = 1.4B
0.00.050.107 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.108 I llm_load_print_meta: model params     = 1.41 B
0.00.050.108 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.108 I llm_load_print_meta: general.name     = 1.4B
0.00.050.109 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.110 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: LF token         = 128 ''
0.00.050.111 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: max token length = 1024
0.00.052.036 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.036 I llm_load_tensors: offloading output layer to GPU
0.00.052.036 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.047 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.048 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.382 I llama_new_context_with_model: n_ctx         = 128
0.00.052.383 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.383 I llama_new_context_with_model: n_batch       = 128
0.00.052.383 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.383 I llama_new_context_with_model: flash_attn    = 0
0.00.052.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.384 I llama_new_context_with_model: freq_scale    = 1
0.00.052.384 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.384 I ggml_metal_init: allocating
0.00.052.387 I ggml_metal_init: found device: Apple M4
0.00.052.389 I ggml_metal_init: picking default device: Apple M4
0.00.052.958 I ggml_metal_init: using embedded metal library
0.00.055.272 I ggml_metal_init: GPU name:   Apple M4
0.00.055.274 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.275 I ggml_metal_init: simdgroup reduction   = true
0.00.055.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.275 I ggml_metal_init: has bfloat            = true
0.00.055.275 I ggml_metal_init: use bfloat            = true
0.00.055.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.276 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.146 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.156 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.177 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.058 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.058 I llama_new_context_with_model: graph nodes  = 967
0.00.067.058 I llama_new_context_with_model: graph splits = 2
0.00.067.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.293 I 
0.00.587.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.337 I perplexity: tokenizing the input ..
0.00.595.177 I perplexity: tokenization took 7.838 ms
0.00.595.181 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.730.809 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.730.846 I llama_perf_context_print:        load time =     577.41 ms
0.00.730.847 I llama_perf_context_print: prompt eval time =     134.25 ms /   128 tokens (    1.05 ms per token,   953.42 tokens per second)
0.00.730.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.848 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.731.317 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.077s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.013.199 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.831 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.461 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.461 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.461 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.462 I llama_model_loader: - type  f32:  194 tensors
0.00.029.462 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.463 I llama_model_loader: - type q6_K:   37 tensors
0.00.050.147 I llm_load_vocab: special tokens cache size = 25
0.00.056.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.233 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.233 I llm_load_print_meta: arch             = gptneox
0.00.056.234 I llm_load_print_meta: vocab type       = BPE
0.00.056.234 I llm_load_print_meta: n_vocab          = 50304
0.00.056.234 I llm_load_print_meta: n_merges         = 50009
0.00.056.235 I llm_load_print_meta: vocab_only       = 0
0.00.056.235 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.235 I llm_load_print_meta: n_embd           = 2048
0.00.056.235 I llm_load_print_meta: n_layer          = 24
0.00.056.239 I llm_load_print_meta: n_head           = 16
0.00.056.239 I llm_load_print_meta: n_head_kv        = 16
0.00.056.240 I llm_load_print_meta: n_rot            = 32
0.00.056.240 I llm_load_print_meta: n_swa            = 0
0.00.056.240 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.241 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.241 I llm_load_print_meta: n_gqa            = 1
0.00.056.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.243 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.245 I llm_load_print_meta: n_ff             = 8192
0.00.056.245 I llm_load_print_meta: n_expert         = 0
0.00.056.245 I llm_load_print_meta: n_expert_used    = 0
0.00.056.245 I llm_load_print_meta: causal attn      = 1
0.00.056.246 I llm_load_print_meta: pooling type     = 0
0.00.056.247 I llm_load_print_meta: rope type        = 2
0.00.056.247 I llm_load_print_meta: rope scaling     = linear
0.00.056.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.248 I llm_load_print_meta: freq_scale_train = 1
0.00.056.248 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.249 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.249 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.249 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.249 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.249 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.249 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.250 I llm_load_print_meta: model type       = 1.4B
0.00.056.262 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.056.262 I llm_load_print_meta: model params     = 1.41 B
0.00.056.262 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.056.263 I llm_load_print_meta: general.name     = 1.4B
0.00.056.263 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.263 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.263 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.263 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.263 I llm_load_print_meta: LF token         = 128 ''
0.00.056.264 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.264 I llm_load_print_meta: max token length = 1024
0.00.058.284 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.284 I llm_load_tensors: offloading output layer to GPU
0.00.058.284 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.294 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.058.296 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.058.630 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.630 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.630 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.630 I llama_new_context_with_model: n_batch       = 2048
0.00.058.631 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.631 I llama_new_context_with_model: flash_attn    = 0
0.00.058.631 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.631 I llama_new_context_with_model: freq_scale    = 1
0.00.058.632 I ggml_metal_init: allocating
0.00.058.635 I ggml_metal_init: found device: Apple M4
0.00.058.636 I ggml_metal_init: picking default device: Apple M4
0.00.059.225 I ggml_metal_init: using embedded metal library
0.00.061.554 I ggml_metal_init: GPU name:   Apple M4
0.00.061.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.557 I ggml_metal_init: simdgroup reduction   = true
0.00.061.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.557 I ggml_metal_init: has bfloat            = true
0.00.061.557 I ggml_metal_init: use bfloat            = true
0.00.061.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.167 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.188 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.307 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.309 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.309 I llama_new_context_with_model: graph nodes  = 967
0.00.093.309 I llama_new_context_with_model: graph splits = 2
0.00.093.312 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.941 I main: llama threadpool init, n_threads = 4
0.00.690.989 I 
0.00.691.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.012 I 
0.00.691.238 I sampler seed: 1234
0.00.691.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.286 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.286 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.286 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.538.555 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.538.556 I llama_perf_context_print:        load time =     677.74 ms
0.01.538.557 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.59 tokens per second)
0.01.538.558 I llama_perf_context_print:        eval time =     792.53 ms /    63 runs   (   12.58 ms per token,    79.49 tokens per second)
0.01.538.558 I llama_perf_context_print:       total time =     847.62 ms /    70 tokens
0.01.538.769 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.789 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.249 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.250 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.250 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.251 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.251 I llama_model_loader: - type  f32:  194 tensors
0.00.024.251 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.252 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.934 I llm_load_vocab: special tokens cache size = 25
0.00.049.860 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.863 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.863 I llm_load_print_meta: arch             = gptneox
0.00.049.864 I llm_load_print_meta: vocab type       = BPE
0.00.049.864 I llm_load_print_meta: n_vocab          = 50304
0.00.049.864 I llm_load_print_meta: n_merges         = 50009
0.00.049.864 I llm_load_print_meta: vocab_only       = 0
0.00.049.864 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.864 I llm_load_print_meta: n_embd           = 2048
0.00.049.865 I llm_load_print_meta: n_layer          = 24
0.00.049.868 I llm_load_print_meta: n_head           = 16
0.00.049.868 I llm_load_print_meta: n_head_kv        = 16
0.00.049.869 I llm_load_print_meta: n_rot            = 32
0.00.049.869 I llm_load_print_meta: n_swa            = 0
0.00.049.869 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.869 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.870 I llm_load_print_meta: n_gqa            = 1
0.00.049.871 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.871 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.872 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.872 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.872 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.873 I llm_load_print_meta: n_ff             = 8192
0.00.049.873 I llm_load_print_meta: n_expert         = 0
0.00.049.874 I llm_load_print_meta: n_expert_used    = 0
0.00.049.874 I llm_load_print_meta: causal attn      = 1
0.00.049.874 I llm_load_print_meta: pooling type     = 0
0.00.049.874 I llm_load_print_meta: rope type        = 2
0.00.049.874 I llm_load_print_meta: rope scaling     = linear
0.00.049.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.877 I llm_load_print_meta: freq_scale_train = 1
0.00.049.877 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.878 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.878 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.878 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.878 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.878 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.879 I llm_load_print_meta: model type       = 1.4B
0.00.049.890 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.891 I llm_load_print_meta: model params     = 1.41 B
0.00.049.891 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.891 I llm_load_print_meta: general.name     = 1.4B
0.00.049.892 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.893 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.893 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.893 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.893 I llm_load_print_meta: LF token         = 128 ''
0.00.049.894 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.894 I llm_load_print_meta: max token length = 1024
0.00.051.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.804 I llm_load_tensors: offloading output layer to GPU
0.00.051.805 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.815 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.816 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.172 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.172 I llama_new_context_with_model: n_ctx         = 128
0.00.052.172 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.173 I llama_new_context_with_model: n_batch       = 128
0.00.052.173 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.173 I llama_new_context_with_model: flash_attn    = 0
0.00.052.173 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.174 I llama_new_context_with_model: freq_scale    = 1
0.00.052.174 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.175 I ggml_metal_init: allocating
0.00.052.182 I ggml_metal_init: found device: Apple M4
0.00.052.184 I ggml_metal_init: picking default device: Apple M4
0.00.052.759 I ggml_metal_init: using embedded metal library
0.00.055.085 I ggml_metal_init: GPU name:   Apple M4
0.00.055.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.088 I ggml_metal_init: simdgroup reduction   = true
0.00.055.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.088 I ggml_metal_init: has bfloat            = true
0.00.055.088 I ggml_metal_init: use bfloat            = true
0.00.055.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.564 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.840 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.842 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.856 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.747 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.748 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.749 I llama_new_context_with_model: graph nodes  = 967
0.00.066.749 I llama_new_context_with_model: graph splits = 2
0.00.066.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.412 I 
0.00.699.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.463 I perplexity: tokenizing the input ..
0.00.707.636 I perplexity: tokenization took 8.171 ms
0.00.707.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.625 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.849.895 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.849.926 I llama_perf_context_print:        load time =     690.61 ms
0.00.849.927 I llama_perf_context_print: prompt eval time =     140.75 ms /   128 tokens (    1.10 ms per token,   909.39 tokens per second)
0.00.849.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.928 I llama_perf_context_print:       total time =     150.52 ms /   129 tokens
0.00.850.438 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.078s
sys	0m0.117s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.715 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.376 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.147 I llama_model_loader: - type  f32:  194 tensors
0.00.024.147 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.852 I llm_load_vocab: special tokens cache size = 25
0.00.050.867 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.870 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.870 I llm_load_print_meta: arch             = gptneox
0.00.050.871 I llm_load_print_meta: vocab type       = BPE
0.00.050.871 I llm_load_print_meta: n_vocab          = 50304
0.00.050.871 I llm_load_print_meta: n_merges         = 50009
0.00.050.871 I llm_load_print_meta: vocab_only       = 0
0.00.050.871 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.872 I llm_load_print_meta: n_embd           = 2048
0.00.050.872 I llm_load_print_meta: n_layer          = 24
0.00.050.874 I llm_load_print_meta: n_head           = 16
0.00.050.876 I llm_load_print_meta: n_head_kv        = 16
0.00.050.876 I llm_load_print_meta: n_rot            = 32
0.00.050.877 I llm_load_print_meta: n_swa            = 0
0.00.050.877 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.877 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.879 I llm_load_print_meta: n_gqa            = 1
0.00.050.880 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.881 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.882 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.882 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.882 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.882 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.883 I llm_load_print_meta: n_ff             = 8192
0.00.050.883 I llm_load_print_meta: n_expert         = 0
0.00.050.883 I llm_load_print_meta: n_expert_used    = 0
0.00.050.884 I llm_load_print_meta: causal attn      = 1
0.00.050.884 I llm_load_print_meta: pooling type     = 0
0.00.050.884 I llm_load_print_meta: rope type        = 2
0.00.050.884 I llm_load_print_meta: rope scaling     = linear
0.00.050.886 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.886 I llm_load_print_meta: freq_scale_train = 1
0.00.050.886 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.886 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.887 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.887 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.887 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.887 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.887 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.887 I llm_load_print_meta: model type       = 1.4B
0.00.050.899 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.899 I llm_load_print_meta: model params     = 1.41 B
0.00.050.900 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.900 I llm_load_print_meta: general.name     = 1.4B
0.00.050.900 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.900 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: LF token         = 128 ''
0.00.050.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: max token length = 1024
0.00.052.946 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.946 I llm_load_tensors: offloading output layer to GPU
0.00.052.946 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.957 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.958 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.318 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.318 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.318 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.318 I llama_new_context_with_model: n_batch       = 2048
0.00.053.319 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.319 I llama_new_context_with_model: flash_attn    = 0
0.00.053.319 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.319 I llama_new_context_with_model: freq_scale    = 1
0.00.053.320 I ggml_metal_init: allocating
0.00.053.323 I ggml_metal_init: found device: Apple M4
0.00.053.325 I ggml_metal_init: picking default device: Apple M4
0.00.053.921 I ggml_metal_init: using embedded metal library
0.00.056.302 I ggml_metal_init: GPU name:   Apple M4
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.305 I ggml_metal_init: simdgroup reduction   = true
0.00.056.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.305 I ggml_metal_init: has bfloat            = true
0.00.056.305 I ggml_metal_init: use bfloat            = true
0.00.056.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.076 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.085 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.108 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.127 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.128 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.128 I llama_new_context_with_model: graph nodes  = 967
0.00.086.129 I llama_new_context_with_model: graph splits = 2
0.00.086.131 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.424 I main: llama threadpool init, n_threads = 4
0.00.769.470 I 
0.00.769.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.520 I 
0.00.769.760 I sampler seed: 1234
0.00.769.765 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.807 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.808 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.644.419 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.644.420 I llama_perf_context_print:        load time =     760.70 ms
0.01.644.421 I llama_perf_context_print: prompt eval time =      54.39 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.01.644.422 I llama_perf_context_print:        eval time =     817.12 ms /    63 runs   (   12.97 ms per token,    77.10 tokens per second)
0.01.644.422 I llama_perf_context_print:       total time =     875.00 ms /    70 tokens
0.01.644.623 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.110s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4462 (c05e8c99) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.851 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.330 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.625 I llama_model_loader: - type  f32:  194 tensors
0.00.023.626 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.289 I llm_load_vocab: special tokens cache size = 25
0.00.049.264 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.267 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.267 I llm_load_print_meta: arch             = gptneox
0.00.049.267 I llm_load_print_meta: vocab type       = BPE
0.00.049.268 I llm_load_print_meta: n_vocab          = 50304
0.00.049.268 I llm_load_print_meta: n_merges         = 50009
0.00.049.268 I llm_load_print_meta: vocab_only       = 0
0.00.049.268 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.268 I llm_load_print_meta: n_embd           = 2048
0.00.049.268 I llm_load_print_meta: n_layer          = 24
0.00.049.272 I llm_load_print_meta: n_head           = 16
0.00.049.272 I llm_load_print_meta: n_head_kv        = 16
0.00.049.272 I llm_load_print_meta: n_rot            = 32
0.00.049.274 I llm_load_print_meta: n_swa            = 0
0.00.049.274 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.275 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.275 I llm_load_print_meta: n_gqa            = 1
0.00.049.276 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.277 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.277 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.278 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.281 I llm_load_print_meta: n_ff             = 8192
0.00.049.281 I llm_load_print_meta: n_expert         = 0
0.00.049.281 I llm_load_print_meta: n_expert_used    = 0
0.00.049.281 I llm_load_print_meta: causal attn      = 1
0.00.049.281 I llm_load_print_meta: pooling type     = 0
0.00.049.281 I llm_load_print_meta: rope type        = 2
0.00.049.282 I llm_load_print_meta: rope scaling     = linear
0.00.049.282 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.282 I llm_load_print_meta: freq_scale_train = 1
0.00.049.283 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.283 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.283 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.283 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.283 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.288 I llm_load_print_meta: model type       = 1.4B
0.00.049.300 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.300 I llm_load_print_meta: model params     = 1.41 B
0.00.049.300 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.300 I llm_load_print_meta: general.name     = 1.4B
0.00.049.301 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.301 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.301 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.301 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.301 I llm_load_print_meta: LF token         = 128 ''
0.00.049.302 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.302 I llm_load_print_meta: max token length = 1024
0.00.051.289 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.289 I llm_load_tensors: offloading output layer to GPU
0.00.051.290 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.300 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.302 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.638 I llama_new_context_with_model: n_ctx         = 128
0.00.051.638 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.638 I llama_new_context_with_model: n_batch       = 128
0.00.051.638 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.639 I llama_new_context_with_model: flash_attn    = 0
0.00.051.639 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.639 I llama_new_context_with_model: freq_scale    = 1
0.00.051.640 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.640 I ggml_metal_init: allocating
0.00.051.643 I ggml_metal_init: found device: Apple M4
0.00.051.644 I ggml_metal_init: picking default device: Apple M4
0.00.052.199 I ggml_metal_init: using embedded metal library
0.00.054.493 I ggml_metal_init: GPU name:   Apple M4
0.00.054.495 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.495 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.496 I ggml_metal_init: simdgroup reduction   = true
0.00.054.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.496 I ggml_metal_init: has bfloat            = true
0.00.054.496 I ggml_metal_init: use bfloat            = true
0.00.054.496 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.497 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.785 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.036 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.050 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.892 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.893 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.893 I llama_new_context_with_model: graph nodes  = 967
0.00.065.893 I llama_new_context_with_model: graph splits = 2
0.00.065.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.466 I 
0.00.701.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.504 I perplexity: tokenizing the input ..
0.00.709.637 I perplexity: tokenization took 8.13 ms
0.00.709.640 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.633 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.850.814 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.850.841 I llama_perf_context_print:        load time =     692.61 ms
0.00.850.843 I llama_perf_context_print: prompt eval time =     139.77 ms /   128 tokens (    1.09 ms per token,   915.82 tokens per second)
0.00.850.844 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.844 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.851.335 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.078s
sys	0m0.131s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4462 (c05e8c99)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cc0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cc0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cc0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cc0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cc0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cc0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cc0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cc0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cc0d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cc0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cc0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cc0e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cc0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cc0f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cc0ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cc10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cc10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cc11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cc11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cc12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cc12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cc13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cc138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cc14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cc14870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cc14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cc15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cc15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cc162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cc165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cc16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cc16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cc175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cc17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cc17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cc18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cc186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cc18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cc19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cc194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cc19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cc19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cc1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cc1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cc1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cc1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cc1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cc1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cc1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cc1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cc1d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cc1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cc1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cc1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cc1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cc1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cc1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cc1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cc1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cc20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cc20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cc20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cc21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cc21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cc21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cc21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cc22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cc228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cc22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cc231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cc23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cc23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cc23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cc24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cc24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cc24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cc25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cc25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cc25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cc26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cc26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cc26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cc274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cc27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cc27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cc284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cc28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cc28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cc294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cc29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cc29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cc2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cc2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cc2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cc2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cc2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cc2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cc1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cc2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cc2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cc2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cc2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cc2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cc2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cc2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cc2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cc2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cc2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cc2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cc30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cc305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cc30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cc31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cc31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cc319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cc31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cc32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cc327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cc32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cc330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cc33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cc33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cc33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cc34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cc34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cc34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cc35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cd04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cd044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cd04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cd04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cd05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cd056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cd05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cd05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cd06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cd06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cd06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cd07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cd075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cd07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cd07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cd08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cd08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cd08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cd09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cd094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cd09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cd09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cd0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cd0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cd0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cd0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cd0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cd0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cd0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cd0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cd0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cd0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cd0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cd0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cd0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cd0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cd0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cd0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cd0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cd0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cd0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cd0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cd0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cd0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cd103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cd10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cd10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cd11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cd11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cd119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cd11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cd122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cd12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cd12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cd13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cd13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cd13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cd13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cd141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cd14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cd14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cd14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cd153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cd15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cd15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cd160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cd16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cd169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cd16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cd172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cd17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cd17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cd18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cd18470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cd188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cd18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cd191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cd19630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cd19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cd19f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cd1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cd1a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cd1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cd1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cd1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cd1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cd1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cd1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cd1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cd1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cd1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cd1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cd1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cd1ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cd1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cd1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cd20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cd205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cd20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cd21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cd216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cd21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cd22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cd227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cd22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cd23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cd238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cd23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cd24450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cd24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cd24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cd25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cd25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cd260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cd26670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cd26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cd271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cd27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cd27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cd282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cd28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cd28e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cd293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cd299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cd29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cd2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cd2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cd2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cd2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cd2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cd2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cd2c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cd2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cd2d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cd2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cd2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cd2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cd2e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cd2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cd2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cd2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cd2ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cd30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cd30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cd30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cd31350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cd31850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cd31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cd32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cd32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cd32c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cd33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cd33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cd33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cd34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cd34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cd34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cd35680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cd35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cd364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cd36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cd36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cd37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cd37840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.151.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.151.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cc1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cc1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cc1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cc14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cc1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cc1c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cc1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cc1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cc1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cc1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cc1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cc13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cc20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cc16fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cc17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cc15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cc15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cc35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cc356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cc35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cc35c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cc35f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cc361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cc36480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cc36740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cc36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cc36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cc36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cc37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cc37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cc377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cc37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cc37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cc38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cc382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cc38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cc38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cc38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cc38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cc39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cc39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cc39600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cc398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cc39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cc39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cc3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cc3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cc3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cc3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cc3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cc3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cc3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cc3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cc3b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cc3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cc3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cc3bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cc3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cc3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cc3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cc3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cc3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cc3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cc3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cc3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cc3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cc3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cc3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cc3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cc3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cc3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cc3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cc40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cc404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cc40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cc40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cc41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cc41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cc41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cc41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cc423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cc42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cc42cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cc43120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cc43590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cc43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cc43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cc442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cc44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cc44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cc45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cc454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cc45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cc45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cc461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cc46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cc46ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cc46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cc473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cc47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cc47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cc48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cc48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cc489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cc48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cc492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cc49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cc49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cc4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cc4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cc4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cc4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cc4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cc4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cc4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cc4bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cc4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cc4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cc4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cc4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cc4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cc4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cc4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cc4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cc4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cc4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cc4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cc4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cc4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cc4fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cc501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cc50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cc50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cc50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cc51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cc517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cc51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cc520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cc52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cc529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cc52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cc53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cc536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cc53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cc53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cc54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cc548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cc54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cc55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cc55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cc55a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cc55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cc56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cc567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cc56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cc570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cc57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cc57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cc57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cc58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cc586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cc58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cc58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cc59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cc59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cc59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cc5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cc5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cc5ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cc5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cc5b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cc5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cc5c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cc5c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cc5cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cc5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cc5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cc5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cc5e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cc5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cc5ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cc5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cc5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cc5f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cc5fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cc60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cc60840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cc60d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cc61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cc61740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cc61c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cc62140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cc62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cc62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cc630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cc636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cc63c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cc64200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cc64810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cc64e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cc65430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cc65c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cc660c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cc66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cc66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cc66fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cc67790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cc67c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cc680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cc68570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cc68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cc69270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cc697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cc69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cc6a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cc6a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cc6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cc6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cc6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cc6bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cc6c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cc6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cc6cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cc6d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cc6d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cc6dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cc6e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cc6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cc6ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cc6f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cc6f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cc6fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cc70200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cc70750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cc70ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cc711f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cc71740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cc71c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cc721e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cc72730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cc72c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cc731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cc73720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cc73c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cc741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cc74710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cc74c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cc751b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cc75700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cc75c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cc761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cc766f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cc76c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cc77190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cc776e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cc77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cc78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cc786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cc78c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cc79170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cc796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cc79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cc7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cc7a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cc7ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cc7b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cc7b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cc7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cc7bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cc7c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cc7c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cc7cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cc7d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cc7d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cc7dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cc7e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cc7e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cc7e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cc7ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cc7f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cc7f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cc7fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cc80150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cc80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cc80f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cc816b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cc81dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cc82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cc82880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cc82b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cc83150 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cd24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cd213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cd1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cd2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cd2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cd296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cd27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cd1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cd1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cd21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cd23050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cd285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cd25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cd2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cd1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cd27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cd22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cd29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cd26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cd2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cd1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cd1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cd2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cd34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cd224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cd24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cd28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cd202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cd2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cd1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cd2d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cd2ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cd20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cd26930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cd2f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cd1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cd2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cd1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cd2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cd27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cd29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cd2c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cd2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cd23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cd1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cd37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cd37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cd38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cd386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cd389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cd38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cd38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cd391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cd394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cd39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cd39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cd39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cd39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cd3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cd3a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cd3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cd3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cd3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cd3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cd3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cd3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cd3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cd3bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cd3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cd3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cd3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cd3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cd3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cd3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cd3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cd3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cd3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cd3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cd3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cd3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cd3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cd3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cd3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cd3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cd3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cd3eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cd3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cd3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cd3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cd3f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cd3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cd3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cd3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cd402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cd40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cd40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cd40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cd40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cd41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cd415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cd41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cd42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cd425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cd42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cd42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cd431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cd43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cd43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cd43f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cd44370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cd447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cd44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cd450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cd45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cd45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cd45f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cd46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cd467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cd46c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cd470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cd47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cd479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cd47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cd48290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cd48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cd48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cd48fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cd49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cd498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cd49d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cd4a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cd4a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cd4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cd4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cd4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cd4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cd4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cd4c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cd4c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cd4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cd4ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cd4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cd4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cd4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cd4dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cd4e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cd4e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cd4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cd4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cd4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cd4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cd4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cd50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cd507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cd50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cd51090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cd51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cd51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cd51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cd52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cd526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cd52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cd52fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cd53410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cd53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cd53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cd54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cd545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cd54a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cd54eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cd55320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cd55790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cd55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cd56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cd564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cd56950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cd56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cd57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cd576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cd57b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cd57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cd583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cd58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cd58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cd59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cd595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cd59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cd59e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cd5a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cd5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cd5abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cd5b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cd5b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cd5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cd5bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cd5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cd5c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cd5caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cd5cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cd5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cd5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cd5dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cd5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cd5e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cd5ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cd5f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cd5f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cd5f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cd5fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cd60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cd60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cd61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cd61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cd61cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cd622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cd62870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cd62e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cd633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cd639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cd63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cd64530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cd64af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cd650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cd65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cd65c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cd661f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cd667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cd66d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cd67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cd678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cd67eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cd68470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cd68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cd68ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cd695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cd69b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cd6a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cd6a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cd6acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cd6b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cd6b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cd6bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cd6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cd6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cd6cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cd6d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cd6dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cd6e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cd6e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cd6ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cd6f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cd6f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cd6fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cd702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cd708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cd70e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cd71430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cd719f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cd71fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cd72570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cd72b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cd730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cd736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cd73c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cd74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cd747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cd74db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cd75370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cd75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cd75d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cd76270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cd76770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cd76c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cd77170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cd77670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cd77b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cd78070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cd78570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cd78a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cd78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cd79470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cd79970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cd79e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cd7a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cd7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cd7b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cd7bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cd7c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cd7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cd7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cd7d160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.799s
user	0m0.293s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4462 (c05e8c99)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a60e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a60edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a60f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a60f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a60fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a610490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a610a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a610ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a6115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a611aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a6124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a612fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a613770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a613f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a6146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a614dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a6154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a615c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a6163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a6181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a6188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a618bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a6191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a619e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a61a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a61a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a61aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a61ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a61b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a61bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a61be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a61c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a61c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a61cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a61d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a61d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a61d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a61de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a61e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a61e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a61ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a61f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a61f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a61ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a6205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a620be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a6211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a621800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a621e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a6230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a623550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a623810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a623e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a6248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a624d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a625210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a6256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a625ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a626490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a626930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a627710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a627bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a628050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a6285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a628af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a629040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a629590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a629ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a62a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a62a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a62aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a62b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a62b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a62bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a62c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a62c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a62cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a62d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a62d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a62daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a62dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a62e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a62ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a62efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a62f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a62fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a62ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a630440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a630bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a631140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a631690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a631be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a632130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a632bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a633bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a634110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a634660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a634bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a635100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a6355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a635a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a635ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a636380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a636820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a636cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a637160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a637600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a637aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a637f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a6383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a638880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a638d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a6391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a639b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a639fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a63a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a63a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a63ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a63b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a63b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a63bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a63c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a63c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a63c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a63cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a63d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a63d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a63dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a63e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a63e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a63e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a63ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a63f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a63f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a63fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a6400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a640560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a640a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a640ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a641340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a6417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a641c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a642120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a6425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a642a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a642f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a6433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a643840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a643ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a644180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a644620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a644ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a645400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a6458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a645d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a6461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a646680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a646b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a646fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a647460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a647900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a647da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a648240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a6486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a648b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a649020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a6494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a649960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a649e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a64a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a64a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a64abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a64b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a64b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a64b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a64be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a64c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a64c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a64cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a64d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a64d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a64db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a64e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a64e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a64ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a64f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a64f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a64fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a650290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a6508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a651090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a651530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a6519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a652620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a652b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a6530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a653610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a653b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a6540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a654600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a654b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a6550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a6555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a655b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a656090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a6565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a656b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a657080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a6575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a657b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a658070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a6585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a658b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a659060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a6595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a659b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a65a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a65a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a65aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a65b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a65b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a65bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a65c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a65c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a65cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a65d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a65d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a65dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a65e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a65e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a65eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a65f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a65f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a65faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a65fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a660540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a660a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a660fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a661530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a661a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a661fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a662520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a662a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a662fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a663510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a663a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a663fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a664500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a664a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a664fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a665440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a6658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a666220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a6666c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a666b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a667000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a6674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a667940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a667de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a668280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a668720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a668bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a669060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a669500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a669a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a66a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a66a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a66afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a66b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a66b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a66c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a66c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a66ca50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a66c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a64e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a64ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a64e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a6214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a618e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a61f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a61ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a620ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a617e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a60dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a6226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a6240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a66bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a61b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a650b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a64eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a619480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a619a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a66ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a66d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a66d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a66d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a66d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a66dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a66df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a66e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a66e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a66e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a66ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a66ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a66efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a66f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a66f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a66f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a66fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a66fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a670030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a6702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a6705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a670870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a670b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a670df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a6710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a671370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a671630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a6718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a671bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a671e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a672130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a6723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a6726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a672970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a672c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a672ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a6731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a673470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a673730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a6739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a673cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a673f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a674230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a6744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a6747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a674a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a674d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a674ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a6752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a675570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a675830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a675af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a675db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a676070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a676330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a6765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a6768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a676b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a676e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a6770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a6773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a677670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a677930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a677bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a677eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a678170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a678430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a6786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a6789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a678c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a678f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a6791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a6794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a679770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a679a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a679cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a679fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a67a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a67a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a67a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a67aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a67ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a67b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a67b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a67b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a67b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a67bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a67bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a67c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a67c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a67c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a67c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a67cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a67ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a67d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a67d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a67d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a67d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a67dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a67def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a67e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a67e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a67e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a67e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a67ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a67ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a67f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a67f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a67f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a67fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a67fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a67fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a6802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a680570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a680830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a680af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a680db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a681070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a681330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a6815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a6818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a681b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a681e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a6820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a6823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a682670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a682930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a682bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a682eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a683170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a683430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a6836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a6839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a683c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a683f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a6841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a6844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a684770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a684a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a684cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a684fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a685270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a685530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a6857f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a685ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a685d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a686030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a6862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a6865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a686870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a686b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a686df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a6870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a687370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a687630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a6878f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a687bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a687e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a688130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a6883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a6886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a688970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a688c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a688ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a6891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a689470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a689730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a6899f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a689cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a689f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a68a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a68a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a68a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a68aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a68ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a68aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a68b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a68b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a68b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a68baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a68bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a68c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a68c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a68c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a68cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a68ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a68d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a68d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a68d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a68d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a68dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a68df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a68e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a68e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a68e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a68ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a68ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a68ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a68f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a68f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a68f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a68fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a68fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a690000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a6902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a690580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a690840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a690b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a690dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a691080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a691340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a691600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a6918c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a691e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a692360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a6928b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a692e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a693350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a6938a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a693df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a694340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a694890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a694de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a695330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a695880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a695dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a696320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a696870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a696dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a697310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a697860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a697db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a698300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a698850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a698da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a6992f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a699840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a699d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a69a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a69a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a69aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a69adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a69b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a69b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a69bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a69c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a69c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a69cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a69d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a69d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a69dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a69dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a69e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a69e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a69eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a69f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a69fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a6a04e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a6a0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a6a1320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a6a15e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a6a1dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a6a2090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a6a26a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a70aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a708b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a70b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a70b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a70baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a70c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a70c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a70cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a70d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a70d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a70db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a70e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a70eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a70f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a70fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a710260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a710980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a7110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a7117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a711f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a7126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a712dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a7134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a714330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a7145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a714c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a715210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a715820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a716010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a7164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a716770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a717000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a717540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a717ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a718140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a7185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a718a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a718f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a7193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a719860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a719d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a71a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a71a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a71aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a71b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a71bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a71c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a71c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a71ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a71daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a71e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a71e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a71ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a71eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a71f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a71fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a720180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a720ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a720f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a721400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a7218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a721d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a7221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a722b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a722fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a723460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a723e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a7243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a7248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a724e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a725390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a7258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a726380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a7268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a726e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a727370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a7278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a727e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a728360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a7288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a729350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a7298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a729df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a72a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a72a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a72ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a72b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a72b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a72bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a72c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a72c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a72cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a72d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a72d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a72ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a72e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a72e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a72eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a72f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a72f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a72fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a7302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a730830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a730d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a7316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a731b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a732000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a7324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a732940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a732de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a733280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a733720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a733bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a734060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a734500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a7349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a7352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a735780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a7360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a736560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a736a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a736ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a737340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a7377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a737c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a738120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a7385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a738a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a738f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a7393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a739ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a73a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a73a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a73aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a73af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a73b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a73b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a73bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a73c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a73c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a73cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a73cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a73d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a73d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a73dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a73e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a73e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a73eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a73f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a73f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a73f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a73fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a7402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a740740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a740be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a741080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a741520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a7419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a742300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a7427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a742c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a7430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a743580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a743a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a743ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a744360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a744800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a744ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a745140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a7455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a745f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a7463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a746860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a746d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a7471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a747640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a747ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a7484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a748a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a748f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a7494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a749780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a749d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a74a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a74a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a74b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a74b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a74b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a74bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a74c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a74cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a74d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a74d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a74daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a74e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a74e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a74ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a74f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a74f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a74fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a750280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a7507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a750d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a751270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a7517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a751d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a752260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a7527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a752d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a753250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a7537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a753cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a754790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a754ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a755230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a755780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a755cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a756770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a756cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a757210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a757760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a757cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a758200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a758750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a758ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a7591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a759740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a759c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a75a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a75a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a75ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a75b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a75b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a75bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a75c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a75c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a75cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a75d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a75d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a75dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a75e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a75e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a75ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a75f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a75f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a75fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a760180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a7606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a760c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a7610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a761560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a761a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a761ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a762340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a7627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a762c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a763120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a7635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a763a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a763f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a7643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a764840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a764ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a765180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a7656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a765df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a766510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a766c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a767350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a767610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a767e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a7680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a7686d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.243s
sys	0m0.135s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.18 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.15 user         0.04 sys
```
