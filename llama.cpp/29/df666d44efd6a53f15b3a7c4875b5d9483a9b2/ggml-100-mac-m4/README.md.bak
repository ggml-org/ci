### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.64 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.82 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.48 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.01 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.28 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.26 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.33 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.21 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.72 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.97 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.98 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.56 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.35 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.68 sec*proc (28 tests)

Total Test time (real) = 224.69 sec

real	3m44.718s
user	7m34.360s
sys	0m6.845s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.20 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.24 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.32 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.59 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.41 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.24 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.07 sec*proc (28 tests)

Total Test time (real) =  52.08 sec

real	0m52.091s
user	1m12.447s
sys	0m5.716s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.119 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.362 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.417 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.029.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.435 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.029.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.437 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.029.437 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.029.438 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.029.440 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.029.441 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.029.442 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.029.446 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.029.446 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.029.450 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.029.451 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.029.451 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.029.452 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.029.452 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.029.453 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.029.454 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.035.450 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.457 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.035.458 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.035.458 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.035.459 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.035.460 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.035.460 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.035.461 I llama_model_loader: - type  f32:  124 tensors
0.00.035.470 I llama_model_loader: - type  f16:   73 tensors
0.00.041.191 I llm_load_vocab: special tokens cache size = 5
0.00.044.013 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.044.020 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.044.021 I llm_load_print_meta: arch             = bert
0.00.044.023 I llm_load_print_meta: vocab type       = WPM
0.00.044.030 I llm_load_print_meta: n_vocab          = 30522
0.00.044.030 I llm_load_print_meta: n_merges         = 0
0.00.044.030 I llm_load_print_meta: vocab_only       = 0
0.00.044.031 I llm_load_print_meta: n_ctx_train      = 512
0.00.044.031 I llm_load_print_meta: n_embd           = 384
0.00.044.031 I llm_load_print_meta: n_layer          = 12
0.00.044.035 I llm_load_print_meta: n_head           = 12
0.00.044.036 I llm_load_print_meta: n_head_kv        = 12
0.00.044.037 I llm_load_print_meta: n_rot            = 32
0.00.044.037 I llm_load_print_meta: n_swa            = 0
0.00.044.037 I llm_load_print_meta: n_embd_head_k    = 32
0.00.044.038 I llm_load_print_meta: n_embd_head_v    = 32
0.00.044.039 I llm_load_print_meta: n_gqa            = 1
0.00.044.040 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.044.041 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.044.042 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.044.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.044.043 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.044.044 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.044.044 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.044.048 I llm_load_print_meta: n_ff             = 1536
0.00.044.048 I llm_load_print_meta: n_expert         = 0
0.00.044.048 I llm_load_print_meta: n_expert_used    = 0
0.00.044.049 I llm_load_print_meta: causal attn      = 0
0.00.044.049 I llm_load_print_meta: pooling type     = 2
0.00.044.049 I llm_load_print_meta: rope type        = 2
0.00.044.050 I llm_load_print_meta: rope scaling     = linear
0.00.044.050 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.044.051 I llm_load_print_meta: freq_scale_train = 1
0.00.044.051 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.044.052 I llm_load_print_meta: rope_finetuned   = unknown
0.00.044.052 I llm_load_print_meta: ssm_d_conv       = 0
0.00.044.052 I llm_load_print_meta: ssm_d_inner      = 0
0.00.044.053 I llm_load_print_meta: ssm_d_state      = 0
0.00.044.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.044.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.044.059 I llm_load_print_meta: model type       = 33M
0.00.044.060 I llm_load_print_meta: model ftype      = F16
0.00.044.061 I llm_load_print_meta: model params     = 33.21 M
0.00.044.062 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.044.063 I llm_load_print_meta: general.name     = Bge Small
0.00.044.064 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.044.064 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.044.064 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.044.065 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.044.065 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.044.065 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.044.066 I llm_load_print_meta: max token length = 21
0.00.046.495 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.046.497 I llm_load_tensors: offloading output layer to GPU
0.00.046.497 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.046.526 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.046.527 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.047.250 I llama_new_context_with_model: n_seq_max     = 1
0.00.047.251 I llama_new_context_with_model: n_ctx         = 512
0.00.047.251 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.047.252 I llama_new_context_with_model: n_batch       = 2048
0.00.047.252 I llama_new_context_with_model: n_ubatch      = 2048
0.00.047.252 I llama_new_context_with_model: flash_attn    = 0
0.00.047.253 I llama_new_context_with_model: freq_base     = 10000.0
0.00.047.253 I llama_new_context_with_model: freq_scale    = 1
0.00.047.254 I ggml_metal_init: allocating
0.00.047.260 I ggml_metal_init: found device: Apple M4
0.00.047.264 I ggml_metal_init: picking default device: Apple M4
0.00.048.351 I ggml_metal_init: using embedded metal library
0.00.053.141 I ggml_metal_init: GPU name:   Apple M4
0.00.053.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.145 I ggml_metal_init: simdgroup reduction   = true
0.00.053.145 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.145 I ggml_metal_init: has bfloat            = true
0.00.053.145 I ggml_metal_init: use bfloat            = true
0.00.053.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.283 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.067.977 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.067.980 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.067.981 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.068.846 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.068.848 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.068.848 I llama_new_context_with_model: graph nodes  = 429
0.00.068.848 I llama_new_context_with_model: graph splits = 2
0.00.068.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.068.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.077.613 I 
0.00.077.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.078.488 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.083.827 I llama_perf_context_print:        load time =      52.23 ms
0.00.083.828 I llama_perf_context_print: prompt eval time =       5.18 ms /     9 tokens (    0.58 ms per token,  1736.11 tokens per second)
0.00.083.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.083.829 I llama_perf_context_print:       total time =       6.21 ms /    10 tokens
0.00.083.990 I ggml_metal_free: deallocating

real	0m0.292s
user	0m0.055s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.613 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.781 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.786 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.787 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.791 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.792 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.792 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.793 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.793 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.794 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.794 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.794 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.797 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.797 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.797 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.798 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.798 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.798 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.798 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.228 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.229 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.229 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.230 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.230 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.230 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.231 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.231 I llama_model_loader: - type  f32:  124 tensors
0.00.015.231 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.790 I llm_load_vocab: special tokens cache size = 5
0.00.019.138 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.141 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.141 I llm_load_print_meta: arch             = bert
0.00.019.142 I llm_load_print_meta: vocab type       = WPM
0.00.019.142 I llm_load_print_meta: n_vocab          = 30522
0.00.019.142 I llm_load_print_meta: n_merges         = 0
0.00.019.142 I llm_load_print_meta: vocab_only       = 0
0.00.019.142 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.142 I llm_load_print_meta: n_embd           = 384
0.00.019.143 I llm_load_print_meta: n_layer          = 12
0.00.019.145 I llm_load_print_meta: n_head           = 12
0.00.019.146 I llm_load_print_meta: n_head_kv        = 12
0.00.019.146 I llm_load_print_meta: n_rot            = 32
0.00.019.146 I llm_load_print_meta: n_swa            = 0
0.00.019.146 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.146 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.147 I llm_load_print_meta: n_gqa            = 1
0.00.019.148 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.149 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.150 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.154 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.154 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.154 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.154 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.155 I llm_load_print_meta: n_ff             = 1536
0.00.019.155 I llm_load_print_meta: n_expert         = 0
0.00.019.155 I llm_load_print_meta: n_expert_used    = 0
0.00.019.155 I llm_load_print_meta: causal attn      = 0
0.00.019.155 I llm_load_print_meta: pooling type     = 2
0.00.019.155 I llm_load_print_meta: rope type        = 2
0.00.019.155 I llm_load_print_meta: rope scaling     = linear
0.00.019.156 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.156 I llm_load_print_meta: freq_scale_train = 1
0.00.019.156 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.156 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.157 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.157 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.157 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.157 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.157 I llm_load_print_meta: model type       = 33M
0.00.019.158 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.158 I llm_load_print_meta: model params     = 33.21 M
0.00.019.158 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.159 I llm_load_print_meta: general.name     = Bge Small
0.00.019.159 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.159 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.159 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.159 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.159 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.160 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.160 I llm_load_print_meta: max token length = 21
0.00.020.484 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.485 I llm_load_tensors: offloading output layer to GPU
0.00.020.485 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.490 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.491 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.831 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.832 I llama_new_context_with_model: n_ctx         = 512
0.00.020.832 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.832 I llama_new_context_with_model: n_batch       = 2048
0.00.020.832 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.833 I llama_new_context_with_model: flash_attn    = 0
0.00.020.833 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.833 I llama_new_context_with_model: freq_scale    = 1
0.00.020.834 I ggml_metal_init: allocating
0.00.020.836 I ggml_metal_init: found device: Apple M4
0.00.020.838 I ggml_metal_init: picking default device: Apple M4
0.00.021.421 I ggml_metal_init: using embedded metal library
0.00.023.998 I ggml_metal_init: GPU name:   Apple M4
0.00.024.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.002 I ggml_metal_init: simdgroup reduction   = true
0.00.024.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.002 I ggml_metal_init: has bfloat            = true
0.00.024.003 I ggml_metal_init: use bfloat            = true
0.00.024.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.095 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.034.564 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.566 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.567 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.135 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.136 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.136 I llama_new_context_with_model: graph nodes  = 429
0.00.035.136 I llama_new_context_with_model: graph splits = 2
0.00.035.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.198 I 
0.00.040.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.743 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.206 I llama_perf_context_print:        load time =      30.58 ms
0.00.045.207 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2075.17 tokens per second)
0.00.045.207 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.208 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.045.399 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.161 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.537 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.941 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.946 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.949 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.959 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.963 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.964 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.965 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.966 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.967 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.967 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.968 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.971 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.972 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.973 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.936 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.975 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.975 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.976 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.976 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.976 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.977 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.977 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.977 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.978 I llama_model_loader: - type  f32:   40 tensors
0.00.047.978 I llama_model_loader: - type  f16:   30 tensors
0.00.065.567 W llm_load_vocab: empty token at index 5
0.00.069.979 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.249 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.279 I llm_load_vocab: special tokens cache size = 5
0.00.331.796 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.808 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.808 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.809 I llm_load_print_meta: vocab type       = BPE
0.00.331.809 I llm_load_print_meta: n_vocab          = 61056
0.00.331.813 I llm_load_print_meta: n_merges         = 39382
0.00.331.813 I llm_load_print_meta: vocab_only       = 0
0.00.331.813 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.813 I llm_load_print_meta: n_embd           = 384
0.00.331.814 I llm_load_print_meta: n_layer          = 4
0.00.331.820 I llm_load_print_meta: n_head           = 12
0.00.331.820 I llm_load_print_meta: n_head_kv        = 12
0.00.331.821 I llm_load_print_meta: n_rot            = 32
0.00.331.821 I llm_load_print_meta: n_swa            = 0
0.00.331.821 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.821 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.821 I llm_load_print_meta: n_gqa            = 1
0.00.331.822 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.823 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.823 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.825 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.825 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.826 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.826 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.826 I llm_load_print_meta: n_ff             = 1536
0.00.331.826 I llm_load_print_meta: n_expert         = 0
0.00.331.826 I llm_load_print_meta: n_expert_used    = 0
0.00.331.827 I llm_load_print_meta: causal attn      = 0
0.00.331.827 I llm_load_print_meta: pooling type     = -1
0.00.331.827 I llm_load_print_meta: rope type        = -1
0.00.331.827 I llm_load_print_meta: rope scaling     = linear
0.00.331.827 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.828 I llm_load_print_meta: freq_scale_train = 1
0.00.331.828 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.830 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.830 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.830 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.830 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.830 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.831 I llm_load_print_meta: model type       = 33M
0.00.331.832 I llm_load_print_meta: model ftype      = F16
0.00.331.833 I llm_load_print_meta: model params     = 32.90 M
0.00.331.833 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.833 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.834 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.835 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.840 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.840 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.843 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.843 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.844 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.844 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.844 I llm_load_print_meta: max token length = 45
0.00.333.039 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.039 I llm_load_tensors: offloading output layer to GPU
0.00.333.040 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.064 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.066 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.971 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.972 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.972 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.972 I llama_new_context_with_model: n_batch       = 2048
0.00.333.972 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.972 I llama_new_context_with_model: flash_attn    = 0
0.00.333.973 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.973 I llama_new_context_with_model: freq_scale    = 1
0.00.333.974 I ggml_metal_init: allocating
0.00.333.977 I ggml_metal_init: found device: Apple M4
0.00.333.979 I ggml_metal_init: picking default device: Apple M4
0.00.335.010 I ggml_metal_init: using embedded metal library
0.00.337.971 I ggml_metal_init: GPU name:   Apple M4
0.00.337.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.973 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.974 I ggml_metal_init: simdgroup reduction   = true
0.00.337.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.974 I ggml_metal_init: has bfloat            = true
0.00.337.974 I ggml_metal_init: use bfloat            = true
0.00.337.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.563 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.350.142 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.144 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.146 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.729 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.730 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.730 I llama_new_context_with_model: graph nodes  = 154
0.00.350.730 I llama_new_context_with_model: graph splits = 2
0.00.350.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.839 I 
0.00.363.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.154 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.364.155 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.364.158 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.364.158 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.364.160 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.364.160 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.675 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.368.361 I llama_perf_context_print:        load time =     341.30 ms
0.00.368.362 I llama_perf_context_print: prompt eval time =       3.68 ms /    62 tokens (    0.06 ms per token, 16856.99 tokens per second)
0.00.368.363 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.368.363 I llama_perf_context_print:       total time =       4.52 ms /    63 tokens
0.00.368.564 I ggml_metal_free: deallocating

real	0m1.190s
user	0m0.339s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.236 I main: llama backend init
0.00.000.242 I main: load the model and apply lora adapter, if any
0.00.031.150 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.237 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.273 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.927 I llama_model_loader: - type  f32:  194 tensors
0.00.063.927 I llama_model_loader: - type  f16:   98 tensors
0.00.094.764 I llm_load_vocab: special tokens cache size = 25
0.00.101.820 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.823 I llm_load_print_meta: arch             = gptneox
0.00.101.823 I llm_load_print_meta: vocab type       = BPE
0.00.101.823 I llm_load_print_meta: n_vocab          = 50304
0.00.101.824 I llm_load_print_meta: n_merges         = 50009
0.00.101.824 I llm_load_print_meta: vocab_only       = 0
0.00.101.824 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.824 I llm_load_print_meta: n_embd           = 2048
0.00.101.824 I llm_load_print_meta: n_layer          = 24
0.00.101.827 I llm_load_print_meta: n_head           = 16
0.00.101.828 I llm_load_print_meta: n_head_kv        = 16
0.00.101.828 I llm_load_print_meta: n_rot            = 32
0.00.101.828 I llm_load_print_meta: n_swa            = 0
0.00.101.828 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.829 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.829 I llm_load_print_meta: n_gqa            = 1
0.00.101.830 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.831 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.831 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.831 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.832 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.832 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.832 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.833 I llm_load_print_meta: n_ff             = 8192
0.00.101.833 I llm_load_print_meta: n_expert         = 0
0.00.101.833 I llm_load_print_meta: n_expert_used    = 0
0.00.101.833 I llm_load_print_meta: causal attn      = 1
0.00.101.833 I llm_load_print_meta: pooling type     = 0
0.00.101.833 I llm_load_print_meta: rope type        = 2
0.00.101.836 I llm_load_print_meta: rope scaling     = linear
0.00.101.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.836 I llm_load_print_meta: freq_scale_train = 1
0.00.101.836 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.837 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.837 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.837 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.837 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.837 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.837 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.838 I llm_load_print_meta: model type       = 1.4B
0.00.101.838 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.101.839 I llm_load_print_meta: model params     = 1.41 B
0.00.101.839 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.101.839 I llm_load_print_meta: general.name     = 1.4B
0.00.101.840 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.840 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.840 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.840 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.840 I llm_load_print_meta: LF token         = 128 ''
0.00.101.841 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.841 I llm_load_print_meta: max token length = 1024
0.00.104.514 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.104.514 I llm_load_tensors: offloading output layer to GPU
0.00.104.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.533 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.534 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.497 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.498 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.499 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.499 I llama_new_context_with_model: n_batch       = 2048
0.00.105.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.499 I llama_new_context_with_model: flash_attn    = 0
0.00.105.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.500 I llama_new_context_with_model: freq_scale    = 1
0.00.105.500 I ggml_metal_init: allocating
0.00.105.509 I ggml_metal_init: found device: Apple M4
0.00.105.514 I ggml_metal_init: picking default device: Apple M4
0.00.106.193 I ggml_metal_init: using embedded metal library
0.00.116.290 I ggml_metal_init: GPU name:   Apple M4
0.00.116.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.293 I ggml_metal_init: simdgroup reduction   = true
0.00.116.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.294 I ggml_metal_init: has bfloat            = true
0.00.116.294 I ggml_metal_init: use bfloat            = true
0.00.116.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.456 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.160.258 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.263 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.282 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.230 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.232 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.232 I llama_new_context_with_model: graph nodes  = 967
0.00.161.232 I llama_new_context_with_model: graph splits = 2
0.00.161.257 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.229 I main: llama threadpool init, n_threads = 4
0.00.243.263 I 
0.00.243.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.296 I 
0.00.243.368 I sampler seed: 1234
0.00.243.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.406 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.407 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.407 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.087.990 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.02.087.990 I llama_perf_context_print:        load time =     212.07 ms
0.02.087.991 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.58 tokens per second)
0.02.087.992 I llama_perf_context_print:        eval time =    1797.79 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.087.992 I llama_perf_context_print:       total time =    1844.76 ms /    70 tokens
0.02.088.189 I ggml_metal_free: deallocating

real	0m2.458s
user	0m0.147s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.632 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.473 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.443 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.449 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.451 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.452 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.457 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.460 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.461 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.740 I llama_model_loader: - type  f32:  194 tensors
0.00.051.740 I llama_model_loader: - type  f16:   98 tensors
0.00.078.967 I llm_load_vocab: special tokens cache size = 25
0.00.085.207 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.209 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.209 I llm_load_print_meta: arch             = gptneox
0.00.085.210 I llm_load_print_meta: vocab type       = BPE
0.00.085.210 I llm_load_print_meta: n_vocab          = 50304
0.00.085.210 I llm_load_print_meta: n_merges         = 50009
0.00.085.210 I llm_load_print_meta: vocab_only       = 0
0.00.085.211 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.211 I llm_load_print_meta: n_embd           = 2048
0.00.085.211 I llm_load_print_meta: n_layer          = 24
0.00.085.213 I llm_load_print_meta: n_head           = 16
0.00.085.214 I llm_load_print_meta: n_head_kv        = 16
0.00.085.214 I llm_load_print_meta: n_rot            = 32
0.00.085.215 I llm_load_print_meta: n_swa            = 0
0.00.085.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.217 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.217 I llm_load_print_meta: n_gqa            = 1
0.00.085.218 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.219 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.219 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.219 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.220 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.220 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.222 I llm_load_print_meta: n_ff             = 8192
0.00.085.222 I llm_load_print_meta: n_expert         = 0
0.00.085.222 I llm_load_print_meta: n_expert_used    = 0
0.00.085.222 I llm_load_print_meta: causal attn      = 1
0.00.085.222 I llm_load_print_meta: pooling type     = 0
0.00.085.222 I llm_load_print_meta: rope type        = 2
0.00.085.222 I llm_load_print_meta: rope scaling     = linear
0.00.085.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.223 I llm_load_print_meta: freq_scale_train = 1
0.00.085.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.223 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.223 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.224 I llm_load_print_meta: model type       = 1.4B
0.00.085.224 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.225 I llm_load_print_meta: model params     = 1.41 B
0.00.085.226 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.226 I llm_load_print_meta: general.name     = 1.4B
0.00.085.227 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.227 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.227 I llm_load_print_meta: LF token         = 128 ''
0.00.085.227 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.228 I llm_load_print_meta: max token length = 1024
0.00.087.159 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.160 I llm_load_tensors: offloading output layer to GPU
0.00.087.160 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.166 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.166 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.072 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.073 I llama_new_context_with_model: n_ctx         = 128
0.00.088.073 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.074 I llama_new_context_with_model: n_batch       = 128
0.00.088.074 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.074 I llama_new_context_with_model: flash_attn    = 0
0.00.088.075 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.075 I llama_new_context_with_model: freq_scale    = 1
0.00.088.075 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.076 I ggml_metal_init: allocating
0.00.088.081 I ggml_metal_init: found device: Apple M4
0.00.088.083 I ggml_metal_init: picking default device: Apple M4
0.00.088.671 I ggml_metal_init: using embedded metal library
0.00.091.160 I ggml_metal_init: GPU name:   Apple M4
0.00.091.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.163 I ggml_metal_init: simdgroup reduction   = true
0.00.091.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.163 I ggml_metal_init: has bfloat            = true
0.00.091.164 I ggml_metal_init: use bfloat            = true
0.00.091.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.508 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.771 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.773 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.786 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.546 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.547 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.547 I llama_new_context_with_model: graph nodes  = 967
0.00.102.547 I llama_new_context_with_model: graph splits = 2
0.00.102.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.034.265 I 
0.01.034.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.034.357 I perplexity: tokenizing the input ..
0.01.046.336 I perplexity: tokenization took 11.975 ms
0.01.046.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.166.157 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.167.652 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.167.678 I llama_perf_context_print:        load time =    1011.78 ms
0.01.167.680 I llama_perf_context_print: prompt eval time =     119.44 ms /   128 tokens (    0.93 ms per token,  1071.71 tokens per second)
0.01.167.681 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.167.681 I llama_perf_context_print:       total time =     133.42 ms /   129 tokens
0.01.168.188 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.115s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.861 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.254 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.433 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.434 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.434 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.435 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.436 I llama_model_loader: - type  f32:  194 tensors
0.00.031.436 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.090 I llm_load_vocab: special tokens cache size = 25
0.00.059.048 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.052 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.052 I llm_load_print_meta: arch             = gptneox
0.00.059.053 I llm_load_print_meta: vocab type       = BPE
0.00.059.053 I llm_load_print_meta: n_vocab          = 50304
0.00.059.053 I llm_load_print_meta: n_merges         = 50009
0.00.059.053 I llm_load_print_meta: vocab_only       = 0
0.00.059.053 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.056 I llm_load_print_meta: n_embd           = 2048
0.00.059.056 I llm_load_print_meta: n_layer          = 24
0.00.059.062 I llm_load_print_meta: n_head           = 16
0.00.059.063 I llm_load_print_meta: n_head_kv        = 16
0.00.059.067 I llm_load_print_meta: n_rot            = 32
0.00.059.069 I llm_load_print_meta: n_swa            = 0
0.00.059.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.069 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.070 I llm_load_print_meta: n_gqa            = 1
0.00.059.071 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.072 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.073 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.074 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.074 I llm_load_print_meta: n_ff             = 8192
0.00.059.074 I llm_load_print_meta: n_expert         = 0
0.00.059.075 I llm_load_print_meta: n_expert_used    = 0
0.00.059.076 I llm_load_print_meta: causal attn      = 1
0.00.059.076 I llm_load_print_meta: pooling type     = 0
0.00.059.076 I llm_load_print_meta: rope type        = 2
0.00.059.076 I llm_load_print_meta: rope scaling     = linear
0.00.059.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.077 I llm_load_print_meta: freq_scale_train = 1
0.00.059.077 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.077 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.077 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.077 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.077 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.077 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.078 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.078 I llm_load_print_meta: model type       = 1.4B
0.00.059.078 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.079 I llm_load_print_meta: model params     = 1.41 B
0.00.059.079 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.079 I llm_load_print_meta: general.name     = 1.4B
0.00.059.079 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.080 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.080 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.080 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.080 I llm_load_print_meta: LF token         = 128 ''
0.00.059.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.080 I llm_load_print_meta: max token length = 1024
0.00.061.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.560 I llm_load_tensors: offloading output layer to GPU
0.00.061.560 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.572 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.573 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.592 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.593 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.593 I llama_new_context_with_model: n_batch       = 2048
0.00.062.593 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.593 I llama_new_context_with_model: flash_attn    = 0
0.00.062.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.594 I llama_new_context_with_model: freq_scale    = 1
0.00.062.594 I ggml_metal_init: allocating
0.00.062.598 I ggml_metal_init: found device: Apple M4
0.00.062.600 I ggml_metal_init: picking default device: Apple M4
0.00.063.335 I ggml_metal_init: using embedded metal library
0.00.065.944 I ggml_metal_init: GPU name:   Apple M4
0.00.065.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.946 I ggml_metal_init: simdgroup reduction   = true
0.00.065.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.947 I ggml_metal_init: has bfloat            = true
0.00.065.947 I ggml_metal_init: use bfloat            = true
0.00.065.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.100.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.752 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.779 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.796 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.798 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.798 I llama_new_context_with_model: graph nodes  = 967
0.00.101.798 I llama_new_context_with_model: graph splits = 2
0.00.101.811 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.348.037 I main: llama threadpool init, n_threads = 4
0.01.348.072 I 
0.01.348.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.348.105 I 
0.01.348.327 I sampler seed: 1234
0.01.348.332 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.348.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.348.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.348.348 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.437.129 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.02.437.130 I llama_perf_context_print:        load time =    1338.17 ms
0.02.437.131 I llama_perf_context_print: prompt eval time =      42.80 ms /     7 tokens (    6.11 ms per token,   163.54 tokens per second)
0.02.437.132 I llama_perf_context_print:        eval time =    1043.04 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.437.132 I llama_perf_context_print:       total time =    1089.10 ms /    70 tokens
0.02.437.332 I ggml_metal_free: deallocating

real	0m2.455s
user	0m0.113s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.720 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.798 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.799 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.809 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.810 I llama_model_loader: - type  f32:  194 tensors
0.00.025.810 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.750 I llm_load_vocab: special tokens cache size = 25
0.00.053.981 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.985 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.986 I llm_load_print_meta: arch             = gptneox
0.00.053.986 I llm_load_print_meta: vocab type       = BPE
0.00.053.986 I llm_load_print_meta: n_vocab          = 50304
0.00.053.986 I llm_load_print_meta: n_merges         = 50009
0.00.053.987 I llm_load_print_meta: vocab_only       = 0
0.00.053.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.989 I llm_load_print_meta: n_embd           = 2048
0.00.053.989 I llm_load_print_meta: n_layer          = 24
0.00.053.994 I llm_load_print_meta: n_head           = 16
0.00.053.995 I llm_load_print_meta: n_head_kv        = 16
0.00.053.995 I llm_load_print_meta: n_rot            = 32
0.00.053.995 I llm_load_print_meta: n_swa            = 0
0.00.053.995 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.995 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.996 I llm_load_print_meta: n_gqa            = 1
0.00.053.997 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.997 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.998 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.998 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.998 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.998 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.998 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.999 I llm_load_print_meta: n_ff             = 8192
0.00.053.999 I llm_load_print_meta: n_expert         = 0
0.00.053.999 I llm_load_print_meta: n_expert_used    = 0
0.00.053.999 I llm_load_print_meta: causal attn      = 1
0.00.053.999 I llm_load_print_meta: pooling type     = 0
0.00.054.000 I llm_load_print_meta: rope type        = 2
0.00.054.000 I llm_load_print_meta: rope scaling     = linear
0.00.054.000 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.002 I llm_load_print_meta: freq_scale_train = 1
0.00.054.002 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.002 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.002 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.002 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.003 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.003 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.003 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.003 I llm_load_print_meta: model type       = 1.4B
0.00.054.003 I llm_load_print_meta: model ftype      = Q8_0
0.00.054.004 I llm_load_print_meta: model params     = 1.41 B
0.00.054.004 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.054.004 I llm_load_print_meta: general.name     = 1.4B
0.00.054.006 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.006 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.006 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.006 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.006 I llm_load_print_meta: LF token         = 128 ''
0.00.054.007 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.007 I llm_load_print_meta: max token length = 1024
0.00.056.141 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.142 I llm_load_tensors: offloading output layer to GPU
0.00.056.142 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.153 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.155 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.057.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.099 I llama_new_context_with_model: n_ctx         = 128
0.00.057.099 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.099 I llama_new_context_with_model: n_batch       = 128
0.00.057.100 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.100 I llama_new_context_with_model: flash_attn    = 0
0.00.057.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.100 I llama_new_context_with_model: freq_scale    = 1
0.00.057.101 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.101 I ggml_metal_init: allocating
0.00.057.106 I ggml_metal_init: found device: Apple M4
0.00.057.110 I ggml_metal_init: picking default device: Apple M4
0.00.057.759 I ggml_metal_init: using embedded metal library
0.00.060.229 I ggml_metal_init: GPU name:   Apple M4
0.00.060.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.232 I ggml_metal_init: simdgroup reduction   = true
0.00.060.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.232 I ggml_metal_init: has bfloat            = true
0.00.060.232 I ggml_metal_init: use bfloat            = true
0.00.060.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.235 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.717 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.071.088 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.092 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.105 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.996 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.997 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.997 I llama_new_context_with_model: graph nodes  = 967
0.00.071.997 I llama_new_context_with_model: graph splits = 2
0.00.072.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.959 I 
0.00.822.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.997 I perplexity: tokenizing the input ..
0.00.831.412 I perplexity: tokenization took 8.414 ms
0.00.831.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.954.493 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.955.797 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.955.810 I llama_perf_context_print:        load time =     812.23 ms
0.00.955.811 I llama_perf_context_print: prompt eval time =     122.83 ms /   128 tokens (    0.96 ms per token,  1042.10 tokens per second)
0.00.955.812 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.955.812 I llama_perf_context_print:       total time =     132.85 ms /   129 tokens
0.00.956.147 I ggml_metal_free: deallocating

real	0m0.972s
user	0m0.082s
sys	0m0.145s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.016.624 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.829 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.653 I llama_model_loader: - type  f32:  194 tensors
0.00.048.654 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.081.870 I llm_load_vocab: special tokens cache size = 25
0.00.092.440 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.443 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.444 I llm_load_print_meta: arch             = gptneox
0.00.092.444 I llm_load_print_meta: vocab type       = BPE
0.00.092.445 I llm_load_print_meta: n_vocab          = 50304
0.00.092.445 I llm_load_print_meta: n_merges         = 50009
0.00.092.445 I llm_load_print_meta: vocab_only       = 0
0.00.092.445 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.445 I llm_load_print_meta: n_embd           = 2048
0.00.092.446 I llm_load_print_meta: n_layer          = 24
0.00.092.450 I llm_load_print_meta: n_head           = 16
0.00.092.451 I llm_load_print_meta: n_head_kv        = 16
0.00.092.451 I llm_load_print_meta: n_rot            = 32
0.00.092.451 I llm_load_print_meta: n_swa            = 0
0.00.092.451 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.452 I llm_load_print_meta: n_gqa            = 1
0.00.092.454 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.456 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.456 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.456 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.457 I llm_load_print_meta: n_ff             = 8192
0.00.092.460 I llm_load_print_meta: n_expert         = 0
0.00.092.460 I llm_load_print_meta: n_expert_used    = 0
0.00.092.460 I llm_load_print_meta: causal attn      = 1
0.00.092.460 I llm_load_print_meta: pooling type     = 0
0.00.092.460 I llm_load_print_meta: rope type        = 2
0.00.092.461 I llm_load_print_meta: rope scaling     = linear
0.00.092.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.462 I llm_load_print_meta: freq_scale_train = 1
0.00.092.462 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.462 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.462 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.463 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.465 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.465 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.465 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.465 I llm_load_print_meta: model type       = 1.4B
0.00.092.466 I llm_load_print_meta: model ftype      = Q4_0
0.00.092.466 I llm_load_print_meta: model params     = 1.41 B
0.00.092.467 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.092.467 I llm_load_print_meta: general.name     = 1.4B
0.00.092.468 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.468 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.468 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.468 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.469 I llm_load_print_meta: LF token         = 128 ''
0.00.092.469 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.469 I llm_load_print_meta: max token length = 1024
0.00.095.418 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.419 I llm_load_tensors: offloading output layer to GPU
0.00.095.419 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.437 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.095.439 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.096.722 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.723 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.723 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.723 I llama_new_context_with_model: n_batch       = 2048
0.00.096.724 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.724 I llama_new_context_with_model: flash_attn    = 0
0.00.096.724 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.725 I llama_new_context_with_model: freq_scale    = 1
0.00.096.725 I ggml_metal_init: allocating
0.00.096.733 I ggml_metal_init: found device: Apple M4
0.00.096.738 I ggml_metal_init: picking default device: Apple M4
0.00.097.628 I ggml_metal_init: using embedded metal library
0.00.101.264 I ggml_metal_init: GPU name:   Apple M4
0.00.101.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.268 I ggml_metal_init: simdgroup reduction   = true
0.00.101.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.268 I ggml_metal_init: has bfloat            = true
0.00.101.268 I ggml_metal_init: use bfloat            = true
0.00.101.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.106 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.136.934 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.941 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.968 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.138.046 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.138.048 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.138.048 I llama_new_context_with_model: graph nodes  = 967
0.00.138.048 I llama_new_context_with_model: graph splits = 2
0.00.138.068 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.138.208 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.249 I main: llama threadpool init, n_threads = 4
0.00.760.368 I 
0.00.760.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.449 I 
0.00.760.847 I sampler seed: 1234
0.00.760.853 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.887 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.889 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.440.401 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49203.05 tokens per second)
0.01.440.402 I llama_perf_context_print:        load time =     743.61 ms
0.01.440.403 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.60 tokens per second)
0.01.440.404 I llama_perf_context_print:        eval time =     630.00 ms /    63 runs   (   10.00 ms per token,   100.00 tokens per second)
0.01.440.405 I llama_perf_context_print:       total time =     680.16 ms /    70 tokens
0.01.440.624 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.139s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.082 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.131 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.100 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.100 I llama_model_loader: - type  f32:  194 tensors
0.00.024.101 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.101 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.953 I llm_load_vocab: special tokens cache size = 25
0.00.052.195 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.200 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.200 I llm_load_print_meta: arch             = gptneox
0.00.052.201 I llm_load_print_meta: vocab type       = BPE
0.00.052.201 I llm_load_print_meta: n_vocab          = 50304
0.00.052.201 I llm_load_print_meta: n_merges         = 50009
0.00.052.201 I llm_load_print_meta: vocab_only       = 0
0.00.052.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.202 I llm_load_print_meta: n_embd           = 2048
0.00.052.202 I llm_load_print_meta: n_layer          = 24
0.00.052.206 I llm_load_print_meta: n_head           = 16
0.00.052.206 I llm_load_print_meta: n_head_kv        = 16
0.00.052.207 I llm_load_print_meta: n_rot            = 32
0.00.052.207 I llm_load_print_meta: n_swa            = 0
0.00.052.207 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.207 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.208 I llm_load_print_meta: n_gqa            = 1
0.00.052.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.209 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.213 I llm_load_print_meta: n_ff             = 8192
0.00.052.214 I llm_load_print_meta: n_expert         = 0
0.00.052.214 I llm_load_print_meta: n_expert_used    = 0
0.00.052.214 I llm_load_print_meta: causal attn      = 1
0.00.052.214 I llm_load_print_meta: pooling type     = 0
0.00.052.214 I llm_load_print_meta: rope type        = 2
0.00.052.214 I llm_load_print_meta: rope scaling     = linear
0.00.052.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.215 I llm_load_print_meta: freq_scale_train = 1
0.00.052.215 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.215 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.215 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.216 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.216 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.216 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.216 I llm_load_print_meta: model type       = 1.4B
0.00.052.216 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.221 I llm_load_print_meta: model params     = 1.41 B
0.00.052.221 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.221 I llm_load_print_meta: general.name     = 1.4B
0.00.052.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.226 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.226 I llm_load_print_meta: LF token         = 128 ''
0.00.052.226 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.227 I llm_load_print_meta: max token length = 1024
0.00.053.857 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.859 I llm_load_tensors: offloading output layer to GPU
0.00.053.859 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.869 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.870 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.779 I llama_new_context_with_model: n_ctx         = 128
0.00.054.779 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.779 I llama_new_context_with_model: n_batch       = 128
0.00.054.780 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.780 I llama_new_context_with_model: flash_attn    = 0
0.00.054.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.782 I llama_new_context_with_model: freq_scale    = 1
0.00.054.782 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.783 I ggml_metal_init: allocating
0.00.054.787 I ggml_metal_init: found device: Apple M4
0.00.054.789 I ggml_metal_init: picking default device: Apple M4
0.00.055.460 I ggml_metal_init: using embedded metal library
0.00.058.087 I ggml_metal_init: GPU name:   Apple M4
0.00.058.089 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.090 I ggml_metal_init: simdgroup reduction   = true
0.00.058.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.090 I ggml_metal_init: has bfloat            = true
0.00.058.090 I ggml_metal_init: use bfloat            = true
0.00.058.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.703 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.033 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.986 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.987 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.987 I llama_new_context_with_model: graph nodes  = 967
0.00.069.987 I llama_new_context_with_model: graph splits = 2
0.00.070.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.878 I 
0.00.600.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.940 I perplexity: tokenizing the input ..
0.00.608.652 I perplexity: tokenization took 7.711 ms
0.00.608.656 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.712 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.731.856 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.731.870 I llama_perf_context_print:        load time =     591.79 ms
0.00.731.871 I llama_perf_context_print: prompt eval time =     121.83 ms /   128 tokens (    0.95 ms per token,  1050.64 tokens per second)
0.00.731.871 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.872 I llama_perf_context_print:       total time =     131.00 ms /   129 tokens
0.00.732.251 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.081s
sys	0m0.105s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.699 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.701 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.778 I llama_model_loader: - type  f32:  194 tensors
0.00.033.778 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.587 I llm_load_vocab: special tokens cache size = 25
0.00.061.624 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.627 I llm_load_print_meta: arch             = gptneox
0.00.061.627 I llm_load_print_meta: vocab type       = BPE
0.00.061.627 I llm_load_print_meta: n_vocab          = 50304
0.00.061.628 I llm_load_print_meta: n_merges         = 50009
0.00.061.628 I llm_load_print_meta: vocab_only       = 0
0.00.061.628 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.628 I llm_load_print_meta: n_embd           = 2048
0.00.061.628 I llm_load_print_meta: n_layer          = 24
0.00.061.631 I llm_load_print_meta: n_head           = 16
0.00.061.632 I llm_load_print_meta: n_head_kv        = 16
0.00.061.632 I llm_load_print_meta: n_rot            = 32
0.00.061.632 I llm_load_print_meta: n_swa            = 0
0.00.061.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.633 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.633 I llm_load_print_meta: n_gqa            = 1
0.00.061.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.635 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.636 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.636 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.637 I llm_load_print_meta: n_ff             = 8192
0.00.061.638 I llm_load_print_meta: n_expert         = 0
0.00.061.638 I llm_load_print_meta: n_expert_used    = 0
0.00.061.642 I llm_load_print_meta: causal attn      = 1
0.00.061.643 I llm_load_print_meta: pooling type     = 0
0.00.061.643 I llm_load_print_meta: rope type        = 2
0.00.061.643 I llm_load_print_meta: rope scaling     = linear
0.00.061.644 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.644 I llm_load_print_meta: freq_scale_train = 1
0.00.061.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.645 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.645 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.645 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.645 I llm_load_print_meta: model type       = 1.4B
0.00.061.649 I llm_load_print_meta: model ftype      = Q4_1
0.00.061.649 I llm_load_print_meta: model params     = 1.41 B
0.00.061.650 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.061.650 I llm_load_print_meta: general.name     = 1.4B
0.00.061.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.650 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: LF token         = 128 ''
0.00.061.653 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.653 I llm_load_print_meta: max token length = 1024
0.00.063.744 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.745 I llm_load_tensors: offloading output layer to GPU
0.00.063.745 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.755 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.063.756 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.064.665 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.666 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.666 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.667 I llama_new_context_with_model: n_batch       = 2048
0.00.064.667 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.667 I llama_new_context_with_model: flash_attn    = 0
0.00.064.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.668 I llama_new_context_with_model: freq_scale    = 1
0.00.064.668 I ggml_metal_init: allocating
0.00.064.678 I ggml_metal_init: found device: Apple M4
0.00.064.681 I ggml_metal_init: picking default device: Apple M4
0.00.065.324 I ggml_metal_init: using embedded metal library
0.00.067.703 I ggml_metal_init: GPU name:   Apple M4
0.00.067.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.705 I ggml_metal_init: simdgroup reduction   = true
0.00.067.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.706 I ggml_metal_init: has bfloat            = true
0.00.067.707 I ggml_metal_init: use bfloat            = true
0.00.067.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.196 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.098.746 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.751 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.768 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.768 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.769 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.769 I llama_new_context_with_model: graph nodes  = 967
0.00.099.770 I llama_new_context_with_model: graph splits = 2
0.00.099.785 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.871 I main: llama threadpool init, n_threads = 4
0.00.936.965 I 
0.00.937.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.937.077 I 
0.00.937.612 I sampler seed: 1234
0.00.937.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.937.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.937.665 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.937.665 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.682.322 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.01.682.323 I llama_perf_context_print:        load time =     927.91 ms
0.01.682.324 I llama_perf_context_print: prompt eval time =      50.36 ms /     7 tokens (    7.19 ms per token,   138.99 tokens per second)
0.01.682.324 I llama_perf_context_print:        eval time =     691.29 ms /    63 runs   (   10.97 ms per token,    91.13 tokens per second)
0.01.682.326 I llama_perf_context_print:       total time =     745.46 ms /    70 tokens
0.01.682.525 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.125s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.629 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.645 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.646 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.459 I llama_model_loader: - type  f32:  194 tensors
0.00.023.459 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.460 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.717 I llm_load_vocab: special tokens cache size = 25
0.00.050.753 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.756 I llm_load_print_meta: arch             = gptneox
0.00.050.756 I llm_load_print_meta: vocab type       = BPE
0.00.050.756 I llm_load_print_meta: n_vocab          = 50304
0.00.050.756 I llm_load_print_meta: n_merges         = 50009
0.00.050.757 I llm_load_print_meta: vocab_only       = 0
0.00.050.757 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.757 I llm_load_print_meta: n_embd           = 2048
0.00.050.757 I llm_load_print_meta: n_layer          = 24
0.00.050.760 I llm_load_print_meta: n_head           = 16
0.00.050.761 I llm_load_print_meta: n_head_kv        = 16
0.00.050.761 I llm_load_print_meta: n_rot            = 32
0.00.050.764 I llm_load_print_meta: n_swa            = 0
0.00.050.764 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.764 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.765 I llm_load_print_meta: n_gqa            = 1
0.00.050.766 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.766 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.767 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.767 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.767 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.768 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.768 I llm_load_print_meta: n_ff             = 8192
0.00.050.769 I llm_load_print_meta: n_expert         = 0
0.00.050.769 I llm_load_print_meta: n_expert_used    = 0
0.00.050.769 I llm_load_print_meta: causal attn      = 1
0.00.050.769 I llm_load_print_meta: pooling type     = 0
0.00.050.769 I llm_load_print_meta: rope type        = 2
0.00.050.778 I llm_load_print_meta: rope scaling     = linear
0.00.050.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.780 I llm_load_print_meta: freq_scale_train = 1
0.00.050.780 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.780 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.780 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.781 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.781 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.781 I llm_load_print_meta: model type       = 1.4B
0.00.050.781 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.782 I llm_load_print_meta: model params     = 1.41 B
0.00.050.782 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.782 I llm_load_print_meta: general.name     = 1.4B
0.00.050.784 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: LF token         = 128 ''
0.00.050.786 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.787 I llm_load_print_meta: max token length = 1024
0.00.052.796 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.797 I llm_load_tensors: offloading output layer to GPU
0.00.052.797 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.808 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.809 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.696 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.697 I llama_new_context_with_model: n_ctx         = 128
0.00.053.697 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.697 I llama_new_context_with_model: n_batch       = 128
0.00.053.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.698 I llama_new_context_with_model: flash_attn    = 0
0.00.053.698 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.698 I llama_new_context_with_model: freq_scale    = 1
0.00.053.699 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.699 I ggml_metal_init: allocating
0.00.053.703 I ggml_metal_init: found device: Apple M4
0.00.053.705 I ggml_metal_init: picking default device: Apple M4
0.00.054.273 I ggml_metal_init: using embedded metal library
0.00.056.657 I ggml_metal_init: GPU name:   Apple M4
0.00.056.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.659 I ggml_metal_init: simdgroup reduction   = true
0.00.056.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.659 I ggml_metal_init: has bfloat            = true
0.00.056.660 I ggml_metal_init: use bfloat            = true
0.00.056.660 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.811 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.062 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.079 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.980 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.981 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.981 I llama_new_context_with_model: graph nodes  = 967
0.00.068.981 I llama_new_context_with_model: graph splits = 2
0.00.068.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.053 I 
0.00.675.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.096 I perplexity: tokenizing the input ..
0.00.683.454 I perplexity: tokenization took 8.356 ms
0.00.683.462 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.129 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.808.287 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.808.304 I llama_perf_context_print:        load time =     666.43 ms
0.00.808.305 I llama_perf_context_print: prompt eval time =     123.44 ms /   128 tokens (    0.96 ms per token,  1036.92 tokens per second)
0.00.808.308 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.308 I llama_perf_context_print:       total time =     133.25 ms /   129 tokens
0.00.808.757 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.080s
sys	0m0.113s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.014.288 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.042.383 I llama_model_loader: - type  f32:  194 tensors
0.00.042.384 I llama_model_loader: - type q5_0:   97 tensors
0.00.042.384 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.834 I llm_load_vocab: special tokens cache size = 25
0.00.077.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.211 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.212 I llm_load_print_meta: arch             = gptneox
0.00.077.212 I llm_load_print_meta: vocab type       = BPE
0.00.077.212 I llm_load_print_meta: n_vocab          = 50304
0.00.077.213 I llm_load_print_meta: n_merges         = 50009
0.00.077.213 I llm_load_print_meta: vocab_only       = 0
0.00.077.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.213 I llm_load_print_meta: n_embd           = 2048
0.00.077.213 I llm_load_print_meta: n_layer          = 24
0.00.077.217 I llm_load_print_meta: n_head           = 16
0.00.077.218 I llm_load_print_meta: n_head_kv        = 16
0.00.077.220 I llm_load_print_meta: n_rot            = 32
0.00.077.220 I llm_load_print_meta: n_swa            = 0
0.00.077.220 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.220 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.221 I llm_load_print_meta: n_gqa            = 1
0.00.077.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.225 I llm_load_print_meta: n_ff             = 8192
0.00.077.226 I llm_load_print_meta: n_expert         = 0
0.00.077.226 I llm_load_print_meta: n_expert_used    = 0
0.00.077.228 I llm_load_print_meta: causal attn      = 1
0.00.077.229 I llm_load_print_meta: pooling type     = 0
0.00.077.229 I llm_load_print_meta: rope type        = 2
0.00.077.229 I llm_load_print_meta: rope scaling     = linear
0.00.077.230 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.230 I llm_load_print_meta: freq_scale_train = 1
0.00.077.230 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.230 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.230 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.231 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.231 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.231 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.231 I llm_load_print_meta: model type       = 1.4B
0.00.077.237 I llm_load_print_meta: model ftype      = Q5_0
0.00.077.238 I llm_load_print_meta: model params     = 1.41 B
0.00.077.239 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.077.239 I llm_load_print_meta: general.name     = 1.4B
0.00.077.239 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.240 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.240 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.242 I llm_load_print_meta: LF token         = 128 ''
0.00.077.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.243 I llm_load_print_meta: max token length = 1024
0.00.079.848 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.849 I llm_load_tensors: offloading output layer to GPU
0.00.079.850 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.861 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.079.863 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.081.296 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.298 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.298 I llama_new_context_with_model: n_batch       = 2048
0.00.081.299 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.299 I llama_new_context_with_model: flash_attn    = 0
0.00.081.300 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.300 I llama_new_context_with_model: freq_scale    = 1
0.00.081.301 I ggml_metal_init: allocating
0.00.081.308 I ggml_metal_init: found device: Apple M4
0.00.081.312 I ggml_metal_init: picking default device: Apple M4
0.00.082.230 I ggml_metal_init: using embedded metal library
0.00.086.237 I ggml_metal_init: GPU name:   Apple M4
0.00.086.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.241 I ggml_metal_init: simdgroup reduction   = true
0.00.086.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.242 I ggml_metal_init: has bfloat            = true
0.00.086.242 I ggml_metal_init: use bfloat            = true
0.00.086.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.245 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.914 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.120.528 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.539 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.563 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.592 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.593 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.594 I llama_new_context_with_model: graph nodes  = 967
0.00.121.594 I llama_new_context_with_model: graph splits = 2
0.00.121.610 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.355 I main: llama threadpool init, n_threads = 4
0.00.863.395 I 
0.00.863.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.426 I 
0.00.863.655 I sampler seed: 1234
0.00.863.662 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.678 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.679 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.680 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.659.890 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.659.891 I llama_perf_context_print:        load time =     849.06 ms
0.01.659.892 I llama_perf_context_print: prompt eval time =      47.54 ms /     7 tokens (    6.79 ms per token,   147.25 tokens per second)
0.01.659.892 I llama_perf_context_print:        eval time =     745.69 ms /    63 runs   (   11.84 ms per token,    84.49 tokens per second)
0.01.659.893 I llama_perf_context_print:       total time =     796.54 ms /    70 tokens
0.01.660.120 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.127s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.120 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.012 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.013 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.014 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.017 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.892 I llama_model_loader: - type  f32:  194 tensors
0.00.024.892 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.399 I llm_load_vocab: special tokens cache size = 25
0.00.051.443 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.445 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.446 I llm_load_print_meta: arch             = gptneox
0.00.051.446 I llm_load_print_meta: vocab type       = BPE
0.00.051.446 I llm_load_print_meta: n_vocab          = 50304
0.00.051.446 I llm_load_print_meta: n_merges         = 50009
0.00.051.446 I llm_load_print_meta: vocab_only       = 0
0.00.051.447 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.447 I llm_load_print_meta: n_embd           = 2048
0.00.051.447 I llm_load_print_meta: n_layer          = 24
0.00.051.449 I llm_load_print_meta: n_head           = 16
0.00.051.450 I llm_load_print_meta: n_head_kv        = 16
0.00.051.450 I llm_load_print_meta: n_rot            = 32
0.00.051.450 I llm_load_print_meta: n_swa            = 0
0.00.051.450 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.452 I llm_load_print_meta: n_gqa            = 1
0.00.051.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.454 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.456 I llm_load_print_meta: n_ff             = 8192
0.00.051.456 I llm_load_print_meta: n_expert         = 0
0.00.051.456 I llm_load_print_meta: n_expert_used    = 0
0.00.051.457 I llm_load_print_meta: causal attn      = 1
0.00.051.457 I llm_load_print_meta: pooling type     = 0
0.00.051.457 I llm_load_print_meta: rope type        = 2
0.00.051.457 I llm_load_print_meta: rope scaling     = linear
0.00.051.458 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.458 I llm_load_print_meta: freq_scale_train = 1
0.00.051.458 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.458 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.458 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.459 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.459 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.459 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.461 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.461 I llm_load_print_meta: model type       = 1.4B
0.00.051.461 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.461 I llm_load_print_meta: model params     = 1.41 B
0.00.051.462 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.462 I llm_load_print_meta: general.name     = 1.4B
0.00.051.462 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.463 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.463 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.463 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.467 I llm_load_print_meta: LF token         = 128 ''
0.00.051.467 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.467 I llm_load_print_meta: max token length = 1024
0.00.053.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.440 I llm_load_tensors: offloading output layer to GPU
0.00.053.441 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.451 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.452 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.332 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.333 I llama_new_context_with_model: n_ctx         = 128
0.00.054.333 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.333 I llama_new_context_with_model: n_batch       = 128
0.00.054.334 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.334 I llama_new_context_with_model: flash_attn    = 0
0.00.054.334 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.334 I llama_new_context_with_model: freq_scale    = 1
0.00.054.335 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.335 I ggml_metal_init: allocating
0.00.054.338 I ggml_metal_init: found device: Apple M4
0.00.054.340 I ggml_metal_init: picking default device: Apple M4
0.00.054.913 I ggml_metal_init: using embedded metal library
0.00.057.270 I ggml_metal_init: GPU name:   Apple M4
0.00.057.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.273 I ggml_metal_init: simdgroup reduction   = true
0.00.057.273 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.273 I ggml_metal_init: has bfloat            = true
0.00.057.273 I ggml_metal_init: use bfloat            = true
0.00.057.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.078 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.318 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.321 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.290 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.291 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.292 I llama_new_context_with_model: graph nodes  = 967
0.00.069.292 I llama_new_context_with_model: graph splits = 2
0.00.069.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.395 I 
0.00.683.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.439 I perplexity: tokenizing the input ..
0.00.690.912 I perplexity: tokenization took 7.471 ms
0.00.690.917 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.067 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.827.252 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.827.268 I llama_perf_context_print:        load time =     673.27 ms
0.00.827.269 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.72 tokens per second)
0.00.827.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.270 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.827.712 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.104s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.133 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.171 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.171 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.270 I llama_model_loader: - type  f32:  194 tensors
0.00.027.270 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.270 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.390 I llm_load_vocab: special tokens cache size = 25
0.00.054.386 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.389 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.390 I llm_load_print_meta: arch             = gptneox
0.00.054.390 I llm_load_print_meta: vocab type       = BPE
0.00.054.390 I llm_load_print_meta: n_vocab          = 50304
0.00.054.391 I llm_load_print_meta: n_merges         = 50009
0.00.054.391 I llm_load_print_meta: vocab_only       = 0
0.00.054.391 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.391 I llm_load_print_meta: n_embd           = 2048
0.00.054.391 I llm_load_print_meta: n_layer          = 24
0.00.054.394 I llm_load_print_meta: n_head           = 16
0.00.054.395 I llm_load_print_meta: n_head_kv        = 16
0.00.054.395 I llm_load_print_meta: n_rot            = 32
0.00.054.395 I llm_load_print_meta: n_swa            = 0
0.00.054.395 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.396 I llm_load_print_meta: n_gqa            = 1
0.00.054.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.398 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.402 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.402 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.402 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.403 I llm_load_print_meta: n_ff             = 8192
0.00.054.403 I llm_load_print_meta: n_expert         = 0
0.00.054.403 I llm_load_print_meta: n_expert_used    = 0
0.00.054.405 I llm_load_print_meta: causal attn      = 1
0.00.054.406 I llm_load_print_meta: pooling type     = 0
0.00.054.406 I llm_load_print_meta: rope type        = 2
0.00.054.406 I llm_load_print_meta: rope scaling     = linear
0.00.054.407 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.407 I llm_load_print_meta: freq_scale_train = 1
0.00.054.407 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.408 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.408 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.408 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.412 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.412 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.412 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.413 I llm_load_print_meta: model type       = 1.4B
0.00.054.413 I llm_load_print_meta: model ftype      = Q5_1
0.00.054.413 I llm_load_print_meta: model params     = 1.41 B
0.00.054.414 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.054.414 I llm_load_print_meta: general.name     = 1.4B
0.00.054.414 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.414 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.415 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.415 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.415 I llm_load_print_meta: LF token         = 128 ''
0.00.054.415 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.415 I llm_load_print_meta: max token length = 1024
0.00.056.242 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.243 I llm_load_tensors: offloading output layer to GPU
0.00.056.243 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.249 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.249 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.057.169 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.169 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.170 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.170 I llama_new_context_with_model: n_batch       = 2048
0.00.057.170 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.170 I llama_new_context_with_model: flash_attn    = 0
0.00.057.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.171 I llama_new_context_with_model: freq_scale    = 1
0.00.057.171 I ggml_metal_init: allocating
0.00.057.177 I ggml_metal_init: found device: Apple M4
0.00.057.180 I ggml_metal_init: picking default device: Apple M4
0.00.057.786 I ggml_metal_init: using embedded metal library
0.00.060.086 I ggml_metal_init: GPU name:   Apple M4
0.00.060.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.089 I ggml_metal_init: simdgroup reduction   = true
0.00.060.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.091 I ggml_metal_init: has bfloat            = true
0.00.060.091 I ggml_metal_init: use bfloat            = true
0.00.060.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.821 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.851 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.861 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.879 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.911 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.912 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.913 I llama_new_context_with_model: graph nodes  = 967
0.00.090.913 I llama_new_context_with_model: graph splits = 2
0.00.090.930 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.244 I main: llama threadpool init, n_threads = 4
0.00.787.285 I 
0.00.787.311 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.312 I 
0.00.787.463 I sampler seed: 1234
0.00.787.467 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.482 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.482 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.627.666 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.627.666 I llama_perf_context_print:        load time =     778.11 ms
0.01.627.667 I llama_perf_context_print: prompt eval time =      42.17 ms /     7 tokens (    6.02 ms per token,   165.99 tokens per second)
0.01.627.669 I llama_perf_context_print:        eval time =     794.95 ms /    63 runs   (   12.62 ms per token,    79.25 tokens per second)
0.01.627.670 I llama_perf_context_print:       total time =     840.42 ms /    70 tokens
0.01.627.863 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.896 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.897 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.897 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.897 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.899 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.899 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.727 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.457 I llama_model_loader: - type  f32:  194 tensors
0.00.023.457 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.458 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.801 I llm_load_vocab: special tokens cache size = 25
0.00.050.743 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.746 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.746 I llm_load_print_meta: arch             = gptneox
0.00.050.746 I llm_load_print_meta: vocab type       = BPE
0.00.050.747 I llm_load_print_meta: n_vocab          = 50304
0.00.050.747 I llm_load_print_meta: n_merges         = 50009
0.00.050.747 I llm_load_print_meta: vocab_only       = 0
0.00.050.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.747 I llm_load_print_meta: n_embd           = 2048
0.00.050.748 I llm_load_print_meta: n_layer          = 24
0.00.050.750 I llm_load_print_meta: n_head           = 16
0.00.050.751 I llm_load_print_meta: n_head_kv        = 16
0.00.050.751 I llm_load_print_meta: n_rot            = 32
0.00.050.751 I llm_load_print_meta: n_swa            = 0
0.00.050.752 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.752 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.753 I llm_load_print_meta: n_gqa            = 1
0.00.050.753 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.754 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.756 I llm_load_print_meta: n_ff             = 8192
0.00.050.756 I llm_load_print_meta: n_expert         = 0
0.00.050.756 I llm_load_print_meta: n_expert_used    = 0
0.00.050.756 I llm_load_print_meta: causal attn      = 1
0.00.050.756 I llm_load_print_meta: pooling type     = 0
0.00.050.757 I llm_load_print_meta: rope type        = 2
0.00.050.759 I llm_load_print_meta: rope scaling     = linear
0.00.050.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.759 I llm_load_print_meta: freq_scale_train = 1
0.00.050.760 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.761 I llm_load_print_meta: model type       = 1.4B
0.00.050.761 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.761 I llm_load_print_meta: model params     = 1.41 B
0.00.050.762 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.762 I llm_load_print_meta: general.name     = 1.4B
0.00.050.762 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.767 I llm_load_print_meta: LF token         = 128 ''
0.00.050.767 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.768 I llm_load_print_meta: max token length = 1024
0.00.052.862 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.863 I llm_load_tensors: offloading output layer to GPU
0.00.052.863 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.873 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.874 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.829 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.830 I llama_new_context_with_model: n_ctx         = 128
0.00.053.830 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.830 I llama_new_context_with_model: n_batch       = 128
0.00.053.830 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.830 I llama_new_context_with_model: flash_attn    = 0
0.00.053.831 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.831 I llama_new_context_with_model: freq_scale    = 1
0.00.053.831 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.832 I ggml_metal_init: allocating
0.00.053.838 I ggml_metal_init: found device: Apple M4
0.00.053.841 I ggml_metal_init: picking default device: Apple M4
0.00.054.442 I ggml_metal_init: using embedded metal library
0.00.056.767 I ggml_metal_init: GPU name:   Apple M4
0.00.056.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.770 I ggml_metal_init: simdgroup reduction   = true
0.00.056.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.770 I ggml_metal_init: has bfloat            = true
0.00.056.770 I ggml_metal_init: use bfloat            = true
0.00.056.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.684 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.950 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.952 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.966 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.853 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.854 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.855 I llama_new_context_with_model: graph nodes  = 967
0.00.068.855 I llama_new_context_with_model: graph splits = 2
0.00.068.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.152 I 
0.00.734.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.204 I perplexity: tokenizing the input ..
0.00.742.427 I perplexity: tokenization took 8.222 ms
0.00.742.430 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.397 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.878.566 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.878.582 I llama_perf_context_print:        load time =     725.25 ms
0.00.878.583 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   949.98 tokens per second)
0.00.878.584 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.584 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.879.032 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.079s
sys	0m0.122s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.870 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.404 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.506 I llama_model_loader: - type  f32:  194 tensors
0.00.024.507 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.507 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.507 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.052 I llm_load_vocab: special tokens cache size = 25
0.00.050.874 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.877 I llm_load_print_meta: arch             = gptneox
0.00.050.877 I llm_load_print_meta: vocab type       = BPE
0.00.050.878 I llm_load_print_meta: n_vocab          = 50304
0.00.050.878 I llm_load_print_meta: n_merges         = 50009
0.00.050.878 I llm_load_print_meta: vocab_only       = 0
0.00.050.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.878 I llm_load_print_meta: n_embd           = 2048
0.00.050.879 I llm_load_print_meta: n_layer          = 24
0.00.050.882 I llm_load_print_meta: n_head           = 16
0.00.050.882 I llm_load_print_meta: n_head_kv        = 16
0.00.050.882 I llm_load_print_meta: n_rot            = 32
0.00.050.883 I llm_load_print_meta: n_swa            = 0
0.00.050.884 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.887 I llm_load_print_meta: n_gqa            = 1
0.00.050.888 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.889 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.890 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.890 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.890 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.891 I llm_load_print_meta: n_ff             = 8192
0.00.050.891 I llm_load_print_meta: n_expert         = 0
0.00.050.891 I llm_load_print_meta: n_expert_used    = 0
0.00.050.891 I llm_load_print_meta: causal attn      = 1
0.00.050.891 I llm_load_print_meta: pooling type     = 0
0.00.050.891 I llm_load_print_meta: rope type        = 2
0.00.050.891 I llm_load_print_meta: rope scaling     = linear
0.00.050.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.893 I llm_load_print_meta: freq_scale_train = 1
0.00.050.894 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.894 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.894 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.894 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.895 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.895 I llm_load_print_meta: model type       = 1.4B
0.00.050.895 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.896 I llm_load_print_meta: model params     = 1.41 B
0.00.050.900 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.900 I llm_load_print_meta: general.name     = 1.4B
0.00.050.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: LF token         = 128 ''
0.00.050.902 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: max token length = 1024
0.00.052.669 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.670 I llm_load_tensors: offloading output layer to GPU
0.00.052.670 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.675 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.675 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.736 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.736 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.736 I llama_new_context_with_model: n_batch       = 2048
0.00.053.736 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.737 I llama_new_context_with_model: flash_attn    = 0
0.00.053.737 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.737 I llama_new_context_with_model: freq_scale    = 1
0.00.053.738 I ggml_metal_init: allocating
0.00.053.741 I ggml_metal_init: found device: Apple M4
0.00.053.742 I ggml_metal_init: picking default device: Apple M4
0.00.054.326 I ggml_metal_init: using embedded metal library
0.00.056.664 I ggml_metal_init: GPU name:   Apple M4
0.00.056.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.666 I ggml_metal_init: simdgroup reduction   = true
0.00.056.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.667 I ggml_metal_init: has bfloat            = true
0.00.056.667 I ggml_metal_init: use bfloat            = true
0.00.056.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.692 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.873 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.878 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.903 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.912 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.913 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.914 I llama_new_context_with_model: graph nodes  = 967
0.00.087.914 I llama_new_context_with_model: graph splits = 2
0.00.087.933 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.829 I main: llama threadpool init, n_threads = 4
0.00.465.878 I 
0.00.465.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.908 I 
0.00.466.145 I sampler seed: 1234
0.00.466.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.466.196 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.466.197 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.466.197 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.151.460 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.151.461 I llama_perf_context_print:        load time =     455.95 ms
0.01.151.461 I llama_perf_context_print: prompt eval time =      41.18 ms /     7 tokens (    5.88 ms per token,   169.97 tokens per second)
0.01.151.464 I llama_perf_context_print:        eval time =     641.12 ms /    63 runs   (   10.18 ms per token,    98.27 tokens per second)
0.01.151.464 I llama_perf_context_print:       total time =     685.64 ms /    70 tokens
0.01.151.654 I ggml_metal_free: deallocating

real	0m1.176s
user	0m0.110s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.133 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.420 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.421 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.421 I llama_model_loader: - type  f32:  194 tensors
0.00.023.421 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.422 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.422 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.694 I llm_load_vocab: special tokens cache size = 25
0.00.049.854 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.857 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.857 I llm_load_print_meta: arch             = gptneox
0.00.049.858 I llm_load_print_meta: vocab type       = BPE
0.00.049.858 I llm_load_print_meta: n_vocab          = 50304
0.00.049.858 I llm_load_print_meta: n_merges         = 50009
0.00.049.858 I llm_load_print_meta: vocab_only       = 0
0.00.049.858 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.859 I llm_load_print_meta: n_embd           = 2048
0.00.049.859 I llm_load_print_meta: n_layer          = 24
0.00.049.861 I llm_load_print_meta: n_head           = 16
0.00.049.862 I llm_load_print_meta: n_head_kv        = 16
0.00.049.862 I llm_load_print_meta: n_rot            = 32
0.00.049.862 I llm_load_print_meta: n_swa            = 0
0.00.049.863 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.863 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.863 I llm_load_print_meta: n_gqa            = 1
0.00.049.864 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.865 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.865 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.866 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.867 I llm_load_print_meta: n_ff             = 8192
0.00.049.867 I llm_load_print_meta: n_expert         = 0
0.00.049.868 I llm_load_print_meta: n_expert_used    = 0
0.00.049.868 I llm_load_print_meta: causal attn      = 1
0.00.049.869 I llm_load_print_meta: pooling type     = 0
0.00.049.869 I llm_load_print_meta: rope type        = 2
0.00.049.869 I llm_load_print_meta: rope scaling     = linear
0.00.049.869 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.870 I llm_load_print_meta: freq_scale_train = 1
0.00.049.870 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.870 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.870 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.870 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.871 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.871 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.871 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.871 I llm_load_print_meta: model type       = 1.4B
0.00.049.871 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.874 I llm_load_print_meta: model params     = 1.41 B
0.00.049.874 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.874 I llm_load_print_meta: general.name     = 1.4B
0.00.049.875 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: LF token         = 128 ''
0.00.049.876 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.876 I llm_load_print_meta: max token length = 1024
0.00.051.401 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.402 I llm_load_tensors: offloading output layer to GPU
0.00.051.402 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.412 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.413 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.285 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.286 I llama_new_context_with_model: n_ctx         = 128
0.00.052.286 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.286 I llama_new_context_with_model: n_batch       = 128
0.00.052.286 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.286 I llama_new_context_with_model: flash_attn    = 0
0.00.052.287 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.287 I llama_new_context_with_model: freq_scale    = 1
0.00.052.287 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.288 I ggml_metal_init: allocating
0.00.052.293 I ggml_metal_init: found device: Apple M4
0.00.052.295 I ggml_metal_init: picking default device: Apple M4
0.00.052.846 I ggml_metal_init: using embedded metal library
0.00.055.139 I ggml_metal_init: GPU name:   Apple M4
0.00.055.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.142 I ggml_metal_init: simdgroup reduction   = true
0.00.055.142 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.142 I ggml_metal_init: has bfloat            = true
0.00.055.142 I ggml_metal_init: use bfloat            = true
0.00.055.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.143 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.940 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.185 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.187 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.202 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.126 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.127 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.127 I llama_new_context_with_model: graph nodes  = 967
0.00.067.128 I llama_new_context_with_model: graph splits = 2
0.00.067.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.742 I 
0.00.404.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.801 I perplexity: tokenizing the input ..
0.00.412.810 I perplexity: tokenization took 8.007 ms
0.00.412.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.545.397 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.546.551 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.546.568 I llama_perf_context_print:        load time =     395.60 ms
0.00.546.569 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.08 tokens per second)
0.00.546.570 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.571 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.547.072 I ggml_metal_free: deallocating

real	0m0.562s
user	0m0.079s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.152 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.649 I llama_model_loader: - type  f32:  194 tensors
0.00.025.649 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.649 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.649 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.650 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.574 I llm_load_vocab: special tokens cache size = 25
0.00.053.738 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.742 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.742 I llm_load_print_meta: arch             = gptneox
0.00.053.742 I llm_load_print_meta: vocab type       = BPE
0.00.053.743 I llm_load_print_meta: n_vocab          = 50304
0.00.053.743 I llm_load_print_meta: n_merges         = 50009
0.00.053.743 I llm_load_print_meta: vocab_only       = 0
0.00.053.743 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.743 I llm_load_print_meta: n_embd           = 2048
0.00.053.744 I llm_load_print_meta: n_layer          = 24
0.00.053.747 I llm_load_print_meta: n_head           = 16
0.00.053.748 I llm_load_print_meta: n_head_kv        = 16
0.00.053.748 I llm_load_print_meta: n_rot            = 32
0.00.053.748 I llm_load_print_meta: n_swa            = 0
0.00.053.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.749 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.749 I llm_load_print_meta: n_gqa            = 1
0.00.053.750 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.751 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.752 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.752 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.752 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.753 I llm_load_print_meta: n_ff             = 8192
0.00.053.756 I llm_load_print_meta: n_expert         = 0
0.00.053.758 I llm_load_print_meta: n_expert_used    = 0
0.00.053.759 I llm_load_print_meta: causal attn      = 1
0.00.053.759 I llm_load_print_meta: pooling type     = 0
0.00.053.759 I llm_load_print_meta: rope type        = 2
0.00.053.759 I llm_load_print_meta: rope scaling     = linear
0.00.053.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.760 I llm_load_print_meta: freq_scale_train = 1
0.00.053.760 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.761 I llm_load_print_meta: model type       = 1.4B
0.00.053.761 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.761 I llm_load_print_meta: model params     = 1.41 B
0.00.053.762 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.762 I llm_load_print_meta: general.name     = 1.4B
0.00.053.762 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.762 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.762 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.763 I llm_load_print_meta: LF token         = 128 ''
0.00.053.763 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.763 I llm_load_print_meta: max token length = 1024
0.00.055.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.753 I llm_load_tensors: offloading output layer to GPU
0.00.055.753 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.764 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.765 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.653 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.654 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.654 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.655 I llama_new_context_with_model: n_batch       = 2048
0.00.056.655 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.655 I llama_new_context_with_model: flash_attn    = 0
0.00.056.655 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.656 I llama_new_context_with_model: freq_scale    = 1
0.00.056.656 I ggml_metal_init: allocating
0.00.056.660 I ggml_metal_init: found device: Apple M4
0.00.056.662 I ggml_metal_init: picking default device: Apple M4
0.00.057.295 I ggml_metal_init: using embedded metal library
0.00.059.714 I ggml_metal_init: GPU name:   Apple M4
0.00.059.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.718 I ggml_metal_init: simdgroup reduction   = true
0.00.059.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.718 I ggml_metal_init: has bfloat            = true
0.00.059.718 I ggml_metal_init: use bfloat            = true
0.00.059.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.917 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.013 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.033 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.028 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.029 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.029 I llama_new_context_with_model: graph nodes  = 967
0.00.091.029 I llama_new_context_with_model: graph splits = 2
0.00.091.044 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.779 I main: llama threadpool init, n_threads = 4
0.00.505.829 I 
0.00.505.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.869 I 
0.00.506.112 I sampler seed: 1234
0.00.506.118 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.133 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.134 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.134 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.253.816 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.253.816 I llama_perf_context_print:        load time =     496.62 ms
0.01.253.818 I llama_perf_context_print: prompt eval time =      40.27 ms /     7 tokens (    5.75 ms per token,   173.81 tokens per second)
0.01.253.818 I llama_perf_context_print:        eval time =     704.49 ms /    63 runs   (   11.18 ms per token,    89.43 tokens per second)
0.01.253.820 I llama_perf_context_print:       total time =     748.04 ms /    70 tokens
0.01.254.028 I ggml_metal_free: deallocating

real	0m1.273s
user	0m0.111s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.707 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.542 I llama_model_loader: - type  f32:  194 tensors
0.00.023.543 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.543 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.543 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.777 I llm_load_vocab: special tokens cache size = 25
0.00.050.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.872 I llm_load_print_meta: arch             = gptneox
0.00.050.872 I llm_load_print_meta: vocab type       = BPE
0.00.050.873 I llm_load_print_meta: n_vocab          = 50304
0.00.050.873 I llm_load_print_meta: n_merges         = 50009
0.00.050.873 I llm_load_print_meta: vocab_only       = 0
0.00.050.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.873 I llm_load_print_meta: n_embd           = 2048
0.00.050.874 I llm_load_print_meta: n_layer          = 24
0.00.050.877 I llm_load_print_meta: n_head           = 16
0.00.050.878 I llm_load_print_meta: n_head_kv        = 16
0.00.050.878 I llm_load_print_meta: n_rot            = 32
0.00.050.878 I llm_load_print_meta: n_swa            = 0
0.00.050.878 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.881 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.882 I llm_load_print_meta: n_gqa            = 1
0.00.050.883 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.883 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.884 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.884 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.885 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.886 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.886 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.887 I llm_load_print_meta: n_ff             = 8192
0.00.050.887 I llm_load_print_meta: n_expert         = 0
0.00.050.888 I llm_load_print_meta: n_expert_used    = 0
0.00.050.888 I llm_load_print_meta: causal attn      = 1
0.00.050.888 I llm_load_print_meta: pooling type     = 0
0.00.050.888 I llm_load_print_meta: rope type        = 2
0.00.050.888 I llm_load_print_meta: rope scaling     = linear
0.00.050.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.893 I llm_load_print_meta: freq_scale_train = 1
0.00.050.893 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.894 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.894 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.894 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.894 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.895 I llm_load_print_meta: model type       = 1.4B
0.00.050.895 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.895 I llm_load_print_meta: model params     = 1.41 B
0.00.050.896 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.896 I llm_load_print_meta: general.name     = 1.4B
0.00.050.897 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.897 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.897 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: LF token         = 128 ''
0.00.050.898 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: max token length = 1024
0.00.052.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.886 I llm_load_tensors: offloading output layer to GPU
0.00.052.886 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.897 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.898 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.788 I llama_new_context_with_model: n_ctx         = 128
0.00.053.788 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.788 I llama_new_context_with_model: n_batch       = 128
0.00.053.788 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.788 I llama_new_context_with_model: flash_attn    = 0
0.00.053.789 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.789 I llama_new_context_with_model: freq_scale    = 1
0.00.053.790 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.790 I ggml_metal_init: allocating
0.00.053.793 I ggml_metal_init: found device: Apple M4
0.00.053.795 I ggml_metal_init: picking default device: Apple M4
0.00.054.387 I ggml_metal_init: using embedded metal library
0.00.056.792 I ggml_metal_init: GPU name:   Apple M4
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.794 I ggml_metal_init: simdgroup reduction   = true
0.00.056.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.795 I ggml_metal_init: has bfloat            = true
0.00.056.795 I ggml_metal_init: use bfloat            = true
0.00.056.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.781 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.054 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.058 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.072 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.040 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.041 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.042 I llama_new_context_with_model: graph nodes  = 967
0.00.069.042 I llama_new_context_with_model: graph splits = 2
0.00.069.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.265 I 
0.00.484.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.312 I perplexity: tokenizing the input ..
0.00.492.034 I perplexity: tokenization took 7.72 ms
0.00.492.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.624.226 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.625.527 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.625.548 I llama_perf_context_print:        load time =     475.41 ms
0.00.625.549 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   969.96 tokens per second)
0.00.625.550 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.551 I llama_perf_context_print:       total time =     141.28 ms /   129 tokens
0.00.626.115 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.079s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.011.394 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.025.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.124 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.124 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.125 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.126 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.126 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.034.552 I llama_model_loader: - type  f32:  194 tensors
0.00.034.553 I llama_model_loader: - type q4_K:   61 tensors
0.00.034.553 I llama_model_loader: - type q5_K:   24 tensors
0.00.034.553 I llama_model_loader: - type q6_K:   13 tensors
0.00.055.992 I llm_load_vocab: special tokens cache size = 25
0.00.061.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.930 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.931 I llm_load_print_meta: arch             = gptneox
0.00.061.931 I llm_load_print_meta: vocab type       = BPE
0.00.061.931 I llm_load_print_meta: n_vocab          = 50304
0.00.061.931 I llm_load_print_meta: n_merges         = 50009
0.00.061.932 I llm_load_print_meta: vocab_only       = 0
0.00.061.932 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.932 I llm_load_print_meta: n_embd           = 2048
0.00.061.932 I llm_load_print_meta: n_layer          = 24
0.00.061.935 I llm_load_print_meta: n_head           = 16
0.00.061.935 I llm_load_print_meta: n_head_kv        = 16
0.00.061.936 I llm_load_print_meta: n_rot            = 32
0.00.061.936 I llm_load_print_meta: n_swa            = 0
0.00.061.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.939 I llm_load_print_meta: n_gqa            = 1
0.00.061.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.940 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.942 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.942 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.942 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.943 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.943 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.944 I llm_load_print_meta: n_ff             = 8192
0.00.061.944 I llm_load_print_meta: n_expert         = 0
0.00.061.944 I llm_load_print_meta: n_expert_used    = 0
0.00.061.944 I llm_load_print_meta: causal attn      = 1
0.00.061.944 I llm_load_print_meta: pooling type     = 0
0.00.061.944 I llm_load_print_meta: rope type        = 2
0.00.061.946 I llm_load_print_meta: rope scaling     = linear
0.00.061.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.947 I llm_load_print_meta: freq_scale_train = 1
0.00.061.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.947 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.947 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.948 I llm_load_print_meta: model type       = 1.4B
0.00.061.948 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.061.949 I llm_load_print_meta: model params     = 1.41 B
0.00.061.949 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.061.950 I llm_load_print_meta: general.name     = 1.4B
0.00.061.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.951 I llm_load_print_meta: LF token         = 128 ''
0.00.061.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.952 I llm_load_print_meta: max token length = 1024
0.00.063.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.991 I llm_load_tensors: offloading output layer to GPU
0.00.063.991 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.001 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.064.003 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.064.944 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.945 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.945 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.945 I llama_new_context_with_model: n_batch       = 2048
0.00.064.945 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.946 I llama_new_context_with_model: flash_attn    = 0
0.00.064.946 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.946 I llama_new_context_with_model: freq_scale    = 1
0.00.064.947 I ggml_metal_init: allocating
0.00.064.955 I ggml_metal_init: found device: Apple M4
0.00.064.958 I ggml_metal_init: picking default device: Apple M4
0.00.065.594 I ggml_metal_init: using embedded metal library
0.00.067.968 I ggml_metal_init: GPU name:   Apple M4
0.00.067.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.971 I ggml_metal_init: simdgroup reduction   = true
0.00.067.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.971 I ggml_metal_init: has bfloat            = true
0.00.067.971 I ggml_metal_init: use bfloat            = true
0.00.067.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.992 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.281 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.286 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.304 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.411 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.412 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.413 I llama_new_context_with_model: graph nodes  = 967
0.00.102.413 I llama_new_context_with_model: graph splits = 2
0.00.102.428 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.329 I main: llama threadpool init, n_threads = 4
0.00.740.368 I 
0.00.740.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.399 I 
0.00.740.617 I sampler seed: 1234
0.00.740.621 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.655 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.656 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.656 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.503.287 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.503.288 I llama_perf_context_print:        load time =     728.93 ms
0.01.503.289 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.67 tokens per second)
0.01.503.291 I llama_perf_context_print:        eval time =     712.60 ms /    63 runs   (   11.31 ms per token,    88.41 tokens per second)
0.01.503.292 I llama_perf_context_print:       total time =     762.96 ms /    70 tokens
0.01.503.484 I ggml_metal_free: deallocating

real	0m1.521s
user	0m0.112s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.925 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.023 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.027 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.028 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.028 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.031 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.031 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.999 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.002 I llama_model_loader: - type  f32:  194 tensors
0.00.024.003 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.003 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.003 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.178 I llm_load_vocab: special tokens cache size = 25
0.00.051.334 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.336 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.337 I llm_load_print_meta: arch             = gptneox
0.00.051.337 I llm_load_print_meta: vocab type       = BPE
0.00.051.337 I llm_load_print_meta: n_vocab          = 50304
0.00.051.337 I llm_load_print_meta: n_merges         = 50009
0.00.051.338 I llm_load_print_meta: vocab_only       = 0
0.00.051.338 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.338 I llm_load_print_meta: n_embd           = 2048
0.00.051.338 I llm_load_print_meta: n_layer          = 24
0.00.051.342 I llm_load_print_meta: n_head           = 16
0.00.051.344 I llm_load_print_meta: n_head_kv        = 16
0.00.051.345 I llm_load_print_meta: n_rot            = 32
0.00.051.345 I llm_load_print_meta: n_swa            = 0
0.00.051.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.346 I llm_load_print_meta: n_gqa            = 1
0.00.051.351 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.351 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.354 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.354 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.354 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.355 I llm_load_print_meta: n_ff             = 8192
0.00.051.355 I llm_load_print_meta: n_expert         = 0
0.00.051.355 I llm_load_print_meta: n_expert_used    = 0
0.00.051.355 I llm_load_print_meta: causal attn      = 1
0.00.051.356 I llm_load_print_meta: pooling type     = 0
0.00.051.356 I llm_load_print_meta: rope type        = 2
0.00.051.358 I llm_load_print_meta: rope scaling     = linear
0.00.051.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.360 I llm_load_print_meta: freq_scale_train = 1
0.00.051.360 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.362 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.362 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.363 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.363 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.363 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.363 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.364 I llm_load_print_meta: model type       = 1.4B
0.00.051.365 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.365 I llm_load_print_meta: model params     = 1.41 B
0.00.051.366 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.366 I llm_load_print_meta: general.name     = 1.4B
0.00.051.366 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: LF token         = 128 ''
0.00.051.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: max token length = 1024
0.00.053.468 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.469 I llm_load_tensors: offloading output layer to GPU
0.00.053.469 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.480 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.481 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.425 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.426 I llama_new_context_with_model: n_ctx         = 128
0.00.054.426 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.427 I llama_new_context_with_model: n_batch       = 128
0.00.054.427 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.427 I llama_new_context_with_model: flash_attn    = 0
0.00.054.427 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.428 I llama_new_context_with_model: freq_scale    = 1
0.00.054.428 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.429 I ggml_metal_init: allocating
0.00.054.434 I ggml_metal_init: found device: Apple M4
0.00.054.437 I ggml_metal_init: picking default device: Apple M4
0.00.055.026 I ggml_metal_init: using embedded metal library
0.00.057.433 I ggml_metal_init: GPU name:   Apple M4
0.00.057.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.435 I ggml_metal_init: simdgroup reduction   = true
0.00.057.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.436 I ggml_metal_init: has bfloat            = true
0.00.057.437 I ggml_metal_init: use bfloat            = true
0.00.057.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.070 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.380 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.383 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.400 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.281 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.282 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.283 I llama_new_context_with_model: graph nodes  = 967
0.00.069.283 I llama_new_context_with_model: graph splits = 2
0.00.069.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.556.608 I 
0.00.556.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.556.653 I perplexity: tokenizing the input ..
0.00.564.566 I perplexity: tokenization took 7.912 ms
0.00.564.570 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.051 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.223 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.242 I llama_perf_context_print:        load time =     547.68 ms
0.00.700.243 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.40 tokens per second)
0.00.700.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.244 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.700.714 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.079s
sys	0m0.100s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.766 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.620 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.621 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.547 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.548 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.548 I llama_model_loader: - type  f32:  194 tensors
0.00.024.549 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.549 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.088 I llm_load_vocab: special tokens cache size = 25
0.00.050.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.952 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.952 I llm_load_print_meta: arch             = gptneox
0.00.050.952 I llm_load_print_meta: vocab type       = BPE
0.00.050.953 I llm_load_print_meta: n_vocab          = 50304
0.00.050.953 I llm_load_print_meta: n_merges         = 50009
0.00.050.953 I llm_load_print_meta: vocab_only       = 0
0.00.050.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.953 I llm_load_print_meta: n_embd           = 2048
0.00.050.954 I llm_load_print_meta: n_layer          = 24
0.00.050.956 I llm_load_print_meta: n_head           = 16
0.00.050.957 I llm_load_print_meta: n_head_kv        = 16
0.00.050.957 I llm_load_print_meta: n_rot            = 32
0.00.050.957 I llm_load_print_meta: n_swa            = 0
0.00.050.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.958 I llm_load_print_meta: n_gqa            = 1
0.00.050.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.960 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.961 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.962 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.963 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.963 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.965 I llm_load_print_meta: n_ff             = 8192
0.00.050.965 I llm_load_print_meta: n_expert         = 0
0.00.050.965 I llm_load_print_meta: n_expert_used    = 0
0.00.050.966 I llm_load_print_meta: causal attn      = 1
0.00.050.966 I llm_load_print_meta: pooling type     = 0
0.00.050.967 I llm_load_print_meta: rope type        = 2
0.00.050.967 I llm_load_print_meta: rope scaling     = linear
0.00.050.967 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.967 I llm_load_print_meta: freq_scale_train = 1
0.00.050.967 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.968 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.968 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.968 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.972 I llm_load_print_meta: model type       = 1.4B
0.00.050.973 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.973 I llm_load_print_meta: model params     = 1.41 B
0.00.050.974 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.974 I llm_load_print_meta: general.name     = 1.4B
0.00.050.974 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: LF token         = 128 ''
0.00.050.975 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: max token length = 1024
0.00.053.011 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.012 I llm_load_tensors: offloading output layer to GPU
0.00.053.012 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.022 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.023 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.991 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.992 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.993 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.993 I llama_new_context_with_model: n_batch       = 2048
0.00.053.993 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.993 I llama_new_context_with_model: flash_attn    = 0
0.00.053.994 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.994 I llama_new_context_with_model: freq_scale    = 1
0.00.053.995 I ggml_metal_init: allocating
0.00.054.002 I ggml_metal_init: found device: Apple M4
0.00.054.004 I ggml_metal_init: picking default device: Apple M4
0.00.054.600 I ggml_metal_init: using embedded metal library
0.00.056.925 I ggml_metal_init: GPU name:   Apple M4
0.00.056.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.929 I ggml_metal_init: simdgroup reduction   = true
0.00.056.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.929 I ggml_metal_init: has bfloat            = true
0.00.056.929 I ggml_metal_init: use bfloat            = true
0.00.056.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.930 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.750 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.989 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.005 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.027 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.029 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.029 I llama_new_context_with_model: graph nodes  = 967
0.00.087.029 I llama_new_context_with_model: graph splits = 2
0.00.087.045 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.972 I main: llama threadpool init, n_threads = 4
0.00.713.011 I 
0.00.713.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.044 I 
0.00.713.273 I sampler seed: 1234
0.00.713.277 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.303 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.560.415 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.560.416 I llama_perf_context_print:        load time =     704.20 ms
0.01.560.417 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.91 tokens per second)
0.01.560.417 I llama_perf_context_print:        eval time =     792.68 ms /    63 runs   (   12.58 ms per token,    79.48 tokens per second)
0.01.560.418 I llama_perf_context_print:       total time =     847.44 ms /    70 tokens
0.01.560.609 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.059 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.930 I llama_model_loader: - type  f32:  194 tensors
0.00.024.930 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.930 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.004 I llm_load_vocab: special tokens cache size = 25
0.00.051.976 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.980 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.980 I llm_load_print_meta: arch             = gptneox
0.00.051.980 I llm_load_print_meta: vocab type       = BPE
0.00.051.981 I llm_load_print_meta: n_vocab          = 50304
0.00.051.981 I llm_load_print_meta: n_merges         = 50009
0.00.051.981 I llm_load_print_meta: vocab_only       = 0
0.00.051.981 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.981 I llm_load_print_meta: n_embd           = 2048
0.00.051.981 I llm_load_print_meta: n_layer          = 24
0.00.051.984 I llm_load_print_meta: n_head           = 16
0.00.051.985 I llm_load_print_meta: n_head_kv        = 16
0.00.051.985 I llm_load_print_meta: n_rot            = 32
0.00.051.988 I llm_load_print_meta: n_swa            = 0
0.00.051.988 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.989 I llm_load_print_meta: n_gqa            = 1
0.00.051.990 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.990 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.991 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.994 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.994 I llm_load_print_meta: n_ff             = 8192
0.00.051.994 I llm_load_print_meta: n_expert         = 0
0.00.051.995 I llm_load_print_meta: n_expert_used    = 0
0.00.051.995 I llm_load_print_meta: causal attn      = 1
0.00.051.995 I llm_load_print_meta: pooling type     = 0
0.00.051.995 I llm_load_print_meta: rope type        = 2
0.00.051.995 I llm_load_print_meta: rope scaling     = linear
0.00.051.996 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.996 I llm_load_print_meta: freq_scale_train = 1
0.00.051.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.996 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.996 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.997 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.997 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.999 I llm_load_print_meta: model type       = 1.4B
0.00.052.000 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.000 I llm_load_print_meta: model params     = 1.41 B
0.00.052.001 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.006 I llm_load_print_meta: general.name     = 1.4B
0.00.052.007 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.008 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: LF token         = 128 ''
0.00.052.009 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: max token length = 1024
0.00.053.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.690 I llm_load_tensors: offloading output layer to GPU
0.00.053.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.700 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.701 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.532 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.533 I llama_new_context_with_model: n_ctx         = 128
0.00.054.533 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.533 I llama_new_context_with_model: n_batch       = 128
0.00.054.533 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.534 I llama_new_context_with_model: flash_attn    = 0
0.00.054.534 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.534 I llama_new_context_with_model: freq_scale    = 1
0.00.054.535 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.535 I ggml_metal_init: allocating
0.00.054.538 I ggml_metal_init: found device: Apple M4
0.00.054.540 I ggml_metal_init: picking default device: Apple M4
0.00.055.111 I ggml_metal_init: using embedded metal library
0.00.057.474 I ggml_metal_init: GPU name:   Apple M4
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.477 I ggml_metal_init: simdgroup reduction   = true
0.00.057.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.477 I ggml_metal_init: has bfloat            = true
0.00.057.477 I ggml_metal_init: use bfloat            = true
0.00.057.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.414 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.687 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.703 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.635 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.636 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.636 I llama_new_context_with_model: graph nodes  = 967
0.00.069.636 I llama_new_context_with_model: graph splits = 2
0.00.069.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.229 I 
0.00.662.289 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.318 I perplexity: tokenizing the input ..
0.00.670.546 I perplexity: tokenization took 8.228 ms
0.00.670.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.198 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.353 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.373 I llama_perf_context_print:        load time =     652.15 ms
0.00.812.374 I llama_perf_context_print: prompt eval time =     140.42 ms /   128 tokens (    1.10 ms per token,   911.53 tokens per second)
0.00.812.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.376 I llama_perf_context_print:       total time =     150.16 ms /   129 tokens
0.00.812.735 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.080s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.124 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.317 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.322 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.475 I llama_model_loader: - type  f32:  194 tensors
0.00.026.476 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.215 I llm_load_vocab: special tokens cache size = 25
0.00.053.200 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.203 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.203 I llm_load_print_meta: arch             = gptneox
0.00.053.204 I llm_load_print_meta: vocab type       = BPE
0.00.053.204 I llm_load_print_meta: n_vocab          = 50304
0.00.053.204 I llm_load_print_meta: n_merges         = 50009
0.00.053.204 I llm_load_print_meta: vocab_only       = 0
0.00.053.204 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.205 I llm_load_print_meta: n_embd           = 2048
0.00.053.205 I llm_load_print_meta: n_layer          = 24
0.00.053.207 I llm_load_print_meta: n_head           = 16
0.00.053.208 I llm_load_print_meta: n_head_kv        = 16
0.00.053.208 I llm_load_print_meta: n_rot            = 32
0.00.053.208 I llm_load_print_meta: n_swa            = 0
0.00.053.208 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.209 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.209 I llm_load_print_meta: n_gqa            = 1
0.00.053.210 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.211 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.212 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.213 I llm_load_print_meta: n_ff             = 8192
0.00.053.214 I llm_load_print_meta: n_expert         = 0
0.00.053.216 I llm_load_print_meta: n_expert_used    = 0
0.00.053.216 I llm_load_print_meta: causal attn      = 1
0.00.053.216 I llm_load_print_meta: pooling type     = 0
0.00.053.216 I llm_load_print_meta: rope type        = 2
0.00.053.216 I llm_load_print_meta: rope scaling     = linear
0.00.053.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.217 I llm_load_print_meta: freq_scale_train = 1
0.00.053.217 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.217 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.217 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.218 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.218 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.218 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.218 I llm_load_print_meta: model type       = 1.4B
0.00.053.218 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.219 I llm_load_print_meta: model params     = 1.41 B
0.00.053.219 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.220 I llm_load_print_meta: general.name     = 1.4B
0.00.053.220 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.220 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.220 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.220 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.221 I llm_load_print_meta: LF token         = 128 ''
0.00.053.221 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.221 I llm_load_print_meta: max token length = 1024
0.00.055.068 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.069 I llm_load_tensors: offloading output layer to GPU
0.00.055.069 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.074 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.075 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.958 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.959 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.959 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.959 I llama_new_context_with_model: n_batch       = 2048
0.00.055.959 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.959 I llama_new_context_with_model: flash_attn    = 0
0.00.055.960 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.960 I llama_new_context_with_model: freq_scale    = 1
0.00.055.961 I ggml_metal_init: allocating
0.00.055.967 I ggml_metal_init: found device: Apple M4
0.00.055.969 I ggml_metal_init: picking default device: Apple M4
0.00.056.552 I ggml_metal_init: using embedded metal library
0.00.058.933 I ggml_metal_init: GPU name:   Apple M4
0.00.058.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.936 I ggml_metal_init: simdgroup reduction   = true
0.00.058.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.936 I ggml_metal_init: has bfloat            = true
0.00.058.936 I ggml_metal_init: use bfloat            = true
0.00.058.937 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.761 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.836 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.841 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.859 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.885 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.886 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.886 I llama_new_context_with_model: graph nodes  = 967
0.00.088.886 I llama_new_context_with_model: graph splits = 2
0.00.088.902 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.769 I main: llama threadpool init, n_threads = 4
0.00.745.814 I 
0.00.745.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.854 I 
0.00.746.083 I sampler seed: 1234
0.00.746.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.137 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.142 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.628.681 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.628.681 I llama_perf_context_print:        load time =     735.64 ms
0.01.628.682 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.62 tokens per second)
0.01.628.683 I llama_perf_context_print:        eval time =     825.07 ms /    63 runs   (   13.10 ms per token,    76.36 tokens per second)
0.01.628.683 I llama_perf_context_print:       total time =     882.92 ms /    70 tokens
0.01.628.881 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4398 (29df666d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.563 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.605 I llama_model_loader: - type  f32:  194 tensors
0.00.023.605 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.108 I llm_load_vocab: special tokens cache size = 25
0.00.050.089 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.091 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.092 I llm_load_print_meta: arch             = gptneox
0.00.050.092 I llm_load_print_meta: vocab type       = BPE
0.00.050.092 I llm_load_print_meta: n_vocab          = 50304
0.00.050.092 I llm_load_print_meta: n_merges         = 50009
0.00.050.093 I llm_load_print_meta: vocab_only       = 0
0.00.050.093 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.093 I llm_load_print_meta: n_embd           = 2048
0.00.050.093 I llm_load_print_meta: n_layer          = 24
0.00.050.096 I llm_load_print_meta: n_head           = 16
0.00.050.097 I llm_load_print_meta: n_head_kv        = 16
0.00.050.097 I llm_load_print_meta: n_rot            = 32
0.00.050.097 I llm_load_print_meta: n_swa            = 0
0.00.050.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.098 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.098 I llm_load_print_meta: n_gqa            = 1
0.00.050.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.101 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.101 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.102 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.103 I llm_load_print_meta: n_ff             = 8192
0.00.050.103 I llm_load_print_meta: n_expert         = 0
0.00.050.103 I llm_load_print_meta: n_expert_used    = 0
0.00.050.104 I llm_load_print_meta: causal attn      = 1
0.00.050.104 I llm_load_print_meta: pooling type     = 0
0.00.050.104 I llm_load_print_meta: rope type        = 2
0.00.050.104 I llm_load_print_meta: rope scaling     = linear
0.00.050.104 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.105 I llm_load_print_meta: freq_scale_train = 1
0.00.050.105 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.105 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.105 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.106 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.106 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.106 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.106 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.106 I llm_load_print_meta: model type       = 1.4B
0.00.050.107 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.107 I llm_load_print_meta: model params     = 1.41 B
0.00.050.108 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.108 I llm_load_print_meta: general.name     = 1.4B
0.00.050.108 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.108 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: LF token         = 128 ''
0.00.050.109 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: max token length = 1024
0.00.052.168 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.168 I llm_load_tensors: offloading output layer to GPU
0.00.052.169 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.179 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.180 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.067 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.067 I llama_new_context_with_model: n_ctx         = 128
0.00.053.068 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.068 I llama_new_context_with_model: n_batch       = 128
0.00.053.068 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.068 I llama_new_context_with_model: flash_attn    = 0
0.00.053.069 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.069 I llama_new_context_with_model: freq_scale    = 1
0.00.053.069 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.070 I ggml_metal_init: allocating
0.00.053.073 I ggml_metal_init: found device: Apple M4
0.00.053.075 I ggml_metal_init: picking default device: Apple M4
0.00.053.658 I ggml_metal_init: using embedded metal library
0.00.055.966 I ggml_metal_init: GPU name:   Apple M4
0.00.055.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.968 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.968 I ggml_metal_init: simdgroup reduction   = true
0.00.055.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.969 I ggml_metal_init: has bfloat            = true
0.00.055.969 I ggml_metal_init: use bfloat            = true
0.00.055.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.970 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.722 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.975 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.979 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.992 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.906 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.907 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.907 I llama_new_context_with_model: graph nodes  = 967
0.00.067.908 I llama_new_context_with_model: graph splits = 2
0.00.067.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.304.304 I 
0.00.304.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.304.354 I perplexity: tokenizing the input ..
0.00.311.687 I perplexity: tokenization took 7.331 ms
0.00.311.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.452.049 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.453.264 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.453.279 I llama_perf_context_print:        load time =     295.50 ms
0.00.453.280 I llama_perf_context_print: prompt eval time =     140.13 ms /   128 tokens (    1.09 ms per token,   913.42 tokens per second)
0.00.453.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.453.281 I llama_perf_context_print:       total time =     148.98 ms /   129 tokens
0.00.453.631 I ggml_metal_free: deallocating

real	0m0.466s
user	0m0.078s
sys	0m0.058s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4398 (29df666d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10a60a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10a60a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10a60af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10a60b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10a60ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10a60c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10a60c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10a60cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10a60d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10a60d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10a60db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10a60e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10a60eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10a60f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10a60fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10a610220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10a610940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10a611060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10a611780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10a611f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10a612670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10a612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10a6134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10a613d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10a614470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10a614730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10a614d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10a6159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10a615ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10a6161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10a616650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10a616910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10a6171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10a6176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10a6179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10a617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10a6182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10a618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10a618c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10a6190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10a619560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10a619a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10a619ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10a61a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10a61a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10a61ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10a61b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10a61bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10a61c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10a61c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10a61cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10a61d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10a61d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10a61dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10a61e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10a61ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10a61f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10a61f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10a61f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10a620190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10a620450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10a6208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10a620d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10a621230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10a6216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10a621b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10a622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10a6224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10a622950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10a622df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10a623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10a623730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10a623bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10a624120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10a624670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10a624bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10a625110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10a625660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10a625bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10a626100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10a626650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10a626ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10a6270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10a627640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10a627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10a6280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10a628630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10a628b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10a6290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10a629620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10a629b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10a62a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10a62a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10a62ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10a62b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10a62b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10a62bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10a61b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10a62bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10a62c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10a62ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10a62d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10a62d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10a62dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10a62e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10a62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10a62eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10a62f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10a62f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10a62fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10a6301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10a630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10a630c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10a631120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10a6315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10a631a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10a631f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10a6323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10a632840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10a632ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10a633180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10a633620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10a633ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10a633f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10a634400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10a6348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10a634d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10a6351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10a635680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10a635b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10a635fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10a636460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10a636900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10a636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10a637240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10a6376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10a637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10a638020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10a6384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10a638960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10a638e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10a6392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10a639740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10a639be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10a63a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10a63a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10a63a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10a63ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10a63b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10a63b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10a63bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10a63c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10a63c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10a63ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10a63cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10a63d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10a63d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10a63dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10a63e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10a63e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10a63ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10a63ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10a63f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10a63f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10a63fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10a6401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10a640640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10a640ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10a640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10a641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10a6418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10a641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10a642200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10a6426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10a642b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10a642fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10a643480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10a643920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10a643dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10a644260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10a644700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10a644ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10a645040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10a6454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10a645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10a645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10a6462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10a646760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10a646c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10a6470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10a647540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10a6479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10a647e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10a6483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10a648920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10a648e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10a6493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10a649680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10a649c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10a64a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10a64a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10a64b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10a64b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10a64b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10a64be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10a64c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10a64cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10a64d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10a64d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10a64d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10a64e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10a64e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10a64ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10a64f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10a64f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10a64fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10a650180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10a6506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10a650c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10a651170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10a6516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10a651c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10a652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10a6526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10a652c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10a653150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10a6536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10a653bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10a654140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10a654690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10a654be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10a655130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10a655680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10a655bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10a656120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10a656670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10a656bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10a657110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10a657660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10a657bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10a658100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10a658650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10a658ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10a6590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10a659640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10a659b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10a65a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10a65a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10a65ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10a65b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10a65b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10a65bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10a65c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10a65c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10a65cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10a65d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10a65d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10a65db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10a65e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10a65e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a65eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10a65f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10a65f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a65fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10a660080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a6605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10a660b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10a660fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10a661460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a661900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a661da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10a662240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10a6626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10a662b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10a663020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10a6634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10a663960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a663e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10a6642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a664740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10a664be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10a665080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10a6655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10a665cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10a666410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10a666b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10a667250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10a667510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10a667d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a667fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a6685d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.148.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.148.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109e04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109e05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109e056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109e05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109e05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109e063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109e06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109e06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109e07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109e075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109e07a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109e080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109e08c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109e093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109e09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109e0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x109e0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x109e0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x109e0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x109e0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109e0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109e0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109e0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x109e0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109e0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x109e0e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109e0e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109e0eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109e0f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109e0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109e0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109e10020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109e10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109e10750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109e10bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109e11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109e114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109e11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109e11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109e121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109e12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109e12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109e12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109e133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109e13820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109e13c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109e14100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109e14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109e149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109e14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109e152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109e15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109e15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109e16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109e16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109e168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109e16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109e17360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109e177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109e17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109e180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109e18520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109e18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109e18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109e19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109e196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109e19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109e19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109e1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109e1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109e1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109e1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x109e1b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109e1ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109e1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109e1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109e1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109e1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109e1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109e1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109e1d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109e1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109e1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109e1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x109e1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109e1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109e1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109e1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109e1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109e20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109e205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109e20a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109e20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109e21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109e21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109e22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109e224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109e22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109e22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109e23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109e236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109e23b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109e23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109e243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109e24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109e24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109e25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x109e255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109e25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109e25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109e26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109e26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109e26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109e27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x109e274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109e27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109e27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109e28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109e28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109e28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109e28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109e293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109e29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109e29cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109e2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109e2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109e2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109e2ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x109e2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109e2b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109e2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x109e2c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109e2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109e2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109e2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109e2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109e2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109e2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109e2df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109e2e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109e2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109e2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109e2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109e2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x109e2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109e2fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109e302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109e30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109e30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109e31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109e31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109e318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109e31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109e321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109e32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109e32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109e32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109e33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109e33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109e33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109e340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109e34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109e349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109e34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109e352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x109e35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109e35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109e36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109e36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109e371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109e37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109e37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109e37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109e38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109e387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109e38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109e390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109e39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109e399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109e39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109e3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109e3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109e3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109e3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x109e3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109e3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x109e3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109e3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109e3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109e3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109e3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109e3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109e3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109e3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109e3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109e3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109e3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109e3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109e3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x109e3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109e3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109e40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109e40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109e40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109e41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109e41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109e42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109e42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109e427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109e42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109e430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109e43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109e43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109e43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109e44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109e446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109e44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109e44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109e45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109e458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109e45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109e46180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109e465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109e46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109e46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109e47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109e477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109e47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109e48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109e48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109e48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109e48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109e49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109e496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109e49b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109e49fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109e4a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109e4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109e4acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109e4b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109e4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109e4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109e4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109e4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109e4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109e4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109e4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109e4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x109e4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x109e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109e4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x109e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x109e4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x109e4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x109e4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x109e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x109e50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x109e505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109e50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109e51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109e52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109e524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109e52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109e52da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109e53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109e53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109e53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109e53f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109e543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109e54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109e54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109e55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109e55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109e55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109e568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109e57000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109e57720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109e57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109e58100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109e58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109e58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109e59180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109e04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109e05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109e055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109e05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109e05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109e06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109e06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109e06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109e07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109e074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109e07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109e07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109e08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109e08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109e09770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109e09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x109e0a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x109e0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x109e0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x109e0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109e0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109e0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109e0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x109e0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109e0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x109e0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109e0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109e0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109e0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109e0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109e0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109e0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109e102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109e105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109e10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109e10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109e112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109e11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109e11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109e12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109e124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109e12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109e12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109e13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109e13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109e13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109e13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109e143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109e14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109e14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109e15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109e15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109e159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109e15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109e162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109e16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109e16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109e17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109e17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109e17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109e17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109e181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109e18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109e18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109e18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109e193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109e19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109e19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109e1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109e1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109e1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109e1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x109e1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109e1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109e1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109e1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109e1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109e1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109e1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109e1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109e1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109e1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109e1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109e1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x109e1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109e1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109e1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109e1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109e1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109e20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109e20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109e20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109e20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109e21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109e218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109e21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109e221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109e22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109e22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109e22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109e23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109e237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109e23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109e240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109e24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109e24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109e24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x109e25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109e256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109e25b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109e25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109e26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109e268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109e26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x109e27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109e275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109e27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109e27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109e28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109e287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109e28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109e29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109e29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109e29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109e2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109e2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109e2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x109e2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109e2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109e2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x109e2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109e2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109e2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109e2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109e2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109e2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109e2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109e2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109e2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109e2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109e2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109e2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x109e2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109e2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109e2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109e303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109e30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109e30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109e31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109e315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109e31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109e31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109e32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109e32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109e32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109e33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109e334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109e33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109e33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109e34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109e34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109e34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109e34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109e353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x109e35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109e35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109e36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109e36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109e36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109e372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109e37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109e37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109e38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109e384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109e38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109e391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109e39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109e39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109e39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109e3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109e3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109e3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109e3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x109e3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109e3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x109e3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109e3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109e3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109e3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109e3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109e3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109e3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109e3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109e3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109e3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109e3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x109e3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109e3fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109e400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109e40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109e409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109e40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109e412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109e41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109e41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109e42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109e42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109e42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109e43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109e434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109e43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109e43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109e44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109e44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109e44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109e44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109e453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109e45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109e45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109e46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109e46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109e46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109e46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109e472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109e47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109e47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109e48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109e484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109e48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109e48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109e491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109e49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109e49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109e49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109e4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109e4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109e4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109e4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109e4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109e4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109e4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109e4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109e4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109e4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109e4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109e4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x109e4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x109e4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109e4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x109e4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x109e4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x109e4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x109e4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x109e4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x109e4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x109e500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109e50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109e509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109e50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109e512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109e51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109e51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109e51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109e52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109e528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109e52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109e531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109e53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109e53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109e53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109e54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109e547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109e54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109e550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109e55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109e559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109e56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109e568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109e56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109e576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109e57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109e57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109e58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109e58890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.782s
user	0m0.296s
sys	0m0.304s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4398 (29df666d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143107590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143107ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143108250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143108800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143108db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143109360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143109910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143109ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14310a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14310a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14310ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14310b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14310be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14310c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14310ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14310d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14310dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14310e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14310ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14310f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14310f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1431100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143110800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1431110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1431117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143111a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143112090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143112d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143113240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143113500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1431139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143113c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1431144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143114a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143114cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143115190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143115630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143115ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143115f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143116410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1431168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143116d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1431171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143117690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143117950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143117f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143118570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143118e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1431194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143119ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14311a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14311a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14311ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14311b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14311bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14311bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14311c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14311c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14311ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14311d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14311d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14311dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14311e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14311e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14311ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14311eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14311f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14311f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14311fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143120140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1431205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143120a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143120f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143121470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1431219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143121f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143122460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1431229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143122f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143123450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1431239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143123ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143124440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143124990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143124ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143125430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143125980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143125ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143126420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143126970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143126ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143127410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143127960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143127eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143128400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143128950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143128ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143118b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143129310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143129ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14312a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14312a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14312aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14312b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14312b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14312baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14312bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14312c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14312ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14312cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14312d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14312da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14312dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14312e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14312e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14312edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14312f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14312f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14312fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143130030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1431304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143130970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143130e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1431312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143131750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143131bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143132090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143132530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1431329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143132e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143133310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1431337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143133c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1431340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143134590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143134a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143134ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143135370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143135810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143135cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143136150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1431365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143136a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143136f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1431373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143137870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143137d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1431381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143138650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143138f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143139430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1431398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143139d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14313a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14313a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14313ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14313aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14313b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14313b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14313bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14313c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14313c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14313cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14313d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14313d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14313d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14313de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14313e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14313e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14313ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14313f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14313f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14313f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14313fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143140330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1431407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143140c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143141110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1431415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143141a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143141ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143142390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143142830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143142cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143143170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143143610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143143ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143143f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1431443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143144890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143144d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1431451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143145720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143145c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1431461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143146710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1431469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143146fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1431475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143147c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1431483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143148890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143148b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143149160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143149770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143149f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14314a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14314a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14314ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14314b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14314ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14314bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14314c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14314ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14314cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14314d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14314da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14314df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14314e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14314ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14314ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14314f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14314fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14314ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1431504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1431509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143150f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143151490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1431519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143151f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143152480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1431529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143152f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143153470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1431539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143153f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143154460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1431549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143154f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143155450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1431559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143155ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143156440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143156990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143156ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143157430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143157980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143157ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143158420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143158970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143158ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143159410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143159960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143159eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14315a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14315a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14315aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14315b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14315b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14315be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14315c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14315c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14315ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14315d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14315d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14315de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14315e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14315e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14315ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14315f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14315f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14315fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14315fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143160370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143160810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143160cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143161150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1431615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143161a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143161f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1431623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143162920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143163040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143163760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143163e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1431645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143164860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143165050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143165310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143165920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1430053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1430069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1430072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1430090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14300a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14300a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14300ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14300b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14300bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14300c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14300cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14300d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14300d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14300e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14300e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14300e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14300eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14300ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14300f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14300f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14300fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1430101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1430111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1430123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1430130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1430139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1430142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141e08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141e08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141e08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141e09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141e096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141e0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141e0cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141e0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141e0da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141e0dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141e0e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141e0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141e0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141e0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141e0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141e0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141e10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141e106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141e10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141e11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141e114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141e11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141e11de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141e12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141e12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141e12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141e131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141e13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141e13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141e141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141e14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141e14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141e151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141e156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141e15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141e16190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141e166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141e16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141e17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141e176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141e17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141e18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141e186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141e18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141e19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141e196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141e1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141e1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141e1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141e1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141e1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141e1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141e1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141e1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141e1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141e1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141e1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141e1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141e1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141e1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141e1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141e1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141e1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141e1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141e20040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141e204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141e20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141e220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141e22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141e24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141e245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141e24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141e25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141e25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141e25cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141e26160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141e26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141e26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141e26f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141e273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141e27880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141e27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141e281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141e28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141e28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141e28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141e29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141e298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141e29d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141e2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141e2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141e2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141e2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141e2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141e2b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141e2bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141e2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141e2c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141e2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141e2d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141e2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141e2d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141e2de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141e2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141e2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141e2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141e2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141e2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141e2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141e2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141e30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141e307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141e30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141e31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141e315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141e31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141e31f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1430158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1430161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143016aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143016f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143017380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1430177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143017c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1430180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143018540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1430189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143018e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143019290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143019b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143019fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14301a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14301a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14301ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14301b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14301b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14301ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14301bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14301c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14301c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14301cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14301d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14301d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14301dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14301e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14301e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14301eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14301eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14301f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14301f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14301fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1430201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143020620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143020f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143021370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1430217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143021c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1430220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143022530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1430229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143022e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143023280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1430236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143023fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143024440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1430248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143024d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143025190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143025600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143025a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143025ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143026350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1430267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143026c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1430270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143027510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143027980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143027df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143028260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1430286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143028b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143028fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143029420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143029890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143029d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14302a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14302a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14302aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14302aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14302b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14302b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14302bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14302c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14302c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14302c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14302cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14302d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14302d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14302db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14302df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14302e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14302e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14302ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14302f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14302f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14302fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14302fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143030310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143030780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143030bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143031060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1430314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143031940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143031db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143032220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143032c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1430333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143033ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1430341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1430344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143034920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143034f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143035530 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141f07160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141f075d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141f07a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141f07eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141f08320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141f08790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141f08c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141f09070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141f094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141f09dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141f0a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141f0bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141f0c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141f0ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141f0d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141f0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141f0e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141f0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141f0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141f109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141f10c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141f11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141f11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141f119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141f11e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141f12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141f127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141f12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141f12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141f13390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141f13800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141f13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141f140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141f14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141f149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141f14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141f152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141f15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141f15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141f15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141f16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141f168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141f16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141f171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141f17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141f17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141f17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141f18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141f187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141f18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141f191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141f196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141f19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141f19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141f1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141f1a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141f1acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141f1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141f1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141f1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141f1beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141f1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141f1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141f1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141f1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141f1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141f1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141f1ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141f1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141f1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141f1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141f1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141f1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141f1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141f1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141f20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141f205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141f20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141f20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141f21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141f21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141f21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141f22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141f224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141f22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141f22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141f23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141f23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141f23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141f23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141f243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141f24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141f25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141f25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141f25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141f25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141f262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141f26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141f26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141f27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141f274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141f27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141f27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141f281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141f28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141f28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141f28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141f293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141f29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141f29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141f2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141f2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141f2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141f2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141f2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141f2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141f2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141f2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141f2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141f2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141f2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141f2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141f2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141f2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141f2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141f2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141f2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141f2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141f2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141f2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141f302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141f30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141f30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141f30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141f31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141f318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141f31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141f321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141f32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141f32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141f32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141f33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141f337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141f33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141f340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141f34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141f349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141f34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141f35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141f356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141f35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141f35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141f36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141f368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141f37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141f37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141f37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141f37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141f38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141f387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141f38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141f390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141f39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141f39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141f39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141f3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141f3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141f3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141f3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141f3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141f3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141f3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141f3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141f3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141f3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141f3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141f3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141f3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141f3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141f3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141f3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141f3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141f40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141f40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141f40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141f41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141f415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141f41a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141f41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141f42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141f42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141f42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141f43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141f435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141f43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141f445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141f44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141f44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141f45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141f45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141f45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141f46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141f465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141f46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141f46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141f47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141f47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141f47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141f48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141f484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141f48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141f48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141f49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141f496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141f49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141f49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141f4a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141f4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141f4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141f4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141f4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141f4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141f4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141f4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141f4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141f4d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141f4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141f4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141f4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141f4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141f4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141f4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141f4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141f4fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141f50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141f50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141f508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141f50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141f511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141f51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141f51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141f51f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141f52380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141f527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141f52c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141f530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141f53540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141f539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141f53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141f54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141f54700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141f54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141f54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141f55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141f558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141f55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141f561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141f56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141f56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141f56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141f57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141f577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141f57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141f580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141f58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141f58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141f596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141f59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141f5a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141f5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141f5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141f5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141f5b830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.940s
user	0m0.245s
sys	0m0.144s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.18 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
