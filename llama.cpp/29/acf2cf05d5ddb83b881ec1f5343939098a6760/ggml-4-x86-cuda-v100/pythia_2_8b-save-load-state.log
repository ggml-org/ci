+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.299 I build: 4912 (29acf2cf0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.264.894 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.280.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.280.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.280.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.280.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.280.956 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.280.957 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.280.958 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.280.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.280.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.280.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.280.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.280.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.280.969 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.280.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.280.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.280.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.280.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.287.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.289.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.297.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.297.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.297.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.297.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.297.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.297.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.297.060 I llama_model_loader: - type  f32:  258 tensors
0.00.297.060 I llama_model_loader: - type q4_0:  129 tensors
0.00.297.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.297.064 I print_info: file format = GGUF V3 (latest)
0.00.297.066 I print_info: file type   = Q4_0
0.00.297.070 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.344.121 I load: special tokens cache size = 25
0.00.366.361 I load: token to piece cache size = 0.2984 MB
0.00.366.382 I print_info: arch             = gptneox
0.00.366.383 I print_info: vocab_only       = 0
0.00.366.384 I print_info: n_ctx_train      = 2048
0.00.366.384 I print_info: n_embd           = 2560
0.00.366.384 I print_info: n_layer          = 32
0.00.366.405 I print_info: n_head           = 32
0.00.366.407 I print_info: n_head_kv        = 32
0.00.366.408 I print_info: n_rot            = 20
0.00.366.408 I print_info: n_swa            = 0
0.00.366.409 I print_info: n_swa_pattern    = 1
0.00.366.409 I print_info: n_embd_head_k    = 80
0.00.366.410 I print_info: n_embd_head_v    = 80
0.00.366.412 I print_info: n_gqa            = 1
0.00.366.414 I print_info: n_embd_k_gqa     = 2560
0.00.366.416 I print_info: n_embd_v_gqa     = 2560
0.00.366.418 I print_info: f_norm_eps       = 1.0e-05
0.00.366.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.366.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.366.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.366.421 I print_info: f_logit_scale    = 0.0e+00
0.00.366.422 I print_info: f_attn_scale     = 0.0e+00
0.00.366.423 I print_info: n_ff             = 10240
0.00.366.424 I print_info: n_expert         = 0
0.00.366.424 I print_info: n_expert_used    = 0
0.00.366.425 I print_info: causal attn      = 1
0.00.366.425 I print_info: pooling type     = 0
0.00.366.426 I print_info: rope type        = 2
0.00.366.427 I print_info: rope scaling     = linear
0.00.366.429 I print_info: freq_base_train  = 10000.0
0.00.366.430 I print_info: freq_scale_train = 1
0.00.366.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.366.431 I print_info: rope_finetuned   = unknown
0.00.366.432 I print_info: ssm_d_conv       = 0
0.00.366.432 I print_info: ssm_d_inner      = 0
0.00.366.432 I print_info: ssm_d_state      = 0
0.00.366.433 I print_info: ssm_dt_rank      = 0
0.00.366.433 I print_info: ssm_dt_b_c_rms   = 0
0.00.366.435 I print_info: model type       = 2.8B
0.00.366.436 I print_info: model params     = 2.78 B
0.00.366.438 I print_info: general.name     = 2.8B
0.00.366.440 I print_info: vocab type       = BPE
0.00.366.442 I print_info: n_vocab          = 50304
0.00.366.442 I print_info: n_merges         = 50009
0.00.366.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.366.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.366.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.366.445 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.366.446 I print_info: LF token         = 187 'Ċ'
0.00.366.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.366.447 I print_info: max token length = 1024
0.00.366.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.905 I load_tensors: offloading 10 repeating layers to GPU
0.00.457.917 I load_tensors: offloaded 10/33 layers to GPU
0.00.457.926 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.457.927 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.457.929 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.096.867 I llama_context: constructing llama_context
0.01.096.873 I llama_context: n_seq_max     = 1
0.01.096.873 I llama_context: n_ctx         = 2048
0.01.096.874 I llama_context: n_ctx_per_seq = 2048
0.01.096.874 I llama_context: n_batch       = 2048
0.01.096.875 I llama_context: n_ubatch      = 512
0.01.096.875 I llama_context: causal_attn   = 1
0.01.096.876 I llama_context: flash_attn    = 0
0.01.096.882 I llama_context: freq_base     = 10000.0
0.01.096.883 I llama_context: freq_scale    = 1
0.01.096.981 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.096.993 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.097.730 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.232.699 I init:        CPU KV buffer size =   440.00 MiB
0.01.232.729 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.261.051 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.261.064 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.261.064 I llama_context: graph nodes  = 1287
0.01.261.065 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.261.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.261.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.392.662 I llama_context: constructing llama_context
0.02.392.682 I llama_context: n_seq_max     = 1
0.02.392.682 I llama_context: n_ctx         = 2048
0.02.392.683 I llama_context: n_ctx_per_seq = 2048
0.02.392.683 I llama_context: n_batch       = 2048
0.02.392.684 I llama_context: n_ubatch      = 512
0.02.392.684 I llama_context: causal_attn   = 1
0.02.392.685 I llama_context: flash_attn    = 0
0.02.392.691 I llama_context: freq_base     = 10000.0
0.02.392.692 I llama_context: freq_scale    = 1
0.02.392.751 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.392.761 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.393.514 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.528.000 I init:        CPU KV buffer size =   440.00 MiB
0.02.528.025 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.556.166 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.556.176 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.556.177 I llama_context: graph nodes  = 1287
0.02.556.178 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.03.432.023 I llama_context: constructing llama_context
0.03.432.042 I llama_context: n_seq_max     = 1
0.03.432.043 I llama_context: n_ctx         = 2048
0.03.432.043 I llama_context: n_ctx_per_seq = 2048
0.03.432.044 I llama_context: n_batch       = 2048
0.03.432.044 I llama_context: n_ubatch      = 512
0.03.432.045 I llama_context: causal_attn   = 1
0.03.432.045 I llama_context: flash_attn    = 0
0.03.432.051 I llama_context: freq_base     = 10000.0
0.03.432.053 I llama_context: freq_scale    = 1
0.03.432.111 I llama_context:        CPU  output buffer size =     0.19 MiB
0.03.432.123 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.03.432.918 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.566.857 I init:        CPU KV buffer size =   440.00 MiB
0.03.566.879 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.594.887 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.594.898 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.594.899 I llama_context: graph nodes  = 1287
0.03.594.900 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


second run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


single seq run: The quick brown fox
            Gigot the wall from the wall,
            Scraped

real	0m5.257s
user	0m12.924s
sys	0m1.358s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.291 I build: 4912 (29acf2cf0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.261.361 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.277.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.277.392 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.277.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.277.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.277.408 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.277.409 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.277.410 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.277.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.277.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.277.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.277.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.277.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.277.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.277.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.277.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.277.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.277.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.284.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.286.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.292.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.292.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.292.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.292.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.292.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.292.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.292.852 I llama_model_loader: - type  f32:  258 tensors
0.00.292.853 I llama_model_loader: - type q4_0:  129 tensors
0.00.292.853 I llama_model_loader: - type q6_K:    1 tensors
0.00.292.856 I print_info: file format = GGUF V3 (latest)
0.00.292.857 I print_info: file type   = Q4_0
0.00.292.859 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.337.034 I load: special tokens cache size = 25
0.00.359.266 I load: token to piece cache size = 0.2984 MB
0.00.359.283 I print_info: arch             = gptneox
0.00.359.284 I print_info: vocab_only       = 0
0.00.359.285 I print_info: n_ctx_train      = 2048
0.00.359.286 I print_info: n_embd           = 2560
0.00.359.286 I print_info: n_layer          = 32
0.00.359.303 I print_info: n_head           = 32
0.00.359.305 I print_info: n_head_kv        = 32
0.00.359.305 I print_info: n_rot            = 20
0.00.359.306 I print_info: n_swa            = 0
0.00.359.307 I print_info: n_swa_pattern    = 1
0.00.359.308 I print_info: n_embd_head_k    = 80
0.00.359.308 I print_info: n_embd_head_v    = 80
0.00.359.310 I print_info: n_gqa            = 1
0.00.359.312 I print_info: n_embd_k_gqa     = 2560
0.00.359.317 I print_info: n_embd_v_gqa     = 2560
0.00.359.318 I print_info: f_norm_eps       = 1.0e-05
0.00.359.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.359.320 I print_info: f_clamp_kqv      = 0.0e+00
0.00.359.320 I print_info: f_max_alibi_bias = 0.0e+00
0.00.359.321 I print_info: f_logit_scale    = 0.0e+00
0.00.359.321 I print_info: f_attn_scale     = 0.0e+00
0.00.359.323 I print_info: n_ff             = 10240
0.00.359.324 I print_info: n_expert         = 0
0.00.359.324 I print_info: n_expert_used    = 0
0.00.359.325 I print_info: causal attn      = 1
0.00.359.325 I print_info: pooling type     = 0
0.00.359.325 I print_info: rope type        = 2
0.00.359.326 I print_info: rope scaling     = linear
0.00.359.327 I print_info: freq_base_train  = 10000.0
0.00.359.328 I print_info: freq_scale_train = 1
0.00.359.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.359.330 I print_info: rope_finetuned   = unknown
0.00.359.331 I print_info: ssm_d_conv       = 0
0.00.359.331 I print_info: ssm_d_inner      = 0
0.00.359.332 I print_info: ssm_d_state      = 0
0.00.359.332 I print_info: ssm_dt_rank      = 0
0.00.359.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.359.333 I print_info: model type       = 2.8B
0.00.359.334 I print_info: model params     = 2.78 B
0.00.359.335 I print_info: general.name     = 2.8B
0.00.359.337 I print_info: vocab type       = BPE
0.00.359.338 I print_info: n_vocab          = 50304
0.00.359.340 I print_info: n_merges         = 50009
0.00.359.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.359.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.359.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.359.352 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.359.354 I print_info: LF token         = 187 'Ċ'
0.00.359.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.359.355 I print_info: max token length = 1024
0.00.359.357 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.450.753 I load_tensors: offloading 10 repeating layers to GPU
0.00.450.763 I load_tensors: offloaded 10/33 layers to GPU
0.00.450.772 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.450.773 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.450.775 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.051.838 I llama_context: constructing llama_context
0.01.051.844 I llama_context: n_seq_max     = 1
0.01.051.844 I llama_context: n_ctx         = 2048
0.01.051.845 I llama_context: n_ctx_per_seq = 2048
0.01.051.845 I llama_context: n_batch       = 2048
0.01.051.846 I llama_context: n_ubatch      = 512
0.01.051.846 I llama_context: causal_attn   = 1
0.01.051.847 I llama_context: flash_attn    = 1
0.01.051.852 I llama_context: freq_base     = 10000.0
0.01.051.854 I llama_context: freq_scale    = 1
0.01.051.946 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.051.957 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.052.663 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.189.359 I init:        CPU KV buffer size =   440.00 MiB
0.01.189.395 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.217.627 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.217.639 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.217.640 I llama_context: graph nodes  = 1160
0.01.217.641 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.217.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.217.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.060.004 I llama_context: constructing llama_context
0.02.060.021 I llama_context: n_seq_max     = 1
0.02.060.022 I llama_context: n_ctx         = 2048
0.02.060.022 I llama_context: n_ctx_per_seq = 2048
0.02.060.023 I llama_context: n_batch       = 2048
0.02.060.023 I llama_context: n_ubatch      = 512
0.02.060.024 I llama_context: causal_attn   = 1
0.02.060.024 I llama_context: flash_attn    = 1
0.02.060.030 I llama_context: freq_base     = 10000.0
0.02.060.031 I llama_context: freq_scale    = 1
0.02.060.091 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.060.101 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.060.914 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.195.086 I init:        CPU KV buffer size =   440.00 MiB
0.02.195.111 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.222.732 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.222.745 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.222.746 I llama_context: graph nodes  = 1160
0.02.222.747 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.03.219.089 I llama_context: constructing llama_context
0.03.219.119 I llama_context: n_seq_max     = 1
0.03.219.120 I llama_context: n_ctx         = 2048
0.03.219.120 I llama_context: n_ctx_per_seq = 2048
0.03.219.121 I llama_context: n_batch       = 2048
0.03.219.121 I llama_context: n_ubatch      = 512
0.03.219.122 I llama_context: causal_attn   = 1
0.03.219.123 I llama_context: flash_attn    = 1
0.03.219.128 I llama_context: freq_base     = 10000.0
0.03.219.129 I llama_context: freq_scale    = 1
0.03.219.190 I llama_context:        CPU  output buffer size =     0.19 MiB
0.03.219.204 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.03.219.973 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.357.706 I init:        CPU KV buffer size =   440.00 MiB
0.03.357.729 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.385.592 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.385.606 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.385.607 I llama_context: graph nodes  = 1160
0.03.385.607 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


second run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


single seq run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the

real	0m4.456s
user	0m12.965s
sys	0m1.577s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.279 I build: 4912 (29acf2cf0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.248.348 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.264.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.264.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.264.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.264.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.264.455 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.264.457 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.264.458 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.264.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.264.463 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.264.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.264.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.264.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.264.467 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.264.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.264.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.264.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.264.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.271.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.273.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.283.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.283.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.283.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.283.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.283.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.283.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.283.285 I llama_model_loader: - type  f32:  258 tensors
0.00.283.304 I llama_model_loader: - type q4_0:  129 tensors
0.00.283.306 I llama_model_loader: - type q6_K:    1 tensors
0.00.283.309 I print_info: file format = GGUF V3 (latest)
0.00.283.310 I print_info: file type   = Q4_0
0.00.283.312 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.329.351 I load: special tokens cache size = 25
0.00.351.713 I load: token to piece cache size = 0.2984 MB
0.00.351.734 I print_info: arch             = gptneox
0.00.351.734 I print_info: vocab_only       = 0
0.00.351.735 I print_info: n_ctx_train      = 2048
0.00.351.736 I print_info: n_embd           = 2560
0.00.351.736 I print_info: n_layer          = 32
0.00.351.756 I print_info: n_head           = 32
0.00.351.758 I print_info: n_head_kv        = 32
0.00.351.758 I print_info: n_rot            = 20
0.00.351.758 I print_info: n_swa            = 0
0.00.351.759 I print_info: n_swa_pattern    = 1
0.00.351.760 I print_info: n_embd_head_k    = 80
0.00.351.761 I print_info: n_embd_head_v    = 80
0.00.351.764 I print_info: n_gqa            = 1
0.00.351.766 I print_info: n_embd_k_gqa     = 2560
0.00.351.771 I print_info: n_embd_v_gqa     = 2560
0.00.351.776 I print_info: f_norm_eps       = 1.0e-05
0.00.351.777 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.351.777 I print_info: f_clamp_kqv      = 0.0e+00
0.00.351.778 I print_info: f_max_alibi_bias = 0.0e+00
0.00.351.779 I print_info: f_logit_scale    = 0.0e+00
0.00.351.779 I print_info: f_attn_scale     = 0.0e+00
0.00.351.781 I print_info: n_ff             = 10240
0.00.351.781 I print_info: n_expert         = 0
0.00.351.782 I print_info: n_expert_used    = 0
0.00.351.782 I print_info: causal attn      = 1
0.00.351.785 I print_info: pooling type     = 0
0.00.351.785 I print_info: rope type        = 2
0.00.351.786 I print_info: rope scaling     = linear
0.00.351.787 I print_info: freq_base_train  = 10000.0
0.00.351.788 I print_info: freq_scale_train = 1
0.00.351.789 I print_info: n_ctx_orig_yarn  = 2048
0.00.351.789 I print_info: rope_finetuned   = unknown
0.00.351.789 I print_info: ssm_d_conv       = 0
0.00.351.790 I print_info: ssm_d_inner      = 0
0.00.351.790 I print_info: ssm_d_state      = 0
0.00.351.790 I print_info: ssm_dt_rank      = 0
0.00.351.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.351.792 I print_info: model type       = 2.8B
0.00.351.792 I print_info: model params     = 2.78 B
0.00.351.793 I print_info: general.name     = 2.8B
0.00.351.796 I print_info: vocab type       = BPE
0.00.351.797 I print_info: n_vocab          = 50304
0.00.351.797 I print_info: n_merges         = 50009
0.00.351.799 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.351.799 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.351.799 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.351.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.351.801 I print_info: LF token         = 187 'Ċ'
0.00.351.802 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.351.804 I print_info: max token length = 1024
0.00.351.806 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.762 I load_tensors: offloading 32 repeating layers to GPU
0.00.444.775 I load_tensors: offloading output layer to GPU
0.00.444.776 I load_tensors: offloaded 33/33 layers to GPU
0.00.444.786 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.444.788 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.705.104 I llama_context: constructing llama_context
0.00.705.111 I llama_context: n_seq_max     = 1
0.00.705.112 I llama_context: n_ctx         = 2048
0.00.705.112 I llama_context: n_ctx_per_seq = 2048
0.00.705.112 I llama_context: n_batch       = 2048
0.00.705.113 I llama_context: n_ubatch      = 512
0.00.705.113 I llama_context: causal_attn   = 1
0.00.705.114 I llama_context: flash_attn    = 0
0.00.705.120 I llama_context: freq_base     = 10000.0
0.00.705.121 I llama_context: freq_scale    = 1
0.00.706.464 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.706.481 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.707.641 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.707.655 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.726.350 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.00.726.359 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.726.360 I llama_context: graph nodes  = 1287
0.00.726.360 I llama_context: graph splits = 2
0.00.726.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.01.703.354 I llama_context: constructing llama_context
0.01.703.366 I llama_context: n_seq_max     = 1
0.01.703.366 I llama_context: n_ctx         = 2048
0.01.703.367 I llama_context: n_ctx_per_seq = 2048
0.01.703.367 I llama_context: n_batch       = 2048
0.01.703.368 I llama_context: n_ubatch      = 512
0.01.703.368 I llama_context: causal_attn   = 1
0.01.703.369 I llama_context: flash_attn    = 0
0.01.703.374 I llama_context: freq_base     = 10000.0
0.01.703.375 I llama_context: freq_scale    = 1
0.01.703.446 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.703.455 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.706.602 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.706.611 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.724.891 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.01.724.903 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.724.904 I llama_context: graph nodes  = 1287
0.01.724.905 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.02.422.126 I llama_context: constructing llama_context
0.02.422.139 I llama_context: n_seq_max     = 1
0.02.422.139 I llama_context: n_ctx         = 2048
0.02.422.140 I llama_context: n_ctx_per_seq = 2048
0.02.422.140 I llama_context: n_batch       = 2048
0.02.422.141 I llama_context: n_ubatch      = 512
0.02.422.141 I llama_context: causal_attn   = 1
0.02.422.141 I llama_context: flash_attn    = 0
0.02.422.148 I llama_context: freq_base     = 10000.0
0.02.422.149 I llama_context: freq_scale    = 1
0.02.422.223 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.02.422.229 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.423.760 I init:      CUDA0 KV buffer size =   640.00 MiB
0.02.423.772 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.440.916 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.02.440.927 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.02.440.927 I llama_context: graph nodes  = 1287
0.02.440.928 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


second run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


single seq run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st

real	0m4.613s
user	0m3.934s
sys	0m0.673s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.279 I build: 4912 (29acf2cf0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.252.171 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.269.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.269.505 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.269.515 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.269.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.269.518 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.269.519 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.269.524 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.269.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.269.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.269.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.269.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.269.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.269.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.269.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.269.549 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.269.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.269.550 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.276.335 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.278.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.284.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.284.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.284.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.284.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.284.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.284.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.284.908 I llama_model_loader: - type  f32:  258 tensors
0.00.284.908 I llama_model_loader: - type q4_0:  129 tensors
0.00.284.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.284.912 I print_info: file format = GGUF V3 (latest)
0.00.284.912 I print_info: file type   = Q4_0
0.00.284.915 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.334.090 I load: special tokens cache size = 25
0.00.356.254 I load: token to piece cache size = 0.2984 MB
0.00.356.271 I print_info: arch             = gptneox
0.00.356.271 I print_info: vocab_only       = 0
0.00.356.272 I print_info: n_ctx_train      = 2048
0.00.356.273 I print_info: n_embd           = 2560
0.00.356.273 I print_info: n_layer          = 32
0.00.356.289 I print_info: n_head           = 32
0.00.356.291 I print_info: n_head_kv        = 32
0.00.356.310 I print_info: n_rot            = 20
0.00.356.313 I print_info: n_swa            = 0
0.00.356.314 I print_info: n_swa_pattern    = 1
0.00.356.314 I print_info: n_embd_head_k    = 80
0.00.356.314 I print_info: n_embd_head_v    = 80
0.00.356.319 I print_info: n_gqa            = 1
0.00.356.321 I print_info: n_embd_k_gqa     = 2560
0.00.356.323 I print_info: n_embd_v_gqa     = 2560
0.00.356.326 I print_info: f_norm_eps       = 1.0e-05
0.00.356.341 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.356.342 I print_info: f_clamp_kqv      = 0.0e+00
0.00.356.342 I print_info: f_max_alibi_bias = 0.0e+00
0.00.356.344 I print_info: f_logit_scale    = 0.0e+00
0.00.356.344 I print_info: f_attn_scale     = 0.0e+00
0.00.356.348 I print_info: n_ff             = 10240
0.00.356.349 I print_info: n_expert         = 0
0.00.356.350 I print_info: n_expert_used    = 0
0.00.356.351 I print_info: causal attn      = 1
0.00.356.351 I print_info: pooling type     = 0
0.00.356.351 I print_info: rope type        = 2
0.00.356.352 I print_info: rope scaling     = linear
0.00.356.354 I print_info: freq_base_train  = 10000.0
0.00.356.354 I print_info: freq_scale_train = 1
0.00.356.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.356.355 I print_info: rope_finetuned   = unknown
0.00.356.356 I print_info: ssm_d_conv       = 0
0.00.356.357 I print_info: ssm_d_inner      = 0
0.00.356.357 I print_info: ssm_d_state      = 0
0.00.356.358 I print_info: ssm_dt_rank      = 0
0.00.356.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.356.360 I print_info: model type       = 2.8B
0.00.356.361 I print_info: model params     = 2.78 B
0.00.356.362 I print_info: general.name     = 2.8B
0.00.356.365 I print_info: vocab type       = BPE
0.00.356.366 I print_info: n_vocab          = 50304
0.00.356.367 I print_info: n_merges         = 50009
0.00.356.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.356.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.356.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.356.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.356.370 I print_info: LF token         = 187 'Ċ'
0.00.356.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.356.371 I print_info: max token length = 1024
0.00.356.377 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.448.150 I load_tensors: offloading 32 repeating layers to GPU
0.00.448.162 I load_tensors: offloading output layer to GPU
0.00.448.163 I load_tensors: offloaded 33/33 layers to GPU
0.00.448.173 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.448.175 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.705.565 I llama_context: constructing llama_context
0.00.705.573 I llama_context: n_seq_max     = 1
0.00.705.573 I llama_context: n_ctx         = 2048
0.00.705.574 I llama_context: n_ctx_per_seq = 2048
0.00.705.574 I llama_context: n_batch       = 2048
0.00.705.575 I llama_context: n_ubatch      = 512
0.00.705.575 I llama_context: causal_attn   = 1
0.00.705.576 I llama_context: flash_attn    = 1
0.00.705.582 I llama_context: freq_base     = 10000.0
0.00.705.583 I llama_context: freq_scale    = 1
0.00.706.938 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.706.956 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.708.088 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.708.103 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.725.219 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.725.229 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.725.230 I llama_context: graph nodes  = 1160
0.00.725.230 I llama_context: graph splits = 2
0.00.725.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.00.940.155 I llama_context: constructing llama_context
0.00.940.167 I llama_context: n_seq_max     = 1
0.00.940.168 I llama_context: n_ctx         = 2048
0.00.940.169 I llama_context: n_ctx_per_seq = 2048
0.00.940.169 I llama_context: n_batch       = 2048
0.00.940.169 I llama_context: n_ubatch      = 512
0.00.940.170 I llama_context: causal_attn   = 1
0.00.940.170 I llama_context: flash_attn    = 1
0.00.940.177 I llama_context: freq_base     = 10000.0
0.00.940.178 I llama_context: freq_scale    = 1
0.00.940.257 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.940.267 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.943.777 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.943.786 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.960.389 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.960.399 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.960.400 I llama_context: graph nodes  = 1160
0.00.960.401 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.01.123.093 I llama_context: constructing llama_context
0.01.123.103 I llama_context: n_seq_max     = 1
0.01.123.104 I llama_context: n_ctx         = 2048
0.01.123.104 I llama_context: n_ctx_per_seq = 2048
0.01.123.105 I llama_context: n_batch       = 2048
0.01.123.105 I llama_context: n_ubatch      = 512
0.01.123.106 I llama_context: causal_attn   = 1
0.01.123.106 I llama_context: flash_attn    = 1
0.01.123.110 I llama_context: freq_base     = 10000.0
0.01.123.111 I llama_context: freq_scale    = 1
0.01.123.181 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.123.190 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.126.444 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.126.455 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.142.687 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.01.142.697 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.142.698 I llama_context: graph nodes  = 1160
0.01.142.698 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


second run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


single seq run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see

real	0m1.572s
user	0m0.916s
sys	0m0.657s
