### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.51 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.89 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.93 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.63 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.02 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.42 sec*proc (29 tests)

Total Test time (real) = 254.43 sec

real	4m14.457s
user	8m32.356s
sys	0m7.172s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.77 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.37 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.92 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.50 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.93 sec*proc (29 tests)

Total Test time (real) =  54.94 sec

real	0m54.951s
user	1m16.938s
sys	0m6.405s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.110 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.098 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.109 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.111 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.112 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.113 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.114 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.115 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.119 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.120 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.120 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.123 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.124 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.125 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.125 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.126 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.127 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.127 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.666 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.668 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.669 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.670 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.670 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.671 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.671 I llama_model_loader: - type  f32:  124 tensors
0.00.029.672 I llama_model_loader: - type  f16:   73 tensors
0.00.029.673 I print_info: file format = GGUF V3 (latest)
0.00.029.674 I print_info: file type   = F16
0.00.029.675 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.034.470 I load: special tokens cache size = 5
0.00.036.939 I load: token to piece cache size = 0.2032 MB
0.00.036.943 I print_info: arch             = bert
0.00.036.944 I print_info: vocab_only       = 0
0.00.036.944 I print_info: n_ctx_train      = 512
0.00.036.944 I print_info: n_embd           = 384
0.00.036.944 I print_info: n_layer          = 12
0.00.036.948 I print_info: n_head           = 12
0.00.036.949 I print_info: n_head_kv        = 12
0.00.036.949 I print_info: n_rot            = 32
0.00.036.949 I print_info: n_swa            = 0
0.00.036.949 I print_info: n_embd_head_k    = 32
0.00.036.950 I print_info: n_embd_head_v    = 32
0.00.036.951 I print_info: n_gqa            = 1
0.00.036.952 I print_info: n_embd_k_gqa     = 384
0.00.036.955 I print_info: n_embd_v_gqa     = 384
0.00.036.956 I print_info: f_norm_eps       = 1.0e-12
0.00.036.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.958 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.959 I print_info: f_logit_scale    = 0.0e+00
0.00.036.959 I print_info: n_ff             = 1536
0.00.036.960 I print_info: n_expert         = 0
0.00.036.962 I print_info: n_expert_used    = 0
0.00.036.962 I print_info: causal attn      = 0
0.00.036.962 I print_info: pooling type     = 2
0.00.036.963 I print_info: rope type        = 2
0.00.036.963 I print_info: rope scaling     = linear
0.00.036.964 I print_info: freq_base_train  = 10000.0
0.00.036.964 I print_info: freq_scale_train = 1
0.00.036.964 I print_info: n_ctx_orig_yarn  = 512
0.00.036.965 I print_info: rope_finetuned   = unknown
0.00.036.965 I print_info: ssm_d_conv       = 0
0.00.036.965 I print_info: ssm_d_inner      = 0
0.00.036.965 I print_info: ssm_d_state      = 0
0.00.036.965 I print_info: ssm_dt_rank      = 0
0.00.036.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.972 I print_info: model type       = 33M
0.00.036.973 I print_info: model params     = 33.21 M
0.00.036.973 I print_info: general.name     = Bge Small
0.00.036.974 I print_info: vocab type       = WPM
0.00.036.975 I print_info: n_vocab          = 30522
0.00.036.975 I print_info: n_merges         = 0
0.00.036.975 I print_info: BOS token        = 101 '[CLS]'
0.00.036.975 I print_info: UNK token        = 100 '[UNK]'
0.00.036.976 I print_info: SEP token        = 102 '[SEP]'
0.00.036.976 I print_info: PAD token        = 0 '[PAD]'
0.00.036.976 I print_info: MASK token       = 103 '[MASK]'
0.00.036.977 I print_info: LF token         = 0 '[PAD]'
0.00.036.977 I print_info: max token length = 21
0.00.036.978 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.527 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.529 I load_tensors: offloading output layer to GPU
0.00.040.529 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.556 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.558 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.839 I llama_init_from_model: n_seq_max     = 1
0.00.040.840 I llama_init_from_model: n_ctx         = 512
0.00.040.841 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.841 I llama_init_from_model: n_batch       = 2048
0.00.040.841 I llama_init_from_model: n_ubatch      = 2048
0.00.040.841 I llama_init_from_model: flash_attn    = 0
0.00.040.842 I llama_init_from_model: freq_base     = 10000.0
0.00.040.843 I llama_init_from_model: freq_scale    = 1
0.00.040.843 I ggml_metal_init: allocating
0.00.040.849 I ggml_metal_init: found device: Apple M4
0.00.040.856 I ggml_metal_init: picking default device: Apple M4
0.00.041.720 I ggml_metal_init: using embedded metal library
0.00.046.062 I ggml_metal_init: GPU name:   Apple M4
0.00.046.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.066 I ggml_metal_init: simdgroup reduction   = true
0.00.046.066 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.066 I ggml_metal_init: has residency sets    = true
0.00.046.067 I ggml_metal_init: has bfloat            = true
0.00.046.067 I ggml_metal_init: use bfloat            = true
0.00.046.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.821 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.527 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.529 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.551 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.790 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.791 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.791 I llama_init_from_model: graph nodes  = 429
0.00.060.792 I llama_init_from_model: graph splits = 2
0.00.060.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.502 I 
0.00.066.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.233 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.506 I llama_perf_context_print:        load time =      49.02 ms
0.00.072.507 I llama_perf_context_print: prompt eval time =       5.13 ms /     9 tokens (    0.57 ms per token,  1755.75 tokens per second)
0.00.072.509 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.510 I llama_perf_context_print:       total time =       6.00 ms /    10 tokens
0.00.072.662 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.053s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.043 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.156 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.162 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.164 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.164 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.164 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.165 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.166 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.166 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.166 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.169 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.171 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.171 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.172 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.172 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.172 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.173 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.635 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.298 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.299 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.299 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.300 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.300 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.300 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.301 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.301 I llama_model_loader: - type  f32:  124 tensors
0.00.015.301 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.302 I print_info: file format = GGUF V3 (latest)
0.00.015.302 I print_info: file type   = Q8_0
0.00.015.307 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.704 I load: special tokens cache size = 5
0.00.018.890 I load: token to piece cache size = 0.2032 MB
0.00.018.892 I print_info: arch             = bert
0.00.018.893 I print_info: vocab_only       = 0
0.00.018.893 I print_info: n_ctx_train      = 512
0.00.018.893 I print_info: n_embd           = 384
0.00.018.893 I print_info: n_layer          = 12
0.00.018.896 I print_info: n_head           = 12
0.00.018.897 I print_info: n_head_kv        = 12
0.00.018.897 I print_info: n_rot            = 32
0.00.018.897 I print_info: n_swa            = 0
0.00.018.897 I print_info: n_embd_head_k    = 32
0.00.018.898 I print_info: n_embd_head_v    = 32
0.00.018.898 I print_info: n_gqa            = 1
0.00.018.899 I print_info: n_embd_k_gqa     = 384
0.00.018.899 I print_info: n_embd_v_gqa     = 384
0.00.018.900 I print_info: f_norm_eps       = 1.0e-12
0.00.018.900 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.901 I print_info: f_logit_scale    = 0.0e+00
0.00.018.901 I print_info: n_ff             = 1536
0.00.018.902 I print_info: n_expert         = 0
0.00.018.902 I print_info: n_expert_used    = 0
0.00.018.902 I print_info: causal attn      = 0
0.00.018.902 I print_info: pooling type     = 2
0.00.018.902 I print_info: rope type        = 2
0.00.018.902 I print_info: rope scaling     = linear
0.00.018.903 I print_info: freq_base_train  = 10000.0
0.00.018.903 I print_info: freq_scale_train = 1
0.00.018.903 I print_info: n_ctx_orig_yarn  = 512
0.00.018.903 I print_info: rope_finetuned   = unknown
0.00.018.904 I print_info: ssm_d_conv       = 0
0.00.018.904 I print_info: ssm_d_inner      = 0
0.00.018.904 I print_info: ssm_d_state      = 0
0.00.018.907 I print_info: ssm_dt_rank      = 0
0.00.018.907 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.907 I print_info: model type       = 33M
0.00.018.907 I print_info: model params     = 33.21 M
0.00.018.907 I print_info: general.name     = Bge Small
0.00.018.908 I print_info: vocab type       = WPM
0.00.018.908 I print_info: n_vocab          = 30522
0.00.018.908 I print_info: n_merges         = 0
0.00.018.909 I print_info: BOS token        = 101 '[CLS]'
0.00.018.909 I print_info: UNK token        = 100 '[UNK]'
0.00.018.909 I print_info: SEP token        = 102 '[SEP]'
0.00.018.909 I print_info: PAD token        = 0 '[PAD]'
0.00.018.909 I print_info: MASK token       = 103 '[MASK]'
0.00.018.909 I print_info: LF token         = 0 '[PAD]'
0.00.018.910 I print_info: max token length = 21
0.00.018.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.633 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.634 I load_tensors: offloading output layer to GPU
0.00.020.634 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.641 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.642 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.812 I llama_init_from_model: n_seq_max     = 1
0.00.020.813 I llama_init_from_model: n_ctx         = 512
0.00.020.813 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.813 I llama_init_from_model: n_batch       = 2048
0.00.020.813 I llama_init_from_model: n_ubatch      = 2048
0.00.020.814 I llama_init_from_model: flash_attn    = 0
0.00.020.814 I llama_init_from_model: freq_base     = 10000.0
0.00.020.814 I llama_init_from_model: freq_scale    = 1
0.00.020.815 I ggml_metal_init: allocating
0.00.020.818 I ggml_metal_init: found device: Apple M4
0.00.020.822 I ggml_metal_init: picking default device: Apple M4
0.00.021.348 I ggml_metal_init: using embedded metal library
0.00.023.877 I ggml_metal_init: GPU name:   Apple M4
0.00.023.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.880 I ggml_metal_init: simdgroup reduction   = true
0.00.023.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.880 I ggml_metal_init: has residency sets    = true
0.00.023.880 I ggml_metal_init: has bfloat            = true
0.00.023.881 I ggml_metal_init: use bfloat            = true
0.00.023.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.085 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.681 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.683 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.698 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.627 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.628 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.628 I llama_init_from_model: graph nodes  = 429
0.00.035.629 I llama_init_from_model: graph splits = 2
0.00.035.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.827 I 
0.00.039.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.382 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.815 I llama_perf_context_print:        load time =      30.37 ms
0.00.044.816 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2096.92 tokens per second)
0.00.044.817 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.818 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.045.030 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.254 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.577 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.067 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.074 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.083 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.084 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.084 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.085 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.086 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.087 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.087 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.088 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.091 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.091 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.092 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.885 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.885 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.886 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.886 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.886 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.887 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.887 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.888 I llama_model_loader: - type  f32:   40 tensors
0.00.047.888 I llama_model_loader: - type  f16:   30 tensors
0.00.047.893 I print_info: file format = GGUF V3 (latest)
0.00.047.894 I print_info: file type   = F16
0.00.047.895 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.056 W load: empty token at index 5
0.00.056.957 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.361 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.395 I load: special tokens cache size = 5
0.00.321.739 I load: token to piece cache size = 1.5060 MB
0.00.321.745 I print_info: arch             = jina-bert-v2
0.00.321.745 I print_info: vocab_only       = 0
0.00.321.746 I print_info: n_ctx_train      = 8192
0.00.321.751 I print_info: n_embd           = 384
0.00.321.752 I print_info: n_layer          = 4
0.00.321.758 I print_info: n_head           = 12
0.00.321.758 I print_info: n_head_kv        = 12
0.00.321.758 I print_info: n_rot            = 32
0.00.321.759 I print_info: n_swa            = 0
0.00.321.759 I print_info: n_embd_head_k    = 32
0.00.321.759 I print_info: n_embd_head_v    = 32
0.00.321.760 I print_info: n_gqa            = 1
0.00.321.760 I print_info: n_embd_k_gqa     = 384
0.00.321.761 I print_info: n_embd_v_gqa     = 384
0.00.321.764 I print_info: f_norm_eps       = 1.0e-12
0.00.321.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.321.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.321.765 I print_info: f_max_alibi_bias = 8.0e+00
0.00.321.765 I print_info: f_logit_scale    = 0.0e+00
0.00.321.765 I print_info: n_ff             = 1536
0.00.321.766 I print_info: n_expert         = 0
0.00.321.767 I print_info: n_expert_used    = 0
0.00.321.767 I print_info: causal attn      = 0
0.00.321.767 I print_info: pooling type     = -1
0.00.321.767 I print_info: rope type        = -1
0.00.321.767 I print_info: rope scaling     = linear
0.00.321.768 I print_info: freq_base_train  = 10000.0
0.00.321.768 I print_info: freq_scale_train = 1
0.00.321.768 I print_info: n_ctx_orig_yarn  = 8192
0.00.321.768 I print_info: rope_finetuned   = unknown
0.00.321.768 I print_info: ssm_d_conv       = 0
0.00.321.768 I print_info: ssm_d_inner      = 0
0.00.321.770 I print_info: ssm_d_state      = 0
0.00.321.770 I print_info: ssm_dt_rank      = 0
0.00.321.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.321.770 I print_info: model type       = 33M
0.00.321.771 I print_info: model params     = 32.90 M
0.00.321.771 I print_info: general.name     = Jina Bert Implementation
0.00.321.772 I print_info: vocab type       = BPE
0.00.321.772 I print_info: n_vocab          = 61056
0.00.321.772 I print_info: n_merges         = 39382
0.00.321.773 I print_info: BOS token        = 0 '<s>'
0.00.321.775 I print_info: EOS token        = 2 '</s>'
0.00.321.776 I print_info: UNK token        = 3 '<unk>'
0.00.321.776 I print_info: SEP token        = 2 '</s>'
0.00.321.776 I print_info: PAD token        = 1 '<pad>'
0.00.321.776 I print_info: MASK token       = 4 '<mask>'
0.00.321.777 I print_info: EOG token        = 2 '</s>'
0.00.321.777 I print_info: max token length = 45
0.00.321.777 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.323.914 I load_tensors: offloading 4 repeating layers to GPU
0.00.323.915 I load_tensors: offloading output layer to GPU
0.00.323.915 I load_tensors: offloaded 5/5 layers to GPU
0.00.323.939 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.323.940 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.258 I llama_init_from_model: n_seq_max     = 1
0.00.324.259 I llama_init_from_model: n_ctx         = 8192
0.00.324.259 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.259 I llama_init_from_model: n_batch       = 2048
0.00.324.259 I llama_init_from_model: n_ubatch      = 2048
0.00.324.260 I llama_init_from_model: flash_attn    = 0
0.00.324.260 I llama_init_from_model: freq_base     = 10000.0
0.00.324.260 I llama_init_from_model: freq_scale    = 1
0.00.324.261 I ggml_metal_init: allocating
0.00.324.264 I ggml_metal_init: found device: Apple M4
0.00.324.268 I ggml_metal_init: picking default device: Apple M4
0.00.325.121 I ggml_metal_init: using embedded metal library
0.00.327.584 I ggml_metal_init: GPU name:   Apple M4
0.00.327.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.586 I ggml_metal_init: simdgroup reduction   = true
0.00.327.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.587 I ggml_metal_init: has residency sets    = true
0.00.327.587 I ggml_metal_init: has bfloat            = true
0.00.327.587 I ggml_metal_init: use bfloat            = true
0.00.327.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.336.961 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.339.942 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.339.943 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.339.964 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.346.474 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.346.475 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.346.476 I llama_init_from_model: graph nodes  = 154
0.00.346.476 I llama_init_from_model: graph splits = 2
0.00.346.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.346.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.353.855 I 
0.00.353.890 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.353.989 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.353.990 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.353.993 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.353.993 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.353.996 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.353.996 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.354.503 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.358.210 I llama_perf_context_print:        load time =     332.24 ms
0.00.358.211 I llama_perf_context_print: prompt eval time =       3.70 ms /    62 tokens (    0.06 ms per token, 16765.82 tokens per second)
0.00.358.214 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.215 I llama_perf_context_print:       total time =       4.35 ms /    63 tokens
0.00.358.473 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.329s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.176 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.356 I main: llama backend init
0.00.000.363 I main: load the model and apply lora adapter, if any
0.00.054.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.728 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.075.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.082.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.477 I llama_model_loader: - type  f32:  194 tensors
0.00.082.478 I llama_model_loader: - type  f16:   98 tensors
0.00.082.480 I print_info: file format = GGUF V3 (latest)
0.00.082.486 I print_info: file type   = all F32 (guessed)
0.00.082.489 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.101.000 I load: special tokens cache size = 25
0.00.111.622 I load: token to piece cache size = 0.2984 MB
0.00.111.628 I print_info: arch             = gptneox
0.00.111.628 I print_info: vocab_only       = 0
0.00.111.628 I print_info: n_ctx_train      = 2048
0.00.111.628 I print_info: n_embd           = 2048
0.00.111.628 I print_info: n_layer          = 24
0.00.111.634 I print_info: n_head           = 16
0.00.111.635 I print_info: n_head_kv        = 16
0.00.111.636 I print_info: n_rot            = 32
0.00.111.639 I print_info: n_swa            = 0
0.00.111.639 I print_info: n_embd_head_k    = 128
0.00.111.640 I print_info: n_embd_head_v    = 128
0.00.111.640 I print_info: n_gqa            = 1
0.00.111.641 I print_info: n_embd_k_gqa     = 2048
0.00.111.642 I print_info: n_embd_v_gqa     = 2048
0.00.111.643 I print_info: f_norm_eps       = 1.0e-05
0.00.111.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.644 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.644 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.644 I print_info: f_logit_scale    = 0.0e+00
0.00.111.645 I print_info: n_ff             = 8192
0.00.111.646 I print_info: n_expert         = 0
0.00.111.646 I print_info: n_expert_used    = 0
0.00.111.646 I print_info: causal attn      = 1
0.00.111.646 I print_info: pooling type     = 0
0.00.111.646 I print_info: rope type        = 2
0.00.111.647 I print_info: rope scaling     = linear
0.00.111.647 I print_info: freq_base_train  = 10000.0
0.00.111.648 I print_info: freq_scale_train = 1
0.00.111.648 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.648 I print_info: rope_finetuned   = unknown
0.00.111.649 I print_info: ssm_d_conv       = 0
0.00.111.649 I print_info: ssm_d_inner      = 0
0.00.111.649 I print_info: ssm_d_state      = 0
0.00.111.654 I print_info: ssm_dt_rank      = 0
0.00.111.654 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.655 I print_info: model type       = 1.4B
0.00.111.655 I print_info: model params     = 1.41 B
0.00.111.655 I print_info: general.name     = 1.4B
0.00.111.656 I print_info: vocab type       = BPE
0.00.111.657 I print_info: n_vocab          = 50304
0.00.111.657 I print_info: n_merges         = 50009
0.00.111.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.657 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.658 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.658 I print_info: LF token         = 187 ''
0.00.111.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.659 I print_info: max token length = 1024
0.00.111.659 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.162.523 I load_tensors: offloading 24 repeating layers to GPU
0.00.162.526 I load_tensors: offloading output layer to GPU
0.00.162.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.162.552 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.162.553 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.163.178 I llama_init_from_model: n_seq_max     = 1
0.00.163.179 I llama_init_from_model: n_ctx         = 2048
0.00.163.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.163.179 I llama_init_from_model: n_batch       = 2048
0.00.163.180 I llama_init_from_model: n_ubatch      = 512
0.00.163.180 I llama_init_from_model: flash_attn    = 0
0.00.163.180 I llama_init_from_model: freq_base     = 10000.0
0.00.163.181 I llama_init_from_model: freq_scale    = 1
0.00.163.181 I ggml_metal_init: allocating
0.00.163.207 I ggml_metal_init: found device: Apple M4
0.00.163.212 I ggml_metal_init: picking default device: Apple M4
0.00.163.854 I ggml_metal_init: using embedded metal library
0.00.196.331 I ggml_metal_init: GPU name:   Apple M4
0.00.196.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.196.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.196.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.196.334 I ggml_metal_init: simdgroup reduction   = true
0.00.196.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.196.334 I ggml_metal_init: has residency sets    = true
0.00.196.334 I ggml_metal_init: has bfloat            = true
0.00.196.335 I ggml_metal_init: use bfloat            = true
0.00.196.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.196.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.405.532 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.405.539 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.405.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.409.250 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.409.253 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.409.253 I llama_init_from_model: graph nodes  = 967
0.00.409.253 I llama_init_from_model: graph splits = 2
0.00.409.261 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.409.388 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.409.389 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.169 I main: llama threadpool init, n_threads = 4
0.00.476.214 I 
0.00.476.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.249 I 
0.00.476.390 I sampler seed: 1234
0.00.476.395 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.476.419 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.476.421 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.476.421 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.311.746 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.02.311.746 I llama_perf_context_print:        load time =     420.94 ms
0.02.311.747 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.49 tokens per second)
0.02.311.748 I llama_perf_context_print:        eval time =    1788.85 ms /    63 runs   (   28.39 ms per token,    35.22 tokens per second)
0.02.311.748 I llama_perf_context_print:       total time =    1836.46 ms /    70 tokens
0.02.311.964 I ggml_metal_free: deallocating

real	0m2.630s
user	0m0.141s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.603 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.029.082 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.189 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.741 I llama_model_loader: - type  f32:  194 tensors
0.00.061.741 I llama_model_loader: - type  f16:   98 tensors
0.00.061.742 I print_info: file format = GGUF V3 (latest)
0.00.061.743 I print_info: file type   = all F32 (guessed)
0.00.061.744 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.074.890 I load: special tokens cache size = 25
0.00.083.127 I load: token to piece cache size = 0.2984 MB
0.00.083.131 I print_info: arch             = gptneox
0.00.083.131 I print_info: vocab_only       = 0
0.00.083.131 I print_info: n_ctx_train      = 2048
0.00.083.131 I print_info: n_embd           = 2048
0.00.083.131 I print_info: n_layer          = 24
0.00.083.135 I print_info: n_head           = 16
0.00.083.136 I print_info: n_head_kv        = 16
0.00.083.136 I print_info: n_rot            = 32
0.00.083.136 I print_info: n_swa            = 0
0.00.083.136 I print_info: n_embd_head_k    = 128
0.00.083.136 I print_info: n_embd_head_v    = 128
0.00.083.137 I print_info: n_gqa            = 1
0.00.083.138 I print_info: n_embd_k_gqa     = 2048
0.00.083.140 I print_info: n_embd_v_gqa     = 2048
0.00.083.140 I print_info: f_norm_eps       = 1.0e-05
0.00.083.141 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.083.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.083.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.083.143 I print_info: f_logit_scale    = 0.0e+00
0.00.083.144 I print_info: n_ff             = 8192
0.00.083.144 I print_info: n_expert         = 0
0.00.083.144 I print_info: n_expert_used    = 0
0.00.083.144 I print_info: causal attn      = 1
0.00.083.144 I print_info: pooling type     = 0
0.00.083.144 I print_info: rope type        = 2
0.00.083.145 I print_info: rope scaling     = linear
0.00.083.145 I print_info: freq_base_train  = 10000.0
0.00.083.146 I print_info: freq_scale_train = 1
0.00.083.146 I print_info: n_ctx_orig_yarn  = 2048
0.00.083.146 I print_info: rope_finetuned   = unknown
0.00.083.146 I print_info: ssm_d_conv       = 0
0.00.083.146 I print_info: ssm_d_inner      = 0
0.00.083.146 I print_info: ssm_d_state      = 0
0.00.083.147 I print_info: ssm_dt_rank      = 0
0.00.083.147 I print_info: ssm_dt_b_c_rms   = 0
0.00.083.147 I print_info: model type       = 1.4B
0.00.083.147 I print_info: model params     = 1.41 B
0.00.083.148 I print_info: general.name     = 1.4B
0.00.083.153 I print_info: vocab type       = BPE
0.00.083.153 I print_info: n_vocab          = 50304
0.00.083.153 I print_info: n_merges         = 50009
0.00.083.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.083.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.083.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.083.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.083.155 I print_info: LF token         = 187 ''
0.00.083.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.083.155 I print_info: max token length = 1024
0.00.083.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.428.677 I load_tensors: offloading 24 repeating layers to GPU
0.01.428.680 I load_tensors: offloading output layer to GPU
0.01.428.681 I load_tensors: offloaded 25/25 layers to GPU
0.01.428.706 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.428.708 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.429.904 I llama_init_from_model: n_seq_max     = 1
0.01.429.906 I llama_init_from_model: n_ctx         = 128
0.01.429.906 I llama_init_from_model: n_ctx_per_seq = 128
0.01.429.906 I llama_init_from_model: n_batch       = 128
0.01.429.906 I llama_init_from_model: n_ubatch      = 128
0.01.429.907 I llama_init_from_model: flash_attn    = 0
0.01.429.907 I llama_init_from_model: freq_base     = 10000.0
0.01.429.907 I llama_init_from_model: freq_scale    = 1
0.01.429.908 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.429.909 I ggml_metal_init: allocating
0.01.429.966 I ggml_metal_init: found device: Apple M4
0.01.429.973 I ggml_metal_init: picking default device: Apple M4
0.01.431.087 I ggml_metal_init: using embedded metal library
0.01.434.893 I ggml_metal_init: GPU name:   Apple M4
0.01.434.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.434.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.434.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.434.897 I ggml_metal_init: simdgroup reduction   = true
0.01.434.897 I ggml_metal_init: simdgroup matrix mul. = true
0.01.434.897 I ggml_metal_init: has residency sets    = true
0.01.434.897 I ggml_metal_init: has bfloat            = true
0.01.434.897 I ggml_metal_init: use bfloat            = true
0.01.434.898 I ggml_metal_init: hasUnifiedMemory      = true
0.01.434.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.445.422 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.447.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.447.128 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.447.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.448.786 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.448.787 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.448.787 I llama_init_from_model: graph nodes  = 967
0.01.448.788 I llama_init_from_model: graph splits = 2
0.01.448.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.448.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.484.198 I 
0.01.484.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.484.244 I perplexity: tokenizing the input ..
0.01.489.425 I perplexity: tokenization took 5.179 ms
0.01.489.430 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.607.931 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.609.273 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.304 I llama_perf_context_print:        load time =    1455.09 ms
0.01.609.306 I llama_perf_context_print: prompt eval time =     118.19 ms /   128 tokens (    0.92 ms per token,  1082.97 tokens per second)
0.01.609.306 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.307 I llama_perf_context_print:       total time =     125.11 ms /   129 tokens
0.01.609.694 I ggml_metal_free: deallocating

real	0m1.796s
user	0m0.100s
sys	0m0.262s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.035.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.943 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.944 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.946 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.947 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.045.148 I llama_model_loader: - type  f32:  194 tensors
0.00.045.149 I llama_model_loader: - type q8_0:   98 tensors
0.00.045.150 I print_info: file format = GGUF V3 (latest)
0.00.045.151 I print_info: file type   = Q8_0
0.00.045.151 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.151 I load: special tokens cache size = 25
0.00.062.554 I load: token to piece cache size = 0.2984 MB
0.00.062.559 I print_info: arch             = gptneox
0.00.062.559 I print_info: vocab_only       = 0
0.00.062.560 I print_info: n_ctx_train      = 2048
0.00.062.560 I print_info: n_embd           = 2048
0.00.062.560 I print_info: n_layer          = 24
0.00.062.565 I print_info: n_head           = 16
0.00.062.568 I print_info: n_head_kv        = 16
0.00.062.568 I print_info: n_rot            = 32
0.00.062.569 I print_info: n_swa            = 0
0.00.062.569 I print_info: n_embd_head_k    = 128
0.00.062.569 I print_info: n_embd_head_v    = 128
0.00.062.570 I print_info: n_gqa            = 1
0.00.062.571 I print_info: n_embd_k_gqa     = 2048
0.00.062.571 I print_info: n_embd_v_gqa     = 2048
0.00.062.572 I print_info: f_norm_eps       = 1.0e-05
0.00.062.572 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.573 I print_info: f_logit_scale    = 0.0e+00
0.00.062.574 I print_info: n_ff             = 8192
0.00.062.574 I print_info: n_expert         = 0
0.00.062.574 I print_info: n_expert_used    = 0
0.00.062.574 I print_info: causal attn      = 1
0.00.062.574 I print_info: pooling type     = 0
0.00.062.575 I print_info: rope type        = 2
0.00.062.575 I print_info: rope scaling     = linear
0.00.062.575 I print_info: freq_base_train  = 10000.0
0.00.062.576 I print_info: freq_scale_train = 1
0.00.062.576 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.576 I print_info: rope_finetuned   = unknown
0.00.062.579 I print_info: ssm_d_conv       = 0
0.00.062.579 I print_info: ssm_d_inner      = 0
0.00.062.579 I print_info: ssm_d_state      = 0
0.00.062.579 I print_info: ssm_dt_rank      = 0
0.00.062.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.579 I print_info: model type       = 1.4B
0.00.062.580 I print_info: model params     = 1.41 B
0.00.062.580 I print_info: general.name     = 1.4B
0.00.062.581 I print_info: vocab type       = BPE
0.00.062.581 I print_info: n_vocab          = 50304
0.00.062.581 I print_info: n_merges         = 50009
0.00.062.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.587 I print_info: LF token         = 187 ''
0.00.062.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.587 I print_info: max token length = 1024
0.00.062.588 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.059.518 I load_tensors: offloading 24 repeating layers to GPU
0.01.059.524 I load_tensors: offloading output layer to GPU
0.01.059.525 I load_tensors: offloaded 25/25 layers to GPU
0.01.059.549 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.059.550 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.060.748 I llama_init_from_model: n_seq_max     = 1
0.01.060.749 I llama_init_from_model: n_ctx         = 2048
0.01.060.749 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.060.750 I llama_init_from_model: n_batch       = 2048
0.01.060.750 I llama_init_from_model: n_ubatch      = 512
0.01.060.750 I llama_init_from_model: flash_attn    = 0
0.01.060.751 I llama_init_from_model: freq_base     = 10000.0
0.01.060.751 I llama_init_from_model: freq_scale    = 1
0.01.060.752 I ggml_metal_init: allocating
0.01.060.766 I ggml_metal_init: found device: Apple M4
0.01.060.774 I ggml_metal_init: picking default device: Apple M4
0.01.062.067 I ggml_metal_init: using embedded metal library
0.01.067.531 I ggml_metal_init: GPU name:   Apple M4
0.01.067.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.067.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.067.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.067.536 I ggml_metal_init: simdgroup reduction   = true
0.01.067.536 I ggml_metal_init: simdgroup matrix mul. = true
0.01.067.536 I ggml_metal_init: has residency sets    = true
0.01.067.536 I ggml_metal_init: has bfloat            = true
0.01.067.537 I ggml_metal_init: use bfloat            = true
0.01.067.537 I ggml_metal_init: hasUnifiedMemory      = true
0.01.067.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.082.674 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.138.352 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.138.358 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.138.392 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.143.538 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.143.540 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.143.541 I llama_init_from_model: graph nodes  = 967
0.01.143.541 I llama_init_from_model: graph splits = 2
0.01.143.546 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.143.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.143.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.197.405 I main: llama threadpool init, n_threads = 4
0.01.197.444 I 
0.01.197.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.197.465 I 
0.01.197.641 I sampler seed: 1234
0.01.197.645 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.197.679 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.197.682 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.197.682 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.285.705 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46988.75 tokens per second)
0.02.285.706 I llama_perf_context_print:        load time =    1186.68 ms
0.02.285.706 I llama_perf_context_print: prompt eval time =      39.83 ms /     7 tokens (    5.69 ms per token,   175.76 tokens per second)
0.02.285.708 I llama_perf_context_print:        eval time =    1045.62 ms /    63 runs   (   16.60 ms per token,    60.25 tokens per second)
0.02.285.708 I llama_perf_context_print:       total time =    1089.02 ms /    70 tokens
0.02.285.965 I ggml_metal_free: deallocating

real	0m2.303s
user	0m0.113s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.256 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.387 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.389 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.390 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.392 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.203 I llama_model_loader: - type  f32:  194 tensors
0.00.025.203 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.204 I print_info: file format = GGUF V3 (latest)
0.00.025.205 I print_info: file type   = Q8_0
0.00.025.206 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.225 I load: special tokens cache size = 25
0.00.039.286 I load: token to piece cache size = 0.2984 MB
0.00.039.290 I print_info: arch             = gptneox
0.00.039.290 I print_info: vocab_only       = 0
0.00.039.291 I print_info: n_ctx_train      = 2048
0.00.039.291 I print_info: n_embd           = 2048
0.00.039.291 I print_info: n_layer          = 24
0.00.039.296 I print_info: n_head           = 16
0.00.039.296 I print_info: n_head_kv        = 16
0.00.039.297 I print_info: n_rot            = 32
0.00.039.297 I print_info: n_swa            = 0
0.00.039.297 I print_info: n_embd_head_k    = 128
0.00.039.297 I print_info: n_embd_head_v    = 128
0.00.039.298 I print_info: n_gqa            = 1
0.00.039.299 I print_info: n_embd_k_gqa     = 2048
0.00.039.299 I print_info: n_embd_v_gqa     = 2048
0.00.039.300 I print_info: f_norm_eps       = 1.0e-05
0.00.039.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.300 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.301 I print_info: f_logit_scale    = 0.0e+00
0.00.039.301 I print_info: n_ff             = 8192
0.00.039.301 I print_info: n_expert         = 0
0.00.039.301 I print_info: n_expert_used    = 0
0.00.039.302 I print_info: causal attn      = 1
0.00.039.302 I print_info: pooling type     = 0
0.00.039.302 I print_info: rope type        = 2
0.00.039.302 I print_info: rope scaling     = linear
0.00.039.302 I print_info: freq_base_train  = 10000.0
0.00.039.303 I print_info: freq_scale_train = 1
0.00.039.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.303 I print_info: rope_finetuned   = unknown
0.00.039.303 I print_info: ssm_d_conv       = 0
0.00.039.303 I print_info: ssm_d_inner      = 0
0.00.039.305 I print_info: ssm_d_state      = 0
0.00.039.305 I print_info: ssm_dt_rank      = 0
0.00.039.305 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.306 I print_info: model type       = 1.4B
0.00.039.306 I print_info: model params     = 1.41 B
0.00.039.306 I print_info: general.name     = 1.4B
0.00.039.307 I print_info: vocab type       = BPE
0.00.039.307 I print_info: n_vocab          = 50304
0.00.039.307 I print_info: n_merges         = 50009
0.00.039.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.308 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.310 I print_info: LF token         = 187 ''
0.00.039.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.310 I print_info: max token length = 1024
0.00.039.310 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.834.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.834.777 I load_tensors: offloading output layer to GPU
0.00.834.778 I load_tensors: offloaded 25/25 layers to GPU
0.00.834.805 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.834.807 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.836.246 I llama_init_from_model: n_seq_max     = 1
0.00.836.248 I llama_init_from_model: n_ctx         = 128
0.00.836.249 I llama_init_from_model: n_ctx_per_seq = 128
0.00.836.249 I llama_init_from_model: n_batch       = 128
0.00.836.249 I llama_init_from_model: n_ubatch      = 128
0.00.836.250 I llama_init_from_model: flash_attn    = 0
0.00.836.250 I llama_init_from_model: freq_base     = 10000.0
0.00.836.251 I llama_init_from_model: freq_scale    = 1
0.00.836.251 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.836.253 I ggml_metal_init: allocating
0.00.836.286 I ggml_metal_init: found device: Apple M4
0.00.836.295 I ggml_metal_init: picking default device: Apple M4
0.00.837.653 I ggml_metal_init: using embedded metal library
0.00.843.345 I ggml_metal_init: GPU name:   Apple M4
0.00.843.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.843.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.843.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.843.351 I ggml_metal_init: simdgroup reduction   = true
0.00.843.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.843.351 I ggml_metal_init: has residency sets    = true
0.00.843.352 I ggml_metal_init: has bfloat            = true
0.00.843.352 I ggml_metal_init: use bfloat            = true
0.00.843.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.843.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.858.872 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.862.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.862.018 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.862.065 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.865.050 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.865.051 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.865.052 I llama_init_from_model: graph nodes  = 967
0.00.865.052 I llama_init_from_model: graph splits = 2
0.00.865.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.865.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.322 I 
0.00.896.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.406 I perplexity: tokenizing the input ..
0.00.903.576 I perplexity: tokenization took 7.168 ms
0.00.903.586 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.043.451 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.044.883 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.044.911 I llama_perf_context_print:        load time =     887.05 ms
0.01.044.912 I llama_perf_context_print: prompt eval time =     138.91 ms /   128 tokens (    1.09 ms per token,   921.48 tokens per second)
0.01.044.912 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.044.913 I llama_perf_context_print:       total time =     148.59 ms /   129 tokens
0.01.045.269 I ggml_metal_free: deallocating

real	0m1.060s
user	0m0.077s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.659 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.636 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.637 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.644 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.312 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.312 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.313 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.313 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.314 I llama_model_loader: - type  f32:  194 tensors
0.00.027.314 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.315 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.315 I print_info: file format = GGUF V3 (latest)
0.00.027.316 I print_info: file type   = Q4_0
0.00.027.317 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.470 I load: special tokens cache size = 25
0.00.041.748 I load: token to piece cache size = 0.2984 MB
0.00.041.752 I print_info: arch             = gptneox
0.00.041.752 I print_info: vocab_only       = 0
0.00.041.753 I print_info: n_ctx_train      = 2048
0.00.041.753 I print_info: n_embd           = 2048
0.00.041.753 I print_info: n_layer          = 24
0.00.041.758 I print_info: n_head           = 16
0.00.041.758 I print_info: n_head_kv        = 16
0.00.041.759 I print_info: n_rot            = 32
0.00.041.759 I print_info: n_swa            = 0
0.00.041.759 I print_info: n_embd_head_k    = 128
0.00.041.759 I print_info: n_embd_head_v    = 128
0.00.041.760 I print_info: n_gqa            = 1
0.00.041.761 I print_info: n_embd_k_gqa     = 2048
0.00.041.761 I print_info: n_embd_v_gqa     = 2048
0.00.041.762 I print_info: f_norm_eps       = 1.0e-05
0.00.041.762 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.762 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.763 I print_info: f_logit_scale    = 0.0e+00
0.00.041.763 I print_info: n_ff             = 8192
0.00.041.763 I print_info: n_expert         = 0
0.00.041.764 I print_info: n_expert_used    = 0
0.00.041.764 I print_info: causal attn      = 1
0.00.041.764 I print_info: pooling type     = 0
0.00.041.764 I print_info: rope type        = 2
0.00.041.764 I print_info: rope scaling     = linear
0.00.041.765 I print_info: freq_base_train  = 10000.0
0.00.041.765 I print_info: freq_scale_train = 1
0.00.041.765 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.765 I print_info: rope_finetuned   = unknown
0.00.041.765 I print_info: ssm_d_conv       = 0
0.00.041.765 I print_info: ssm_d_inner      = 0
0.00.041.766 I print_info: ssm_d_state      = 0
0.00.041.766 I print_info: ssm_dt_rank      = 0
0.00.041.766 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.766 I print_info: model type       = 1.4B
0.00.041.766 I print_info: model params     = 1.41 B
0.00.041.767 I print_info: general.name     = 1.4B
0.00.041.769 I print_info: vocab type       = BPE
0.00.041.770 I print_info: n_vocab          = 50304
0.00.041.770 I print_info: n_merges         = 50009
0.00.041.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: LF token         = 187 ''
0.00.041.771 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: max token length = 1024
0.00.041.771 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.550.612 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.630 I load_tensors: offloading output layer to GPU
0.00.550.631 I load_tensors: offloaded 25/25 layers to GPU
0.00.550.664 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.550.665 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.552.381 I llama_init_from_model: n_seq_max     = 1
0.00.552.386 I llama_init_from_model: n_ctx         = 2048
0.00.552.387 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.552.388 I llama_init_from_model: n_batch       = 2048
0.00.552.388 I llama_init_from_model: n_ubatch      = 512
0.00.552.389 I llama_init_from_model: flash_attn    = 0
0.00.552.391 I llama_init_from_model: freq_base     = 10000.0
0.00.552.392 I llama_init_from_model: freq_scale    = 1
0.00.552.394 I ggml_metal_init: allocating
0.00.552.511 I ggml_metal_init: found device: Apple M4
0.00.552.523 I ggml_metal_init: picking default device: Apple M4
0.00.554.379 I ggml_metal_init: using embedded metal library
0.00.559.393 I ggml_metal_init: GPU name:   Apple M4
0.00.559.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.404 I ggml_metal_init: simdgroup reduction   = true
0.00.559.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.405 I ggml_metal_init: has residency sets    = true
0.00.559.405 I ggml_metal_init: has bfloat            = true
0.00.559.405 I ggml_metal_init: use bfloat            = true
0.00.559.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.938 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.601.937 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.601.972 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.208 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.606.210 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.606.210 I llama_init_from_model: graph nodes  = 967
0.00.606.210 I llama_init_from_model: graph splits = 2
0.00.606.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.606.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.606.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.973 I main: llama threadpool init, n_threads = 4
0.00.664.019 I 
0.00.664.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.045 I 
0.00.664.206 I sampler seed: 1234
0.00.664.210 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.231 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.231 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.231 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.353.445 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.353.446 I llama_perf_context_print:        load time =     652.58 ms
0.01.353.447 I llama_perf_context_print: prompt eval time =      49.41 ms /     7 tokens (    7.06 ms per token,   141.69 tokens per second)
0.01.353.447 I llama_perf_context_print:        eval time =     637.01 ms /    63 runs   (   10.11 ms per token,    98.90 tokens per second)
0.01.353.448 I llama_perf_context_print:       total time =     690.19 ms /    70 tokens
0.01.353.670 I ggml_metal_free: deallocating

real	0m1.372s
user	0m0.103s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.028 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.443 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.328 I llama_model_loader: - type  f32:  194 tensors
0.00.026.329 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.330 I print_info: file format = GGUF V3 (latest)
0.00.026.330 I print_info: file type   = Q4_0
0.00.026.331 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.813 I load: special tokens cache size = 25
0.00.041.054 I load: token to piece cache size = 0.2984 MB
0.00.041.059 I print_info: arch             = gptneox
0.00.041.059 I print_info: vocab_only       = 0
0.00.041.059 I print_info: n_ctx_train      = 2048
0.00.041.059 I print_info: n_embd           = 2048
0.00.041.059 I print_info: n_layer          = 24
0.00.041.064 I print_info: n_head           = 16
0.00.041.065 I print_info: n_head_kv        = 16
0.00.041.065 I print_info: n_rot            = 32
0.00.041.065 I print_info: n_swa            = 0
0.00.041.065 I print_info: n_embd_head_k    = 128
0.00.041.065 I print_info: n_embd_head_v    = 128
0.00.041.066 I print_info: n_gqa            = 1
0.00.041.067 I print_info: n_embd_k_gqa     = 2048
0.00.041.068 I print_info: n_embd_v_gqa     = 2048
0.00.041.068 I print_info: f_norm_eps       = 1.0e-05
0.00.041.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.071 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.071 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.071 I print_info: f_logit_scale    = 0.0e+00
0.00.041.072 I print_info: n_ff             = 8192
0.00.041.072 I print_info: n_expert         = 0
0.00.041.072 I print_info: n_expert_used    = 0
0.00.041.073 I print_info: causal attn      = 1
0.00.041.073 I print_info: pooling type     = 0
0.00.041.073 I print_info: rope type        = 2
0.00.041.074 I print_info: rope scaling     = linear
0.00.041.081 I print_info: freq_base_train  = 10000.0
0.00.041.082 I print_info: freq_scale_train = 1
0.00.041.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.082 I print_info: rope_finetuned   = unknown
0.00.041.082 I print_info: ssm_d_conv       = 0
0.00.041.082 I print_info: ssm_d_inner      = 0
0.00.041.082 I print_info: ssm_d_state      = 0
0.00.041.083 I print_info: ssm_dt_rank      = 0
0.00.041.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.083 I print_info: model type       = 1.4B
0.00.041.083 I print_info: model params     = 1.41 B
0.00.041.083 I print_info: general.name     = 1.4B
0.00.041.084 I print_info: vocab type       = BPE
0.00.041.084 I print_info: n_vocab          = 50304
0.00.041.084 I print_info: n_merges         = 50009
0.00.041.085 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.085 I print_info: LF token         = 187 ''
0.00.041.088 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.088 I print_info: max token length = 1024
0.00.041.089 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.550.747 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.764 I load_tensors: offloading output layer to GPU
0.00.550.765 I load_tensors: offloaded 25/25 layers to GPU
0.00.550.803 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.550.804 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.552.511 I llama_init_from_model: n_seq_max     = 1
0.00.552.513 I llama_init_from_model: n_ctx         = 128
0.00.552.514 I llama_init_from_model: n_ctx_per_seq = 128
0.00.552.515 I llama_init_from_model: n_batch       = 128
0.00.552.515 I llama_init_from_model: n_ubatch      = 128
0.00.552.515 I llama_init_from_model: flash_attn    = 0
0.00.552.518 I llama_init_from_model: freq_base     = 10000.0
0.00.552.518 I llama_init_from_model: freq_scale    = 1
0.00.552.519 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.552.521 I ggml_metal_init: allocating
0.00.552.621 I ggml_metal_init: found device: Apple M4
0.00.552.635 I ggml_metal_init: picking default device: Apple M4
0.00.554.559 I ggml_metal_init: using embedded metal library
0.00.560.671 I ggml_metal_init: GPU name:   Apple M4
0.00.560.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.560.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.560.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.560.682 I ggml_metal_init: simdgroup reduction   = true
0.00.560.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.560.683 I ggml_metal_init: has residency sets    = true
0.00.560.683 I ggml_metal_init: has bfloat            = true
0.00.560.683 I ggml_metal_init: use bfloat            = true
0.00.560.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.560.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.579.326 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.582.928 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.586.100 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.586.102 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.586.102 I llama_init_from_model: graph nodes  = 967
0.00.586.103 I llama_init_from_model: graph splits = 2
0.00.586.106 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.586.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.126 I 
0.00.617.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.222 I perplexity: tokenizing the input ..
0.00.623.802 I perplexity: tokenization took 6.577 ms
0.00.623.807 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.681 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.015 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.041 I llama_perf_context_print:        load time =     607.08 ms
0.00.754.042 I llama_perf_context_print: prompt eval time =     128.27 ms /   128 tokens (    1.00 ms per token,   997.91 tokens per second)
0.00.754.043 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.043 I llama_perf_context_print:       total time =     136.92 ms /   129 tokens
0.00.754.463 I ggml_metal_free: deallocating

real	0m0.770s
user	0m0.079s
sys	0m0.125s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.200 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.064 I llama_model_loader: - type  f32:  194 tensors
0.00.029.064 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.065 I print_info: file format = GGUF V3 (latest)
0.00.029.066 I print_info: file type   = Q4_1
0.00.029.067 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.037.412 I load: special tokens cache size = 25
0.00.043.616 I load: token to piece cache size = 0.2984 MB
0.00.043.620 I print_info: arch             = gptneox
0.00.043.620 I print_info: vocab_only       = 0
0.00.043.621 I print_info: n_ctx_train      = 2048
0.00.043.621 I print_info: n_embd           = 2048
0.00.043.621 I print_info: n_layer          = 24
0.00.043.624 I print_info: n_head           = 16
0.00.043.625 I print_info: n_head_kv        = 16
0.00.043.625 I print_info: n_rot            = 32
0.00.043.626 I print_info: n_swa            = 0
0.00.043.627 I print_info: n_embd_head_k    = 128
0.00.043.628 I print_info: n_embd_head_v    = 128
0.00.043.628 I print_info: n_gqa            = 1
0.00.043.629 I print_info: n_embd_k_gqa     = 2048
0.00.043.630 I print_info: n_embd_v_gqa     = 2048
0.00.043.630 I print_info: f_norm_eps       = 1.0e-05
0.00.043.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.631 I print_info: f_logit_scale    = 0.0e+00
0.00.043.632 I print_info: n_ff             = 8192
0.00.043.632 I print_info: n_expert         = 0
0.00.043.632 I print_info: n_expert_used    = 0
0.00.043.632 I print_info: causal attn      = 1
0.00.043.632 I print_info: pooling type     = 0
0.00.043.634 I print_info: rope type        = 2
0.00.043.636 I print_info: rope scaling     = linear
0.00.043.636 I print_info: freq_base_train  = 10000.0
0.00.043.636 I print_info: freq_scale_train = 1
0.00.043.637 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.637 I print_info: rope_finetuned   = unknown
0.00.043.637 I print_info: ssm_d_conv       = 0
0.00.043.637 I print_info: ssm_d_inner      = 0
0.00.043.637 I print_info: ssm_d_state      = 0
0.00.043.637 I print_info: ssm_dt_rank      = 0
0.00.043.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.638 I print_info: model type       = 1.4B
0.00.043.638 I print_info: model params     = 1.41 B
0.00.043.638 I print_info: general.name     = 1.4B
0.00.043.639 I print_info: vocab type       = BPE
0.00.043.639 I print_info: n_vocab          = 50304
0.00.043.640 I print_info: n_merges         = 50009
0.00.043.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.641 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.641 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.641 I print_info: LF token         = 187 ''
0.00.043.641 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.642 I print_info: max token length = 1024
0.00.043.642 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.907 I load_tensors: offloading output layer to GPU
0.00.613.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.941 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.613.942 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.615.012 I llama_init_from_model: n_seq_max     = 1
0.00.615.015 I llama_init_from_model: n_ctx         = 2048
0.00.615.016 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.017 I llama_init_from_model: n_batch       = 2048
0.00.615.017 I llama_init_from_model: n_ubatch      = 512
0.00.615.018 I llama_init_from_model: flash_attn    = 0
0.00.615.020 I llama_init_from_model: freq_base     = 10000.0
0.00.615.020 I llama_init_from_model: freq_scale    = 1
0.00.615.023 I ggml_metal_init: allocating
0.00.615.123 I ggml_metal_init: found device: Apple M4
0.00.615.137 I ggml_metal_init: picking default device: Apple M4
0.00.617.114 I ggml_metal_init: using embedded metal library
0.00.623.740 I ggml_metal_init: GPU name:   Apple M4
0.00.623.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.751 I ggml_metal_init: simdgroup reduction   = true
0.00.623.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.752 I ggml_metal_init: has residency sets    = true
0.00.623.752 I ggml_metal_init: has bfloat            = true
0.00.623.753 I ggml_metal_init: use bfloat            = true
0.00.623.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.990 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.841 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.877 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.913 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.915 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.915 I llama_init_from_model: graph nodes  = 967
0.00.704.915 I llama_init_from_model: graph splits = 2
0.00.704.923 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.705.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.705.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.058 I main: llama threadpool init, n_threads = 4
0.00.762.106 I 
0.00.762.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.131 I 
0.00.762.314 I sampler seed: 1234
0.00.762.318 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.340 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.340 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.341 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.487.050 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.487.051 I llama_perf_context_print:        load time =     752.13 ms
0.01.487.052 I llama_perf_context_print: prompt eval time =      48.89 ms /     7 tokens (    6.98 ms per token,   143.18 tokens per second)
0.01.487.053 I llama_perf_context_print:        eval time =     673.10 ms /    63 runs   (   10.68 ms per token,    93.60 tokens per second)
0.01.487.053 I llama_perf_context_print:       total time =     725.71 ms /    70 tokens
0.01.487.313 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.111s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.833 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.129 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.021 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.888 I llama_model_loader: - type  f32:  194 tensors
0.00.024.888 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.889 I print_info: file format = GGUF V3 (latest)
0.00.024.890 I print_info: file type   = Q4_1
0.00.024.891 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.628 I load: special tokens cache size = 25
0.00.039.678 I load: token to piece cache size = 0.2984 MB
0.00.039.682 I print_info: arch             = gptneox
0.00.039.682 I print_info: vocab_only       = 0
0.00.039.682 I print_info: n_ctx_train      = 2048
0.00.039.682 I print_info: n_embd           = 2048
0.00.039.683 I print_info: n_layer          = 24
0.00.039.687 I print_info: n_head           = 16
0.00.039.688 I print_info: n_head_kv        = 16
0.00.039.688 I print_info: n_rot            = 32
0.00.039.688 I print_info: n_swa            = 0
0.00.039.688 I print_info: n_embd_head_k    = 128
0.00.039.688 I print_info: n_embd_head_v    = 128
0.00.039.689 I print_info: n_gqa            = 1
0.00.039.690 I print_info: n_embd_k_gqa     = 2048
0.00.039.690 I print_info: n_embd_v_gqa     = 2048
0.00.039.691 I print_info: f_norm_eps       = 1.0e-05
0.00.039.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.692 I print_info: f_logit_scale    = 0.0e+00
0.00.039.693 I print_info: n_ff             = 8192
0.00.039.693 I print_info: n_expert         = 0
0.00.039.693 I print_info: n_expert_used    = 0
0.00.039.693 I print_info: causal attn      = 1
0.00.039.693 I print_info: pooling type     = 0
0.00.039.693 I print_info: rope type        = 2
0.00.039.694 I print_info: rope scaling     = linear
0.00.039.694 I print_info: freq_base_train  = 10000.0
0.00.039.694 I print_info: freq_scale_train = 1
0.00.039.694 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.695 I print_info: rope_finetuned   = unknown
0.00.039.695 I print_info: ssm_d_conv       = 0
0.00.039.695 I print_info: ssm_d_inner      = 0
0.00.039.695 I print_info: ssm_d_state      = 0
0.00.039.695 I print_info: ssm_dt_rank      = 0
0.00.039.695 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.696 I print_info: model type       = 1.4B
0.00.039.696 I print_info: model params     = 1.41 B
0.00.039.696 I print_info: general.name     = 1.4B
0.00.039.696 I print_info: vocab type       = BPE
0.00.039.697 I print_info: n_vocab          = 50304
0.00.039.697 I print_info: n_merges         = 50009
0.00.039.697 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.701 I print_info: LF token         = 187 ''
0.00.039.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.701 I print_info: max token length = 1024
0.00.039.701 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.526 I load_tensors: offloading output layer to GPU
0.00.620.526 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.561 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.620.563 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.622.311 I llama_init_from_model: n_seq_max     = 1
0.00.622.314 I llama_init_from_model: n_ctx         = 128
0.00.622.314 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.315 I llama_init_from_model: n_batch       = 128
0.00.622.315 I llama_init_from_model: n_ubatch      = 128
0.00.622.315 I llama_init_from_model: flash_attn    = 0
0.00.622.318 I llama_init_from_model: freq_base     = 10000.0
0.00.622.319 I llama_init_from_model: freq_scale    = 1
0.00.622.319 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.322 I ggml_metal_init: allocating
0.00.622.405 I ggml_metal_init: found device: Apple M4
0.00.622.422 I ggml_metal_init: picking default device: Apple M4
0.00.624.294 I ggml_metal_init: using embedded metal library
0.00.630.994 I ggml_metal_init: GPU name:   Apple M4
0.00.631.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.005 I ggml_metal_init: simdgroup reduction   = true
0.00.631.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.005 I ggml_metal_init: has residency sets    = true
0.00.631.005 I ggml_metal_init: has bfloat            = true
0.00.631.006 I ggml_metal_init: use bfloat            = true
0.00.631.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.218 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.652.789 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.079 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.656.081 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.656.082 I llama_init_from_model: graph nodes  = 967
0.00.656.082 I llama_init_from_model: graph splits = 2
0.00.656.085 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.980 I 
0.00.684.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.069 I perplexity: tokenizing the input ..
0.00.691.504 I perplexity: tokenization took 7.432 ms
0.00.691.510 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.505 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.826.840 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.826.865 I llama_perf_context_print:        load time =     675.13 ms
0.00.826.866 I llama_perf_context_print: prompt eval time =     133.09 ms /   128 tokens (    1.04 ms per token,   961.77 tokens per second)
0.00.826.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.867 I llama_perf_context_print:       total time =     142.89 ms /   129 tokens
0.00.827.294 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.081s
sys	0m0.126s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.115 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.117 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.118 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.118 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.119 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.121 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.666 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.669 I llama_model_loader: - type  f32:  194 tensors
0.00.025.669 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.670 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.670 I print_info: file format = GGUF V3 (latest)
0.00.025.671 I print_info: file type   = Q5_0
0.00.025.675 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.476 I load: special tokens cache size = 25
0.00.039.571 I load: token to piece cache size = 0.2984 MB
0.00.039.574 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.574 I print_info: n_ctx_train      = 2048
0.00.039.575 I print_info: n_embd           = 2048
0.00.039.575 I print_info: n_layer          = 24
0.00.039.577 I print_info: n_head           = 16
0.00.039.578 I print_info: n_head_kv        = 16
0.00.039.578 I print_info: n_rot            = 32
0.00.039.578 I print_info: n_swa            = 0
0.00.039.579 I print_info: n_embd_head_k    = 128
0.00.039.581 I print_info: n_embd_head_v    = 128
0.00.039.582 I print_info: n_gqa            = 1
0.00.039.582 I print_info: n_embd_k_gqa     = 2048
0.00.039.583 I print_info: n_embd_v_gqa     = 2048
0.00.039.584 I print_info: f_norm_eps       = 1.0e-05
0.00.039.584 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.584 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.584 I print_info: f_logit_scale    = 0.0e+00
0.00.039.585 I print_info: n_ff             = 8192
0.00.039.585 I print_info: n_expert         = 0
0.00.039.585 I print_info: n_expert_used    = 0
0.00.039.585 I print_info: causal attn      = 1
0.00.039.586 I print_info: pooling type     = 0
0.00.039.586 I print_info: rope type        = 2
0.00.039.586 I print_info: rope scaling     = linear
0.00.039.592 I print_info: freq_base_train  = 10000.0
0.00.039.593 I print_info: freq_scale_train = 1
0.00.039.593 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.593 I print_info: rope_finetuned   = unknown
0.00.039.594 I print_info: ssm_d_conv       = 0
0.00.039.594 I print_info: ssm_d_inner      = 0
0.00.039.594 I print_info: ssm_d_state      = 0
0.00.039.594 I print_info: ssm_dt_rank      = 0
0.00.039.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.595 I print_info: model type       = 1.4B
0.00.039.595 I print_info: model params     = 1.41 B
0.00.039.595 I print_info: general.name     = 1.4B
0.00.039.600 I print_info: vocab type       = BPE
0.00.039.600 I print_info: n_vocab          = 50304
0.00.039.600 I print_info: n_merges         = 50009
0.00.039.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.601 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.602 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.602 I print_info: LF token         = 187 ''
0.00.039.602 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.603 I print_info: max token length = 1024
0.00.039.603 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.310 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.327 I load_tensors: offloading output layer to GPU
0.00.668.328 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.362 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.668.364 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.670.004 I llama_init_from_model: n_seq_max     = 1
0.00.670.006 I llama_init_from_model: n_ctx         = 2048
0.00.670.007 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.007 I llama_init_from_model: n_batch       = 2048
0.00.670.008 I llama_init_from_model: n_ubatch      = 512
0.00.670.008 I llama_init_from_model: flash_attn    = 0
0.00.670.010 I llama_init_from_model: freq_base     = 10000.0
0.00.670.010 I llama_init_from_model: freq_scale    = 1
0.00.670.013 I ggml_metal_init: allocating
0.00.670.094 I ggml_metal_init: found device: Apple M4
0.00.670.109 I ggml_metal_init: picking default device: Apple M4
0.00.672.040 I ggml_metal_init: using embedded metal library
0.00.678.697 I ggml_metal_init: GPU name:   Apple M4
0.00.678.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.704 I ggml_metal_init: simdgroup reduction   = true
0.00.678.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.704 I ggml_metal_init: has residency sets    = true
0.00.678.705 I ggml_metal_init: has bfloat            = true
0.00.678.705 I ggml_metal_init: use bfloat            = true
0.00.678.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.201 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.058 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.094 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.316 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.318 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.318 I llama_init_from_model: graph nodes  = 967
0.00.756.319 I llama_init_from_model: graph splits = 2
0.00.756.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.711 I main: llama threadpool init, n_threads = 4
0.00.814.756 I 
0.00.814.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.782 I 
0.00.814.933 I sampler seed: 1234
0.00.814.938 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.993 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.603.536 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.603.537 I llama_perf_context_print:        load time =     805.09 ms
0.01.603.537 I llama_perf_context_print: prompt eval time =      53.04 ms /     7 tokens (    7.58 ms per token,   131.97 tokens per second)
0.01.603.538 I llama_perf_context_print:        eval time =     732.63 ms /    63 runs   (   11.63 ms per token,    85.99 tokens per second)
0.01.603.540 I llama_perf_context_print:       total time =     789.55 ms /    70 tokens
0.01.603.798 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.110s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.554 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.619 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.423 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.425 I print_info: file format = GGUF V3 (latest)
0.00.025.425 I print_info: file type   = Q5_0
0.00.025.426 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.686 I load: special tokens cache size = 25
0.00.039.919 I load: token to piece cache size = 0.2984 MB
0.00.039.923 I print_info: arch             = gptneox
0.00.039.924 I print_info: vocab_only       = 0
0.00.039.924 I print_info: n_ctx_train      = 2048
0.00.039.924 I print_info: n_embd           = 2048
0.00.039.924 I print_info: n_layer          = 24
0.00.039.929 I print_info: n_head           = 16
0.00.039.929 I print_info: n_head_kv        = 16
0.00.039.930 I print_info: n_rot            = 32
0.00.039.930 I print_info: n_swa            = 0
0.00.039.930 I print_info: n_embd_head_k    = 128
0.00.039.930 I print_info: n_embd_head_v    = 128
0.00.039.931 I print_info: n_gqa            = 1
0.00.039.932 I print_info: n_embd_k_gqa     = 2048
0.00.039.932 I print_info: n_embd_v_gqa     = 2048
0.00.039.933 I print_info: f_norm_eps       = 1.0e-05
0.00.039.934 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.934 I print_info: f_logit_scale    = 0.0e+00
0.00.039.935 I print_info: n_ff             = 8192
0.00.039.935 I print_info: n_expert         = 0
0.00.039.935 I print_info: n_expert_used    = 0
0.00.039.938 I print_info: causal attn      = 1
0.00.039.938 I print_info: pooling type     = 0
0.00.039.939 I print_info: rope type        = 2
0.00.039.939 I print_info: rope scaling     = linear
0.00.039.939 I print_info: freq_base_train  = 10000.0
0.00.039.940 I print_info: freq_scale_train = 1
0.00.039.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.940 I print_info: rope_finetuned   = unknown
0.00.039.940 I print_info: ssm_d_conv       = 0
0.00.039.940 I print_info: ssm_d_inner      = 0
0.00.039.941 I print_info: ssm_d_state      = 0
0.00.039.941 I print_info: ssm_dt_rank      = 0
0.00.039.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.941 I print_info: model type       = 1.4B
0.00.039.942 I print_info: model params     = 1.41 B
0.00.039.942 I print_info: general.name     = 1.4B
0.00.039.943 I print_info: vocab type       = BPE
0.00.039.943 I print_info: n_vocab          = 50304
0.00.039.943 I print_info: n_merges         = 50009
0.00.039.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.944 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.944 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: LF token         = 187 ''
0.00.039.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.949 I print_info: max token length = 1024
0.00.039.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.978 I load_tensors: offloading output layer to GPU
0.00.653.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.010 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.654.011 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.655.456 I llama_init_from_model: n_seq_max     = 1
0.00.655.462 I llama_init_from_model: n_ctx         = 128
0.00.655.462 I llama_init_from_model: n_ctx_per_seq = 128
0.00.655.463 I llama_init_from_model: n_batch       = 128
0.00.655.463 I llama_init_from_model: n_ubatch      = 128
0.00.655.464 I llama_init_from_model: flash_attn    = 0
0.00.655.465 I llama_init_from_model: freq_base     = 10000.0
0.00.655.465 I llama_init_from_model: freq_scale    = 1
0.00.655.466 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.655.468 I ggml_metal_init: allocating
0.00.655.528 I ggml_metal_init: found device: Apple M4
0.00.655.541 I ggml_metal_init: picking default device: Apple M4
0.00.657.253 I ggml_metal_init: using embedded metal library
0.00.664.096 I ggml_metal_init: GPU name:   Apple M4
0.00.664.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.108 I ggml_metal_init: simdgroup reduction   = true
0.00.664.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.108 I ggml_metal_init: has residency sets    = true
0.00.664.109 I ggml_metal_init: has bfloat            = true
0.00.664.109 I ggml_metal_init: use bfloat            = true
0.00.664.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.910 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.685.920 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.685.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.214 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.216 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.216 I llama_init_from_model: graph nodes  = 967
0.00.689.217 I llama_init_from_model: graph splits = 2
0.00.689.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.778 I 
0.00.717.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.859 I perplexity: tokenizing the input ..
0.00.725.380 I perplexity: tokenization took 7.519 ms
0.00.725.387 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.880 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.863.308 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.863.335 I llama_perf_context_print:        load time =     708.21 ms
0.00.863.336 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.66 tokens per second)
0.00.863.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.337 I llama_perf_context_print:       total time =     145.56 ms /   129 tokens
0.00.863.765 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.080s
sys	0m0.138s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.852 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.854 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.855 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.860 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.861 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.562 I llama_model_loader: - type  f32:  194 tensors
0.00.026.563 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.563 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.564 I print_info: file format = GGUF V3 (latest)
0.00.026.564 I print_info: file type   = Q5_1
0.00.026.565 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.658 I load: special tokens cache size = 25
0.00.040.628 I load: token to piece cache size = 0.2984 MB
0.00.040.631 I print_info: arch             = gptneox
0.00.040.631 I print_info: vocab_only       = 0
0.00.040.632 I print_info: n_ctx_train      = 2048
0.00.040.632 I print_info: n_embd           = 2048
0.00.040.632 I print_info: n_layer          = 24
0.00.040.635 I print_info: n_head           = 16
0.00.040.636 I print_info: n_head_kv        = 16
0.00.040.636 I print_info: n_rot            = 32
0.00.040.636 I print_info: n_swa            = 0
0.00.040.637 I print_info: n_embd_head_k    = 128
0.00.040.637 I print_info: n_embd_head_v    = 128
0.00.040.637 I print_info: n_gqa            = 1
0.00.040.638 I print_info: n_embd_k_gqa     = 2048
0.00.040.639 I print_info: n_embd_v_gqa     = 2048
0.00.040.639 I print_info: f_norm_eps       = 1.0e-05
0.00.040.640 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.640 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.640 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.640 I print_info: f_logit_scale    = 0.0e+00
0.00.040.641 I print_info: n_ff             = 8192
0.00.040.641 I print_info: n_expert         = 0
0.00.040.641 I print_info: n_expert_used    = 0
0.00.040.641 I print_info: causal attn      = 1
0.00.040.642 I print_info: pooling type     = 0
0.00.040.643 I print_info: rope type        = 2
0.00.040.645 I print_info: rope scaling     = linear
0.00.040.645 I print_info: freq_base_train  = 10000.0
0.00.040.646 I print_info: freq_scale_train = 1
0.00.040.646 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.646 I print_info: rope_finetuned   = unknown
0.00.040.648 I print_info: ssm_d_conv       = 0
0.00.040.648 I print_info: ssm_d_inner      = 0
0.00.040.648 I print_info: ssm_d_state      = 0
0.00.040.648 I print_info: ssm_dt_rank      = 0
0.00.040.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.648 I print_info: model type       = 1.4B
0.00.040.649 I print_info: model params     = 1.41 B
0.00.040.649 I print_info: general.name     = 1.4B
0.00.040.649 I print_info: vocab type       = BPE
0.00.040.649 I print_info: n_vocab          = 50304
0.00.040.650 I print_info: n_merges         = 50009
0.00.040.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.654 I print_info: LF token         = 187 ''
0.00.040.654 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.655 I print_info: max token length = 1024
0.00.040.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.732.328 I load_tensors: offloading 24 repeating layers to GPU
0.00.732.343 I load_tensors: offloading output layer to GPU
0.00.732.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.732.378 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.732.380 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.734.033 I llama_init_from_model: n_seq_max     = 1
0.00.734.035 I llama_init_from_model: n_ctx         = 2048
0.00.734.036 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.734.036 I llama_init_from_model: n_batch       = 2048
0.00.734.037 I llama_init_from_model: n_ubatch      = 512
0.00.734.037 I llama_init_from_model: flash_attn    = 0
0.00.734.038 I llama_init_from_model: freq_base     = 10000.0
0.00.734.039 I llama_init_from_model: freq_scale    = 1
0.00.734.040 I ggml_metal_init: allocating
0.00.734.056 I ggml_metal_init: found device: Apple M4
0.00.734.065 I ggml_metal_init: picking default device: Apple M4
0.00.735.617 I ggml_metal_init: using embedded metal library
0.00.741.930 I ggml_metal_init: GPU name:   Apple M4
0.00.741.934 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.741.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.741.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.741.936 I ggml_metal_init: simdgroup reduction   = true
0.00.741.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.741.936 I ggml_metal_init: has residency sets    = true
0.00.741.937 I ggml_metal_init: has bfloat            = true
0.00.741.937 I ggml_metal_init: use bfloat            = true
0.00.741.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.741.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.759.557 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.811.921 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.811.928 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.812.013 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.817.249 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.817.249 I llama_init_from_model: graph nodes  = 967
0.00.817.250 I llama_init_from_model: graph splits = 2
0.00.817.256 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.817.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.817.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.045 I main: llama threadpool init, n_threads = 4
0.00.875.090 I 
0.00.875.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.115 I 
0.00.875.282 I sampler seed: 1234
0.00.875.287 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.309 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.309 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.714.191 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.714.192 I llama_perf_context_print:        load time =     864.43 ms
0.01.714.192 I llama_perf_context_print: prompt eval time =      52.28 ms /     7 tokens (    7.47 ms per token,   133.89 tokens per second)
0.01.714.193 I llama_perf_context_print:        eval time =     783.65 ms /    63 runs   (   12.44 ms per token,    80.39 tokens per second)
0.01.714.193 I llama_perf_context_print:       total time =     839.86 ms /    70 tokens
0.01.714.437 I ggml_metal_free: deallocating

real	0m1.732s
user	0m0.109s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.976 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.374 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.374 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.374 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.375 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.224 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.224 I llama_model_loader: - type  f32:  194 tensors
0.00.027.225 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.225 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.226 I print_info: file format = GGUF V3 (latest)
0.00.027.227 I print_info: file type   = Q5_1
0.00.027.228 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.863 I load: special tokens cache size = 25
0.00.041.989 I load: token to piece cache size = 0.2984 MB
0.00.041.993 I print_info: arch             = gptneox
0.00.041.994 I print_info: vocab_only       = 0
0.00.041.994 I print_info: n_ctx_train      = 2048
0.00.041.994 I print_info: n_embd           = 2048
0.00.041.994 I print_info: n_layer          = 24
0.00.041.999 I print_info: n_head           = 16
0.00.041.999 I print_info: n_head_kv        = 16
0.00.042.000 I print_info: n_rot            = 32
0.00.042.000 I print_info: n_swa            = 0
0.00.042.003 I print_info: n_embd_head_k    = 128
0.00.042.004 I print_info: n_embd_head_v    = 128
0.00.042.004 I print_info: n_gqa            = 1
0.00.042.005 I print_info: n_embd_k_gqa     = 2048
0.00.042.006 I print_info: n_embd_v_gqa     = 2048
0.00.042.007 I print_info: f_norm_eps       = 1.0e-05
0.00.042.008 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.009 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.009 I print_info: f_logit_scale    = 0.0e+00
0.00.042.010 I print_info: n_ff             = 8192
0.00.042.011 I print_info: n_expert         = 0
0.00.042.011 I print_info: n_expert_used    = 0
0.00.042.011 I print_info: causal attn      = 1
0.00.042.011 I print_info: pooling type     = 0
0.00.042.011 I print_info: rope type        = 2
0.00.042.011 I print_info: rope scaling     = linear
0.00.042.012 I print_info: freq_base_train  = 10000.0
0.00.042.012 I print_info: freq_scale_train = 1
0.00.042.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.012 I print_info: rope_finetuned   = unknown
0.00.042.013 I print_info: ssm_d_conv       = 0
0.00.042.013 I print_info: ssm_d_inner      = 0
0.00.042.013 I print_info: ssm_d_state      = 0
0.00.042.013 I print_info: ssm_dt_rank      = 0
0.00.042.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.013 I print_info: model type       = 1.4B
0.00.042.014 I print_info: model params     = 1.41 B
0.00.042.015 I print_info: general.name     = 1.4B
0.00.042.015 I print_info: vocab type       = BPE
0.00.042.016 I print_info: n_vocab          = 50304
0.00.042.016 I print_info: n_merges         = 50009
0.00.042.016 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.017 I print_info: LF token         = 187 ''
0.00.042.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.017 I print_info: max token length = 1024
0.00.042.018 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.691.625 I load_tensors: offloading 24 repeating layers to GPU
0.00.691.640 I load_tensors: offloading output layer to GPU
0.00.691.641 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.676 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.691.678 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.693.311 I llama_init_from_model: n_seq_max     = 1
0.00.693.315 I llama_init_from_model: n_ctx         = 128
0.00.693.316 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.316 I llama_init_from_model: n_batch       = 128
0.00.693.316 I llama_init_from_model: n_ubatch      = 128
0.00.693.317 I llama_init_from_model: flash_attn    = 0
0.00.693.318 I llama_init_from_model: freq_base     = 10000.0
0.00.693.319 I llama_init_from_model: freq_scale    = 1
0.00.693.320 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.322 I ggml_metal_init: allocating
0.00.693.369 I ggml_metal_init: found device: Apple M4
0.00.693.381 I ggml_metal_init: picking default device: Apple M4
0.00.694.905 I ggml_metal_init: using embedded metal library
0.00.701.087 I ggml_metal_init: GPU name:   Apple M4
0.00.701.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.094 I ggml_metal_init: simdgroup reduction   = true
0.00.701.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.094 I ggml_metal_init: has residency sets    = true
0.00.701.095 I ggml_metal_init: has bfloat            = true
0.00.701.095 I ggml_metal_init: use bfloat            = true
0.00.701.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.810 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.721.818 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.868 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.942 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.724.944 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.724.944 I llama_init_from_model: graph nodes  = 967
0.00.724.945 I llama_init_from_model: graph splits = 2
0.00.724.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.482 I 
0.00.752.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.574 I perplexity: tokenizing the input ..
0.00.760.179 I perplexity: tokenization took 7.601 ms
0.00.760.188 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.089 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.897.427 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.897.456 I llama_perf_context_print:        load time =     741.49 ms
0.00.897.457 I llama_perf_context_print: prompt eval time =     135.01 ms /   128 tokens (    1.05 ms per token,   948.06 tokens per second)
0.00.897.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.458 I llama_perf_context_print:       total time =     144.98 ms /   129 tokens
0.00.897.848 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.080s
sys	0m0.136s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.766 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.640 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.559 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.560 I llama_model_loader: - type  f32:  194 tensors
0.00.024.560 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.560 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.560 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.561 I print_info: file format = GGUF V3 (latest)
0.00.024.562 I print_info: file type   = Q2_K - Medium
0.00.024.562 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.791 I load: special tokens cache size = 25
0.00.038.928 I load: token to piece cache size = 0.2984 MB
0.00.038.930 I print_info: arch             = gptneox
0.00.038.931 I print_info: vocab_only       = 0
0.00.038.931 I print_info: n_ctx_train      = 2048
0.00.038.931 I print_info: n_embd           = 2048
0.00.038.931 I print_info: n_layer          = 24
0.00.038.934 I print_info: n_head           = 16
0.00.038.935 I print_info: n_head_kv        = 16
0.00.038.935 I print_info: n_rot            = 32
0.00.038.935 I print_info: n_swa            = 0
0.00.038.936 I print_info: n_embd_head_k    = 128
0.00.038.936 I print_info: n_embd_head_v    = 128
0.00.038.939 I print_info: n_gqa            = 1
0.00.038.940 I print_info: n_embd_k_gqa     = 2048
0.00.038.941 I print_info: n_embd_v_gqa     = 2048
0.00.038.941 I print_info: f_norm_eps       = 1.0e-05
0.00.038.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.942 I print_info: f_logit_scale    = 0.0e+00
0.00.038.943 I print_info: n_ff             = 8192
0.00.038.944 I print_info: n_expert         = 0
0.00.038.944 I print_info: n_expert_used    = 0
0.00.038.944 I print_info: causal attn      = 1
0.00.038.945 I print_info: pooling type     = 0
0.00.038.945 I print_info: rope type        = 2
0.00.038.945 I print_info: rope scaling     = linear
0.00.038.945 I print_info: freq_base_train  = 10000.0
0.00.038.946 I print_info: freq_scale_train = 1
0.00.038.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.946 I print_info: rope_finetuned   = unknown
0.00.038.946 I print_info: ssm_d_conv       = 0
0.00.038.946 I print_info: ssm_d_inner      = 0
0.00.038.947 I print_info: ssm_d_state      = 0
0.00.038.947 I print_info: ssm_dt_rank      = 0
0.00.038.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.947 I print_info: model type       = 1.4B
0.00.038.947 I print_info: model params     = 1.41 B
0.00.038.947 I print_info: general.name     = 1.4B
0.00.038.948 I print_info: vocab type       = BPE
0.00.038.948 I print_info: n_vocab          = 50304
0.00.038.949 I print_info: n_merges         = 50009
0.00.038.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: LF token         = 187 ''
0.00.038.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: max token length = 1024
0.00.038.955 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.369 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.383 I load_tensors: offloading output layer to GPU
0.00.348.383 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.417 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.419 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.947 I llama_init_from_model: n_seq_max     = 1
0.00.349.951 I llama_init_from_model: n_ctx         = 2048
0.00.349.952 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.349.952 I llama_init_from_model: n_batch       = 2048
0.00.349.952 I llama_init_from_model: n_ubatch      = 512
0.00.349.953 I llama_init_from_model: flash_attn    = 0
0.00.349.955 I llama_init_from_model: freq_base     = 10000.0
0.00.349.955 I llama_init_from_model: freq_scale    = 1
0.00.349.957 I ggml_metal_init: allocating
0.00.350.054 I ggml_metal_init: found device: Apple M4
0.00.350.068 I ggml_metal_init: picking default device: Apple M4
0.00.351.935 I ggml_metal_init: using embedded metal library
0.00.357.643 I ggml_metal_init: GPU name:   Apple M4
0.00.357.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.357.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.357.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.357.665 I ggml_metal_init: simdgroup reduction   = true
0.00.357.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.357.666 I ggml_metal_init: has residency sets    = true
0.00.357.666 I ggml_metal_init: has bfloat            = true
0.00.357.666 I ggml_metal_init: use bfloat            = true
0.00.357.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.357.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.379.317 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.439.852 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.439.858 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.439.888 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.444.023 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.444.024 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.444.025 I llama_init_from_model: graph nodes  = 967
0.00.444.025 I llama_init_from_model: graph splits = 2
0.00.444.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.444.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.444.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.846 I main: llama threadpool init, n_threads = 4
0.00.505.884 I 
0.00.505.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.908 I 
0.00.506.090 I sampler seed: 1234
0.00.506.094 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.132 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.133 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.133 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.183.850 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.183.851 I llama_perf_context_print:        load time =     496.28 ms
0.01.183.852 I llama_perf_context_print: prompt eval time =      40.11 ms /     7 tokens (    5.73 ms per token,   174.50 tokens per second)
0.01.183.853 I llama_perf_context_print:        eval time =     634.64 ms /    63 runs   (   10.07 ms per token,    99.27 tokens per second)
0.01.183.853 I llama_perf_context_print:       total time =     678.71 ms /    70 tokens
0.01.184.058 I ggml_metal_free: deallocating

real	0m1.201s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.937 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.758 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.758 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.758 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.759 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q2_K - Medium
0.00.024.761 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.800 I load: special tokens cache size = 25
0.00.039.001 I load: token to piece cache size = 0.2984 MB
0.00.039.004 I print_info: arch             = gptneox
0.00.039.005 I print_info: vocab_only       = 0
0.00.039.005 I print_info: n_ctx_train      = 2048
0.00.039.005 I print_info: n_embd           = 2048
0.00.039.005 I print_info: n_layer          = 24
0.00.039.010 I print_info: n_head           = 16
0.00.039.011 I print_info: n_head_kv        = 16
0.00.039.011 I print_info: n_rot            = 32
0.00.039.011 I print_info: n_swa            = 0
0.00.039.011 I print_info: n_embd_head_k    = 128
0.00.039.011 I print_info: n_embd_head_v    = 128
0.00.039.012 I print_info: n_gqa            = 1
0.00.039.013 I print_info: n_embd_k_gqa     = 2048
0.00.039.014 I print_info: n_embd_v_gqa     = 2048
0.00.039.014 I print_info: f_norm_eps       = 1.0e-05
0.00.039.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.015 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.015 I print_info: f_logit_scale    = 0.0e+00
0.00.039.016 I print_info: n_ff             = 8192
0.00.039.017 I print_info: n_expert         = 0
0.00.039.017 I print_info: n_expert_used    = 0
0.00.039.017 I print_info: causal attn      = 1
0.00.039.017 I print_info: pooling type     = 0
0.00.039.017 I print_info: rope type        = 2
0.00.039.018 I print_info: rope scaling     = linear
0.00.039.018 I print_info: freq_base_train  = 10000.0
0.00.039.018 I print_info: freq_scale_train = 1
0.00.039.018 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.019 I print_info: rope_finetuned   = unknown
0.00.039.019 I print_info: ssm_d_conv       = 0
0.00.039.019 I print_info: ssm_d_inner      = 0
0.00.039.019 I print_info: ssm_d_state      = 0
0.00.039.019 I print_info: ssm_dt_rank      = 0
0.00.039.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.020 I print_info: model type       = 1.4B
0.00.039.020 I print_info: model params     = 1.41 B
0.00.039.020 I print_info: general.name     = 1.4B
0.00.039.021 I print_info: vocab type       = BPE
0.00.039.021 I print_info: n_vocab          = 50304
0.00.039.021 I print_info: n_merges         = 50009
0.00.039.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.023 I print_info: LF token         = 187 ''
0.00.039.023 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.023 I print_info: max token length = 1024
0.00.039.024 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.031 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.047 I load_tensors: offloading output layer to GPU
0.00.344.047 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.082 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.083 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.345.830 I llama_init_from_model: n_seq_max     = 1
0.00.345.834 I llama_init_from_model: n_ctx         = 128
0.00.345.835 I llama_init_from_model: n_ctx_per_seq = 128
0.00.345.835 I llama_init_from_model: n_batch       = 128
0.00.345.836 I llama_init_from_model: n_ubatch      = 128
0.00.345.836 I llama_init_from_model: flash_attn    = 0
0.00.345.837 I llama_init_from_model: freq_base     = 10000.0
0.00.345.838 I llama_init_from_model: freq_scale    = 1
0.00.345.838 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.345.843 I ggml_metal_init: allocating
0.00.345.936 I ggml_metal_init: found device: Apple M4
0.00.345.950 I ggml_metal_init: picking default device: Apple M4
0.00.347.846 I ggml_metal_init: using embedded metal library
0.00.353.261 I ggml_metal_init: GPU name:   Apple M4
0.00.353.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.277 I ggml_metal_init: simdgroup reduction   = true
0.00.353.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.278 I ggml_metal_init: has residency sets    = true
0.00.353.278 I ggml_metal_init: has bfloat            = true
0.00.353.279 I ggml_metal_init: use bfloat            = true
0.00.353.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.745 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.378.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.378.471 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.378.524 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.081 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.382.083 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.382.083 I llama_init_from_model: graph nodes  = 967
0.00.382.084 I llama_init_from_model: graph splits = 2
0.00.382.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.382.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.415.619 I 
0.00.415.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.415.719 I perplexity: tokenizing the input ..
0.00.422.172 I perplexity: tokenization took 6.452 ms
0.00.422.178 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.561.208 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.537 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.558 I llama_perf_context_print:        load time =     406.67 ms
0.00.562.559 I llama_perf_context_print: prompt eval time =     138.74 ms /   128 tokens (    1.08 ms per token,   922.58 tokens per second)
0.00.562.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.560 I llama_perf_context_print:       total time =     146.94 ms /   129 tokens
0.00.562.938 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.081s
sys	0m0.093s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.323 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.333 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.334 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.335 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.337 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.337 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.959 I llama_model_loader: - type  f32:  194 tensors
0.00.023.959 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.960 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.960 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.961 I print_info: file format = GGUF V3 (latest)
0.00.023.961 I print_info: file type   = Q3_K - Medium
0.00.023.962 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.778 I load: special tokens cache size = 25
0.00.037.763 I load: token to piece cache size = 0.2984 MB
0.00.037.766 I print_info: arch             = gptneox
0.00.037.766 I print_info: vocab_only       = 0
0.00.037.766 I print_info: n_ctx_train      = 2048
0.00.037.766 I print_info: n_embd           = 2048
0.00.037.766 I print_info: n_layer          = 24
0.00.037.769 I print_info: n_head           = 16
0.00.037.770 I print_info: n_head_kv        = 16
0.00.037.770 I print_info: n_rot            = 32
0.00.037.770 I print_info: n_swa            = 0
0.00.037.770 I print_info: n_embd_head_k    = 128
0.00.037.771 I print_info: n_embd_head_v    = 128
0.00.037.771 I print_info: n_gqa            = 1
0.00.037.772 I print_info: n_embd_k_gqa     = 2048
0.00.037.773 I print_info: n_embd_v_gqa     = 2048
0.00.037.773 I print_info: f_norm_eps       = 1.0e-05
0.00.037.774 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.774 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.774 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.774 I print_info: f_logit_scale    = 0.0e+00
0.00.037.775 I print_info: n_ff             = 8192
0.00.037.775 I print_info: n_expert         = 0
0.00.037.775 I print_info: n_expert_used    = 0
0.00.037.775 I print_info: causal attn      = 1
0.00.037.776 I print_info: pooling type     = 0
0.00.037.776 I print_info: rope type        = 2
0.00.037.776 I print_info: rope scaling     = linear
0.00.037.776 I print_info: freq_base_train  = 10000.0
0.00.037.777 I print_info: freq_scale_train = 1
0.00.037.777 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.777 I print_info: rope_finetuned   = unknown
0.00.037.777 I print_info: ssm_d_conv       = 0
0.00.037.778 I print_info: ssm_d_inner      = 0
0.00.037.778 I print_info: ssm_d_state      = 0
0.00.037.778 I print_info: ssm_dt_rank      = 0
0.00.037.778 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.778 I print_info: model type       = 1.4B
0.00.037.779 I print_info: model params     = 1.41 B
0.00.037.779 I print_info: general.name     = 1.4B
0.00.037.780 I print_info: vocab type       = BPE
0.00.037.780 I print_info: n_vocab          = 50304
0.00.037.780 I print_info: n_merges         = 50009
0.00.037.781 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.781 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.781 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.781 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.782 I print_info: LF token         = 187 ''
0.00.037.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.782 I print_info: max token length = 1024
0.00.037.785 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.639 I load_tensors: offloading output layer to GPU
0.00.442.640 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.674 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.676 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.323 I llama_init_from_model: n_seq_max     = 1
0.00.444.329 I llama_init_from_model: n_ctx         = 2048
0.00.444.330 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.331 I llama_init_from_model: n_batch       = 2048
0.00.444.331 I llama_init_from_model: n_ubatch      = 512
0.00.444.332 I llama_init_from_model: flash_attn    = 0
0.00.444.333 I llama_init_from_model: freq_base     = 10000.0
0.00.444.334 I llama_init_from_model: freq_scale    = 1
0.00.444.336 I ggml_metal_init: allocating
0.00.444.422 I ggml_metal_init: found device: Apple M4
0.00.444.435 I ggml_metal_init: picking default device: Apple M4
0.00.446.372 I ggml_metal_init: using embedded metal library
0.00.451.834 I ggml_metal_init: GPU name:   Apple M4
0.00.451.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.841 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.842 I ggml_metal_init: simdgroup reduction   = true
0.00.451.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.843 I ggml_metal_init: has residency sets    = true
0.00.451.843 I ggml_metal_init: has bfloat            = true
0.00.451.843 I ggml_metal_init: use bfloat            = true
0.00.451.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.539 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.528.144 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.528.153 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.528.198 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.533.003 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.533.005 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.533.006 I llama_init_from_model: graph nodes  = 967
0.00.533.006 I llama_init_from_model: graph splits = 2
0.00.533.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.533.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.533.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.399 I main: llama threadpool init, n_threads = 4
0.00.589.437 I 
0.00.589.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.461 I 
0.00.589.629 I sampler seed: 1234
0.00.589.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.690 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.693 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.343.212 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.343.213 I llama_perf_context_print:        load time =     579.89 ms
0.01.343.214 I llama_perf_context_print: prompt eval time =      50.12 ms /     7 tokens (    7.16 ms per token,   139.68 tokens per second)
0.01.343.214 I llama_perf_context_print:        eval time =     700.48 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.343.215 I llama_perf_context_print:       total time =     754.55 ms /    70 tokens
0.01.343.439 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.111s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.483 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.483 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.483 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.484 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.484 I print_info: file format = GGUF V3 (latest)
0.00.024.489 I print_info: file type   = Q3_K - Medium
0.00.024.490 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.447 I load: special tokens cache size = 25
0.00.038.566 I load: token to piece cache size = 0.2984 MB
0.00.038.571 I print_info: arch             = gptneox
0.00.038.571 I print_info: vocab_only       = 0
0.00.038.571 I print_info: n_ctx_train      = 2048
0.00.038.571 I print_info: n_embd           = 2048
0.00.038.571 I print_info: n_layer          = 24
0.00.038.575 I print_info: n_head           = 16
0.00.038.576 I print_info: n_head_kv        = 16
0.00.038.576 I print_info: n_rot            = 32
0.00.038.576 I print_info: n_swa            = 0
0.00.038.577 I print_info: n_embd_head_k    = 128
0.00.038.577 I print_info: n_embd_head_v    = 128
0.00.038.577 I print_info: n_gqa            = 1
0.00.038.578 I print_info: n_embd_k_gqa     = 2048
0.00.038.579 I print_info: n_embd_v_gqa     = 2048
0.00.038.579 I print_info: f_norm_eps       = 1.0e-05
0.00.038.580 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.580 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.580 I print_info: f_logit_scale    = 0.0e+00
0.00.038.581 I print_info: n_ff             = 8192
0.00.038.581 I print_info: n_expert         = 0
0.00.038.581 I print_info: n_expert_used    = 0
0.00.038.581 I print_info: causal attn      = 1
0.00.038.581 I print_info: pooling type     = 0
0.00.038.581 I print_info: rope type        = 2
0.00.038.584 I print_info: rope scaling     = linear
0.00.038.585 I print_info: freq_base_train  = 10000.0
0.00.038.585 I print_info: freq_scale_train = 1
0.00.038.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.585 I print_info: rope_finetuned   = unknown
0.00.038.586 I print_info: ssm_d_conv       = 0
0.00.038.589 I print_info: ssm_d_inner      = 0
0.00.038.617 I print_info: ssm_d_state      = 0
0.00.038.620 I print_info: ssm_dt_rank      = 0
0.00.038.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.620 I print_info: model type       = 1.4B
0.00.038.621 I print_info: model params     = 1.41 B
0.00.038.621 I print_info: general.name     = 1.4B
0.00.038.621 I print_info: vocab type       = BPE
0.00.038.622 I print_info: n_vocab          = 50304
0.00.038.622 I print_info: n_merges         = 50009
0.00.038.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: LF token         = 187 ''
0.00.038.623 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: max token length = 1024
0.00.038.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.808 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.817 I load_tensors: offloading output layer to GPU
0.00.457.818 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.848 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.850 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.459.458 I llama_init_from_model: n_seq_max     = 1
0.00.459.463 I llama_init_from_model: n_ctx         = 128
0.00.459.464 I llama_init_from_model: n_ctx_per_seq = 128
0.00.459.464 I llama_init_from_model: n_batch       = 128
0.00.459.465 I llama_init_from_model: n_ubatch      = 128
0.00.459.465 I llama_init_from_model: flash_attn    = 0
0.00.459.466 I llama_init_from_model: freq_base     = 10000.0
0.00.459.467 I llama_init_from_model: freq_scale    = 1
0.00.459.468 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.459.470 I ggml_metal_init: allocating
0.00.459.520 I ggml_metal_init: found device: Apple M4
0.00.459.533 I ggml_metal_init: picking default device: Apple M4
0.00.461.192 I ggml_metal_init: using embedded metal library
0.00.466.765 I ggml_metal_init: GPU name:   Apple M4
0.00.466.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.781 I ggml_metal_init: simdgroup reduction   = true
0.00.466.782 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.782 I ggml_metal_init: has residency sets    = true
0.00.466.782 I ggml_metal_init: has bfloat            = true
0.00.466.783 I ggml_metal_init: use bfloat            = true
0.00.466.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.256 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.490.947 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.490.960 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.491.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.494.407 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.494.408 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.494.409 I llama_init_from_model: graph nodes  = 967
0.00.494.409 I llama_init_from_model: graph splits = 2
0.00.494.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.494.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.344 I 
0.00.524.426 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.433 I perplexity: tokenizing the input ..
0.00.531.544 I perplexity: tokenization took 7.109 ms
0.00.531.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.674.480 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.675.809 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.675.834 I llama_perf_context_print:        load time =     515.60 ms
0.00.675.835 I llama_perf_context_print: prompt eval time =     141.96 ms /   128 tokens (    1.11 ms per token,   901.69 tokens per second)
0.00.675.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.675.836 I llama_perf_context_print:       total time =     151.50 ms /   129 tokens
0.00.676.248 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.080s
sys	0m0.123s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.734 I llama_model_loader: - type  f32:  194 tensors
0.00.024.734 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.734 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.735 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.735 I print_info: file format = GGUF V3 (latest)
0.00.024.735 I print_info: file type   = Q4_K - Medium
0.00.024.736 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.600 I load: special tokens cache size = 25
0.00.038.318 I load: token to piece cache size = 0.2984 MB
0.00.038.321 I print_info: arch             = gptneox
0.00.038.321 I print_info: vocab_only       = 0
0.00.038.321 I print_info: n_ctx_train      = 2048
0.00.038.322 I print_info: n_embd           = 2048
0.00.038.322 I print_info: n_layer          = 24
0.00.038.325 I print_info: n_head           = 16
0.00.038.326 I print_info: n_head_kv        = 16
0.00.038.326 I print_info: n_rot            = 32
0.00.038.326 I print_info: n_swa            = 0
0.00.038.327 I print_info: n_embd_head_k    = 128
0.00.038.329 I print_info: n_embd_head_v    = 128
0.00.038.330 I print_info: n_gqa            = 1
0.00.038.330 I print_info: n_embd_k_gqa     = 2048
0.00.038.331 I print_info: n_embd_v_gqa     = 2048
0.00.038.332 I print_info: f_norm_eps       = 1.0e-05
0.00.038.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.333 I print_info: f_logit_scale    = 0.0e+00
0.00.038.333 I print_info: n_ff             = 8192
0.00.038.334 I print_info: n_expert         = 0
0.00.038.334 I print_info: n_expert_used    = 0
0.00.038.335 I print_info: causal attn      = 1
0.00.038.335 I print_info: pooling type     = 0
0.00.038.335 I print_info: rope type        = 2
0.00.038.336 I print_info: rope scaling     = linear
0.00.038.336 I print_info: freq_base_train  = 10000.0
0.00.038.336 I print_info: freq_scale_train = 1
0.00.038.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.337 I print_info: rope_finetuned   = unknown
0.00.038.337 I print_info: ssm_d_conv       = 0
0.00.038.337 I print_info: ssm_d_inner      = 0
0.00.038.337 I print_info: ssm_d_state      = 0
0.00.038.337 I print_info: ssm_dt_rank      = 0
0.00.038.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.339 I print_info: model type       = 1.4B
0.00.038.339 I print_info: model params     = 1.41 B
0.00.038.340 I print_info: general.name     = 1.4B
0.00.038.340 I print_info: vocab type       = BPE
0.00.038.340 I print_info: n_vocab          = 50304
0.00.038.340 I print_info: n_merges         = 50009
0.00.038.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: LF token         = 187 ''
0.00.038.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: max token length = 1024
0.00.038.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.518.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.518.867 I load_tensors: offloading output layer to GPU
0.00.518.868 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.905 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.906 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.520.626 I llama_init_from_model: n_seq_max     = 1
0.00.520.629 I llama_init_from_model: n_ctx         = 2048
0.00.520.629 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.520.630 I llama_init_from_model: n_batch       = 2048
0.00.520.630 I llama_init_from_model: n_ubatch      = 512
0.00.520.631 I llama_init_from_model: flash_attn    = 0
0.00.520.633 I llama_init_from_model: freq_base     = 10000.0
0.00.520.634 I llama_init_from_model: freq_scale    = 1
0.00.520.636 I ggml_metal_init: allocating
0.00.520.724 I ggml_metal_init: found device: Apple M4
0.00.520.736 I ggml_metal_init: picking default device: Apple M4
0.00.522.674 I ggml_metal_init: using embedded metal library
0.00.529.125 I ggml_metal_init: GPU name:   Apple M4
0.00.529.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.132 I ggml_metal_init: simdgroup reduction   = true
0.00.529.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.133 I ggml_metal_init: has residency sets    = true
0.00.529.133 I ggml_metal_init: has bfloat            = true
0.00.529.133 I ggml_metal_init: use bfloat            = true
0.00.529.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.547.419 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.347 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.604.354 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.604.389 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.015 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.609.018 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.609.018 I llama_init_from_model: graph nodes  = 967
0.00.609.018 I llama_init_from_model: graph splits = 2
0.00.609.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.609.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.609.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.748 I main: llama threadpool init, n_threads = 4
0.00.663.787 I 
0.00.663.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.809 I 
0.00.663.974 I sampler seed: 1234
0.00.663.978 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.998 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.999 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.999 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.418.517 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48200.95 tokens per second)
0.01.418.518 I llama_perf_context_print:        load time =     653.17 ms
0.01.418.518 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.37 tokens per second)
0.01.418.519 I llama_perf_context_print:        eval time =     704.32 ms /    63 runs   (   11.18 ms per token,    89.45 tokens per second)
0.01.418.523 I llama_perf_context_print:       total time =     755.54 ms /    70 tokens
0.01.418.754 I ggml_metal_free: deallocating

real	0m1.437s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.001 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.988 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.864 I llama_model_loader: - type  f32:  194 tensors
0.00.025.864 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.864 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.864 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.865 I print_info: file format = GGUF V3 (latest)
0.00.025.866 I print_info: file type   = Q4_K - Medium
0.00.025.867 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.301 I load: special tokens cache size = 25
0.00.040.472 I load: token to piece cache size = 0.2984 MB
0.00.040.476 I print_info: arch             = gptneox
0.00.040.476 I print_info: vocab_only       = 0
0.00.040.477 I print_info: n_ctx_train      = 2048
0.00.040.477 I print_info: n_embd           = 2048
0.00.040.477 I print_info: n_layer          = 24
0.00.040.481 I print_info: n_head           = 16
0.00.040.482 I print_info: n_head_kv        = 16
0.00.040.482 I print_info: n_rot            = 32
0.00.040.482 I print_info: n_swa            = 0
0.00.040.483 I print_info: n_embd_head_k    = 128
0.00.040.483 I print_info: n_embd_head_v    = 128
0.00.040.483 I print_info: n_gqa            = 1
0.00.040.484 I print_info: n_embd_k_gqa     = 2048
0.00.040.485 I print_info: n_embd_v_gqa     = 2048
0.00.040.485 I print_info: f_norm_eps       = 1.0e-05
0.00.040.487 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.488 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.488 I print_info: f_logit_scale    = 0.0e+00
0.00.040.488 I print_info: n_ff             = 8192
0.00.040.489 I print_info: n_expert         = 0
0.00.040.489 I print_info: n_expert_used    = 0
0.00.040.489 I print_info: causal attn      = 1
0.00.040.489 I print_info: pooling type     = 0
0.00.040.489 I print_info: rope type        = 2
0.00.040.489 I print_info: rope scaling     = linear
0.00.040.490 I print_info: freq_base_train  = 10000.0
0.00.040.490 I print_info: freq_scale_train = 1
0.00.040.490 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.490 I print_info: rope_finetuned   = unknown
0.00.040.490 I print_info: ssm_d_conv       = 0
0.00.040.490 I print_info: ssm_d_inner      = 0
0.00.040.491 I print_info: ssm_d_state      = 0
0.00.040.491 I print_info: ssm_dt_rank      = 0
0.00.040.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.491 I print_info: model type       = 1.4B
0.00.040.491 I print_info: model params     = 1.41 B
0.00.040.492 I print_info: general.name     = 1.4B
0.00.040.492 I print_info: vocab type       = BPE
0.00.040.493 I print_info: n_vocab          = 50304
0.00.040.493 I print_info: n_merges         = 50009
0.00.040.493 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: LF token         = 187 ''
0.00.040.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.496 I print_info: max token length = 1024
0.00.040.496 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.523.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.072 I load_tensors: offloading output layer to GPU
0.00.523.073 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.104 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.105 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.524.926 I llama_init_from_model: n_seq_max     = 1
0.00.524.930 I llama_init_from_model: n_ctx         = 128
0.00.524.931 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.931 I llama_init_from_model: n_batch       = 128
0.00.524.931 I llama_init_from_model: n_ubatch      = 128
0.00.524.932 I llama_init_from_model: flash_attn    = 0
0.00.524.934 I llama_init_from_model: freq_base     = 10000.0
0.00.524.934 I llama_init_from_model: freq_scale    = 1
0.00.524.935 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.938 I ggml_metal_init: allocating
0.00.525.002 I ggml_metal_init: found device: Apple M4
0.00.525.015 I ggml_metal_init: picking default device: Apple M4
0.00.526.874 I ggml_metal_init: using embedded metal library
0.00.533.673 I ggml_metal_init: GPU name:   Apple M4
0.00.533.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.680 I ggml_metal_init: simdgroup reduction   = true
0.00.533.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.681 I ggml_metal_init: has residency sets    = true
0.00.533.681 I ggml_metal_init: has bfloat            = true
0.00.533.681 I ggml_metal_init: use bfloat            = true
0.00.533.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.551.504 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.555.056 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.555.102 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.558.278 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.558.280 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.558.280 I llama_init_from_model: graph nodes  = 967
0.00.558.281 I llama_init_from_model: graph splits = 2
0.00.558.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.558.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.963 I 
0.00.585.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.052 I perplexity: tokenizing the input ..
0.00.590.348 I perplexity: tokenization took 5.294 ms
0.00.590.352 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.968 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.723.320 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.723.345 I llama_perf_context_print:        load time =     574.95 ms
0.00.723.345 I llama_perf_context_print: prompt eval time =     131.38 ms /   128 tokens (    1.03 ms per token,   974.24 tokens per second)
0.00.723.346 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.346 I llama_perf_context_print:       total time =     138.38 ms /   129 tokens
0.00.723.744 I ggml_metal_free: deallocating

real	0m0.739s
user	0m0.078s
sys	0m0.123s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.692 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.693 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.693 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.167 I llama_model_loader: - type  f32:  194 tensors
0.00.024.167 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.168 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.168 I print_info: file format = GGUF V3 (latest)
0.00.024.169 I print_info: file type   = Q5_K - Medium
0.00.024.169 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.092 I load: special tokens cache size = 25
0.00.038.140 I load: token to piece cache size = 0.2984 MB
0.00.038.142 I print_info: arch             = gptneox
0.00.038.143 I print_info: vocab_only       = 0
0.00.038.143 I print_info: n_ctx_train      = 2048
0.00.038.143 I print_info: n_embd           = 2048
0.00.038.143 I print_info: n_layer          = 24
0.00.038.146 I print_info: n_head           = 16
0.00.038.147 I print_info: n_head_kv        = 16
0.00.038.147 I print_info: n_rot            = 32
0.00.038.147 I print_info: n_swa            = 0
0.00.038.147 I print_info: n_embd_head_k    = 128
0.00.038.149 I print_info: n_embd_head_v    = 128
0.00.038.150 I print_info: n_gqa            = 1
0.00.038.151 I print_info: n_embd_k_gqa     = 2048
0.00.038.151 I print_info: n_embd_v_gqa     = 2048
0.00.038.156 I print_info: f_norm_eps       = 1.0e-05
0.00.038.156 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.156 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.157 I print_info: f_logit_scale    = 0.0e+00
0.00.038.157 I print_info: n_ff             = 8192
0.00.038.158 I print_info: n_expert         = 0
0.00.038.158 I print_info: n_expert_used    = 0
0.00.038.158 I print_info: causal attn      = 1
0.00.038.158 I print_info: pooling type     = 0
0.00.038.160 I print_info: rope type        = 2
0.00.038.161 I print_info: rope scaling     = linear
0.00.038.161 I print_info: freq_base_train  = 10000.0
0.00.038.162 I print_info: freq_scale_train = 1
0.00.038.162 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.162 I print_info: rope_finetuned   = unknown
0.00.038.162 I print_info: ssm_d_conv       = 0
0.00.038.163 I print_info: ssm_d_inner      = 0
0.00.038.163 I print_info: ssm_d_state      = 0
0.00.038.165 I print_info: ssm_dt_rank      = 0
0.00.038.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.165 I print_info: model type       = 1.4B
0.00.038.165 I print_info: model params     = 1.41 B
0.00.038.166 I print_info: general.name     = 1.4B
0.00.038.166 I print_info: vocab type       = BPE
0.00.038.166 I print_info: n_vocab          = 50304
0.00.038.167 I print_info: n_merges         = 50009
0.00.038.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.168 I print_info: LF token         = 187 ''
0.00.038.168 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.169 I print_info: max token length = 1024
0.00.038.169 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.660 I load_tensors: offloading output layer to GPU
0.00.652.661 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.682 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.652.683 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.653.614 I llama_init_from_model: n_seq_max     = 1
0.00.653.617 I llama_init_from_model: n_ctx         = 2048
0.00.653.618 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.618 I llama_init_from_model: n_batch       = 2048
0.00.653.619 I llama_init_from_model: n_ubatch      = 512
0.00.653.619 I llama_init_from_model: flash_attn    = 0
0.00.653.620 I llama_init_from_model: freq_base     = 10000.0
0.00.653.621 I llama_init_from_model: freq_scale    = 1
0.00.653.622 I ggml_metal_init: allocating
0.00.653.660 I ggml_metal_init: found device: Apple M4
0.00.653.670 I ggml_metal_init: picking default device: Apple M4
0.00.654.770 I ggml_metal_init: using embedded metal library
0.00.658.855 I ggml_metal_init: GPU name:   Apple M4
0.00.658.864 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.866 I ggml_metal_init: simdgroup reduction   = true
0.00.658.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.867 I ggml_metal_init: has residency sets    = true
0.00.658.867 I ggml_metal_init: has bfloat            = true
0.00.658.867 I ggml_metal_init: use bfloat            = true
0.00.658.869 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.545 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.912 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.705.919 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.710.173 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.710.175 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.710.175 I llama_init_from_model: graph nodes  = 967
0.00.710.175 I llama_init_from_model: graph splits = 2
0.00.710.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.710.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.710.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.532 I main: llama threadpool init, n_threads = 4
0.00.775.572 I 
0.00.775.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.600 I 
0.00.775.765 I sampler seed: 1234
0.00.775.770 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.781 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.623.734 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.623.736 I llama_perf_context_print:        load time =     766.05 ms
0.01.623.736 I llama_perf_context_print: prompt eval time =      52.56 ms /     7 tokens (    7.51 ms per token,   133.18 tokens per second)
0.01.623.737 I llama_perf_context_print:        eval time =     792.68 ms /    63 runs   (   12.58 ms per token,    79.48 tokens per second)
0.01.623.738 I llama_perf_context_print:       total time =     848.91 ms /    70 tokens
0.01.623.946 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.106s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.180 I llama_model_loader: - type  f32:  194 tensors
0.00.024.180 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.181 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.181 I print_info: file format = GGUF V3 (latest)
0.00.024.182 I print_info: file type   = Q5_K - Medium
0.00.024.183 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.331 I load: special tokens cache size = 25
0.00.038.492 I load: token to piece cache size = 0.2984 MB
0.00.038.503 I print_info: arch             = gptneox
0.00.038.504 I print_info: vocab_only       = 0
0.00.038.504 I print_info: n_ctx_train      = 2048
0.00.038.504 I print_info: n_embd           = 2048
0.00.038.505 I print_info: n_layer          = 24
0.00.038.510 I print_info: n_head           = 16
0.00.038.511 I print_info: n_head_kv        = 16
0.00.038.511 I print_info: n_rot            = 32
0.00.038.512 I print_info: n_swa            = 0
0.00.038.513 I print_info: n_embd_head_k    = 128
0.00.038.513 I print_info: n_embd_head_v    = 128
0.00.038.513 I print_info: n_gqa            = 1
0.00.038.514 I print_info: n_embd_k_gqa     = 2048
0.00.038.515 I print_info: n_embd_v_gqa     = 2048
0.00.038.516 I print_info: f_norm_eps       = 1.0e-05
0.00.038.516 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.516 I print_info: f_logit_scale    = 0.0e+00
0.00.038.517 I print_info: n_ff             = 8192
0.00.038.517 I print_info: n_expert         = 0
0.00.038.518 I print_info: n_expert_used    = 0
0.00.038.518 I print_info: causal attn      = 1
0.00.038.518 I print_info: pooling type     = 0
0.00.038.518 I print_info: rope type        = 2
0.00.038.518 I print_info: rope scaling     = linear
0.00.038.519 I print_info: freq_base_train  = 10000.0
0.00.038.519 I print_info: freq_scale_train = 1
0.00.038.519 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.519 I print_info: rope_finetuned   = unknown
0.00.038.521 I print_info: ssm_d_conv       = 0
0.00.038.521 I print_info: ssm_d_inner      = 0
0.00.038.521 I print_info: ssm_d_state      = 0
0.00.038.522 I print_info: ssm_dt_rank      = 0
0.00.038.522 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.522 I print_info: model type       = 1.4B
0.00.038.522 I print_info: model params     = 1.41 B
0.00.038.522 I print_info: general.name     = 1.4B
0.00.038.523 I print_info: vocab type       = BPE
0.00.038.524 I print_info: n_vocab          = 50304
0.00.038.528 I print_info: n_merges         = 50009
0.00.038.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: LF token         = 187 ''
0.00.038.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: max token length = 1024
0.00.038.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.812 I load_tensors: offloading output layer to GPU
0.00.580.812 I load_tensors: offloaded 25/25 layers to GPU
0.00.580.845 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.580.847 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.582.588 I llama_init_from_model: n_seq_max     = 1
0.00.582.592 I llama_init_from_model: n_ctx         = 128
0.00.582.592 I llama_init_from_model: n_ctx_per_seq = 128
0.00.582.592 I llama_init_from_model: n_batch       = 128
0.00.582.593 I llama_init_from_model: n_ubatch      = 128
0.00.582.594 I llama_init_from_model: flash_attn    = 0
0.00.582.595 I llama_init_from_model: freq_base     = 10000.0
0.00.582.596 I llama_init_from_model: freq_scale    = 1
0.00.582.597 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.582.599 I ggml_metal_init: allocating
0.00.582.651 I ggml_metal_init: found device: Apple M4
0.00.582.666 I ggml_metal_init: picking default device: Apple M4
0.00.584.160 I ggml_metal_init: using embedded metal library
0.00.590.564 I ggml_metal_init: GPU name:   Apple M4
0.00.590.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.569 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.570 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.570 I ggml_metal_init: simdgroup reduction   = true
0.00.590.570 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.571 I ggml_metal_init: has residency sets    = true
0.00.590.571 I ggml_metal_init: has bfloat            = true
0.00.590.571 I ggml_metal_init: use bfloat            = true
0.00.590.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.087 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.650 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.611.654 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.611.709 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.983 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.614.986 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.614.987 I llama_init_from_model: graph nodes  = 967
0.00.614.987 I llama_init_from_model: graph splits = 2
0.00.614.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.614.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.395 I 
0.00.648.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.486 I perplexity: tokenizing the input ..
0.00.656.109 I perplexity: tokenization took 7.619 ms
0.00.656.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.216 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.578 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.602 I llama_perf_context_print:        load time =     639.62 ms
0.00.795.602 I llama_perf_context_print: prompt eval time =     137.18 ms /   128 tokens (    1.07 ms per token,   933.08 tokens per second)
0.00.795.603 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.603 I llama_perf_context_print:       total time =     147.21 ms /   129 tokens
0.00.795.998 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.080s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.131 I llama_model_loader: - type  f32:  194 tensors
0.00.024.132 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.132 I print_info: file format = GGUF V3 (latest)
0.00.024.133 I print_info: file type   = Q6_K
0.00.024.133 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.964 I load: special tokens cache size = 25
0.00.038.063 I load: token to piece cache size = 0.2984 MB
0.00.038.066 I print_info: arch             = gptneox
0.00.038.066 I print_info: vocab_only       = 0
0.00.038.066 I print_info: n_ctx_train      = 2048
0.00.038.067 I print_info: n_embd           = 2048
0.00.038.067 I print_info: n_layer          = 24
0.00.038.069 I print_info: n_head           = 16
0.00.038.070 I print_info: n_head_kv        = 16
0.00.038.070 I print_info: n_rot            = 32
0.00.038.071 I print_info: n_swa            = 0
0.00.038.071 I print_info: n_embd_head_k    = 128
0.00.038.071 I print_info: n_embd_head_v    = 128
0.00.038.072 I print_info: n_gqa            = 1
0.00.038.072 I print_info: n_embd_k_gqa     = 2048
0.00.038.073 I print_info: n_embd_v_gqa     = 2048
0.00.038.074 I print_info: f_norm_eps       = 1.0e-05
0.00.038.074 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.075 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.075 I print_info: f_logit_scale    = 0.0e+00
0.00.038.077 I print_info: n_ff             = 8192
0.00.038.077 I print_info: n_expert         = 0
0.00.038.078 I print_info: n_expert_used    = 0
0.00.038.078 I print_info: causal attn      = 1
0.00.038.078 I print_info: pooling type     = 0
0.00.038.078 I print_info: rope type        = 2
0.00.038.080 I print_info: rope scaling     = linear
0.00.038.082 I print_info: freq_base_train  = 10000.0
0.00.038.082 I print_info: freq_scale_train = 1
0.00.038.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.082 I print_info: rope_finetuned   = unknown
0.00.038.083 I print_info: ssm_d_conv       = 0
0.00.038.083 I print_info: ssm_d_inner      = 0
0.00.038.083 I print_info: ssm_d_state      = 0
0.00.038.083 I print_info: ssm_dt_rank      = 0
0.00.038.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.087 I print_info: model type       = 1.4B
0.00.038.087 I print_info: model params     = 1.41 B
0.00.038.087 I print_info: general.name     = 1.4B
0.00.038.088 I print_info: vocab type       = BPE
0.00.038.089 I print_info: n_vocab          = 50304
0.00.038.089 I print_info: n_merges         = 50009
0.00.038.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.090 I print_info: LF token         = 187 ''
0.00.038.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.091 I print_info: max token length = 1024
0.00.038.091 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.065 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.079 I load_tensors: offloading output layer to GPU
0.00.641.079 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.113 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.641.115 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.642.614 I llama_init_from_model: n_seq_max     = 1
0.00.642.618 I llama_init_from_model: n_ctx         = 2048
0.00.642.618 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.642.618 I llama_init_from_model: n_batch       = 2048
0.00.642.619 I llama_init_from_model: n_ubatch      = 512
0.00.642.620 I llama_init_from_model: flash_attn    = 0
0.00.642.621 I llama_init_from_model: freq_base     = 10000.0
0.00.642.621 I llama_init_from_model: freq_scale    = 1
0.00.642.623 I ggml_metal_init: allocating
0.00.642.637 I ggml_metal_init: found device: Apple M4
0.00.642.646 I ggml_metal_init: picking default device: Apple M4
0.00.644.198 I ggml_metal_init: using embedded metal library
0.00.650.475 I ggml_metal_init: GPU name:   Apple M4
0.00.650.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.480 I ggml_metal_init: simdgroup reduction   = true
0.00.650.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.481 I ggml_metal_init: has residency sets    = true
0.00.650.481 I ggml_metal_init: has bfloat            = true
0.00.650.481 I ggml_metal_init: use bfloat            = true
0.00.650.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.199 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.036 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.722.047 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.369 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.371 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.372 I llama_init_from_model: graph nodes  = 967
0.00.726.372 I llama_init_from_model: graph splits = 2
0.00.726.377 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.518 I main: llama threadpool init, n_threads = 4
0.00.792.564 I 
0.00.792.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.589 I 
0.00.792.767 I sampler seed: 1234
0.00.792.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.793 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.793 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.667.879 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.667.880 I llama_perf_context_print:        load time =     782.94 ms
0.01.667.881 I llama_perf_context_print: prompt eval time =      57.53 ms /     7 tokens (    8.22 ms per token,   121.68 tokens per second)
0.01.667.881 I llama_perf_context_print:        eval time =     814.76 ms /    63 runs   (   12.93 ms per token,    77.32 tokens per second)
0.01.667.883 I llama_perf_context_print:       total time =     876.09 ms /    70 tokens
0.01.668.145 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4751 (ecc8e3ae) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.456 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.459 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.461 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.486 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.385 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.387 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.388 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.389 I llama_model_loader: - type  f32:  194 tensors
0.00.024.389 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.390 I print_info: file format = GGUF V3 (latest)
0.00.024.391 I print_info: file type   = Q6_K
0.00.024.392 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.344 I load: special tokens cache size = 25
0.00.038.393 I load: token to piece cache size = 0.2984 MB
0.00.038.398 I print_info: arch             = gptneox
0.00.038.398 I print_info: vocab_only       = 0
0.00.038.398 I print_info: n_ctx_train      = 2048
0.00.038.398 I print_info: n_embd           = 2048
0.00.038.399 I print_info: n_layer          = 24
0.00.038.403 I print_info: n_head           = 16
0.00.038.404 I print_info: n_head_kv        = 16
0.00.038.404 I print_info: n_rot            = 32
0.00.038.405 I print_info: n_swa            = 0
0.00.038.405 I print_info: n_embd_head_k    = 128
0.00.038.405 I print_info: n_embd_head_v    = 128
0.00.038.406 I print_info: n_gqa            = 1
0.00.038.407 I print_info: n_embd_k_gqa     = 2048
0.00.038.407 I print_info: n_embd_v_gqa     = 2048
0.00.038.408 I print_info: f_norm_eps       = 1.0e-05
0.00.038.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.408 I print_info: f_logit_scale    = 0.0e+00
0.00.038.409 I print_info: n_ff             = 8192
0.00.038.409 I print_info: n_expert         = 0
0.00.038.409 I print_info: n_expert_used    = 0
0.00.038.410 I print_info: causal attn      = 1
0.00.038.410 I print_info: pooling type     = 0
0.00.038.410 I print_info: rope type        = 2
0.00.038.410 I print_info: rope scaling     = linear
0.00.038.411 I print_info: freq_base_train  = 10000.0
0.00.038.411 I print_info: freq_scale_train = 1
0.00.038.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.411 I print_info: rope_finetuned   = unknown
0.00.038.411 I print_info: ssm_d_conv       = 0
0.00.038.411 I print_info: ssm_d_inner      = 0
0.00.038.412 I print_info: ssm_d_state      = 0
0.00.038.412 I print_info: ssm_dt_rank      = 0
0.00.038.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.412 I print_info: model type       = 1.4B
0.00.038.412 I print_info: model params     = 1.41 B
0.00.038.412 I print_info: general.name     = 1.4B
0.00.038.413 I print_info: vocab type       = BPE
0.00.038.413 I print_info: n_vocab          = 50304
0.00.038.416 I print_info: n_merges         = 50009
0.00.038.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: LF token         = 187 ''
0.00.038.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: max token length = 1024
0.00.038.418 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.697 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.702 I load_tensors: offloading output layer to GPU
0.00.615.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.732 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.615.736 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.617.296 I llama_init_from_model: n_seq_max     = 1
0.00.617.299 I llama_init_from_model: n_ctx         = 128
0.00.617.299 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.300 I llama_init_from_model: n_batch       = 128
0.00.617.300 I llama_init_from_model: n_ubatch      = 128
0.00.617.300 I llama_init_from_model: flash_attn    = 0
0.00.617.301 I llama_init_from_model: freq_base     = 10000.0
0.00.617.302 I llama_init_from_model: freq_scale    = 1
0.00.617.303 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.305 I ggml_metal_init: allocating
0.00.617.376 I ggml_metal_init: found device: Apple M4
0.00.617.390 I ggml_metal_init: picking default device: Apple M4
0.00.618.994 I ggml_metal_init: using embedded metal library
0.00.625.063 I ggml_metal_init: GPU name:   Apple M4
0.00.625.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.068 I ggml_metal_init: simdgroup reduction   = true
0.00.625.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.069 I ggml_metal_init: has residency sets    = true
0.00.625.069 I ggml_metal_init: has bfloat            = true
0.00.625.069 I ggml_metal_init: use bfloat            = true
0.00.625.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.302 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.826 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.644.830 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.644.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.108 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.110 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.110 I llama_init_from_model: graph nodes  = 967
0.00.648.111 I llama_init_from_model: graph splits = 2
0.00.648.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.459 I 
0.00.686.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.545 I perplexity: tokenizing the input ..
0.00.693.438 I perplexity: tokenization took 6.89 ms
0.00.693.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.474 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.825.880 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.825.898 I llama_perf_context_print:        load time =     677.65 ms
0.00.825.898 I llama_perf_context_print: prompt eval time =     130.07 ms /   128 tokens (    1.02 ms per token,   984.12 tokens per second)
0.00.825.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.899 I llama_perf_context_print:       total time =     139.44 ms /   129 tokens
0.00.826.257 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.077s
sys	0m0.147s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4751 (ecc8e3ae)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1296083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129608ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129609090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129609640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129609bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12960a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12960a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12960ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12960b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12960b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12960bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12960c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12960ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12960d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12960dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12960e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12960ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12960f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12960f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1296100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129611640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129611ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1296128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129612ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129613b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129614080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1296147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129614aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129615330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129615870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129616470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129616910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129617250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1296176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129618030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1296184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129618790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1296193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129619cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12961a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12961a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12961af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12961b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12961bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12961c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12961c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12961cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12961d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12961d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12961db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12961e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12961e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12961ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12961ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12961f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12961f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12961fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1296201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129620640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129620ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129620f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129621420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1296218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129621d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1296222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1296232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1296237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1296247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1296257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1296267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129627260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1296277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1296287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129629790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1296199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12962a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12962a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12962ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12962b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12962b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12962be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12962c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12962c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12962ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12962d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12962d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12962de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12962e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12962e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12962ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12962f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12962f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12962fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129630090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129630530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1296309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129630e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129631310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1296317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129631c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1296320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129632590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129632a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129632ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129633810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129634150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1296345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1296353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129635870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129635d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1296361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129636650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1296378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129637d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1296386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129639490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12963a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12963a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12963abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12963b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12963b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12963b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12963be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12963c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12963c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12963cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12963d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12963d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12963d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12963de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12963e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12963e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12963ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12963f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12963f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12963fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12963fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129640cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129641170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129641610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129641f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1296423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129642d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1296431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129643b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129643fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129644450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1296448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129644d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129645230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1296456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129645b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129646560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129646ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129647000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129647810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129647e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129648430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129648a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1296496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129649fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12964a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12964ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12964b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12964b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12964bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12964c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12964c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12964cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12964d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12964d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12964ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12964e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12964e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12964edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12964f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12964f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12964fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1296502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1296512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1296522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129652d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1296532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1296542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1296552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1296557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1296567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129656d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1296577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129658270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1296587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1296597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129659d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12965a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12965a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12965acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12965b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12965b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12965bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12965c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12965c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12965ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12965d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12965d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12965dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12965e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12965e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12965ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12965f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12965f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12965fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12965ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1296603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129660870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129660d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1296611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129661650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129661af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129661f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129662430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1296628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129662d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129663210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1296645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129664cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1296653e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1296656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129665e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129666150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129666760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.847.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.847.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d504ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d504f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d5053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d505830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d505ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d506110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d506580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d5069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d506e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d5073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d507850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d507ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d5089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d5091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d5099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d50a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d50a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d50af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d50b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d50be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d50c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d50cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d50d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d50da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d50e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d50e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d50e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d50eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d50f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d50f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d50f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d50fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d510280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d510540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d5109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d510e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d511290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d511700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d511b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d511fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d512450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d5128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d512d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d5131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d513610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d513a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d513ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d514360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d5147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d514c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d5150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d515520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d515990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d515e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d516270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d5166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d516c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d517150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d5175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d517a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d517ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d518310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d518780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d518bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d519060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d5194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d519940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d519db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d51a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d51a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d51ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d51af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d51b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d51b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d51bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d51c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d51c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d51ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d51ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d51d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d51d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d51dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d51e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d51e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d51e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d51ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d51f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d51f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d51fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d51ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d5203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d520830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d520ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d521110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d521580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d5219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d521e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d5222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d522740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d522bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d523020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d523490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d523900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d523d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d5241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d524650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d524ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d524f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d5253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d525810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d525c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d5260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d526560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d5269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d526e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d5272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d527720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d527b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d528000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d528470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d5288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d528d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d5291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d529630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d529aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d529f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d52a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d52a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d52ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d52b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d52b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d52b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d52be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d52c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d52c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d52cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d52cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d52d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d52d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d52dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d52e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d52e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d52ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d52eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d52f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d52f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d52fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d5300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d530520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d530990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d530e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d531270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d5316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d531b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d531fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d532430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d5328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d532d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d533180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d5335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d533a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d533ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d534340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d5347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d534c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d535090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d535cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d535f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d536240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d5366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d536b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d536f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d537400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d537870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d537ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d538150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d5385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d538a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d538ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d539310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d539780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d539bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d53a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d53a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d53a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d53adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d53b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d53b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d53bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d53bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d53c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d53c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d53ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d53d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d53d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d53da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d53de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d53e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d53e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d53ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d53f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d53f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d53fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d53ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d540390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d540800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d540c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d5410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d541600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d541b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d542680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d542940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d542f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d5434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d543a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d544040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d544600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d544bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d545180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d545740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d545d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d5462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d546880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d546e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d547400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d5479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d547f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d548540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d548b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d5490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d549680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d549c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d54a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d54a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d54ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d54b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d54b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d54bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d54c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d54ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d54d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d54d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d54db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d54e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d54e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d54ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d54f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d54f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d54fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d5503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d550980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d550f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d551500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d551ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d552080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d552640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d552c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d5531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d553780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d553d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d554300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d5548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d554e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d555440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d555a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d555fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d556580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d556b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d557040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d557540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d557a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d557f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d558440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d558940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d558e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d559340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d559840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d559d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d55a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d55a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d55ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d55b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d55b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d55c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d55c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d55ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d55d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d55d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d55e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d55e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d55e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129666410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1296480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1296486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12961b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12961b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12961d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12964a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129612b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12961a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129618a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12961abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129611b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12961ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12962a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129665960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129614d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129615020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12964a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129648d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129613190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129613710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129666bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129666e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129667140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129667400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1296676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129667980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129667c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129667f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1296681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129668480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129668740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129668a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129668cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129668f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129669240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129669500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1296697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129669a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129669d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12966a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12966a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12966a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12966a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12966ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12966adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12966b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12966b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12966b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12966b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12966bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12966be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12966c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12966c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12966c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12966c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12966cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12966cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12966d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12966d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12966d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12966d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12966dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12966df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12966e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12966e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12966e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12966ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12966ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12966efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12966f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12966f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12966f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12966fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12966fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129670040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129670300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1296705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129670880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129670b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129670e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1296710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129671380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129671640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129671900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129671bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129671e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129672140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129672400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1296726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129672980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129672c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129672f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1296731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129673480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129673740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129673a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129673cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129673f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129674240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129674500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1296747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129674a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129674d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129675000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1296752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129675580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129675840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129675b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129675dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129676080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129676340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129676600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1296768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129676b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129676e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129677100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1296773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129677680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129677940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129677c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129677ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129678180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129678440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129678700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1296789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129678c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129678f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129679200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1296794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129679780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129679a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129679d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129679fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12967a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12967a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12967a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12967aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12967ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12967b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12967b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12967b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12967b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12967bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12967be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12967c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12967c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12967c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12967c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12967cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12967ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12967d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12967d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12967d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12967d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12967dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12967df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12967e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12967e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12967e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12967ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12967ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12967ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12967f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12967f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12967f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12967fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12967fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129680000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1296802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129680580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129680840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129680b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129680dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129681080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129681340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129681600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1296818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129681b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129681e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129682100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1296823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129682680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129682940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129682c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129682ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129683180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129683440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129683700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1296839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129683c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129683f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129684200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1296844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129684780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129684a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129684d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129684fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129685280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129685540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129685800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129685ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129685d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129686040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129686440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1296868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129687090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129687350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129687610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129687a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129687ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129688360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1296887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129688c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1296890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129689520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129689990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129689e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12968a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12968a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12968ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12968afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12968b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12968b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12968bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12968c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12968c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12968ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12968ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12968d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12968d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12968dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12968e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12968e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12968e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12968ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12968f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12968f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12968fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12968ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129690410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129690880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129690cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129691160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1296915d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129691a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129691eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129692320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129692790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129692c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129693070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1296934e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129693950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129693dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129694230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1296946a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129694b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129694f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1296953f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129695860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129695cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129696140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1296965b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129696a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129696e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129697300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129697770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129697be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129698050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1296984c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129698930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129698da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129699210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129699680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129699af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129699f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12969a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12969a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12969acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12969b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12969be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12969c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12969cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12969cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12969d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12969d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12969e000 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.930s
user	0m0.282s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4751 (ecc8e3ae)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12570e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12570f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12570f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12570fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1257101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125710750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125710d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1257112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125711860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125711d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125712260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125713280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125713a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125714240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125714960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125715080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1257157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125716690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125716db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1257174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125717bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125718490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125718bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125718e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125719480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12571a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12571a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12571a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12571ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12571b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12571b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12571be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12571c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12571c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12571ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12571cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12571d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12571d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12571dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12571e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12571e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12571ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12571ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12571f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12571f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125720890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125720ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1257214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125721ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1257220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1257226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125722ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125723810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125723ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1257240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1257248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1257254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125725970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125725e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1257262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125726750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125726bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125727090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125727530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1257279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125727e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125728310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125728860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125728db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125729300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125729850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125729da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12572a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12572a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12572ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12572b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12572b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12572bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12572c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12572c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12572cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12572d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12572d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12572dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12572e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12572e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12572ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12572f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12572f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12572fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125730290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12571ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125731400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125731950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125731ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1257323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125732940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125732e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1257333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125733930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1257343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125734920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125734e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1257353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125735860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125735d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1257361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125736640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125736f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125737420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1257378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125737d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125738200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1257386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125738b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125738fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125739480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125739920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125739dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12573a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12573a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12573aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12573b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12573b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12573b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12573be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12573c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12573c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12573cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12573d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12573d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12573d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12573de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12573e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12573e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12573ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12573f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12573f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12573fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12573fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125740380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125740820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125740cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125741160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125741600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125741aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125741f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1257423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125742880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125742d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1257431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125743660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125743b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125743fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125744440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1257448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125744d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125745220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1257456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125745b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125746000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1257464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125746940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125746de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125747280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125747720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125747bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125748060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125748500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1257489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1257492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125749780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125749c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12574a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12574a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12574aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12574aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12574b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12574b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12574bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12574c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12574c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12574cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12574d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12574d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12574db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12574ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12574e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12574e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12574eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12574f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12574fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12574ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125750550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125751350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1257517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125751c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1257528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125752e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125753380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1257538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125753e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125754370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1257548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125754e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125755360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1257558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125755e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125756350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1257568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125756df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125757340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125757890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125757de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125758330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125758880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125758dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125759320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125759870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125759dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12575a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12575a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12575adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12575b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12575b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12575bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12575c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12575c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12575cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12575d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12575d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12575dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12575e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12575e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12575ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12575f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12575f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12575fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1257602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125760800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125760d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1257612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1257617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125761d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125762290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1257627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125762d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125763280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1257637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125763d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1257647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125764d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125765260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125765700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125765ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125766040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1257664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125766980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125766e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1257672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125767760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125767c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1257680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125768540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1257689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125768e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125769320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1257697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125769d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12576a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12576ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12576b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12576b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12576bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12576c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12576c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12576cd10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.141.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120a04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120a04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120a053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120a05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120a05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120a06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120a06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120a069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120a06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120a073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120a07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120a07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120a089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120a091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120a099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120a0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120a0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120a0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120a0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120a0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120a0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120a0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120a0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120a0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120a0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120a0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120a0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120a0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120a0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120a0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120a0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120a0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120a10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120a10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120a109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120a10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120a11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120a11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120a11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120a11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120a12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120a128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120a12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120a131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120a13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120a13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120a13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120a14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120a147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120a14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120a150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120a15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120a15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120a15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120a16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120a166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120a16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120a17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120a175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120a17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120a17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120a18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120a18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120a18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120a19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120a194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120a19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120a19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120a1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120a1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120a1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120a1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120a1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120a1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120a1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120a1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120a1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120a1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120a1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120a1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120a1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120a1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120a1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120a1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120a1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120a1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120a1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120a1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120a1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120a1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120a203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120a20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120a20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120a21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120a21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120a219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120a21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120a222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120a22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120a22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120a23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120a23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120a23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120a23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120a241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120a24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120a24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120a24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120a253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120a25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120a25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120a260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120a26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120a269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120a26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120a272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120a27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120a27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120a28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120a28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120a288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120a28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120a291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120a29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120a29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120a29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120a2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120a2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120a2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120a2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120a2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120a2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120a2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120a2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120a2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120a2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120a2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120a2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120a2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120a2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120a2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120a2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120a2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120a2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120a2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120a2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120a2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120a300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120a30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120a30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120a30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120a31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120a316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120a31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120a31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120a32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120a328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120a32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120a33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120a335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120a33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120a33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120a34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120a347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120a34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120a35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120a35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120a35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120a36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120a366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120a36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120a36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120a37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120a37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120a37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120a38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120a385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120a38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120a38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120a39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120a39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120a39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120a3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120a3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120a3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120a3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120a3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120a3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120a3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120a3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120a3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120a3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120a3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120a3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120a3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120a3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120a3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120a3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120a3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120a3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120a3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120a3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120a3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120a3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120a40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120a40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120a40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120a410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120a41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120a41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120a42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120a42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120a42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120a434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120a43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120a44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120a44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120a44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120a45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120a45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120a45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120a462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120a46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120a46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120a47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120a479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120a47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120a48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120a48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120a490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120a49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120a49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120a4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120a4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120a4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120a4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120a4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120a4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120a4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120a4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120a4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120a4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120a4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120a4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120a4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120a4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120a4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120a4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120a4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120a503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120a50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120a50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120a51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120a51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120a52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120a52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120a52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120a531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120a53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120a53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120a54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120a548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120a54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120a55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120a55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120a55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120a56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120a56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120a57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120a57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120a57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120a57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120a58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120a58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120a58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120a59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120a59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120a59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120a5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120a5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120a5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120a5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120a5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120a5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120a5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120a5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120a5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120a5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120a5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120a5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120a5e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127004c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1270050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127005540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1270059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127005e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127006290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127006700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127006b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127006fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1270074f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127007960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127007fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127008b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1270092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127009ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12700a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12700a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12700b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12700b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12700bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12700c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12700cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12700d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12700db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12700e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12700e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12700e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12700eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12700f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12700f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12700f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12700ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127010390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127010650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127010ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127010f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1270113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127011810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127011c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1270120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127012560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1270129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127012e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1270132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127013720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127013b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127014000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127014470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1270148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127014d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1270151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127015630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127015aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127015f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127016380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1270167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127016d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127017260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1270176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127017b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127017fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127018420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127018890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127018d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127019170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1270195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127019a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127019ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12701a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12701a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12701ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12701b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12701b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12701b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12701bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12701c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12701c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12701cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12701cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12701d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12701d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12701dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12701e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12701e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12701ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12701eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12701f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12701f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12701fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127020060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1270204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127020940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127020db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127021220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127021690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127021b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127021f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1270223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127022850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127022cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127023130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1270235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127023a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127023e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1270247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127024ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127024f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127025390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127025800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127025c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1270260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127026550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1270269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127026e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1270272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127027710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127027b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127027ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127028460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1270288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127028d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1270291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127029620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127029a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127029f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12702a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12702a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12702ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12702b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12702b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12702b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12702be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12702c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12702c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12702cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12702cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12702d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12702d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12702dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12702e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12702e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12702ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12702eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12702f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12702f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12702fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1270300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127030510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127030980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127030df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127031260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1270316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127031fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127032420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127032890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127032d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127033170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1270335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127033ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127034330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1270347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127034c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127035080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1270354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127035960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127035dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127036240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1270366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127036b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127036f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127037400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127037870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127037ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127038150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1270385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127038a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127038ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127039310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127039780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127039bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12703a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12703a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12703a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12703adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12703b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12703b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12703bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12703bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12703c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12703c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12703ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12703d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12703d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12703da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12703de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12703e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12703e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12703ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12703f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12703f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12703f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12703fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127040200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127040670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127040ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127041070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1270414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127041950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1270424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127042760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127042a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127042e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127043300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127043770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127043be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127044050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1270444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127044930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127044da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127045210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127045680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127045af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127045f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1270463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127046840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127046cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127047120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127047590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127047a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127047e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1270482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127048750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127048bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127049030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1270494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127049910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127049d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12704a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12704a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12704aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12704af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12704b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12704b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12704bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12704c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12704c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12704c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12704ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12704d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12704d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12704dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12704e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12704e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12704e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12704ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12704f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12704f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12704fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12704ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127050390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127050800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127050c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1270510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127051550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1270519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127051e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1270522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127052710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127052b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127052ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127053460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1270538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127053d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1270541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127054620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127054a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127054f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127055370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1270557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127055c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1270560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127056b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127057250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127057970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127058350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1270587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127058dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1270593d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m1.024s
user	0m0.251s
sys	0m0.204s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.11 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.54 sec*proc (2 tests)

Total Test time (real) =   1.56 sec
        1.58 real         0.51 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
