Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.150s
user	0m1.063s
sys	0m1.473s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf-hash
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Linking CXX shared library ../bin/libllama.dylib
[ 24%] Built target llama-gguf
[ 24%] Built target llama-gguf-hash
[ 24%] Built target llama
[ 24%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 49%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-chat
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Built target test-log
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-arg-parser
[ 60%] Built target test-backend-ops
[ 60%] Built target test-chat-template
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Built target test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-barrier
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Built target test-quantize-perf
[ 68%] Built target test-quantize-fns
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Built target test-rope
[ 68%] Built target llama-batched-bench
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Built target llama-batched
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 77%] Built target llama-imatrix
[ 77%] Built target llama-bench
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookup
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-retrieval
[ 81%] Built target llama-parallel
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Built target llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-tokenize
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-speculative-simple
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-cvector-generator
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-tts
[ 95%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-gemma3-cli
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-gemma3-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.125s
user	0m6.580s
sys	0m10.069s

main: quantize time =  3310.00 ms
main:    total time =  3310.00 ms

main: quantize time =  1539.67 ms
main:    total time =  1539.67 ms

main: quantize time =  1595.62 ms
main:    total time =  1595.62 ms

main: quantize time =  2254.88 ms
main:    total time =  2254.88 ms

main: quantize time =  2440.91 ms
main:    total time =  2440.91 ms

main: quantize time =  5284.88 ms
main:    total time =  5284.88 ms

main: quantize time =  5867.72 ms
main:    total time =  5867.72 ms

main: quantize time =  6986.47 ms
main:    total time =  6986.47 ms

main: quantize time =  6290.24 ms
main:    total time =  6290.24 ms

main: quantize time =  4673.16 ms
main:    total time =  4673.16 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.194 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.354 I main: llama backend init
0.00.000.360 I main: load the model and apply lora adapter, if any
0.00.049.065 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.605 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.644 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.081.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.062 I llama_model_loader: - type  f32:  194 tensors
0.00.081.062 I llama_model_loader: - type  f16:   98 tensors
0.00.081.064 I print_info: file format = GGUF V3 (latest)
0.00.081.067 I print_info: file type   = all F32 (guessed)
0.00.081.069 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.095.948 I load: special tokens cache size = 25
0.00.104.681 I load: token to piece cache size = 0.2984 MB
0.00.104.709 I print_info: arch             = gptneox
0.00.104.710 I print_info: vocab_only       = 0
0.00.104.711 I print_info: n_ctx_train      = 2048
0.00.104.711 I print_info: n_embd           = 2048
0.00.104.711 I print_info: n_layer          = 24
0.00.104.716 I print_info: n_head           = 16
0.00.104.717 I print_info: n_head_kv        = 16
0.00.104.717 I print_info: n_rot            = 32
0.00.104.717 I print_info: n_swa            = 0
0.00.104.718 I print_info: n_embd_head_k    = 128
0.00.104.720 I print_info: n_embd_head_v    = 128
0.00.104.721 I print_info: n_gqa            = 1
0.00.104.722 I print_info: n_embd_k_gqa     = 2048
0.00.104.723 I print_info: n_embd_v_gqa     = 2048
0.00.104.724 I print_info: f_norm_eps       = 1.0e-05
0.00.104.725 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.725 I print_info: f_logit_scale    = 0.0e+00
0.00.104.725 I print_info: f_attn_scale     = 0.0e+00
0.00.104.726 I print_info: n_ff             = 8192
0.00.104.726 I print_info: n_expert         = 0
0.00.104.726 I print_info: n_expert_used    = 0
0.00.104.727 I print_info: causal attn      = 1
0.00.104.727 I print_info: pooling type     = 0
0.00.104.727 I print_info: rope type        = 2
0.00.104.727 I print_info: rope scaling     = linear
0.00.104.728 I print_info: freq_base_train  = 10000.0
0.00.104.728 I print_info: freq_scale_train = 1
0.00.104.728 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.728 I print_info: rope_finetuned   = unknown
0.00.104.729 I print_info: ssm_d_conv       = 0
0.00.104.729 I print_info: ssm_d_inner      = 0
0.00.104.729 I print_info: ssm_d_state      = 0
0.00.104.729 I print_info: ssm_dt_rank      = 0
0.00.104.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.729 I print_info: model type       = 1.4B
0.00.104.730 I print_info: model params     = 1.41 B
0.00.104.730 I print_info: general.name     = 1.4B
0.00.104.731 I print_info: vocab type       = BPE
0.00.104.731 I print_info: n_vocab          = 50304
0.00.104.731 I print_info: n_merges         = 50009
0.00.104.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.732 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.737 I print_info: LF token         = 187 'Ċ'
0.00.104.737 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.738 I print_info: max token length = 1024
0.00.104.738 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.202.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.202.062 I load_tensors: offloading output layer to GPU
0.00.202.063 I load_tensors: offloaded 25/25 layers to GPU
0.00.202.091 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.202.093 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.202.650 I llama_init_from_model: n_seq_max     = 1
0.00.202.651 I llama_init_from_model: n_ctx         = 2048
0.00.202.651 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.202.651 I llama_init_from_model: n_batch       = 2048
0.00.202.651 I llama_init_from_model: n_ubatch      = 512
0.00.202.651 I llama_init_from_model: flash_attn    = 0
0.00.202.652 I llama_init_from_model: freq_base     = 10000.0
0.00.202.652 I llama_init_from_model: freq_scale    = 1
0.00.202.653 I ggml_metal_init: allocating
0.00.202.736 I ggml_metal_init: found device: Apple M4
0.00.202.740 I ggml_metal_init: picking default device: Apple M4
0.00.203.449 I ggml_metal_load_library: using embedded metal library
0.00.227.922 I ggml_metal_init: GPU name:   Apple M4
0.00.227.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.227.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.227.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.227.925 I ggml_metal_init: simdgroup reduction   = true
0.00.227.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.227.925 I ggml_metal_init: has residency sets    = true
0.00.227.925 I ggml_metal_init: has bfloat            = true
0.00.227.925 I ggml_metal_init: use bfloat            = true
0.00.227.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.227.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.744 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.443.555 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.443.562 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.443.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.447.381 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.447.383 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.447.384 I llama_init_from_model: graph nodes  = 967
0.00.447.384 I llama_init_from_model: graph splits = 2
0.00.447.392 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.447.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.447.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.023 I main: llama threadpool init, n_threads = 4
0.00.513.086 I 
0.00.513.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.117 I 
0.00.513.305 I sampler seed: 1234
0.00.513.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.345 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.345 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.343.417 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.343.418 I llama_perf_context_print:        load time =     463.08 ms
0.02.343.418 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.55 tokens per second)
0.02.343.420 I llama_perf_context_print:        eval time =    1783.18 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.343.421 I llama_perf_context_print:       total time =    1831.27 ms /    70 tokens
0.02.343.633 I ggml_metal_free: deallocating

real	0m2.687s
user	0m0.138s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.575 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.357 I llama_model_loader: - type  f32:  194 tensors
0.00.036.357 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.358 I print_info: file format = GGUF V3 (latest)
0.00.036.359 I print_info: file type   = Q8_0
0.00.036.360 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.141 I load: special tokens cache size = 25
0.00.051.874 I load: token to piece cache size = 0.2984 MB
0.00.051.890 I print_info: arch             = gptneox
0.00.051.891 I print_info: vocab_only       = 0
0.00.051.892 I print_info: n_ctx_train      = 2048
0.00.051.892 I print_info: n_embd           = 2048
0.00.051.892 I print_info: n_layer          = 24
0.00.051.898 I print_info: n_head           = 16
0.00.051.899 I print_info: n_head_kv        = 16
0.00.051.899 I print_info: n_rot            = 32
0.00.051.899 I print_info: n_swa            = 0
0.00.051.900 I print_info: n_embd_head_k    = 128
0.00.051.900 I print_info: n_embd_head_v    = 128
0.00.051.900 I print_info: n_gqa            = 1
0.00.051.901 I print_info: n_embd_k_gqa     = 2048
0.00.051.902 I print_info: n_embd_v_gqa     = 2048
0.00.051.903 I print_info: f_norm_eps       = 1.0e-05
0.00.051.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.903 I print_info: f_logit_scale    = 0.0e+00
0.00.051.903 I print_info: f_attn_scale     = 0.0e+00
0.00.051.904 I print_info: n_ff             = 8192
0.00.051.904 I print_info: n_expert         = 0
0.00.051.904 I print_info: n_expert_used    = 0
0.00.051.904 I print_info: causal attn      = 1
0.00.051.905 I print_info: pooling type     = 0
0.00.051.905 I print_info: rope type        = 2
0.00.051.905 I print_info: rope scaling     = linear
0.00.051.905 I print_info: freq_base_train  = 10000.0
0.00.051.906 I print_info: freq_scale_train = 1
0.00.051.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.907 I print_info: rope_finetuned   = unknown
0.00.051.907 I print_info: ssm_d_conv       = 0
0.00.051.908 I print_info: ssm_d_inner      = 0
0.00.051.908 I print_info: ssm_d_state      = 0
0.00.051.908 I print_info: ssm_dt_rank      = 0
0.00.051.908 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.908 I print_info: model type       = 1.4B
0.00.051.909 I print_info: model params     = 1.41 B
0.00.051.909 I print_info: general.name     = 1.4B
0.00.051.909 I print_info: vocab type       = BPE
0.00.051.909 I print_info: n_vocab          = 50304
0.00.051.910 I print_info: n_merges         = 50009
0.00.051.910 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.910 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.910 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.910 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.910 I print_info: LF token         = 187 'Ċ'
0.00.051.911 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.911 I print_info: max token length = 1024
0.00.051.912 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.301.464 I load_tensors: offloading 24 repeating layers to GPU
0.01.301.473 I load_tensors: offloading output layer to GPU
0.01.301.474 I load_tensors: offloaded 25/25 layers to GPU
0.01.301.496 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.301.497 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.302.375 I llama_init_from_model: n_seq_max     = 1
0.01.302.380 I llama_init_from_model: n_ctx         = 2048
0.01.302.380 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.302.381 I llama_init_from_model: n_batch       = 2048
0.01.302.381 I llama_init_from_model: n_ubatch      = 512
0.01.302.381 I llama_init_from_model: flash_attn    = 0
0.01.302.383 I llama_init_from_model: freq_base     = 10000.0
0.01.302.383 I llama_init_from_model: freq_scale    = 1
0.01.302.385 I ggml_metal_init: allocating
0.01.302.427 I ggml_metal_init: found device: Apple M4
0.01.302.439 I ggml_metal_init: picking default device: Apple M4
0.01.303.524 I ggml_metal_load_library: using embedded metal library
0.01.306.462 I ggml_metal_init: GPU name:   Apple M4
0.01.306.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.306.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.306.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.306.467 I ggml_metal_init: simdgroup reduction   = true
0.01.306.467 I ggml_metal_init: simdgroup matrix mul. = true
0.01.306.467 I ggml_metal_init: has residency sets    = true
0.01.306.467 I ggml_metal_init: has bfloat            = true
0.01.306.468 I ggml_metal_init: use bfloat            = true
0.01.306.468 I ggml_metal_init: hasUnifiedMemory      = true
0.01.306.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.318.230 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.350.568 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.350.575 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.350.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.355.295 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.355.297 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.355.297 I llama_init_from_model: graph nodes  = 967
0.01.355.298 I llama_init_from_model: graph splits = 2
0.01.355.302 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.355.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.355.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.405.063 I main: llama threadpool init, n_threads = 4
0.01.405.108 I 
0.01.405.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.405.129 I 
0.01.405.252 I sampler seed: 1234
0.01.405.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.405.269 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.405.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.405.272 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.563.001 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.02.563.001 I llama_perf_context_print:        load time =    1394.28 ms
0.02.563.002 I llama_perf_context_print: prompt eval time =      49.01 ms /     7 tokens (    7.00 ms per token,   142.83 tokens per second)
0.02.563.003 I llama_perf_context_print:        eval time =    1105.76 ms /    63 runs   (   17.55 ms per token,    56.97 tokens per second)
0.02.563.003 I llama_perf_context_print:       total time =    1158.67 ms /    70 tokens
0.02.563.204 I ggml_metal_free: deallocating

real	0m2.581s
user	0m0.100s
sys	0m0.243s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.015.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.262 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.263 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.115 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.117 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.117 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.118 I llama_model_loader: - type  f32:  194 tensors
0.00.041.119 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.119 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.120 I print_info: file format = GGUF V3 (latest)
0.00.041.120 I print_info: file type   = Q4_0
0.00.041.122 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.355 I load: special tokens cache size = 25
0.00.057.542 I load: token to piece cache size = 0.2984 MB
0.00.057.557 I print_info: arch             = gptneox
0.00.057.558 I print_info: vocab_only       = 0
0.00.057.559 I print_info: n_ctx_train      = 2048
0.00.057.559 I print_info: n_embd           = 2048
0.00.057.559 I print_info: n_layer          = 24
0.00.057.563 I print_info: n_head           = 16
0.00.057.564 I print_info: n_head_kv        = 16
0.00.057.564 I print_info: n_rot            = 32
0.00.057.564 I print_info: n_swa            = 0
0.00.057.564 I print_info: n_embd_head_k    = 128
0.00.057.567 I print_info: n_embd_head_v    = 128
0.00.057.568 I print_info: n_gqa            = 1
0.00.057.569 I print_info: n_embd_k_gqa     = 2048
0.00.057.570 I print_info: n_embd_v_gqa     = 2048
0.00.057.570 I print_info: f_norm_eps       = 1.0e-05
0.00.057.571 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.571 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.571 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.571 I print_info: f_logit_scale    = 0.0e+00
0.00.057.572 I print_info: f_attn_scale     = 0.0e+00
0.00.057.572 I print_info: n_ff             = 8192
0.00.057.573 I print_info: n_expert         = 0
0.00.057.573 I print_info: n_expert_used    = 0
0.00.057.573 I print_info: causal attn      = 1
0.00.057.573 I print_info: pooling type     = 0
0.00.057.573 I print_info: rope type        = 2
0.00.057.573 I print_info: rope scaling     = linear
0.00.057.574 I print_info: freq_base_train  = 10000.0
0.00.057.574 I print_info: freq_scale_train = 1
0.00.057.574 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.575 I print_info: rope_finetuned   = unknown
0.00.057.575 I print_info: ssm_d_conv       = 0
0.00.057.575 I print_info: ssm_d_inner      = 0
0.00.057.575 I print_info: ssm_d_state      = 0
0.00.057.575 I print_info: ssm_dt_rank      = 0
0.00.057.576 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.576 I print_info: model type       = 1.4B
0.00.057.577 I print_info: model params     = 1.41 B
0.00.057.577 I print_info: general.name     = 1.4B
0.00.057.578 I print_info: vocab type       = BPE
0.00.057.578 I print_info: n_vocab          = 50304
0.00.057.578 I print_info: n_merges         = 50009
0.00.057.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.579 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.579 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.580 I print_info: LF token         = 187 'Ċ'
0.00.057.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.580 I print_info: max token length = 1024
0.00.057.580 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.118 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.139 I load_tensors: offloading output layer to GPU
0.00.634.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.184 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.634.186 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.635.491 I llama_init_from_model: n_seq_max     = 1
0.00.635.495 I llama_init_from_model: n_ctx         = 2048
0.00.635.495 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.496 I llama_init_from_model: n_batch       = 2048
0.00.635.496 I llama_init_from_model: n_ubatch      = 512
0.00.635.497 I llama_init_from_model: flash_attn    = 0
0.00.635.500 I llama_init_from_model: freq_base     = 10000.0
0.00.635.500 I llama_init_from_model: freq_scale    = 1
0.00.635.515 I ggml_metal_init: allocating
0.00.635.622 I ggml_metal_init: found device: Apple M4
0.00.635.650 I ggml_metal_init: picking default device: Apple M4
0.00.637.347 I ggml_metal_load_library: using embedded metal library
0.00.643.093 I ggml_metal_init: GPU name:   Apple M4
0.00.643.117 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.117 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.118 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.118 I ggml_metal_init: simdgroup reduction   = true
0.00.643.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.119 I ggml_metal_init: has residency sets    = true
0.00.643.119 I ggml_metal_init: has bfloat            = true
0.00.643.119 I ggml_metal_init: use bfloat            = true
0.00.643.122 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.704 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.745 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.752 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.775 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.003 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.005 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.006 I llama_init_from_model: graph nodes  = 967
0.00.717.006 I llama_init_from_model: graph splits = 2
0.00.717.012 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.046 I main: llama threadpool init, n_threads = 4
0.00.764.100 I 
0.00.764.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.122 I 
0.00.764.264 I sampler seed: 1234
0.00.764.269 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.284 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.284 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.486.794 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.486.795 I llama_perf_context_print:        load time =     747.59 ms
0.01.486.797 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.19 tokens per second)
0.01.486.802 I llama_perf_context_print:        eval time =     670.47 ms /    63 runs   (   10.64 ms per token,    93.96 tokens per second)
0.01.486.804 I llama_perf_context_print:       total time =     723.45 ms /    70 tokens
0.01.487.040 I ggml_metal_free: deallocating

real	0m1.504s
user	0m0.116s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.014.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.537 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.042.985 I llama_model_loader: - type  f32:  194 tensors
0.00.042.985 I llama_model_loader: - type q4_1:   97 tensors
0.00.042.985 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.986 I print_info: file format = GGUF V3 (latest)
0.00.042.986 I print_info: file type   = Q4_1
0.00.042.987 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.052.426 I load: special tokens cache size = 25
0.00.059.906 I load: token to piece cache size = 0.2984 MB
0.00.059.920 I print_info: arch             = gptneox
0.00.059.922 I print_info: vocab_only       = 0
0.00.059.922 I print_info: n_ctx_train      = 2048
0.00.059.922 I print_info: n_embd           = 2048
0.00.059.923 I print_info: n_layer          = 24
0.00.059.932 I print_info: n_head           = 16
0.00.059.933 I print_info: n_head_kv        = 16
0.00.059.933 I print_info: n_rot            = 32
0.00.059.933 I print_info: n_swa            = 0
0.00.059.934 I print_info: n_embd_head_k    = 128
0.00.059.934 I print_info: n_embd_head_v    = 128
0.00.059.935 I print_info: n_gqa            = 1
0.00.059.935 I print_info: n_embd_k_gqa     = 2048
0.00.059.936 I print_info: n_embd_v_gqa     = 2048
0.00.059.937 I print_info: f_norm_eps       = 1.0e-05
0.00.059.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.939 I print_info: f_logit_scale    = 0.0e+00
0.00.059.940 I print_info: f_attn_scale     = 0.0e+00
0.00.059.940 I print_info: n_ff             = 8192
0.00.059.940 I print_info: n_expert         = 0
0.00.059.941 I print_info: n_expert_used    = 0
0.00.059.941 I print_info: causal attn      = 1
0.00.059.942 I print_info: pooling type     = 0
0.00.059.943 I print_info: rope type        = 2
0.00.059.943 I print_info: rope scaling     = linear
0.00.059.944 I print_info: freq_base_train  = 10000.0
0.00.059.944 I print_info: freq_scale_train = 1
0.00.059.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.944 I print_info: rope_finetuned   = unknown
0.00.059.945 I print_info: ssm_d_conv       = 0
0.00.059.945 I print_info: ssm_d_inner      = 0
0.00.059.945 I print_info: ssm_d_state      = 0
0.00.059.945 I print_info: ssm_dt_rank      = 0
0.00.059.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.945 I print_info: model type       = 1.4B
0.00.059.946 I print_info: model params     = 1.41 B
0.00.059.946 I print_info: general.name     = 1.4B
0.00.059.946 I print_info: vocab type       = BPE
0.00.059.947 I print_info: n_vocab          = 50304
0.00.059.947 I print_info: n_merges         = 50009
0.00.059.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.948 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.948 I print_info: LF token         = 187 'Ċ'
0.00.059.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.949 I print_info: max token length = 1024
0.00.059.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.276 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.297 I load_tensors: offloading output layer to GPU
0.00.679.298 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.332 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.679.333 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.680.753 I llama_init_from_model: n_seq_max     = 1
0.00.680.757 I llama_init_from_model: n_ctx         = 2048
0.00.680.758 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.759 I llama_init_from_model: n_batch       = 2048
0.00.680.759 I llama_init_from_model: n_ubatch      = 512
0.00.680.759 I llama_init_from_model: flash_attn    = 0
0.00.680.762 I llama_init_from_model: freq_base     = 10000.0
0.00.680.763 I llama_init_from_model: freq_scale    = 1
0.00.680.765 I ggml_metal_init: allocating
0.00.680.848 I ggml_metal_init: found device: Apple M4
0.00.680.862 I ggml_metal_init: picking default device: Apple M4
0.00.682.535 I ggml_metal_load_library: using embedded metal library
0.00.688.395 I ggml_metal_init: GPU name:   Apple M4
0.00.688.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.419 I ggml_metal_init: simdgroup reduction   = true
0.00.688.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.419 I ggml_metal_init: has residency sets    = true
0.00.688.420 I ggml_metal_init: has bfloat            = true
0.00.688.420 I ggml_metal_init: use bfloat            = true
0.00.688.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.593 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.782.603 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.782.612 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.782.648 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.787.188 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.787.190 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.787.191 I llama_init_from_model: graph nodes  = 967
0.00.787.191 I llama_init_from_model: graph splits = 2
0.00.787.197 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.787.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.787.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.856 I main: llama threadpool init, n_threads = 4
0.00.836.907 I 
0.00.836.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.928 I 
0.00.837.066 I sampler seed: 1234
0.00.837.071 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.087 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.087 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.087 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.606.026 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.606.027 I llama_perf_context_print:        load time =     821.24 ms
0.01.606.028 I llama_perf_context_print: prompt eval time =      49.59 ms /     7 tokens (    7.08 ms per token,   141.17 tokens per second)
0.01.606.029 I llama_perf_context_print:        eval time =     716.62 ms /    63 runs   (   11.38 ms per token,    87.91 tokens per second)
0.01.606.030 I llama_perf_context_print:       total time =     769.89 ms /    70 tokens
0.01.606.219 I ggml_metal_free: deallocating

real	0m1.623s
user	0m0.116s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.558 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.096 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.664 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.665 I llama_model_loader: - type  f32:  194 tensors
0.00.027.665 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.666 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.666 I print_info: file format = GGUF V3 (latest)
0.00.027.667 I print_info: file type   = Q5_0
0.00.027.667 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.455 I load: special tokens cache size = 25
0.00.041.202 I load: token to piece cache size = 0.2984 MB
0.00.041.211 I print_info: arch             = gptneox
0.00.041.212 I print_info: vocab_only       = 0
0.00.041.212 I print_info: n_ctx_train      = 2048
0.00.041.212 I print_info: n_embd           = 2048
0.00.041.212 I print_info: n_layer          = 24
0.00.041.215 I print_info: n_head           = 16
0.00.041.216 I print_info: n_head_kv        = 16
0.00.041.216 I print_info: n_rot            = 32
0.00.041.216 I print_info: n_swa            = 0
0.00.041.216 I print_info: n_embd_head_k    = 128
0.00.041.218 I print_info: n_embd_head_v    = 128
0.00.041.219 I print_info: n_gqa            = 1
0.00.041.220 I print_info: n_embd_k_gqa     = 2048
0.00.041.220 I print_info: n_embd_v_gqa     = 2048
0.00.041.221 I print_info: f_norm_eps       = 1.0e-05
0.00.041.221 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.221 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.222 I print_info: f_logit_scale    = 0.0e+00
0.00.041.222 I print_info: f_attn_scale     = 0.0e+00
0.00.041.223 I print_info: n_ff             = 8192
0.00.041.223 I print_info: n_expert         = 0
0.00.041.223 I print_info: n_expert_used    = 0
0.00.041.223 I print_info: causal attn      = 1
0.00.041.224 I print_info: pooling type     = 0
0.00.041.226 I print_info: rope type        = 2
0.00.041.226 I print_info: rope scaling     = linear
0.00.041.227 I print_info: freq_base_train  = 10000.0
0.00.041.227 I print_info: freq_scale_train = 1
0.00.041.227 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.227 I print_info: rope_finetuned   = unknown
0.00.041.227 I print_info: ssm_d_conv       = 0
0.00.041.228 I print_info: ssm_d_inner      = 0
0.00.041.228 I print_info: ssm_d_state      = 0
0.00.041.228 I print_info: ssm_dt_rank      = 0
0.00.041.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.228 I print_info: model type       = 1.4B
0.00.041.228 I print_info: model params     = 1.41 B
0.00.041.228 I print_info: general.name     = 1.4B
0.00.041.229 I print_info: vocab type       = BPE
0.00.041.229 I print_info: n_vocab          = 50304
0.00.041.230 I print_info: n_merges         = 50009
0.00.041.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.231 I print_info: LF token         = 187 'Ċ'
0.00.041.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.231 I print_info: max token length = 1024
0.00.041.231 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.702.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.491 I load_tensors: offloading output layer to GPU
0.00.702.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.528 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.702.530 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.704.147 I llama_init_from_model: n_seq_max     = 1
0.00.704.150 I llama_init_from_model: n_ctx         = 2048
0.00.704.151 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.704.151 I llama_init_from_model: n_batch       = 2048
0.00.704.152 I llama_init_from_model: n_ubatch      = 512
0.00.704.152 I llama_init_from_model: flash_attn    = 0
0.00.704.154 I llama_init_from_model: freq_base     = 10000.0
0.00.704.154 I llama_init_from_model: freq_scale    = 1
0.00.704.157 I ggml_metal_init: allocating
0.00.704.240 I ggml_metal_init: found device: Apple M4
0.00.704.253 I ggml_metal_init: picking default device: Apple M4
0.00.705.871 I ggml_metal_load_library: using embedded metal library
0.00.712.373 I ggml_metal_init: GPU name:   Apple M4
0.00.712.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.712.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.712.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.712.378 I ggml_metal_init: simdgroup reduction   = true
0.00.712.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.712.379 I ggml_metal_init: has residency sets    = true
0.00.712.379 I ggml_metal_init: has bfloat            = true
0.00.712.379 I ggml_metal_init: use bfloat            = true
0.00.712.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.712.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.729.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.778.368 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.778.374 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.778.397 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.783.096 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.783.098 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.783.099 I llama_init_from_model: graph nodes  = 967
0.00.783.099 I llama_init_from_model: graph splits = 2
0.00.783.105 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.783.234 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.783.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.152 I main: llama threadpool init, n_threads = 4
0.00.842.202 I 
0.00.842.231 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.232 I 
0.00.842.382 I sampler seed: 1234
0.00.842.386 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.401 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.403 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.403 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.631.276 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.631.277 I llama_perf_context_print:        load time =     829.86 ms
0.01.631.278 I llama_perf_context_print: prompt eval time =      52.88 ms /     7 tokens (    7.55 ms per token,   132.38 tokens per second)
0.01.631.278 I llama_perf_context_print:        eval time =     733.05 ms /    63 runs   (   11.64 ms per token,    85.94 tokens per second)
0.01.631.279 I llama_perf_context_print:       total time =     789.85 ms /    70 tokens
0.01.631.480 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.108s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.186 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.164 I llama_model_loader: - type  f32:  194 tensors
0.00.025.164 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.165 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.165 I print_info: file format = GGUF V3 (latest)
0.00.025.166 I print_info: file type   = Q5_1
0.00.025.167 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.994 I load: special tokens cache size = 25
0.00.039.091 I load: token to piece cache size = 0.2984 MB
0.00.039.104 I print_info: arch             = gptneox
0.00.039.105 I print_info: vocab_only       = 0
0.00.039.106 I print_info: n_ctx_train      = 2048
0.00.039.106 I print_info: n_embd           = 2048
0.00.039.106 I print_info: n_layer          = 24
0.00.039.109 I print_info: n_head           = 16
0.00.039.110 I print_info: n_head_kv        = 16
0.00.039.110 I print_info: n_rot            = 32
0.00.039.110 I print_info: n_swa            = 0
0.00.039.110 I print_info: n_embd_head_k    = 128
0.00.039.110 I print_info: n_embd_head_v    = 128
0.00.039.111 I print_info: n_gqa            = 1
0.00.039.112 I print_info: n_embd_k_gqa     = 2048
0.00.039.113 I print_info: n_embd_v_gqa     = 2048
0.00.039.114 I print_info: f_norm_eps       = 1.0e-05
0.00.039.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.114 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.115 I print_info: f_logit_scale    = 0.0e+00
0.00.039.115 I print_info: f_attn_scale     = 0.0e+00
0.00.039.115 I print_info: n_ff             = 8192
0.00.039.115 I print_info: n_expert         = 0
0.00.039.116 I print_info: n_expert_used    = 0
0.00.039.121 I print_info: causal attn      = 1
0.00.039.123 I print_info: pooling type     = 0
0.00.039.124 I print_info: rope type        = 2
0.00.039.125 I print_info: rope scaling     = linear
0.00.039.125 I print_info: freq_base_train  = 10000.0
0.00.039.126 I print_info: freq_scale_train = 1
0.00.039.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.126 I print_info: rope_finetuned   = unknown
0.00.039.126 I print_info: ssm_d_conv       = 0
0.00.039.126 I print_info: ssm_d_inner      = 0
0.00.039.127 I print_info: ssm_d_state      = 0
0.00.039.127 I print_info: ssm_dt_rank      = 0
0.00.039.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.127 I print_info: model type       = 1.4B
0.00.039.128 I print_info: model params     = 1.41 B
0.00.039.128 I print_info: general.name     = 1.4B
0.00.039.128 I print_info: vocab type       = BPE
0.00.039.129 I print_info: n_vocab          = 50304
0.00.039.129 I print_info: n_merges         = 50009
0.00.039.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.134 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.137 I print_info: LF token         = 187 'Ċ'
0.00.039.137 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.137 I print_info: max token length = 1024
0.00.039.138 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.816 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.825 I load_tensors: offloading output layer to GPU
0.00.608.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.856 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.857 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.131 I llama_init_from_model: n_seq_max     = 1
0.00.610.134 I llama_init_from_model: n_ctx         = 2048
0.00.610.135 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.135 I llama_init_from_model: n_batch       = 2048
0.00.610.136 I llama_init_from_model: n_ubatch      = 512
0.00.610.136 I llama_init_from_model: flash_attn    = 0
0.00.610.137 I llama_init_from_model: freq_base     = 10000.0
0.00.610.138 I llama_init_from_model: freq_scale    = 1
0.00.610.139 I ggml_metal_init: allocating
0.00.610.153 I ggml_metal_init: found device: Apple M4
0.00.610.162 I ggml_metal_init: picking default device: Apple M4
0.00.611.402 I ggml_metal_load_library: using embedded metal library
0.00.617.692 I ggml_metal_init: GPU name:   Apple M4
0.00.617.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.697 I ggml_metal_init: simdgroup reduction   = true
0.00.617.697 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.697 I ggml_metal_init: has residency sets    = true
0.00.617.698 I ggml_metal_init: has bfloat            = true
0.00.617.698 I ggml_metal_init: use bfloat            = true
0.00.617.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.319 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.155 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.161 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.838 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.691.839 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.691.840 I llama_init_from_model: graph nodes  = 967
0.00.691.840 I llama_init_from_model: graph splits = 2
0.00.691.846 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.390 I main: llama threadpool init, n_threads = 4
0.00.748.437 I 
0.00.748.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.459 I 
0.00.748.614 I sampler seed: 1234
0.00.748.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.660 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.663 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.664 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.585.761 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.585.761 I llama_perf_context_print:        load time =     738.49 ms
0.01.585.763 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.55 tokens per second)
0.01.585.764 I llama_perf_context_print:        eval time =     791.88 ms /    63 runs   (   12.57 ms per token,    79.56 tokens per second)
0.01.585.764 I llama_perf_context_print:       total time =     838.09 ms /    70 tokens
0.01.585.960 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.358 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.183 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.184 I llama_model_loader: - type  f32:  194 tensors
0.00.026.185 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.185 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.185 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.186 I print_info: file format = GGUF V3 (latest)
0.00.026.186 I print_info: file type   = Q2_K - Medium
0.00.026.187 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.323 I load: special tokens cache size = 25
0.00.040.139 I load: token to piece cache size = 0.2984 MB
0.00.040.153 I print_info: arch             = gptneox
0.00.040.154 I print_info: vocab_only       = 0
0.00.040.155 I print_info: n_ctx_train      = 2048
0.00.040.155 I print_info: n_embd           = 2048
0.00.040.155 I print_info: n_layer          = 24
0.00.040.158 I print_info: n_head           = 16
0.00.040.159 I print_info: n_head_kv        = 16
0.00.040.159 I print_info: n_rot            = 32
0.00.040.159 I print_info: n_swa            = 0
0.00.040.159 I print_info: n_embd_head_k    = 128
0.00.040.159 I print_info: n_embd_head_v    = 128
0.00.040.160 I print_info: n_gqa            = 1
0.00.040.161 I print_info: n_embd_k_gqa     = 2048
0.00.040.162 I print_info: n_embd_v_gqa     = 2048
0.00.040.162 I print_info: f_norm_eps       = 1.0e-05
0.00.040.164 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.164 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.164 I print_info: f_logit_scale    = 0.0e+00
0.00.040.164 I print_info: f_attn_scale     = 0.0e+00
0.00.040.165 I print_info: n_ff             = 8192
0.00.040.165 I print_info: n_expert         = 0
0.00.040.165 I print_info: n_expert_used    = 0
0.00.040.167 I print_info: causal attn      = 1
0.00.040.168 I print_info: pooling type     = 0
0.00.040.168 I print_info: rope type        = 2
0.00.040.168 I print_info: rope scaling     = linear
0.00.040.169 I print_info: freq_base_train  = 10000.0
0.00.040.169 I print_info: freq_scale_train = 1
0.00.040.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.170 I print_info: rope_finetuned   = unknown
0.00.040.171 I print_info: ssm_d_conv       = 0
0.00.040.171 I print_info: ssm_d_inner      = 0
0.00.040.171 I print_info: ssm_d_state      = 0
0.00.040.171 I print_info: ssm_dt_rank      = 0
0.00.040.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.171 I print_info: model type       = 1.4B
0.00.040.174 I print_info: model params     = 1.41 B
0.00.040.175 I print_info: general.name     = 1.4B
0.00.040.175 I print_info: vocab type       = BPE
0.00.040.175 I print_info: n_vocab          = 50304
0.00.040.178 I print_info: n_merges         = 50009
0.00.040.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: LF token         = 187 'Ċ'
0.00.040.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: max token length = 1024
0.00.040.183 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.608 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.621 I load_tensors: offloading output layer to GPU
0.00.334.622 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.657 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.334.663 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.336.310 I llama_init_from_model: n_seq_max     = 1
0.00.336.313 I llama_init_from_model: n_ctx         = 2048
0.00.336.314 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.336.314 I llama_init_from_model: n_batch       = 2048
0.00.336.315 I llama_init_from_model: n_ubatch      = 512
0.00.336.315 I llama_init_from_model: flash_attn    = 0
0.00.336.317 I llama_init_from_model: freq_base     = 10000.0
0.00.336.318 I llama_init_from_model: freq_scale    = 1
0.00.336.321 I ggml_metal_init: allocating
0.00.336.400 I ggml_metal_init: found device: Apple M4
0.00.336.415 I ggml_metal_init: picking default device: Apple M4
0.00.338.018 I ggml_metal_load_library: using embedded metal library
0.00.343.679 I ggml_metal_init: GPU name:   Apple M4
0.00.343.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.697 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.698 I ggml_metal_init: simdgroup reduction   = true
0.00.343.698 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.698 I ggml_metal_init: has residency sets    = true
0.00.343.699 I ggml_metal_init: has bfloat            = true
0.00.343.699 I ggml_metal_init: use bfloat            = true
0.00.343.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.706 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.355 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.437.911 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.437.918 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.437.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.442.271 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.442.274 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.442.274 I llama_init_from_model: graph nodes  = 967
0.00.442.274 I llama_init_from_model: graph splits = 2
0.00.442.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.442.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.442.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.693 I main: llama threadpool init, n_threads = 4
0.00.503.735 I 
0.00.503.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.756 I 
0.00.503.917 I sampler seed: 1234
0.00.503.921 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.503.947 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.503.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.503.947 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.443 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.191.445 I llama_perf_context_print:        load time =     493.18 ms
0.01.191.446 I llama_perf_context_print: prompt eval time =      44.22 ms /     7 tokens (    6.32 ms per token,   158.29 tokens per second)
0.01.191.446 I llama_perf_context_print:        eval time =     640.41 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.191.446 I llama_perf_context_print:       total time =     688.46 ms /    70 tokens
0.01.191.689 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.111s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.011.574 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.449 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.450 I llama_model_loader: - type  f32:  194 tensors
0.00.027.450 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.450 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.450 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.451 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.451 I print_info: file format = GGUF V3 (latest)
0.00.027.452 I print_info: file type   = Q3_K - Medium
0.00.027.453 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.373 I load: special tokens cache size = 25
0.00.041.199 I load: token to piece cache size = 0.2984 MB
0.00.041.213 I print_info: arch             = gptneox
0.00.041.214 I print_info: vocab_only       = 0
0.00.041.215 I print_info: n_ctx_train      = 2048
0.00.041.215 I print_info: n_embd           = 2048
0.00.041.215 I print_info: n_layer          = 24
0.00.041.217 I print_info: n_head           = 16
0.00.041.218 I print_info: n_head_kv        = 16
0.00.041.218 I print_info: n_rot            = 32
0.00.041.219 I print_info: n_swa            = 0
0.00.041.219 I print_info: n_embd_head_k    = 128
0.00.041.219 I print_info: n_embd_head_v    = 128
0.00.041.220 I print_info: n_gqa            = 1
0.00.041.221 I print_info: n_embd_k_gqa     = 2048
0.00.041.222 I print_info: n_embd_v_gqa     = 2048
0.00.041.223 I print_info: f_norm_eps       = 1.0e-05
0.00.041.223 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.223 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.223 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.223 I print_info: f_logit_scale    = 0.0e+00
0.00.041.224 I print_info: f_attn_scale     = 0.0e+00
0.00.041.224 I print_info: n_ff             = 8192
0.00.041.224 I print_info: n_expert         = 0
0.00.041.226 I print_info: n_expert_used    = 0
0.00.041.227 I print_info: causal attn      = 1
0.00.041.227 I print_info: pooling type     = 0
0.00.041.227 I print_info: rope type        = 2
0.00.041.228 I print_info: rope scaling     = linear
0.00.041.228 I print_info: freq_base_train  = 10000.0
0.00.041.230 I print_info: freq_scale_train = 1
0.00.041.230 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.230 I print_info: rope_finetuned   = unknown
0.00.041.230 I print_info: ssm_d_conv       = 0
0.00.041.230 I print_info: ssm_d_inner      = 0
0.00.041.230 I print_info: ssm_d_state      = 0
0.00.041.231 I print_info: ssm_dt_rank      = 0
0.00.041.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.232 I print_info: model type       = 1.4B
0.00.041.232 I print_info: model params     = 1.41 B
0.00.041.232 I print_info: general.name     = 1.4B
0.00.041.233 I print_info: vocab type       = BPE
0.00.041.233 I print_info: n_vocab          = 50304
0.00.041.233 I print_info: n_merges         = 50009
0.00.041.233 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.237 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.237 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: LF token         = 187 'Ċ'
0.00.041.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: max token length = 1024
0.00.041.238 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.422.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.422.984 I load_tensors: offloading output layer to GPU
0.00.422.985 I load_tensors: offloaded 25/25 layers to GPU
0.00.423.019 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.423.020 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.424.284 I llama_init_from_model: n_seq_max     = 1
0.00.424.293 I llama_init_from_model: n_ctx         = 2048
0.00.424.293 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.424.294 I llama_init_from_model: n_batch       = 2048
0.00.424.294 I llama_init_from_model: n_ubatch      = 512
0.00.424.294 I llama_init_from_model: flash_attn    = 0
0.00.424.297 I llama_init_from_model: freq_base     = 10000.0
0.00.424.297 I llama_init_from_model: freq_scale    = 1
0.00.424.299 I ggml_metal_init: allocating
0.00.424.381 I ggml_metal_init: found device: Apple M4
0.00.424.395 I ggml_metal_init: picking default device: Apple M4
0.00.426.193 I ggml_metal_load_library: using embedded metal library
0.00.431.904 I ggml_metal_init: GPU name:   Apple M4
0.00.431.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.431.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.431.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.431.928 I ggml_metal_init: simdgroup reduction   = true
0.00.431.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.431.929 I ggml_metal_init: has residency sets    = true
0.00.431.929 I ggml_metal_init: has bfloat            = true
0.00.431.930 I ggml_metal_init: use bfloat            = true
0.00.431.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.431.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.452.537 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.509.545 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.509.551 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.509.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.514.117 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.514.119 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.514.119 I llama_init_from_model: graph nodes  = 967
0.00.514.119 I llama_init_from_model: graph splits = 2
0.00.514.125 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.514.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.514.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.394 I main: llama threadpool init, n_threads = 4
0.00.568.438 I 
0.00.568.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.461 I 
0.00.568.629 I sampler seed: 1234
0.00.568.634 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.568.650 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.568.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.568.651 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.301.288 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47651.01 tokens per second)
0.01.301.290 I llama_perf_context_print:        load time =     556.07 ms
0.01.301.291 I llama_perf_context_print: prompt eval time =      40.17 ms /     7 tokens (    5.74 ms per token,   174.27 tokens per second)
0.01.301.292 I llama_perf_context_print:        eval time =     689.96 ms /    63 runs   (   10.95 ms per token,    91.31 tokens per second)
0.01.301.292 I llama_perf_context_print:       total time =     733.65 ms /    70 tokens
0.01.301.546 I ggml_metal_free: deallocating

real	0m1.318s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.011.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.411 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.231 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.232 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.233 I llama_model_loader: - type  f32:  194 tensors
0.00.028.233 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.234 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.234 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.235 I print_info: file format = GGUF V3 (latest)
0.00.028.235 I print_info: file type   = Q4_K - Medium
0.00.028.240 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.369 I load: special tokens cache size = 25
0.00.042.499 I load: token to piece cache size = 0.2984 MB
0.00.042.517 I print_info: arch             = gptneox
0.00.042.519 I print_info: vocab_only       = 0
0.00.042.519 I print_info: n_ctx_train      = 2048
0.00.042.519 I print_info: n_embd           = 2048
0.00.042.519 I print_info: n_layer          = 24
0.00.042.524 I print_info: n_head           = 16
0.00.042.524 I print_info: n_head_kv        = 16
0.00.042.524 I print_info: n_rot            = 32
0.00.042.525 I print_info: n_swa            = 0
0.00.042.525 I print_info: n_embd_head_k    = 128
0.00.042.525 I print_info: n_embd_head_v    = 128
0.00.042.525 I print_info: n_gqa            = 1
0.00.042.526 I print_info: n_embd_k_gqa     = 2048
0.00.042.527 I print_info: n_embd_v_gqa     = 2048
0.00.042.527 I print_info: f_norm_eps       = 1.0e-05
0.00.042.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.531 I print_info: f_logit_scale    = 0.0e+00
0.00.042.531 I print_info: f_attn_scale     = 0.0e+00
0.00.042.532 I print_info: n_ff             = 8192
0.00.042.532 I print_info: n_expert         = 0
0.00.042.532 I print_info: n_expert_used    = 0
0.00.042.532 I print_info: causal attn      = 1
0.00.042.532 I print_info: pooling type     = 0
0.00.042.532 I print_info: rope type        = 2
0.00.042.533 I print_info: rope scaling     = linear
0.00.042.533 I print_info: freq_base_train  = 10000.0
0.00.042.533 I print_info: freq_scale_train = 1
0.00.042.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.534 I print_info: rope_finetuned   = unknown
0.00.042.534 I print_info: ssm_d_conv       = 0
0.00.042.534 I print_info: ssm_d_inner      = 0
0.00.042.534 I print_info: ssm_d_state      = 0
0.00.042.535 I print_info: ssm_dt_rank      = 0
0.00.042.542 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.543 I print_info: model type       = 1.4B
0.00.042.546 I print_info: model params     = 1.41 B
0.00.042.546 I print_info: general.name     = 1.4B
0.00.042.547 I print_info: vocab type       = BPE
0.00.042.547 I print_info: n_vocab          = 50304
0.00.042.548 I print_info: n_merges         = 50009
0.00.042.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.549 I print_info: LF token         = 187 'Ċ'
0.00.042.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.551 I print_info: max token length = 1024
0.00.042.551 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.510.267 I load_tensors: offloading 24 repeating layers to GPU
0.00.510.293 I load_tensors: offloading output layer to GPU
0.00.510.294 I load_tensors: offloaded 25/25 layers to GPU
0.00.510.360 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.510.362 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.512.015 I llama_init_from_model: n_seq_max     = 1
0.00.512.026 I llama_init_from_model: n_ctx         = 2048
0.00.512.027 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.512.027 I llama_init_from_model: n_batch       = 2048
0.00.512.027 I llama_init_from_model: n_ubatch      = 512
0.00.512.028 I llama_init_from_model: flash_attn    = 0
0.00.512.030 I llama_init_from_model: freq_base     = 10000.0
0.00.512.031 I llama_init_from_model: freq_scale    = 1
0.00.512.034 I ggml_metal_init: allocating
0.00.512.160 I ggml_metal_init: found device: Apple M4
0.00.512.183 I ggml_metal_init: picking default device: Apple M4
0.00.514.178 I ggml_metal_load_library: using embedded metal library
0.00.519.160 I ggml_metal_init: GPU name:   Apple M4
0.00.519.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.519.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.519.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.519.170 I ggml_metal_init: simdgroup reduction   = true
0.00.519.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.519.171 I ggml_metal_init: has residency sets    = true
0.00.519.171 I ggml_metal_init: has bfloat            = true
0.00.519.171 I ggml_metal_init: use bfloat            = true
0.00.519.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.519.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.531.367 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.562.610 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.562.619 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.562.641 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.567.299 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.567.301 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.567.301 I llama_init_from_model: graph nodes  = 967
0.00.567.301 I llama_init_from_model: graph splits = 2
0.00.567.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.567.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.567.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.991 I main: llama threadpool init, n_threads = 4
0.00.625.036 I 
0.00.625.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.057 I 
0.00.625.244 I sampler seed: 1234
0.00.625.248 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.264 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.264 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.264 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.385.862 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.01.385.863 I llama_perf_context_print:        load time =     612.44 ms
0.01.385.863 I llama_perf_context_print: prompt eval time =      57.85 ms /     7 tokens (    8.26 ms per token,   121.00 tokens per second)
0.01.385.864 I llama_perf_context_print:        eval time =     699.82 ms /    63 runs   (   11.11 ms per token,    90.02 tokens per second)
0.01.385.865 I llama_perf_context_print:       total time =     761.59 ms /    70 tokens
0.01.386.097 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.103s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.616 I llama_model_loader: - type  f32:  194 tensors
0.00.024.616 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.616 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.617 I print_info: file format = GGUF V3 (latest)
0.00.024.617 I print_info: file type   = Q5_K - Medium
0.00.024.618 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.375 I load: special tokens cache size = 25
0.00.038.473 I load: token to piece cache size = 0.2984 MB
0.00.038.487 I print_info: arch             = gptneox
0.00.038.488 I print_info: vocab_only       = 0
0.00.038.488 I print_info: n_ctx_train      = 2048
0.00.038.488 I print_info: n_embd           = 2048
0.00.038.488 I print_info: n_layer          = 24
0.00.038.491 I print_info: n_head           = 16
0.00.038.492 I print_info: n_head_kv        = 16
0.00.038.492 I print_info: n_rot            = 32
0.00.038.492 I print_info: n_swa            = 0
0.00.038.492 I print_info: n_embd_head_k    = 128
0.00.038.492 I print_info: n_embd_head_v    = 128
0.00.038.493 I print_info: n_gqa            = 1
0.00.038.494 I print_info: n_embd_k_gqa     = 2048
0.00.038.494 I print_info: n_embd_v_gqa     = 2048
0.00.038.495 I print_info: f_norm_eps       = 1.0e-05
0.00.038.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.495 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.496 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.496 I print_info: f_logit_scale    = 0.0e+00
0.00.038.496 I print_info: f_attn_scale     = 0.0e+00
0.00.038.498 I print_info: n_ff             = 8192
0.00.038.498 I print_info: n_expert         = 0
0.00.038.498 I print_info: n_expert_used    = 0
0.00.038.499 I print_info: causal attn      = 1
0.00.038.499 I print_info: pooling type     = 0
0.00.038.499 I print_info: rope type        = 2
0.00.038.500 I print_info: rope scaling     = linear
0.00.038.500 I print_info: freq_base_train  = 10000.0
0.00.038.500 I print_info: freq_scale_train = 1
0.00.038.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.501 I print_info: rope_finetuned   = unknown
0.00.038.501 I print_info: ssm_d_conv       = 0
0.00.038.501 I print_info: ssm_d_inner      = 0
0.00.038.501 I print_info: ssm_d_state      = 0
0.00.038.501 I print_info: ssm_dt_rank      = 0
0.00.038.501 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.504 I print_info: model type       = 1.4B
0.00.038.505 I print_info: model params     = 1.41 B
0.00.038.505 I print_info: general.name     = 1.4B
0.00.038.506 I print_info: vocab type       = BPE
0.00.038.506 I print_info: n_vocab          = 50304
0.00.038.506 I print_info: n_merges         = 50009
0.00.038.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.507 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.507 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.508 I print_info: LF token         = 187 'Ċ'
0.00.038.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.508 I print_info: max token length = 1024
0.00.038.509 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.088 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.102 I load_tensors: offloading output layer to GPU
0.00.625.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.137 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.625.139 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.626.716 I llama_init_from_model: n_seq_max     = 1
0.00.626.718 I llama_init_from_model: n_ctx         = 2048
0.00.626.719 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.719 I llama_init_from_model: n_batch       = 2048
0.00.626.720 I llama_init_from_model: n_ubatch      = 512
0.00.626.720 I llama_init_from_model: flash_attn    = 0
0.00.626.721 I llama_init_from_model: freq_base     = 10000.0
0.00.626.722 I llama_init_from_model: freq_scale    = 1
0.00.626.723 I ggml_metal_init: allocating
0.00.626.739 I ggml_metal_init: found device: Apple M4
0.00.626.748 I ggml_metal_init: picking default device: Apple M4
0.00.628.105 I ggml_metal_load_library: using embedded metal library
0.00.634.358 I ggml_metal_init: GPU name:   Apple M4
0.00.634.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.364 I ggml_metal_init: simdgroup reduction   = true
0.00.634.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.364 I ggml_metal_init: has residency sets    = true
0.00.634.365 I ggml_metal_init: has bfloat            = true
0.00.634.365 I ggml_metal_init: use bfloat            = true
0.00.634.366 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.236 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.243 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.523 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.525 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.526 I llama_init_from_model: graph nodes  = 967
0.00.734.526 I llama_init_from_model: graph splits = 2
0.00.734.532 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.150 I main: llama threadpool init, n_threads = 4
0.00.794.205 I 
0.00.794.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.224 I 
0.00.794.382 I sampler seed: 1234
0.00.794.387 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.402 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.404 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.404 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.635.668 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.635.669 I llama_perf_context_print:        load time =     784.64 ms
0.01.635.670 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.04 tokens per second)
0.01.635.670 I llama_perf_context_print:        eval time =     785.73 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.635.671 I llama_perf_context_print:       total time =     842.24 ms /    70 tokens
0.01.635.872 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.112s
sys	0m0.234s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.219 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.219 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.661 I llama_model_loader: - type  f32:  194 tensors
0.00.025.662 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.662 I print_info: file format = GGUF V3 (latest)
0.00.025.663 I print_info: file type   = Q6_K
0.00.025.664 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.540 I load: special tokens cache size = 25
0.00.039.287 I load: token to piece cache size = 0.2984 MB
0.00.039.301 I print_info: arch             = gptneox
0.00.039.302 I print_info: vocab_only       = 0
0.00.039.302 I print_info: n_ctx_train      = 2048
0.00.039.303 I print_info: n_embd           = 2048
0.00.039.303 I print_info: n_layer          = 24
0.00.039.305 I print_info: n_head           = 16
0.00.039.306 I print_info: n_head_kv        = 16
0.00.039.306 I print_info: n_rot            = 32
0.00.039.306 I print_info: n_swa            = 0
0.00.039.307 I print_info: n_embd_head_k    = 128
0.00.039.307 I print_info: n_embd_head_v    = 128
0.00.039.308 I print_info: n_gqa            = 1
0.00.039.308 I print_info: n_embd_k_gqa     = 2048
0.00.039.311 I print_info: n_embd_v_gqa     = 2048
0.00.039.311 I print_info: f_norm_eps       = 1.0e-05
0.00.039.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.312 I print_info: f_logit_scale    = 0.0e+00
0.00.039.312 I print_info: f_attn_scale     = 0.0e+00
0.00.039.313 I print_info: n_ff             = 8192
0.00.039.313 I print_info: n_expert         = 0
0.00.039.313 I print_info: n_expert_used    = 0
0.00.039.313 I print_info: causal attn      = 1
0.00.039.313 I print_info: pooling type     = 0
0.00.039.313 I print_info: rope type        = 2
0.00.039.313 I print_info: rope scaling     = linear
0.00.039.314 I print_info: freq_base_train  = 10000.0
0.00.039.314 I print_info: freq_scale_train = 1
0.00.039.314 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.315 I print_info: rope_finetuned   = unknown
0.00.039.315 I print_info: ssm_d_conv       = 0
0.00.039.316 I print_info: ssm_d_inner      = 0
0.00.039.316 I print_info: ssm_d_state      = 0
0.00.039.316 I print_info: ssm_dt_rank      = 0
0.00.039.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.316 I print_info: model type       = 1.4B
0.00.039.319 I print_info: model params     = 1.41 B
0.00.039.320 I print_info: general.name     = 1.4B
0.00.039.320 I print_info: vocab type       = BPE
0.00.039.320 I print_info: n_vocab          = 50304
0.00.039.321 I print_info: n_merges         = 50009
0.00.039.321 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.321 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.321 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: LF token         = 187 'Ċ'
0.00.039.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: max token length = 1024
0.00.039.322 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.765 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.780 I load_tensors: offloading output layer to GPU
0.00.643.781 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.814 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.643.816 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.645.153 I llama_init_from_model: n_seq_max     = 1
0.00.645.155 I llama_init_from_model: n_ctx         = 2048
0.00.645.156 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.156 I llama_init_from_model: n_batch       = 2048
0.00.645.157 I llama_init_from_model: n_ubatch      = 512
0.00.645.157 I llama_init_from_model: flash_attn    = 0
0.00.645.158 I llama_init_from_model: freq_base     = 10000.0
0.00.645.159 I llama_init_from_model: freq_scale    = 1
0.00.645.160 I ggml_metal_init: allocating
0.00.645.171 I ggml_metal_init: found device: Apple M4
0.00.645.179 I ggml_metal_init: picking default device: Apple M4
0.00.646.480 I ggml_metal_load_library: using embedded metal library
0.00.652.863 I ggml_metal_init: GPU name:   Apple M4
0.00.652.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.869 I ggml_metal_init: simdgroup reduction   = true
0.00.652.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.869 I ggml_metal_init: has residency sets    = true
0.00.652.869 I ggml_metal_init: has bfloat            = true
0.00.652.870 I ggml_metal_init: use bfloat            = true
0.00.652.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.872 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.907 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.552 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.722.558 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.668 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.670 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.670 I llama_init_from_model: graph nodes  = 967
0.00.726.670 I llama_init_from_model: graph splits = 2
0.00.726.676 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.951 I main: llama threadpool init, n_threads = 4
0.00.791.001 I 
0.00.791.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.037 I 
0.00.791.278 I sampler seed: 1234
0.00.791.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.791.316 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.791.318 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.791.318 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.798 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.673.799 I llama_perf_context_print:        load time =     780.41 ms
0.01.673.801 I llama_perf_context_print: prompt eval time =      57.83 ms /     7 tokens (    8.26 ms per token,   121.04 tokens per second)
0.01.673.801 I llama_perf_context_print:        eval time =     821.81 ms /    63 runs   (   13.04 ms per token,    76.66 tokens per second)
0.01.673.802 I llama_perf_context_print:       total time =     883.63 ms /    70 tokens
0.01.674.001 I ggml_metal_free: deallocating

real	0m1.693s
user	0m0.109s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.625 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.374 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.561 I llama_model_loader: - type  f32:  194 tensors
0.00.054.561 I llama_model_loader: - type  f16:   98 tensors
0.00.054.564 I print_info: file format = GGUF V3 (latest)
0.00.054.565 I print_info: file type   = all F32 (guessed)
0.00.054.568 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.958 I load: special tokens cache size = 25
0.00.076.534 I load: token to piece cache size = 0.2984 MB
0.00.076.549 I print_info: arch             = gptneox
0.00.076.550 I print_info: vocab_only       = 0
0.00.076.551 I print_info: n_ctx_train      = 2048
0.00.076.551 I print_info: n_embd           = 2048
0.00.076.551 I print_info: n_layer          = 24
0.00.076.554 I print_info: n_head           = 16
0.00.076.555 I print_info: n_head_kv        = 16
0.00.076.555 I print_info: n_rot            = 32
0.00.076.555 I print_info: n_swa            = 0
0.00.076.555 I print_info: n_embd_head_k    = 128
0.00.076.555 I print_info: n_embd_head_v    = 128
0.00.076.556 I print_info: n_gqa            = 1
0.00.076.557 I print_info: n_embd_k_gqa     = 2048
0.00.076.558 I print_info: n_embd_v_gqa     = 2048
0.00.076.559 I print_info: f_norm_eps       = 1.0e-05
0.00.076.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.560 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.560 I print_info: f_logit_scale    = 0.0e+00
0.00.076.560 I print_info: f_attn_scale     = 0.0e+00
0.00.076.563 I print_info: n_ff             = 8192
0.00.076.563 I print_info: n_expert         = 0
0.00.076.563 I print_info: n_expert_used    = 0
0.00.076.564 I print_info: causal attn      = 1
0.00.076.564 I print_info: pooling type     = 0
0.00.076.564 I print_info: rope type        = 2
0.00.076.564 I print_info: rope scaling     = linear
0.00.076.565 I print_info: freq_base_train  = 10000.0
0.00.076.565 I print_info: freq_scale_train = 1
0.00.076.565 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.566 I print_info: rope_finetuned   = unknown
0.00.076.566 I print_info: ssm_d_conv       = 0
0.00.076.566 I print_info: ssm_d_inner      = 0
0.00.076.566 I print_info: ssm_d_state      = 0
0.00.076.574 I print_info: ssm_dt_rank      = 0
0.00.076.576 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.576 I print_info: model type       = 1.4B
0.00.076.577 I print_info: model params     = 1.41 B
0.00.076.577 I print_info: general.name     = 1.4B
0.00.076.578 I print_info: vocab type       = BPE
0.00.076.578 I print_info: n_vocab          = 50304
0.00.076.581 I print_info: n_merges         = 50009
0.00.076.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.582 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.582 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.582 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.583 I print_info: LF token         = 187 'Ċ'
0.00.076.583 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.583 I print_info: max token length = 1024
0.00.076.584 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.531.892 I load_tensors: offloading 24 repeating layers to GPU
0.01.531.895 I load_tensors: offloading output layer to GPU
0.01.531.896 I load_tensors: offloaded 25/25 layers to GPU
0.01.531.919 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.531.921 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.533.206 I llama_init_from_model: n_seq_max     = 1
0.01.533.207 I llama_init_from_model: n_ctx         = 128
0.01.533.208 I llama_init_from_model: n_ctx_per_seq = 128
0.01.533.208 I llama_init_from_model: n_batch       = 128
0.01.533.209 I llama_init_from_model: n_ubatch      = 128
0.01.533.209 I llama_init_from_model: flash_attn    = 0
0.01.533.209 I llama_init_from_model: freq_base     = 10000.0
0.01.533.210 I llama_init_from_model: freq_scale    = 1
0.01.533.210 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.533.211 I ggml_metal_init: allocating
0.01.533.255 I ggml_metal_init: found device: Apple M4
0.01.533.262 I ggml_metal_init: picking default device: Apple M4
0.01.534.311 I ggml_metal_load_library: using embedded metal library
0.01.538.454 I ggml_metal_init: GPU name:   Apple M4
0.01.538.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.538.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.538.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.538.458 I ggml_metal_init: simdgroup reduction   = true
0.01.538.458 I ggml_metal_init: simdgroup matrix mul. = true
0.01.538.458 I ggml_metal_init: has residency sets    = true
0.01.538.459 I ggml_metal_init: has bfloat            = true
0.01.538.459 I ggml_metal_init: use bfloat            = true
0.01.538.459 I ggml_metal_init: hasUnifiedMemory      = true
0.01.538.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.549.959 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.551.837 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.551.840 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.551.852 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.553.523 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.553.524 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.553.525 I llama_init_from_model: graph nodes  = 967
0.01.553.525 I llama_init_from_model: graph splits = 2
0.01.553.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.553.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.588.320 I 
0.01.588.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.588.361 I perplexity: tokenizing the input ..
0.01.593.915 I perplexity: tokenization took 5.552 ms
0.01.593.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.712.740 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.714.027 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.714.058 I llama_perf_context_print:        load time =    1565.27 ms
0.01.714.059 I llama_perf_context_print: prompt eval time =     118.55 ms /   128 tokens (    0.93 ms per token,  1079.72 tokens per second)
0.01.714.059 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.714.060 I llama_perf_context_print:       total time =     125.74 ms /   129 tokens
0.01.714.411 I ggml_metal_free: deallocating

real	0m1.927s
user	0m0.099s
sys	0m0.278s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.382 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.270 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.404 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.107 I llama_model_loader: - type  f32:  194 tensors
0.00.029.108 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.109 I print_info: file format = GGUF V3 (latest)
0.00.029.109 I print_info: file type   = Q8_0
0.00.029.110 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.744 I load: special tokens cache size = 25
0.00.043.912 I load: token to piece cache size = 0.2984 MB
0.00.043.929 I print_info: arch             = gptneox
0.00.043.930 I print_info: vocab_only       = 0
0.00.043.930 I print_info: n_ctx_train      = 2048
0.00.043.931 I print_info: n_embd           = 2048
0.00.043.931 I print_info: n_layer          = 24
0.00.043.934 I print_info: n_head           = 16
0.00.043.935 I print_info: n_head_kv        = 16
0.00.043.935 I print_info: n_rot            = 32
0.00.043.935 I print_info: n_swa            = 0
0.00.043.935 I print_info: n_embd_head_k    = 128
0.00.043.936 I print_info: n_embd_head_v    = 128
0.00.043.936 I print_info: n_gqa            = 1
0.00.043.937 I print_info: n_embd_k_gqa     = 2048
0.00.043.937 I print_info: n_embd_v_gqa     = 2048
0.00.043.938 I print_info: f_norm_eps       = 1.0e-05
0.00.043.938 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.938 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.938 I print_info: f_logit_scale    = 0.0e+00
0.00.043.939 I print_info: f_attn_scale     = 0.0e+00
0.00.043.939 I print_info: n_ff             = 8192
0.00.043.939 I print_info: n_expert         = 0
0.00.043.940 I print_info: n_expert_used    = 0
0.00.043.940 I print_info: causal attn      = 1
0.00.043.941 I print_info: pooling type     = 0
0.00.043.941 I print_info: rope type        = 2
0.00.043.941 I print_info: rope scaling     = linear
0.00.043.942 I print_info: freq_base_train  = 10000.0
0.00.043.942 I print_info: freq_scale_train = 1
0.00.043.942 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.943 I print_info: rope_finetuned   = unknown
0.00.043.943 I print_info: ssm_d_conv       = 0
0.00.043.943 I print_info: ssm_d_inner      = 0
0.00.043.943 I print_info: ssm_d_state      = 0
0.00.043.943 I print_info: ssm_dt_rank      = 0
0.00.043.943 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.944 I print_info: model type       = 1.4B
0.00.043.944 I print_info: model params     = 1.41 B
0.00.043.944 I print_info: general.name     = 1.4B
0.00.043.945 I print_info: vocab type       = BPE
0.00.043.945 I print_info: n_vocab          = 50304
0.00.043.945 I print_info: n_merges         = 50009
0.00.043.945 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.946 I print_info: LF token         = 187 'Ċ'
0.00.043.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.946 I print_info: max token length = 1024
0.00.043.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.884.522 I load_tensors: offloading 24 repeating layers to GPU
0.00.884.531 I load_tensors: offloading output layer to GPU
0.00.884.531 I load_tensors: offloaded 25/25 layers to GPU
0.00.884.564 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.884.567 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.886.007 I llama_init_from_model: n_seq_max     = 1
0.00.886.009 I llama_init_from_model: n_ctx         = 128
0.00.886.010 I llama_init_from_model: n_ctx_per_seq = 128
0.00.886.010 I llama_init_from_model: n_batch       = 128
0.00.886.010 I llama_init_from_model: n_ubatch      = 128
0.00.886.010 I llama_init_from_model: flash_attn    = 0
0.00.886.011 I llama_init_from_model: freq_base     = 10000.0
0.00.886.011 I llama_init_from_model: freq_scale    = 1
0.00.886.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.886.016 I ggml_metal_init: allocating
0.00.886.126 I ggml_metal_init: found device: Apple M4
0.00.886.136 I ggml_metal_init: picking default device: Apple M4
0.00.887.393 I ggml_metal_load_library: using embedded metal library
0.00.892.652 I ggml_metal_init: GPU name:   Apple M4
0.00.892.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.892.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.892.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.892.658 I ggml_metal_init: simdgroup reduction   = true
0.00.892.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.892.658 I ggml_metal_init: has residency sets    = true
0.00.892.658 I ggml_metal_init: has bfloat            = true
0.00.892.658 I ggml_metal_init: use bfloat            = true
0.00.892.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.892.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.908.468 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.911.795 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.911.798 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.911.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.914.914 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.914.916 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.914.916 I llama_init_from_model: graph nodes  = 967
0.00.914.917 I llama_init_from_model: graph splits = 2
0.00.914.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.914.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.938.980 I 
0.00.939.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.939.048 I perplexity: tokenizing the input ..
0.00.945.319 I perplexity: tokenization took 6.268 ms
0.00.945.329 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.069.835 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.071.164 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.071.188 I llama_perf_context_print:        load time =     925.70 ms
0.01.071.189 I llama_perf_context_print: prompt eval time =     123.93 ms /   128 tokens (    0.97 ms per token,  1032.83 tokens per second)
0.01.071.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.071.191 I llama_perf_context_print:       total time =     132.21 ms /   129 tokens
0.01.071.549 I ggml_metal_free: deallocating

real	0m1.095s
user	0m0.080s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.287 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.373 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.511 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.145 I llama_model_loader: - type  f32:  194 tensors
0.00.026.145 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.146 I print_info: file format = GGUF V3 (latest)
0.00.026.147 I print_info: file type   = Q4_0
0.00.026.148 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.840 I load: special tokens cache size = 25
0.00.041.056 I load: token to piece cache size = 0.2984 MB
0.00.041.074 I print_info: arch             = gptneox
0.00.041.075 I print_info: vocab_only       = 0
0.00.041.075 I print_info: n_ctx_train      = 2048
0.00.041.075 I print_info: n_embd           = 2048
0.00.041.075 I print_info: n_layer          = 24
0.00.041.079 I print_info: n_head           = 16
0.00.041.080 I print_info: n_head_kv        = 16
0.00.041.080 I print_info: n_rot            = 32
0.00.041.080 I print_info: n_swa            = 0
0.00.041.080 I print_info: n_embd_head_k    = 128
0.00.041.081 I print_info: n_embd_head_v    = 128
0.00.041.084 I print_info: n_gqa            = 1
0.00.041.085 I print_info: n_embd_k_gqa     = 2048
0.00.041.085 I print_info: n_embd_v_gqa     = 2048
0.00.041.086 I print_info: f_norm_eps       = 1.0e-05
0.00.041.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.086 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.087 I print_info: f_logit_scale    = 0.0e+00
0.00.041.087 I print_info: f_attn_scale     = 0.0e+00
0.00.041.088 I print_info: n_ff             = 8192
0.00.041.088 I print_info: n_expert         = 0
0.00.041.088 I print_info: n_expert_used    = 0
0.00.041.088 I print_info: causal attn      = 1
0.00.041.088 I print_info: pooling type     = 0
0.00.041.088 I print_info: rope type        = 2
0.00.041.089 I print_info: rope scaling     = linear
0.00.041.089 I print_info: freq_base_train  = 10000.0
0.00.041.089 I print_info: freq_scale_train = 1
0.00.041.089 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.090 I print_info: rope_finetuned   = unknown
0.00.041.090 I print_info: ssm_d_conv       = 0
0.00.041.090 I print_info: ssm_d_inner      = 0
0.00.041.090 I print_info: ssm_d_state      = 0
0.00.041.092 I print_info: ssm_dt_rank      = 0
0.00.041.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.092 I print_info: model type       = 1.4B
0.00.041.092 I print_info: model params     = 1.41 B
0.00.041.092 I print_info: general.name     = 1.4B
0.00.041.093 I print_info: vocab type       = BPE
0.00.041.093 I print_info: n_vocab          = 50304
0.00.041.093 I print_info: n_merges         = 50009
0.00.041.094 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.094 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.094 I print_info: LF token         = 187 'Ċ'
0.00.041.095 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.095 I print_info: max token length = 1024
0.00.041.095 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.570.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.115 I load_tensors: offloading output layer to GPU
0.00.570.116 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.156 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.570.157 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.571.827 I llama_init_from_model: n_seq_max     = 1
0.00.571.830 I llama_init_from_model: n_ctx         = 128
0.00.571.830 I llama_init_from_model: n_ctx_per_seq = 128
0.00.571.831 I llama_init_from_model: n_batch       = 128
0.00.571.831 I llama_init_from_model: n_ubatch      = 128
0.00.571.831 I llama_init_from_model: flash_attn    = 0
0.00.571.834 I llama_init_from_model: freq_base     = 10000.0
0.00.571.835 I llama_init_from_model: freq_scale    = 1
0.00.571.836 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.571.839 I ggml_metal_init: allocating
0.00.571.931 I ggml_metal_init: found device: Apple M4
0.00.571.945 I ggml_metal_init: picking default device: Apple M4
0.00.573.594 I ggml_metal_load_library: using embedded metal library
0.00.580.000 I ggml_metal_init: GPU name:   Apple M4
0.00.580.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.014 I ggml_metal_init: simdgroup reduction   = true
0.00.580.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.014 I ggml_metal_init: has residency sets    = true
0.00.580.014 I ggml_metal_init: has bfloat            = true
0.00.580.015 I ggml_metal_init: use bfloat            = true
0.00.580.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.598.516 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.602.004 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.602.009 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.602.036 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.605.145 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.605.147 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.605.148 I llama_init_from_model: graph nodes  = 967
0.00.605.148 I llama_init_from_model: graph splits = 2
0.00.605.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.605.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.908 I 
0.00.630.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.993 I perplexity: tokenizing the input ..
0.00.637.924 I perplexity: tokenization took 6.93 ms
0.00.637.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.719 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.774.122 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.774.145 I llama_perf_context_print:        load time =     620.53 ms
0.00.774.146 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.25 tokens per second)
0.00.774.146 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.147 I llama_perf_context_print:       total time =     143.24 ms /   129 tokens
0.00.774.495 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.079s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.986 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.511 I llama_model_loader: - type  f32:  194 tensors
0.00.024.511 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.512 I print_info: file format = GGUF V3 (latest)
0.00.024.513 I print_info: file type   = Q4_1
0.00.024.515 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.919 I load: special tokens cache size = 25
0.00.039.053 I load: token to piece cache size = 0.2984 MB
0.00.039.066 I print_info: arch             = gptneox
0.00.039.066 I print_info: vocab_only       = 0
0.00.039.067 I print_info: n_ctx_train      = 2048
0.00.039.067 I print_info: n_embd           = 2048
0.00.039.067 I print_info: n_layer          = 24
0.00.039.071 I print_info: n_head           = 16
0.00.039.071 I print_info: n_head_kv        = 16
0.00.039.072 I print_info: n_rot            = 32
0.00.039.072 I print_info: n_swa            = 0
0.00.039.072 I print_info: n_embd_head_k    = 128
0.00.039.072 I print_info: n_embd_head_v    = 128
0.00.039.073 I print_info: n_gqa            = 1
0.00.039.073 I print_info: n_embd_k_gqa     = 2048
0.00.039.074 I print_info: n_embd_v_gqa     = 2048
0.00.039.074 I print_info: f_norm_eps       = 1.0e-05
0.00.039.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.075 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.075 I print_info: f_logit_scale    = 0.0e+00
0.00.039.075 I print_info: f_attn_scale     = 0.0e+00
0.00.039.076 I print_info: n_ff             = 8192
0.00.039.076 I print_info: n_expert         = 0
0.00.039.076 I print_info: n_expert_used    = 0
0.00.039.076 I print_info: causal attn      = 1
0.00.039.076 I print_info: pooling type     = 0
0.00.039.076 I print_info: rope type        = 2
0.00.039.077 I print_info: rope scaling     = linear
0.00.039.077 I print_info: freq_base_train  = 10000.0
0.00.039.080 I print_info: freq_scale_train = 1
0.00.039.080 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.080 I print_info: rope_finetuned   = unknown
0.00.039.081 I print_info: ssm_d_conv       = 0
0.00.039.081 I print_info: ssm_d_inner      = 0
0.00.039.081 I print_info: ssm_d_state      = 0
0.00.039.081 I print_info: ssm_dt_rank      = 0
0.00.039.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.081 I print_info: model type       = 1.4B
0.00.039.082 I print_info: model params     = 1.41 B
0.00.039.082 I print_info: general.name     = 1.4B
0.00.039.082 I print_info: vocab type       = BPE
0.00.039.082 I print_info: n_vocab          = 50304
0.00.039.082 I print_info: n_merges         = 50009
0.00.039.083 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: LF token         = 187 'Ċ'
0.00.039.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.084 I print_info: max token length = 1024
0.00.039.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.295.848 I load_tensors: offloading 24 repeating layers to GPU
0.00.295.861 I load_tensors: offloading output layer to GPU
0.00.295.862 I load_tensors: offloaded 25/25 layers to GPU
0.00.295.892 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.295.894 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.297.442 I llama_init_from_model: n_seq_max     = 1
0.00.297.448 I llama_init_from_model: n_ctx         = 128
0.00.297.449 I llama_init_from_model: n_ctx_per_seq = 128
0.00.297.449 I llama_init_from_model: n_batch       = 128
0.00.297.450 I llama_init_from_model: n_ubatch      = 128
0.00.297.450 I llama_init_from_model: flash_attn    = 0
0.00.297.451 I llama_init_from_model: freq_base     = 10000.0
0.00.297.452 I llama_init_from_model: freq_scale    = 1
0.00.297.452 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.297.455 I ggml_metal_init: allocating
0.00.297.509 I ggml_metal_init: found device: Apple M4
0.00.297.522 I ggml_metal_init: picking default device: Apple M4
0.00.299.055 I ggml_metal_load_library: using embedded metal library
0.00.305.959 I ggml_metal_init: GPU name:   Apple M4
0.00.305.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.305.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.305.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.305.971 I ggml_metal_init: simdgroup reduction   = true
0.00.305.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.305.971 I ggml_metal_init: has residency sets    = true
0.00.305.972 I ggml_metal_init: has bfloat            = true
0.00.305.972 I ggml_metal_init: use bfloat            = true
0.00.305.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.305.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.324.178 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.327.788 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.327.795 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.327.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.331.099 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.331.100 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.331.101 I llama_init_from_model: graph nodes  = 967
0.00.331.101 I llama_init_from_model: graph splits = 2
0.00.331.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.331.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.029 I 
0.00.358.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.141 I perplexity: tokenizing the input ..
0.00.364.934 I perplexity: tokenization took 6.791 ms
0.00.364.940 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.494.638 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.495.978 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.496.005 I llama_perf_context_print:        load time =     349.03 ms
0.00.496.006 I llama_perf_context_print: prompt eval time =     128.80 ms /   128 tokens (    1.01 ms per token,   993.78 tokens per second)
0.00.496.007 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.496.007 I llama_perf_context_print:       total time =     137.98 ms /   129 tokens
0.00.496.398 I ggml_metal_free: deallocating

real	0m0.511s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.424 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.864 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.718 I llama_model_loader: - type  f32:  194 tensors
0.00.025.718 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.718 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.719 I print_info: file format = GGUF V3 (latest)
0.00.025.719 I print_info: file type   = Q5_0
0.00.025.721 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.361 I load: special tokens cache size = 25
0.00.040.484 I load: token to piece cache size = 0.2984 MB
0.00.040.501 I print_info: arch             = gptneox
0.00.040.502 I print_info: vocab_only       = 0
0.00.040.502 I print_info: n_ctx_train      = 2048
0.00.040.502 I print_info: n_embd           = 2048
0.00.040.502 I print_info: n_layer          = 24
0.00.040.506 I print_info: n_head           = 16
0.00.040.506 I print_info: n_head_kv        = 16
0.00.040.507 I print_info: n_rot            = 32
0.00.040.507 I print_info: n_swa            = 0
0.00.040.507 I print_info: n_embd_head_k    = 128
0.00.040.507 I print_info: n_embd_head_v    = 128
0.00.040.508 I print_info: n_gqa            = 1
0.00.040.508 I print_info: n_embd_k_gqa     = 2048
0.00.040.509 I print_info: n_embd_v_gqa     = 2048
0.00.040.509 I print_info: f_norm_eps       = 1.0e-05
0.00.040.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.511 I print_info: f_logit_scale    = 0.0e+00
0.00.040.511 I print_info: f_attn_scale     = 0.0e+00
0.00.040.512 I print_info: n_ff             = 8192
0.00.040.513 I print_info: n_expert         = 0
0.00.040.513 I print_info: n_expert_used    = 0
0.00.040.513 I print_info: causal attn      = 1
0.00.040.513 I print_info: pooling type     = 0
0.00.040.513 I print_info: rope type        = 2
0.00.040.513 I print_info: rope scaling     = linear
0.00.040.514 I print_info: freq_base_train  = 10000.0
0.00.040.514 I print_info: freq_scale_train = 1
0.00.040.514 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.514 I print_info: rope_finetuned   = unknown
0.00.040.514 I print_info: ssm_d_conv       = 0
0.00.040.515 I print_info: ssm_d_inner      = 0
0.00.040.515 I print_info: ssm_d_state      = 0
0.00.040.515 I print_info: ssm_dt_rank      = 0
0.00.040.515 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.515 I print_info: model type       = 1.4B
0.00.040.515 I print_info: model params     = 1.41 B
0.00.040.515 I print_info: general.name     = 1.4B
0.00.040.516 I print_info: vocab type       = BPE
0.00.040.516 I print_info: n_vocab          = 50304
0.00.040.516 I print_info: n_merges         = 50009
0.00.040.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.517 I print_info: LF token         = 187 'Ċ'
0.00.040.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.517 I print_info: max token length = 1024
0.00.040.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.673.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.967 I load_tensors: offloading output layer to GPU
0.00.673.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.005 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.674.006 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.675.572 I llama_init_from_model: n_seq_max     = 1
0.00.675.574 I llama_init_from_model: n_ctx         = 128
0.00.675.575 I llama_init_from_model: n_ctx_per_seq = 128
0.00.675.575 I llama_init_from_model: n_batch       = 128
0.00.675.576 I llama_init_from_model: n_ubatch      = 128
0.00.675.576 I llama_init_from_model: flash_attn    = 0
0.00.675.578 I llama_init_from_model: freq_base     = 10000.0
0.00.675.579 I llama_init_from_model: freq_scale    = 1
0.00.675.579 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.675.582 I ggml_metal_init: allocating
0.00.675.679 I ggml_metal_init: found device: Apple M4
0.00.675.694 I ggml_metal_init: picking default device: Apple M4
0.00.677.334 I ggml_metal_load_library: using embedded metal library
0.00.684.409 I ggml_metal_init: GPU name:   Apple M4
0.00.684.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.420 I ggml_metal_init: simdgroup reduction   = true
0.00.684.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.421 I ggml_metal_init: has residency sets    = true
0.00.684.421 I ggml_metal_init: has bfloat            = true
0.00.684.421 I ggml_metal_init: use bfloat            = true
0.00.684.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.489 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.941 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.945 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.970 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.228 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.709.230 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.709.230 I llama_init_from_model: graph nodes  = 967
0.00.709.231 I llama_init_from_model: graph splits = 2
0.00.709.234 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.709.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.882 I 
0.00.740.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.988 I perplexity: tokenizing the input ..
0.00.747.981 I perplexity: tokenization took 6.989 ms
0.00.747.995 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.893.881 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.895.216 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.895.240 I llama_perf_context_print:        load time =     731.45 ms
0.00.895.241 I llama_perf_context_print: prompt eval time =     144.95 ms /   128 tokens (    1.13 ms per token,   883.06 tokens per second)
0.00.895.242 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.242 I llama_perf_context_print:       total time =     154.36 ms /   129 tokens
0.00.895.622 I ggml_metal_free: deallocating

real	0m0.912s
user	0m0.080s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.737 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.605 I llama_model_loader: - type  f32:  194 tensors
0.00.024.605 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.605 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.606 I print_info: file format = GGUF V3 (latest)
0.00.024.606 I print_info: file type   = Q5_1
0.00.024.607 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.975 I load: special tokens cache size = 25
0.00.038.928 I load: token to piece cache size = 0.2984 MB
0.00.038.945 I print_info: arch             = gptneox
0.00.038.946 I print_info: vocab_only       = 0
0.00.038.946 I print_info: n_ctx_train      = 2048
0.00.038.947 I print_info: n_embd           = 2048
0.00.038.947 I print_info: n_layer          = 24
0.00.038.950 I print_info: n_head           = 16
0.00.038.951 I print_info: n_head_kv        = 16
0.00.038.951 I print_info: n_rot            = 32
0.00.038.951 I print_info: n_swa            = 0
0.00.038.951 I print_info: n_embd_head_k    = 128
0.00.038.951 I print_info: n_embd_head_v    = 128
0.00.038.952 I print_info: n_gqa            = 1
0.00.038.953 I print_info: n_embd_k_gqa     = 2048
0.00.038.953 I print_info: n_embd_v_gqa     = 2048
0.00.038.954 I print_info: f_norm_eps       = 1.0e-05
0.00.038.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.958 I print_info: f_logit_scale    = 0.0e+00
0.00.038.958 I print_info: f_attn_scale     = 0.0e+00
0.00.038.958 I print_info: n_ff             = 8192
0.00.038.958 I print_info: n_expert         = 0
0.00.038.959 I print_info: n_expert_used    = 0
0.00.038.959 I print_info: causal attn      = 1
0.00.038.959 I print_info: pooling type     = 0
0.00.038.959 I print_info: rope type        = 2
0.00.038.959 I print_info: rope scaling     = linear
0.00.038.960 I print_info: freq_base_train  = 10000.0
0.00.038.960 I print_info: freq_scale_train = 1
0.00.038.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.960 I print_info: rope_finetuned   = unknown
0.00.038.960 I print_info: ssm_d_conv       = 0
0.00.038.960 I print_info: ssm_d_inner      = 0
0.00.038.961 I print_info: ssm_d_state      = 0
0.00.038.961 I print_info: ssm_dt_rank      = 0
0.00.038.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.962 I print_info: model type       = 1.4B
0.00.038.962 I print_info: model params     = 1.41 B
0.00.038.963 I print_info: general.name     = 1.4B
0.00.038.963 I print_info: vocab type       = BPE
0.00.038.963 I print_info: n_vocab          = 50304
0.00.038.963 I print_info: n_merges         = 50009
0.00.038.964 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: LF token         = 187 'Ċ'
0.00.038.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: max token length = 1024
0.00.038.965 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.105 I load_tensors: offloading output layer to GPU
0.00.619.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.123 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.619.124 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.620.057 I llama_init_from_model: n_seq_max     = 1
0.00.620.061 I llama_init_from_model: n_ctx         = 128
0.00.620.061 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.061 I llama_init_from_model: n_batch       = 128
0.00.620.062 I llama_init_from_model: n_ubatch      = 128
0.00.620.062 I llama_init_from_model: flash_attn    = 0
0.00.620.063 I llama_init_from_model: freq_base     = 10000.0
0.00.620.063 I llama_init_from_model: freq_scale    = 1
0.00.620.064 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.065 I ggml_metal_init: allocating
0.00.620.107 I ggml_metal_init: found device: Apple M4
0.00.620.119 I ggml_metal_init: picking default device: Apple M4
0.00.621.087 I ggml_metal_load_library: using embedded metal library
0.00.625.257 I ggml_metal_init: GPU name:   Apple M4
0.00.625.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.264 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.265 I ggml_metal_init: simdgroup reduction   = true
0.00.625.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.265 I ggml_metal_init: has residency sets    = true
0.00.625.266 I ggml_metal_init: has bfloat            = true
0.00.625.266 I ggml_metal_init: use bfloat            = true
0.00.625.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.227 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.876 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.880 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.486 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.487 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.488 I llama_init_from_model: graph nodes  = 967
0.00.642.488 I llama_init_from_model: graph splits = 2
0.00.642.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.420 I 
0.00.667.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.469 I perplexity: tokenizing the input ..
0.00.671.539 I perplexity: tokenization took 4.068 ms
0.00.671.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.106 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.808.908 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.808.948 I llama_perf_context_print:        load time =     658.52 ms
0.00.808.956 I llama_perf_context_print: prompt eval time =     134.32 ms /   128 tokens (    1.05 ms per token,   952.91 tokens per second)
0.00.808.960 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.960 I llama_perf_context_print:       total time =     141.53 ms /   129 tokens
0.00.810.058 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.228 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.299 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.589 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.116 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.117 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.117 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.118 I llama_model_loader: - type  f32:  194 tensors
0.00.036.118 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.118 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.119 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.119 I print_info: file format = GGUF V3 (latest)
0.00.036.120 I print_info: file type   = Q2_K - Medium
0.00.036.121 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.373 I load: special tokens cache size = 25
0.00.055.046 I load: token to piece cache size = 0.2984 MB
0.00.055.061 I print_info: arch             = gptneox
0.00.055.062 I print_info: vocab_only       = 0
0.00.055.062 I print_info: n_ctx_train      = 2048
0.00.055.062 I print_info: n_embd           = 2048
0.00.055.062 I print_info: n_layer          = 24
0.00.055.064 I print_info: n_head           = 16
0.00.055.065 I print_info: n_head_kv        = 16
0.00.055.065 I print_info: n_rot            = 32
0.00.055.065 I print_info: n_swa            = 0
0.00.055.065 I print_info: n_embd_head_k    = 128
0.00.055.066 I print_info: n_embd_head_v    = 128
0.00.055.066 I print_info: n_gqa            = 1
0.00.055.067 I print_info: n_embd_k_gqa     = 2048
0.00.055.068 I print_info: n_embd_v_gqa     = 2048
0.00.055.069 I print_info: f_norm_eps       = 1.0e-05
0.00.055.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.069 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.070 I print_info: f_logit_scale    = 0.0e+00
0.00.055.070 I print_info: f_attn_scale     = 0.0e+00
0.00.055.071 I print_info: n_ff             = 8192
0.00.055.071 I print_info: n_expert         = 0
0.00.055.071 I print_info: n_expert_used    = 0
0.00.055.072 I print_info: causal attn      = 1
0.00.055.072 I print_info: pooling type     = 0
0.00.055.072 I print_info: rope type        = 2
0.00.055.072 I print_info: rope scaling     = linear
0.00.055.072 I print_info: freq_base_train  = 10000.0
0.00.055.073 I print_info: freq_scale_train = 1
0.00.055.073 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.077 I print_info: rope_finetuned   = unknown
0.00.055.077 I print_info: ssm_d_conv       = 0
0.00.055.077 I print_info: ssm_d_inner      = 0
0.00.055.077 I print_info: ssm_d_state      = 0
0.00.055.077 I print_info: ssm_dt_rank      = 0
0.00.055.079 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.079 I print_info: model type       = 1.4B
0.00.055.079 I print_info: model params     = 1.41 B
0.00.055.079 I print_info: general.name     = 1.4B
0.00.055.080 I print_info: vocab type       = BPE
0.00.055.080 I print_info: n_vocab          = 50304
0.00.055.081 I print_info: n_merges         = 50009
0.00.055.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.082 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.082 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.082 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.082 I print_info: LF token         = 187 'Ċ'
0.00.055.083 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.084 I print_info: max token length = 1024
0.00.055.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.365.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.011 I load_tensors: offloading output layer to GPU
0.00.366.012 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.038 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.039 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.335 I llama_init_from_model: n_seq_max     = 1
0.00.367.338 I llama_init_from_model: n_ctx         = 128
0.00.367.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.367.339 I llama_init_from_model: n_batch       = 128
0.00.367.340 I llama_init_from_model: n_ubatch      = 128
0.00.367.340 I llama_init_from_model: flash_attn    = 0
0.00.367.343 I llama_init_from_model: freq_base     = 10000.0
0.00.367.344 I llama_init_from_model: freq_scale    = 1
0.00.367.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.367.348 I ggml_metal_init: allocating
0.00.367.439 I ggml_metal_init: found device: Apple M4
0.00.367.453 I ggml_metal_init: picking default device: Apple M4
0.00.369.062 I ggml_metal_load_library: using embedded metal library
0.00.374.713 I ggml_metal_init: GPU name:   Apple M4
0.00.374.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.374.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.374.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.374.727 I ggml_metal_init: simdgroup reduction   = true
0.00.374.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.374.728 I ggml_metal_init: has residency sets    = true
0.00.374.728 I ggml_metal_init: has bfloat            = true
0.00.374.728 I ggml_metal_init: use bfloat            = true
0.00.374.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.374.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.189 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.399.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.399.884 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.399.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.403.466 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.403.468 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.403.469 I llama_init_from_model: graph nodes  = 967
0.00.403.469 I llama_init_from_model: graph splits = 2
0.00.403.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.403.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.099 I 
0.00.432.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.189 I perplexity: tokenizing the input ..
0.00.439.184 I perplexity: tokenization took 6.993 ms
0.00.439.190 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.572.234 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.573.587 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.573.611 I llama_perf_context_print:        load time =     419.79 ms
0.00.573.612 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.07 tokens per second)
0.00.573.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.613 I llama_perf_context_print:       total time =     141.52 ms /   129 tokens
0.00.574.005 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.098s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.046 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.946 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.947 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.947 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.948 I llama_model_loader: - type  f32:  194 tensors
0.00.024.948 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.948 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.949 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.949 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.950 I print_info: file format = GGUF V3 (latest)
0.00.024.950 I print_info: file type   = Q3_K - Medium
0.00.024.951 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.292 I load: special tokens cache size = 25
0.00.039.689 I load: token to piece cache size = 0.2984 MB
0.00.039.706 I print_info: arch             = gptneox
0.00.039.707 I print_info: vocab_only       = 0
0.00.039.707 I print_info: n_ctx_train      = 2048
0.00.039.708 I print_info: n_embd           = 2048
0.00.039.708 I print_info: n_layer          = 24
0.00.039.712 I print_info: n_head           = 16
0.00.039.713 I print_info: n_head_kv        = 16
0.00.039.713 I print_info: n_rot            = 32
0.00.039.713 I print_info: n_swa            = 0
0.00.039.713 I print_info: n_embd_head_k    = 128
0.00.039.714 I print_info: n_embd_head_v    = 128
0.00.039.714 I print_info: n_gqa            = 1
0.00.039.715 I print_info: n_embd_k_gqa     = 2048
0.00.039.716 I print_info: n_embd_v_gqa     = 2048
0.00.039.716 I print_info: f_norm_eps       = 1.0e-05
0.00.039.718 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.719 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.719 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.719 I print_info: f_logit_scale    = 0.0e+00
0.00.039.719 I print_info: f_attn_scale     = 0.0e+00
0.00.039.720 I print_info: n_ff             = 8192
0.00.039.720 I print_info: n_expert         = 0
0.00.039.722 I print_info: n_expert_used    = 0
0.00.039.722 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.723 I print_info: rope type        = 2
0.00.039.723 I print_info: rope scaling     = linear
0.00.039.723 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.724 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.724 I print_info: rope_finetuned   = unknown
0.00.039.724 I print_info: ssm_d_conv       = 0
0.00.039.725 I print_info: ssm_d_inner      = 0
0.00.039.725 I print_info: ssm_d_state      = 0
0.00.039.725 I print_info: ssm_dt_rank      = 0
0.00.039.726 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.726 I print_info: model type       = 1.4B
0.00.039.726 I print_info: model params     = 1.41 B
0.00.039.726 I print_info: general.name     = 1.4B
0.00.039.727 I print_info: vocab type       = BPE
0.00.039.727 I print_info: n_vocab          = 50304
0.00.039.727 I print_info: n_merges         = 50009
0.00.039.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: LF token         = 187 'Ċ'
0.00.039.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: max token length = 1024
0.00.039.729 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.105 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.122 I load_tensors: offloading output layer to GPU
0.00.437.123 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.159 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.160 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.985 I llama_init_from_model: n_seq_max     = 1
0.00.438.988 I llama_init_from_model: n_ctx         = 128
0.00.438.988 I llama_init_from_model: n_ctx_per_seq = 128
0.00.438.989 I llama_init_from_model: n_batch       = 128
0.00.438.989 I llama_init_from_model: n_ubatch      = 128
0.00.438.989 I llama_init_from_model: flash_attn    = 0
0.00.438.992 I llama_init_from_model: freq_base     = 10000.0
0.00.438.992 I llama_init_from_model: freq_scale    = 1
0.00.438.993 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.438.995 I ggml_metal_init: allocating
0.00.439.087 I ggml_metal_init: found device: Apple M4
0.00.439.100 I ggml_metal_init: picking default device: Apple M4
0.00.440.723 I ggml_metal_load_library: using embedded metal library
0.00.446.274 I ggml_metal_init: GPU name:   Apple M4
0.00.446.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.293 I ggml_metal_init: simdgroup reduction   = true
0.00.446.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.293 I ggml_metal_init: has residency sets    = true
0.00.446.294 I ggml_metal_init: has bfloat            = true
0.00.446.294 I ggml_metal_init: use bfloat            = true
0.00.446.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.299 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.397 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.471.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.471.048 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.471.099 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.503 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.474.505 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.474.506 I llama_init_from_model: graph nodes  = 967
0.00.474.506 I llama_init_from_model: graph splits = 2
0.00.474.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.474.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.733 I 
0.00.502.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.834 I perplexity: tokenizing the input ..
0.00.509.393 I perplexity: tokenization took 6.556 ms
0.00.509.400 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.652.527 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.653.917 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.653.945 I llama_perf_context_print:        load time =     493.68 ms
0.00.653.946 I llama_perf_context_print: prompt eval time =     142.25 ms /   128 tokens (    1.11 ms per token,   899.85 tokens per second)
0.00.653.947 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.653.947 I llama_perf_context_print:       total time =     151.22 ms /   129 tokens
0.00.654.351 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.081s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.008 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.764 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.779 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.785 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.277 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.277 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.278 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.278 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.279 I llama_model_loader: - type  f32:  194 tensors
0.00.027.279 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.280 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.280 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.280 I print_info: file format = GGUF V3 (latest)
0.00.027.281 I print_info: file type   = Q4_K - Medium
0.00.027.282 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.223 I load: special tokens cache size = 25
0.00.041.297 I load: token to piece cache size = 0.2984 MB
0.00.041.314 I print_info: arch             = gptneox
0.00.041.315 I print_info: vocab_only       = 0
0.00.041.315 I print_info: n_ctx_train      = 2048
0.00.041.315 I print_info: n_embd           = 2048
0.00.041.316 I print_info: n_layer          = 24
0.00.041.320 I print_info: n_head           = 16
0.00.041.321 I print_info: n_head_kv        = 16
0.00.041.324 I print_info: n_rot            = 32
0.00.041.324 I print_info: n_swa            = 0
0.00.041.324 I print_info: n_embd_head_k    = 128
0.00.041.324 I print_info: n_embd_head_v    = 128
0.00.041.325 I print_info: n_gqa            = 1
0.00.041.325 I print_info: n_embd_k_gqa     = 2048
0.00.041.326 I print_info: n_embd_v_gqa     = 2048
0.00.041.327 I print_info: f_norm_eps       = 1.0e-05
0.00.041.327 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.327 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.329 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.329 I print_info: f_logit_scale    = 0.0e+00
0.00.041.329 I print_info: f_attn_scale     = 0.0e+00
0.00.041.329 I print_info: n_ff             = 8192
0.00.041.330 I print_info: n_expert         = 0
0.00.041.330 I print_info: n_expert_used    = 0
0.00.041.330 I print_info: causal attn      = 1
0.00.041.330 I print_info: pooling type     = 0
0.00.041.330 I print_info: rope type        = 2
0.00.041.330 I print_info: rope scaling     = linear
0.00.041.331 I print_info: freq_base_train  = 10000.0
0.00.041.331 I print_info: freq_scale_train = 1
0.00.041.331 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.331 I print_info: rope_finetuned   = unknown
0.00.041.332 I print_info: ssm_d_conv       = 0
0.00.041.332 I print_info: ssm_d_inner      = 0
0.00.041.333 I print_info: ssm_d_state      = 0
0.00.041.333 I print_info: ssm_dt_rank      = 0
0.00.041.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.333 I print_info: model type       = 1.4B
0.00.041.333 I print_info: model params     = 1.41 B
0.00.041.334 I print_info: general.name     = 1.4B
0.00.041.334 I print_info: vocab type       = BPE
0.00.041.334 I print_info: n_vocab          = 50304
0.00.041.335 I print_info: n_merges         = 50009
0.00.041.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: LF token         = 187 'Ċ'
0.00.041.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.336 I print_info: max token length = 1024
0.00.041.336 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.222 I load_tensors: offloading output layer to GPU
0.00.511.223 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.257 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.259 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.000 I llama_init_from_model: n_seq_max     = 1
0.00.513.004 I llama_init_from_model: n_ctx         = 128
0.00.513.005 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.005 I llama_init_from_model: n_batch       = 128
0.00.513.005 I llama_init_from_model: n_ubatch      = 128
0.00.513.006 I llama_init_from_model: flash_attn    = 0
0.00.513.008 I llama_init_from_model: freq_base     = 10000.0
0.00.513.009 I llama_init_from_model: freq_scale    = 1
0.00.513.009 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.012 I ggml_metal_init: allocating
0.00.513.090 I ggml_metal_init: found device: Apple M4
0.00.513.103 I ggml_metal_init: picking default device: Apple M4
0.00.514.727 I ggml_metal_load_library: using embedded metal library
0.00.521.731 I ggml_metal_init: GPU name:   Apple M4
0.00.521.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.742 I ggml_metal_init: simdgroup reduction   = true
0.00.521.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.743 I ggml_metal_init: has residency sets    = true
0.00.521.743 I ggml_metal_init: has bfloat            = true
0.00.521.743 I ggml_metal_init: use bfloat            = true
0.00.521.744 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.211 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.217 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.256 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.557 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.559 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.559 I llama_init_from_model: graph nodes  = 967
0.00.546.559 I llama_init_from_model: graph splits = 2
0.00.546.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.368 I 
0.00.577.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.469 I perplexity: tokenizing the input ..
0.00.585.041 I perplexity: tokenization took 7.568 ms
0.00.585.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.585 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.727.923 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.727.950 I llama_perf_context_print:        load time =     567.35 ms
0.00.727.951 I llama_perf_context_print: prompt eval time =     140.63 ms /   128 tokens (    1.10 ms per token,   910.18 tokens per second)
0.00.727.953 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.954 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.728.345 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.082s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.029.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.336 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.340 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.341 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.038.077 I llama_model_loader: - type  f32:  194 tensors
0.00.038.078 I llama_model_loader: - type q5_K:   61 tensors
0.00.038.078 I llama_model_loader: - type q6_K:   37 tensors
0.00.038.078 I print_info: file format = GGUF V3 (latest)
0.00.038.079 I print_info: file type   = Q5_K - Medium
0.00.038.080 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.050.265 I load: special tokens cache size = 25
0.00.059.893 I load: token to piece cache size = 0.2984 MB
0.00.059.912 I print_info: arch             = gptneox
0.00.059.913 I print_info: vocab_only       = 0
0.00.059.913 I print_info: n_ctx_train      = 2048
0.00.059.914 I print_info: n_embd           = 2048
0.00.059.914 I print_info: n_layer          = 24
0.00.059.919 I print_info: n_head           = 16
0.00.059.921 I print_info: n_head_kv        = 16
0.00.059.921 I print_info: n_rot            = 32
0.00.059.922 I print_info: n_swa            = 0
0.00.059.922 I print_info: n_embd_head_k    = 128
0.00.059.933 I print_info: n_embd_head_v    = 128
0.00.059.935 I print_info: n_gqa            = 1
0.00.059.937 I print_info: n_embd_k_gqa     = 2048
0.00.059.939 I print_info: n_embd_v_gqa     = 2048
0.00.059.940 I print_info: f_norm_eps       = 1.0e-05
0.00.059.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.942 I print_info: f_logit_scale    = 0.0e+00
0.00.059.942 I print_info: f_attn_scale     = 0.0e+00
0.00.059.944 I print_info: n_ff             = 8192
0.00.059.944 I print_info: n_expert         = 0
0.00.059.945 I print_info: n_expert_used    = 0
0.00.059.945 I print_info: causal attn      = 1
0.00.059.945 I print_info: pooling type     = 0
0.00.059.946 I print_info: rope type        = 2
0.00.059.946 I print_info: rope scaling     = linear
0.00.059.947 I print_info: freq_base_train  = 10000.0
0.00.059.948 I print_info: freq_scale_train = 1
0.00.059.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.949 I print_info: rope_finetuned   = unknown
0.00.059.949 I print_info: ssm_d_conv       = 0
0.00.059.950 I print_info: ssm_d_inner      = 0
0.00.059.950 I print_info: ssm_d_state      = 0
0.00.059.950 I print_info: ssm_dt_rank      = 0
0.00.059.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.951 I print_info: model type       = 1.4B
0.00.059.952 I print_info: model params     = 1.41 B
0.00.059.953 I print_info: general.name     = 1.4B
0.00.059.954 I print_info: vocab type       = BPE
0.00.059.954 I print_info: n_vocab          = 50304
0.00.059.959 I print_info: n_merges         = 50009
0.00.059.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.961 I print_info: LF token         = 187 'Ċ'
0.00.059.962 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.962 I print_info: max token length = 1024
0.00.059.963 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.489 I load_tensors: offloading output layer to GPU
0.00.643.490 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.525 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.643.526 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.644.654 I llama_init_from_model: n_seq_max     = 1
0.00.644.656 I llama_init_from_model: n_ctx         = 128
0.00.644.657 I llama_init_from_model: n_ctx_per_seq = 128
0.00.644.658 I llama_init_from_model: n_batch       = 128
0.00.644.658 I llama_init_from_model: n_ubatch      = 128
0.00.644.658 I llama_init_from_model: flash_attn    = 0
0.00.644.660 I llama_init_from_model: freq_base     = 10000.0
0.00.644.661 I llama_init_from_model: freq_scale    = 1
0.00.644.661 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.644.664 I ggml_metal_init: allocating
0.00.644.770 I ggml_metal_init: found device: Apple M4
0.00.644.789 I ggml_metal_init: picking default device: Apple M4
0.00.646.470 I ggml_metal_load_library: using embedded metal library
0.00.653.185 I ggml_metal_init: GPU name:   Apple M4
0.00.653.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.192 I ggml_metal_init: simdgroup reduction   = true
0.00.653.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.193 I ggml_metal_init: has residency sets    = true
0.00.653.193 I ggml_metal_init: has bfloat            = true
0.00.653.193 I ggml_metal_init: use bfloat            = true
0.00.653.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.211 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.792 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.674.799 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.674.826 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.678.072 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.678.074 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.678.075 I llama_init_from_model: graph nodes  = 967
0.00.678.075 I llama_init_from_model: graph splits = 2
0.00.678.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.678.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.618 I 
0.00.714.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.722 I perplexity: tokenizing the input ..
0.00.721.300 I perplexity: tokenization took 6.575 ms
0.00.721.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.164 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.859.478 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.859.500 I llama_perf_context_print:        load time =     692.36 ms
0.00.859.500 I llama_perf_context_print: prompt eval time =     136.46 ms /   128 tokens (    1.07 ms per token,   938.02 tokens per second)
0.00.859.501 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.503 I llama_perf_context_print:       total time =     144.89 ms /   129 tokens
0.00.859.869 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.087s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.082 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.210 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.211 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.211 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.217 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.830 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.832 I llama_model_loader: - type  f32:  194 tensors
0.00.025.832 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.833 I print_info: file format = GGUF V3 (latest)
0.00.025.833 I print_info: file type   = Q6_K
0.00.025.834 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.470 I load: special tokens cache size = 25
0.00.040.699 I load: token to piece cache size = 0.2984 MB
0.00.040.715 I print_info: arch             = gptneox
0.00.040.716 I print_info: vocab_only       = 0
0.00.040.716 I print_info: n_ctx_train      = 2048
0.00.040.717 I print_info: n_embd           = 2048
0.00.040.717 I print_info: n_layer          = 24
0.00.040.720 I print_info: n_head           = 16
0.00.040.721 I print_info: n_head_kv        = 16
0.00.040.721 I print_info: n_rot            = 32
0.00.040.721 I print_info: n_swa            = 0
0.00.040.722 I print_info: n_embd_head_k    = 128
0.00.040.722 I print_info: n_embd_head_v    = 128
0.00.040.722 I print_info: n_gqa            = 1
0.00.040.723 I print_info: n_embd_k_gqa     = 2048
0.00.040.726 I print_info: n_embd_v_gqa     = 2048
0.00.040.727 I print_info: f_norm_eps       = 1.0e-05
0.00.040.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.727 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.727 I print_info: f_logit_scale    = 0.0e+00
0.00.040.727 I print_info: f_attn_scale     = 0.0e+00
0.00.040.728 I print_info: n_ff             = 8192
0.00.040.728 I print_info: n_expert         = 0
0.00.040.728 I print_info: n_expert_used    = 0
0.00.040.730 I print_info: causal attn      = 1
0.00.040.730 I print_info: pooling type     = 0
0.00.040.730 I print_info: rope type        = 2
0.00.040.730 I print_info: rope scaling     = linear
0.00.040.731 I print_info: freq_base_train  = 10000.0
0.00.040.731 I print_info: freq_scale_train = 1
0.00.040.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.732 I print_info: rope_finetuned   = unknown
0.00.040.732 I print_info: ssm_d_conv       = 0
0.00.040.733 I print_info: ssm_d_inner      = 0
0.00.040.733 I print_info: ssm_d_state      = 0
0.00.040.733 I print_info: ssm_dt_rank      = 0
0.00.040.733 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.733 I print_info: model type       = 1.4B
0.00.040.733 I print_info: model params     = 1.41 B
0.00.040.734 I print_info: general.name     = 1.4B
0.00.040.734 I print_info: vocab type       = BPE
0.00.040.734 I print_info: n_vocab          = 50304
0.00.040.735 I print_info: n_merges         = 50009
0.00.040.735 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: LF token         = 187 'Ċ'
0.00.040.736 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.736 I print_info: max token length = 1024
0.00.040.736 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.057 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.064 I load_tensors: offloading output layer to GPU
0.00.686.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.097 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.686.100 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.687.663 I llama_init_from_model: n_seq_max     = 1
0.00.687.666 I llama_init_from_model: n_ctx         = 128
0.00.687.666 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.666 I llama_init_from_model: n_batch       = 128
0.00.687.667 I llama_init_from_model: n_ubatch      = 128
0.00.687.667 I llama_init_from_model: flash_attn    = 0
0.00.687.668 I llama_init_from_model: freq_base     = 10000.0
0.00.687.669 I llama_init_from_model: freq_scale    = 1
0.00.687.669 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.672 I ggml_metal_init: allocating
0.00.687.749 I ggml_metal_init: found device: Apple M4
0.00.687.762 I ggml_metal_init: picking default device: Apple M4
0.00.689.183 I ggml_metal_load_library: using embedded metal library
0.00.695.134 I ggml_metal_init: GPU name:   Apple M4
0.00.695.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.140 I ggml_metal_init: simdgroup reduction   = true
0.00.695.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.140 I ggml_metal_init: has residency sets    = true
0.00.695.141 I ggml_metal_init: has bfloat            = true
0.00.695.141 I ggml_metal_init: use bfloat            = true
0.00.695.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.660 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.035 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.715.048 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.715.077 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.220 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.718.221 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.718.222 I llama_init_from_model: graph nodes  = 967
0.00.718.222 I llama_init_from_model: graph splits = 2
0.00.718.225 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.887 I 
0.00.754.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.007 I perplexity: tokenizing the input ..
0.00.761.902 I perplexity: tokenization took 6.89 ms
0.00.761.910 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.894.619 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.895.971 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.895.996 I llama_perf_context_print:        load time =     744.80 ms
0.00.895.997 I llama_perf_context_print: prompt eval time =     131.85 ms /   128 tokens (    1.03 ms per token,   970.79 tokens per second)
0.00.895.998 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.998 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.00.896.352 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.078s
sys	0m0.147s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4875 (7841fc72) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.660 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.820 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.831 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.834 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.842 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.836 I llama_model_loader: - type  f32:  194 tensors
0.00.052.837 I llama_model_loader: - type  f16:   98 tensors
0.00.052.838 I print_info: file format = GGUF V3 (latest)
0.00.052.838 I print_info: file type   = all F32 (guessed)
0.00.052.840 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.625 I load: special tokens cache size = 25
0.00.072.616 I load: token to piece cache size = 0.2984 MB
0.00.072.631 I print_info: arch             = gptneox
0.00.072.632 I print_info: vocab_only       = 0
0.00.072.632 I print_info: n_ctx_train      = 2048
0.00.072.633 I print_info: n_embd           = 2048
0.00.072.633 I print_info: n_layer          = 24
0.00.072.636 I print_info: n_head           = 16
0.00.072.637 I print_info: n_head_kv        = 16
0.00.072.637 I print_info: n_rot            = 32
0.00.072.637 I print_info: n_swa            = 0
0.00.072.638 I print_info: n_embd_head_k    = 128
0.00.072.639 I print_info: n_embd_head_v    = 128
0.00.072.640 I print_info: n_gqa            = 1
0.00.072.640 I print_info: n_embd_k_gqa     = 2048
0.00.072.641 I print_info: n_embd_v_gqa     = 2048
0.00.072.642 I print_info: f_norm_eps       = 1.0e-05
0.00.072.642 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.643 I print_info: f_logit_scale    = 0.0e+00
0.00.072.643 I print_info: f_attn_scale     = 0.0e+00
0.00.072.643 I print_info: n_ff             = 8192
0.00.072.644 I print_info: n_expert         = 0
0.00.072.644 I print_info: n_expert_used    = 0
0.00.072.644 I print_info: causal attn      = 1
0.00.072.644 I print_info: pooling type     = 0
0.00.072.644 I print_info: rope type        = 2
0.00.072.645 I print_info: rope scaling     = linear
0.00.072.647 I print_info: freq_base_train  = 10000.0
0.00.072.647 I print_info: freq_scale_train = 1
0.00.072.647 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.647 I print_info: rope_finetuned   = unknown
0.00.072.647 I print_info: ssm_d_conv       = 0
0.00.072.648 I print_info: ssm_d_inner      = 0
0.00.072.648 I print_info: ssm_d_state      = 0
0.00.072.648 I print_info: ssm_dt_rank      = 0
0.00.072.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.649 I print_info: model type       = 1.4B
0.00.072.649 I print_info: model params     = 1.41 B
0.00.072.649 I print_info: general.name     = 1.4B
0.00.072.650 I print_info: vocab type       = BPE
0.00.072.650 I print_info: n_vocab          = 50304
0.00.072.650 I print_info: n_merges         = 50009
0.00.072.652 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.652 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.653 I print_info: LF token         = 187 'Ċ'
0.00.072.653 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.653 I print_info: max token length = 1024
0.00.072.653 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.246.761 I load_tensors: offloading 24 repeating layers to GPU
0.01.246.763 I load_tensors: offloading output layer to GPU
0.01.246.764 I load_tensors: offloaded 25/25 layers to GPU
0.01.246.790 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.246.792 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.247.903 I llama_init_from_model: n_seq_max     = 1
0.01.247.905 I llama_init_from_model: n_ctx         = 128
0.01.247.905 I llama_init_from_model: n_ctx_per_seq = 128
0.01.247.905 I llama_init_from_model: n_batch       = 128
0.01.247.905 I llama_init_from_model: n_ubatch      = 128
0.01.247.906 I llama_init_from_model: flash_attn    = 0
0.01.247.906 I llama_init_from_model: freq_base     = 10000.0
0.01.247.906 I llama_init_from_model: freq_scale    = 1
0.01.247.907 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.247.910 I ggml_metal_init: allocating
0.01.247.976 I ggml_metal_init: found device: Apple M4
0.01.247.983 I ggml_metal_init: picking default device: Apple M4
0.01.249.007 I ggml_metal_load_library: using embedded metal library
0.01.252.958 I ggml_metal_init: GPU name:   Apple M4
0.01.252.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.252.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.252.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.252.962 I ggml_metal_init: simdgroup reduction   = true
0.01.252.962 I ggml_metal_init: simdgroup matrix mul. = true
0.01.252.962 I ggml_metal_init: has residency sets    = true
0.01.252.962 I ggml_metal_init: has bfloat            = true
0.01.252.962 I ggml_metal_init: use bfloat            = true
0.01.252.963 I ggml_metal_init: hasUnifiedMemory      = true
0.01.252.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.264.554 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.266.342 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.266.344 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.266.359 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.268.063 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.268.064 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.268.064 I llama_init_from_model: graph nodes  = 967
0.01.268.065 I llama_init_from_model: graph splits = 2
0.01.268.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.268.067 I 
0.01.268.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.268.107 I compute_imatrix: tokenizing the input ..
0.01.272.404 I compute_imatrix: tokenization took 4.296 ms
0.01.272.406 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.534.733 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.537.453 I llama_perf_context_print:        load time =    1513.07 ms
0.01.537.454 I llama_perf_context_print: prompt eval time =     260.50 ms /   128 tokens (    2.04 ms per token,   491.36 tokens per second)
0.01.537.454 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.537.455 I llama_perf_context_print:       total time =    1515.78 ms /   129 tokens
0.01.538.047 I ggml_metal_free: deallocating

real	0m1.763s
user	0m0.125s
sys	0m0.252s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4875 (7841fc72)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119e051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119e058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119e05e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119e06430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119e06f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119e07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119e07af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119e080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119e085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119e08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119e08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119e09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119e0a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119e0aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119e0b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119e0b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119e0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119e0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119e0ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119e0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119e0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119e0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119e0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119e0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119e0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119e0fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119e103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119e10870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119e10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119e10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119e116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119e11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119e11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119e122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119e12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119e130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119e13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119e139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119e13e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119e14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119e14c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119e14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119e15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119e15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119e16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119e167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119e16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119e17120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119e175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119e17a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119e17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119e183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119e18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119e18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119e19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119e196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119e19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119e19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119e1a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119e1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119e1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119e1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119e1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119e1b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119e1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119e1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119e1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119e1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119e1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119e1e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119e1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119e1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119e1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119e20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119e20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119e21020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119e21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119e21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119e22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119e22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119e22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119e23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119e23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119e23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119e23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119e24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119e24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119e24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119e259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119e26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119e266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119e27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119e27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119e28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119e28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119e29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119e29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119e29bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119e2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119e2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119e2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119e2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119e2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119e2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119e2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119e2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119e2c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119e2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119e2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119e2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119e2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119e2e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119e2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119e2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119e2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119e2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119e2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119e2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119e302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119e30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119e30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119e310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119e31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119e31a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119e31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119e32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119e327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119e32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119e33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119e335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119e33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119e33f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119e343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119e34840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119e34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119e35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119e35620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119e35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119e35f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119e36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119e36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119e371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119e37680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119e37b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119e37fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119e38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119e38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119e38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119e39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119e396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119e39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119e3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119e3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119e3a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119e3b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119e3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119e3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119e3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119e3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119e3c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119e3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119e3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119e3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119e3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119e3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119e3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119e3eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119e3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119e3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119e3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119e40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119e405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119e40a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119e40f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119e413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119e41860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119e41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119e42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119e42850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119e42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119e43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119e436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119e43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119e44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119e444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119e44960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119e44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119e45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119e457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119e45c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119e465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119e46a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119e472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119e47810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119e47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119e48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119e48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119e48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119e49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119e49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119e4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119e4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119e4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119e4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119e4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119e4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119e4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119e4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119e4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119e4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119e4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119e4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119e4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119e4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119e4f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119e4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119e4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119e50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119e508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119e50e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119e51430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119e519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119e51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119e52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119e52af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119e53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119e53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119e541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119e54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119e54d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119e552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119e55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119e55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119e563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119e56980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119e56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119e574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119e57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119e58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119e585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119e58ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119e59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119e59700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119e59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119e5a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119e5a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119e5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119e5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119e5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119e5c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119e5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119e5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119e5d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119e5d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119e5db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119e5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119e5e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119e5ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119e5ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119e5f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119e5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x119e5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x119e60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x119e60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x119e60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x119e61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x119e61770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x119e61c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x119e62170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x119e62670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x119e62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119e63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119e63a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119e641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119e648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119e64fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119e652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119e65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119e65ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119e66370 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.487.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.487.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119f04480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119f04870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119f04b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119f04df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119f050b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119f05370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119f05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119f058f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119f05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119f05e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119f06130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119f066b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119f06970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119f06c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119f06ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119f071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119f07470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119f07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119f079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119f07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119f07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119f08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119f084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119f087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119f08a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119f08ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119f092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119f09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119f09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119f09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119f0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119f0a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119f0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119f0ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119f0ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119f0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119f0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119f0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119f0b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119f0bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119f0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119f0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119f0c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119f0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119f0c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119f0cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119f0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119f0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119f0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119f0d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119f0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119f0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119f0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119f0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119f0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119f0e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119f0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119f0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119f0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119f0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119f0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119f0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119f0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119f100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119f10370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119f10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119f108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119f10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119f10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119f11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119f113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119f116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119f11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119f11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119f11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119f12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119f12730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119f129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119f12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119f13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119f134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119f137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119f13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119f142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119f14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119f14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119f14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119f14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119f15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119f15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119f155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119f158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119f15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119f160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119f163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119f16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119f16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119f16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119f17430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119f176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119f17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119f17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119f181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119f184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119f18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119f18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119f19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119f19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119f197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119f19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119f19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119f1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119f1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119f1a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119f1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119f1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119f1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119f1b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119f1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119f1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119f1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119f1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119f1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119f1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119f1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119f1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119f1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119f1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119f1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119f1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119f1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119f1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119f1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119f1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119f1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119f1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119f1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119f20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119f20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119f205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119f208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119f20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119f20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119f210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119f213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119f21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119f21930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119f21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119f21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119f22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119f22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119f226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119f22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119f22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119f234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119f23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119f23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119f23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119f23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119f24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119f24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119f247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119f24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119f24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119f25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119f252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119f255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119f25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119f25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119f25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119f260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119f26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119f26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119f268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119f26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119f26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119f27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119f276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119f27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119f27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119f27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119f28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119f289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119f28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119f28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119f29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119f294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119f297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119f29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119f29ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119f2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119f2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119f2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119f2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119f2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119f2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119f2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119f2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119f2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119f2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119f2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119f2c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119f2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119f2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119f2d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119f2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119f2d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119f2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119f2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119f2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119f2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119f2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119f2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119f2ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119f2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119f2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119f2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119f2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119f2fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119f2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119f302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119f305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119f30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119f30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119f30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119f310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119f31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119f31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119f318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119f31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119f31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119f32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119f323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119f326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119f32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119f32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119f331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119f33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119f339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119f33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119f33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119f34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119f34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x119f34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x119f34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x119f352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x119f35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x119f35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x119f35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x119f35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x119f36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x119f36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x119f365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119f368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119f36b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119f370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119f373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119f37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119f37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119f37eb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118a04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118a04540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118a04aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118a04d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118a05020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118a052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118a055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118a05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118a05de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118a060a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118a06360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118a06620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118a068e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118a06ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118a06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118a07120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118a073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118a076a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118a07960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118a07c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118a07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118a081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118a08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118a08720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118a089e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118a08ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118a08f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118a09220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118a094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118a097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118a09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118a09d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118a09fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118a0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118a0a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118a0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118a0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118a0ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118a0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118a0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118a0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118a0b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118a0bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118a0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118a0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118a0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118a0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118a0c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118a0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118a0cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118a0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118a0d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118a0d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118a0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118a0dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118a0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118a0e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118a0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118a0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118a0ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118a0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118a0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118a0f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118a0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118a0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118a0faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118a0fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118a10020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118a102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118a105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118a10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118a10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118a10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118a110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118a11360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118a11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118a118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118a11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118a11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118a12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118a123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118a126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118a12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118a12c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118a131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118a13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118a13720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118a139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118a13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118a13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118a14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118a144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118a147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118a14a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118a14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118a14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118a152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118a15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118a15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118a15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118a15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118a16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118a16320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118a165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118a168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118a16b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118a16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118a170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118a173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118a17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118a17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118a17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118a18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118a18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118a186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118a189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118a18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118a18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118a191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118a194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118a19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118a19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118a19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118a19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118a1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118a1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118a1a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118a1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118a1ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118a1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118a1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118a1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118a1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118a1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118a1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118a1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118a1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118a1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118a1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118a1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118a1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118a1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118a1d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118a1d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118a1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118a1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118a1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118a1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118a1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118a1e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118a1e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118a1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118a1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118a1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118a1f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118a1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118a1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118a1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118a1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118a202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118a20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118a20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118a20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118a20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118a21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118a21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118a215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118a218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118a21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118a21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118a220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118a223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118a22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118a22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118a22be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118a22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118a23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118a23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118a236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118a239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118a23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118a241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118a244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118a24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118a24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118a24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118a24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118a25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118a25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118a257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118a25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118a25d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118a26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118a262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118a265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118a26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118a26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118a26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118a270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118a27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118a27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118a278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118a27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118a27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118a28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118a283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118a286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118a28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118a28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118a28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118a291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118a29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118a29720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118a299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118a29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118a29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118a2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118a2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118a2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118a2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118a2ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118a2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118a2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118a2b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118a2b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118a2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118a2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118a2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118a2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118a2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118a2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118a2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118a2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118a2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118a2d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118a2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118a2d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118a2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118a2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118a2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118a2e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118a2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118a2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118a2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118a2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118a2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118a2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118a2f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118a2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118a2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118a2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118a30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118a307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118a30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118a30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118a31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118a312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118a315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118a31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118a31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118a31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118a320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118a32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118a32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118a328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118a32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118a32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118a33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118a333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118a336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118a33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118a33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118a33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118a341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118a34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118a34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118a349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x118a34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x118a34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x118a35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x118a354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x118a357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x118a35a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x118a35d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x118a35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x118a362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x118a36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118a36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118a36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118a36da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118a37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118a37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118a375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118a378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118a37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118a37e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.492s
user	0m0.258s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4875 (7841fc72)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12460bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12460c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12460cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12460d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12460d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12460dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12460e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12460e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12460ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12460f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12460f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12460fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124611f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124612680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124612da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1246134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124613c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1246143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124614ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1246151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1246161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124616650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124617190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124617630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124617ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124619080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1246199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124619e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12461a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12461a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12461ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12461b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12461b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12461ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12461bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12461c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12461c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12461d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12461d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12461da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12461dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12461e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12461e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12461ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12461f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12461f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12461faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12461ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124620bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124621090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124621530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1246219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124621e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1246227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124622c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1246230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124623590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124623a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124623ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124624370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1246248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124624e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1246258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124625e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124626350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1246268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124626df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124627340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124627890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124628330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124628880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124629320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124629870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12462a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12462a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12462adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12462b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12462b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12462bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12462c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12461cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12462c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12462cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12462d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12462d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12462df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12462e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12462e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12462eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12462f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12462f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12462fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124630430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124630ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1246318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124631d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124632200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1246326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124632b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124632fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124633480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124633920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124633dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124634260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124634700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124634ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124635040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1246354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124635980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124635e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1246362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124636760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124636c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1246370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124637540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1246379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124637e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124638320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1246387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124638c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124639100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1246395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124639a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124639ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12463a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12463a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12463acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12463b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12463b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12463baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12463bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12463c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12463c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12463cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12463d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12463d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12463db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12463dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12463e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12463e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12463ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12463f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12463f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12463fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124640000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1246404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124640940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124640de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124641280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124641720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124641bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124642060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124642500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1246429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124642e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1246432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124643780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124643c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1246440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124644560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124644a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124644ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124645340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1246457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124645c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1246465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124646a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124646f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1246473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124647840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124647ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124648180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124648620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124648b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1246490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124649610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124649b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12464a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12464a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12464a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12464ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12464b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12464b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12464bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12464c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12464c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12464ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12464cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12464d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12464d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12464e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12464e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12464e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12464ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12464f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12464f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12464ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1246504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124650a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124651030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1246515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124651b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124652140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1246526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124652ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124653800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124653db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124654360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124654ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124655470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124655a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124655fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124656b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1246570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124657690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1246581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1246587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124658d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124659300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1246598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124659e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12465a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12465a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12465af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12465b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12465bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12465c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12465c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12465cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12465d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12465d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12465dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12465e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12465e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12465ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12465f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12465f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12465ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1246604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124661020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1246615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124661b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124662130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124662630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124662b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124663030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124663530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124663a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124663f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124664430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124664930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124664e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124665330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124665830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124665d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124666230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124666730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x124666c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x124667130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x124667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x124667b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x124668030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x124668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x124668a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x124668f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x124669430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x124669930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124669e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12466a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12466af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12466b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12466bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12466c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12466c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12466cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12466d130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12466ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12466e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12466e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12466e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12466ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12466ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12466efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12466f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12466f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12466f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12466faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12466fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124670020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1246702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1246705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124670860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124670b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124670de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1246710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124671360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124671620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1246718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124671ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124671e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124672120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1246723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1246726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124672960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124672c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124672ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1246731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124673460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124673720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1246739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124673ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124673f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124674220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1246744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1246747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124674a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124674d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124674fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1246752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124675560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124675820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124675ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124675da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124676060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124676320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1246765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1246768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124676b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124676e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1246770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1246773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124677660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124677920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124677be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124677ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124678160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124678420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1246786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1246789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124678c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124678f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1246791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1246794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124679760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124679a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124679ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124679fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12467a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12467a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12467a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12467aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12467ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12467b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12467b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12467b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12467b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12467bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12467bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12467c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12467c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12467c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12467c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12467cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12467ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12467d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12467d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12467d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12467d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12467dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12467dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12467e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12467e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12467e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12467e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12467eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12467ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12467f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12467f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12467f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12467fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12467fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12467ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1246802a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124680560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124680820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124680ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124680da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124681060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124681320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1246815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1246818a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124681b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124681e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1246820e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1246823a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124682660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124682920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124682be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124682ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124683160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124683420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1246836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1246839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124683c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124683f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1246841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1246844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124684760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124684a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124684ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124684fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124685260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124685520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1246857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124685aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124685d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124686020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1246862e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1246865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124686860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124686b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124686de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1246870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124687360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124687620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1246878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124687ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124687e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124688120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1246883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1246886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124688960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124688c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124688ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1246891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124689460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124689720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1246899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124689ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124689f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12468a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12468a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12468a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12468aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12468ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12468afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12468b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12468b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12468b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12468bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12468bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12468c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12468c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12468c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12468c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12468cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12468ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12468d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12468d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12468d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12468d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12468dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12468dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12468e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12468e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12468e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12468e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12468ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12468ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12468f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12468f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12468f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12468fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12468fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12468ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124690260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124690520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1246907e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124690aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124690d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124691020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1246912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1246915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124691860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124691b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124691de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1246920a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124692360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124692620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1246928e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124692ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124692e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124693120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1246933e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1246936a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124693960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124693c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124693ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1246941a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124694460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124694720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1246949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124694ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124694f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124695220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1246954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1246957a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124695a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124695d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124695fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1246962a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124696560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124696820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124696ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124696da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124697060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124697320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1246975e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1246978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124697b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124697e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1246980e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1246983a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124698660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124698920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124698be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124698ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124699160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124699420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1246996e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1246999a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124699c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124699f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12469a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12469a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12469a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12469aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12469ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12469afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12469b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12469b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12469b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12469baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12469bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12469c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12469c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12469c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12469c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12469cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12469cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12469d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12469d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12469d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12469d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12469dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12469de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12469e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12469e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12469e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12469e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12469ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12469eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12469f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12469f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12469f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12469f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12469fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12469ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1246a0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1246a04e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1246a07a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1246a0a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1246a0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1246a0fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1246a12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1246a1560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1246a1820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125804820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1258065a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125808480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125808740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125808cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125808f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125809240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125809500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1258097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125809a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125809d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12580a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12580a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12580a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12580a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12580ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12580adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12580b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12580b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12580b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12580b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12580bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12580be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12580c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12580c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12580c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12580c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12580cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12580cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12580d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12580d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12580d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12580d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12580dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12580df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12580e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12580e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12580e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12580ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12580ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12580efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12580f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12580f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12580f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12580fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12580fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125810040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125810300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1258105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125810880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125810b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125810e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1258110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125811380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125811640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125811900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125811bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125811e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125812400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1258126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125812980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125812c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125812f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1258131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125813480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125813740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125813a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125813cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125813f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125814240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125814500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1258147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125814a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125814d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1258152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125815580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125815840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125815b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125815dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125816080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125816340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125816600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1258168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125816b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125816e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125817100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1258173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125817680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125817940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125817c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125817ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125818180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125818440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125818700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1258189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125818c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125818f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125819200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1258194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125819780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125819a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125819fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12581a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12581a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12581a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12581aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12581ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12581b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12581b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12581b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12581b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12581bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12581be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12581c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12581c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12581c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12581c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12581cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12581ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12581d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12581d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12581d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12581d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12581dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12581df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12581e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12581e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12581e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12581ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12581ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12581ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12581f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12581f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12581f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12581fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12581fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125820000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1258202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125820580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125820840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125820b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125820dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125821340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125821600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1258218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125821b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125821e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125822100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1258223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125822680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125822940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125822c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125822ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125823700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1258239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125823f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1258244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125824780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125824d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125824fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125825280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125825540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125825800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125825ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125825d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125826300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1258265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125826880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125826b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125826e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1258270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125827380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125827640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125827900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125827bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125827e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125828140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125828400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1258286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125828980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125828c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125828f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1258291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125829480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125829740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125829a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125829cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125829f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12582a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12582a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12582a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12582aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12582ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12582b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12582b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12582b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12582b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12582bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12582bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12582c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12582c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12582c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12582c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12582cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12582ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12582d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12582d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12582d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12582d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12582dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12582dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12582e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12582e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12582e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12582e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12582ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12582ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12582f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12582f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12582f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12582fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12582fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12582ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125830280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125830800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125830ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125830d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125831040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125831300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1258315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125831880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125831b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125831e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1258320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125832640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125832900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125832bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125832e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125833140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125833400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1258336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125833980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125833c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125833f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1258341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125834740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125834a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125834cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125834f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125835240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125835500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1258357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125835a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125835d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1258362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125836580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125836840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125836b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125836dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125837080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125837340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125837600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1258378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125837b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125838100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1258383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x125838680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x125838940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x125838c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x125838ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x125839180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x125839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x125839700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1258399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x125839c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x125839f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12583a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12583a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12583a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12583aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12583ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12583afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12583b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12583b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12583b800 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.947s
user	0m0.213s
sys	0m0.197s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
