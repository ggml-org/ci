### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.74 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.50 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.46 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  175.65 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.89 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.32 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 218.32 sec*proc (28 tests)

Total Test time (real) = 218.33 sec

real	3m38.364s
user	7m27.364s
sys	0m6.251s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.35 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.34 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.20 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.28 sec*proc (28 tests)

Total Test time (real) =  51.30 sec

real	0m51.306s
user	1m11.493s
sys	0m5.794s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.150 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.132 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.303 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.313 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.314 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.315 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.316 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.318 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.318 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.319 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.319 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.320 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.324 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.324 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.325 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.326 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.327 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.327 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.328 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.732 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.734 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.735 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.735 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.736 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.736 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.737 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.738 I llama_model_loader: - type  f32:  124 tensors
0.00.032.738 I llama_model_loader: - type  f16:   73 tensors
0.00.037.326 I llm_load_vocab: special tokens cache size = 5
0.00.039.554 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.559 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.559 I llm_load_print_meta: arch             = bert
0.00.039.560 I llm_load_print_meta: vocab type       = WPM
0.00.039.560 I llm_load_print_meta: n_vocab          = 30522
0.00.039.560 I llm_load_print_meta: n_merges         = 0
0.00.039.560 I llm_load_print_meta: vocab_only       = 0
0.00.039.561 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.561 I llm_load_print_meta: n_embd           = 384
0.00.039.564 I llm_load_print_meta: n_layer          = 12
0.00.039.567 I llm_load_print_meta: n_head           = 12
0.00.039.568 I llm_load_print_meta: n_head_kv        = 12
0.00.039.568 I llm_load_print_meta: n_rot            = 32
0.00.039.569 I llm_load_print_meta: n_swa            = 0
0.00.039.569 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.569 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.570 I llm_load_print_meta: n_gqa            = 1
0.00.039.571 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.572 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.573 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.573 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.573 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.573 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.575 I llm_load_print_meta: n_ff             = 1536
0.00.039.575 I llm_load_print_meta: n_expert         = 0
0.00.039.575 I llm_load_print_meta: n_expert_used    = 0
0.00.039.575 I llm_load_print_meta: causal attn      = 0
0.00.039.575 I llm_load_print_meta: pooling type     = 2
0.00.039.576 I llm_load_print_meta: rope type        = 2
0.00.039.576 I llm_load_print_meta: rope scaling     = linear
0.00.039.577 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.577 I llm_load_print_meta: freq_scale_train = 1
0.00.039.577 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.578 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.578 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.578 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.579 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.579 I llm_load_print_meta: model type       = 33M
0.00.039.580 I llm_load_print_meta: model ftype      = F16
0.00.039.580 I llm_load_print_meta: model params     = 33.21 M
0.00.039.581 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.583 I llm_load_print_meta: general.name     = Bge Small
0.00.039.584 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.584 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.584 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.584 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.585 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.585 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.585 I llm_load_print_meta: max token length = 21
0.00.041.612 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.613 I llm_load_tensors: offloading output layer to GPU
0.00.041.613 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.640 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.642 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.229 I llama_new_context_with_model: n_seq_max     = 1
0.00.042.231 I llama_new_context_with_model: n_ctx         = 512
0.00.042.231 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.042.231 I llama_new_context_with_model: n_batch       = 2048
0.00.042.232 I llama_new_context_with_model: n_ubatch      = 2048
0.00.042.232 I llama_new_context_with_model: flash_attn    = 0
0.00.042.232 I llama_new_context_with_model: freq_base     = 10000.0
0.00.042.233 I llama_new_context_with_model: freq_scale    = 1
0.00.042.234 I ggml_metal_init: allocating
0.00.042.238 I ggml_metal_init: found device: Apple M4
0.00.042.241 I ggml_metal_init: picking default device: Apple M4
0.00.043.105 I ggml_metal_init: using embedded metal library
0.00.047.515 I ggml_metal_init: GPU name:   Apple M4
0.00.047.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.519 I ggml_metal_init: simdgroup reduction   = true
0.00.047.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.519 I ggml_metal_init: has bfloat            = true
0.00.047.520 I ggml_metal_init: use bfloat            = true
0.00.047.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.167 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.060.773 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.775 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.776 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.573 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.575 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.575 I llama_new_context_with_model: graph nodes  = 429
0.00.061.575 I llama_new_context_with_model: graph splits = 2
0.00.061.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.065 I 
0.00.068.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.750 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.521 I llama_perf_context_print:        load time =      45.93 ms
0.00.073.522 I llama_perf_context_print: prompt eval time =       4.62 ms /     9 tokens (    0.51 ms per token,  1946.37 tokens per second)
0.00.073.523 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.523 I llama_perf_context_print:       total time =       5.46 ms /    10 tokens
0.00.073.677 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.051s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.499 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.430 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.435 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.436 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.436 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.436 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.437 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.438 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.438 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.438 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.439 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.441 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.444 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.444 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.444 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.445 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.445 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.446 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.420 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.421 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.422 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.422 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.422 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.423 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.423 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.423 I llama_model_loader: - type  f32:  124 tensors
0.00.014.424 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.763 I llm_load_vocab: special tokens cache size = 5
0.00.018.075 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.078 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.078 I llm_load_print_meta: arch             = bert
0.00.018.078 I llm_load_print_meta: vocab type       = WPM
0.00.018.079 I llm_load_print_meta: n_vocab          = 30522
0.00.018.079 I llm_load_print_meta: n_merges         = 0
0.00.018.079 I llm_load_print_meta: vocab_only       = 0
0.00.018.079 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.079 I llm_load_print_meta: n_embd           = 384
0.00.018.080 I llm_load_print_meta: n_layer          = 12
0.00.018.082 I llm_load_print_meta: n_head           = 12
0.00.018.083 I llm_load_print_meta: n_head_kv        = 12
0.00.018.083 I llm_load_print_meta: n_rot            = 32
0.00.018.086 I llm_load_print_meta: n_swa            = 0
0.00.018.086 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.086 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.086 I llm_load_print_meta: n_gqa            = 1
0.00.018.087 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.088 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.088 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.089 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.089 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.089 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.089 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.090 I llm_load_print_meta: n_ff             = 1536
0.00.018.090 I llm_load_print_meta: n_expert         = 0
0.00.018.090 I llm_load_print_meta: n_expert_used    = 0
0.00.018.090 I llm_load_print_meta: causal attn      = 0
0.00.018.090 I llm_load_print_meta: pooling type     = 2
0.00.018.090 I llm_load_print_meta: rope type        = 2
0.00.018.091 I llm_load_print_meta: rope scaling     = linear
0.00.018.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.093 I llm_load_print_meta: freq_scale_train = 1
0.00.018.093 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.093 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.093 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.094 I llm_load_print_meta: model type       = 33M
0.00.018.094 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.095 I llm_load_print_meta: model params     = 33.21 M
0.00.018.095 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.095 I llm_load_print_meta: general.name     = Bge Small
0.00.018.095 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.096 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.096 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.096 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.096 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.096 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.097 I llm_load_print_meta: max token length = 21
0.00.019.309 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.310 I llm_load_tensors: offloading output layer to GPU
0.00.019.310 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.318 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.319 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.675 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.676 I llama_new_context_with_model: n_ctx         = 512
0.00.019.677 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.677 I llama_new_context_with_model: n_batch       = 2048
0.00.019.677 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.677 I llama_new_context_with_model: flash_attn    = 0
0.00.019.677 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.678 I llama_new_context_with_model: freq_scale    = 1
0.00.019.678 I ggml_metal_init: allocating
0.00.019.681 I ggml_metal_init: found device: Apple M4
0.00.019.683 I ggml_metal_init: picking default device: Apple M4
0.00.020.299 I ggml_metal_init: using embedded metal library
0.00.022.625 I ggml_metal_init: GPU name:   Apple M4
0.00.022.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.628 I ggml_metal_init: simdgroup reduction   = true
0.00.022.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.628 I ggml_metal_init: has bfloat            = true
0.00.022.628 I ggml_metal_init: use bfloat            = true
0.00.022.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.852 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.349 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.351 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.353 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.974 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.975 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.975 I llama_new_context_with_model: graph nodes  = 429
0.00.033.975 I llama_new_context_with_model: graph splits = 2
0.00.033.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.292 I 
0.00.038.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.849 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.255 I llama_perf_context_print:        load time =      28.79 ms
0.00.043.256 I llama_perf_context_print: prompt eval time =       4.26 ms /     9 tokens (    0.47 ms per token,  2110.69 tokens per second)
0.00.043.257 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.258 I llama_perf_context_print:       total time =       4.97 ms /    10 tokens
0.00.043.466 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.256 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.588 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.069 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.077 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.079 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.080 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.080 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.082 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.083 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.084 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.088 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.089 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.093 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.093 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.094 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.378 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.378 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.379 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.379 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.379 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.380 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.380 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.380 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.381 I llama_model_loader: - type  f32:   40 tensors
0.00.050.381 I llama_model_loader: - type  f16:   30 tensors
0.00.068.483 W llm_load_vocab: empty token at index 5
0.00.073.253 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.536 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.567 I llm_load_vocab: special tokens cache size = 5
0.00.331.744 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.759 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.761 I llm_load_print_meta: vocab type       = BPE
0.00.331.761 I llm_load_print_meta: n_vocab          = 61056
0.00.331.761 I llm_load_print_meta: n_merges         = 39382
0.00.331.762 I llm_load_print_meta: vocab_only       = 0
0.00.331.762 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.762 I llm_load_print_meta: n_embd           = 384
0.00.331.762 I llm_load_print_meta: n_layer          = 4
0.00.331.767 I llm_load_print_meta: n_head           = 12
0.00.331.768 I llm_load_print_meta: n_head_kv        = 12
0.00.331.768 I llm_load_print_meta: n_rot            = 32
0.00.331.768 I llm_load_print_meta: n_swa            = 0
0.00.331.769 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.769 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.769 I llm_load_print_meta: n_gqa            = 1
0.00.331.770 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.770 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.772 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.775 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.775 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.776 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.776 I llm_load_print_meta: n_ff             = 1536
0.00.331.777 I llm_load_print_meta: n_expert         = 0
0.00.331.777 I llm_load_print_meta: n_expert_used    = 0
0.00.331.777 I llm_load_print_meta: causal attn      = 0
0.00.331.777 I llm_load_print_meta: pooling type     = -1
0.00.331.777 I llm_load_print_meta: rope type        = -1
0.00.331.777 I llm_load_print_meta: rope scaling     = linear
0.00.331.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.778 I llm_load_print_meta: freq_scale_train = 1
0.00.331.778 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.780 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.780 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.781 I llm_load_print_meta: model type       = 33M
0.00.331.781 I llm_load_print_meta: model ftype      = F16
0.00.331.781 I llm_load_print_meta: model params     = 32.90 M
0.00.331.782 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.782 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.782 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.782 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.785 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.785 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.786 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.786 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.786 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.786 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.786 I llm_load_print_meta: max token length = 45
0.00.332.744 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.744 I llm_load_tensors: offloading output layer to GPU
0.00.332.744 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.766 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.767 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.532 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.535 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.536 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.536 I llama_new_context_with_model: n_batch       = 2048
0.00.333.536 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.536 I llama_new_context_with_model: flash_attn    = 0
0.00.333.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.537 I llama_new_context_with_model: freq_scale    = 1
0.00.333.537 I ggml_metal_init: allocating
0.00.333.540 I ggml_metal_init: found device: Apple M4
0.00.333.542 I ggml_metal_init: picking default device: Apple M4
0.00.334.392 I ggml_metal_init: using embedded metal library
0.00.337.451 I ggml_metal_init: GPU name:   Apple M4
0.00.337.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.453 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.454 I ggml_metal_init: simdgroup reduction   = true
0.00.337.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.454 I ggml_metal_init: has bfloat            = true
0.00.337.454 I ggml_metal_init: use bfloat            = true
0.00.337.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.196 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.349.640 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.642 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.646 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.272 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.273 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.274 I llama_new_context_with_model: graph nodes  = 154
0.00.350.274 I llama_new_context_with_model: graph splits = 2
0.00.350.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.151 I 
0.00.363.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.462 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.363.463 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.363.465 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.363.466 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.363.473 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.363.474 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.977 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.630 I llama_perf_context_print:        load time =     338.56 ms
0.00.367.631 I llama_perf_context_print: prompt eval time =       3.64 ms /    62 tokens (    0.06 ms per token, 17023.61 tokens per second)
0.00.367.632 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.632 I llama_perf_context_print:       total time =       4.48 ms /    63 tokens
0.00.367.905 I ggml_metal_free: deallocating

real	0m1.099s
user	0m0.338s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.172 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.303 I main: llama backend init
0.00.000.308 I main: load the model and apply lora adapter, if any
0.00.059.120 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.070.745 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.778 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.780 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.078.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.080.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.089.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.772 I llama_model_loader: - type  f32:  194 tensors
0.00.089.773 I llama_model_loader: - type  f16:   98 tensors
0.00.123.384 I llm_load_vocab: special tokens cache size = 25
0.00.130.355 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.130.358 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.130.358 I llm_load_print_meta: arch             = gptneox
0.00.130.358 I llm_load_print_meta: vocab type       = BPE
0.00.130.359 I llm_load_print_meta: n_vocab          = 50304
0.00.130.359 I llm_load_print_meta: n_merges         = 50009
0.00.130.359 I llm_load_print_meta: vocab_only       = 0
0.00.130.359 I llm_load_print_meta: n_ctx_train      = 2048
0.00.130.359 I llm_load_print_meta: n_embd           = 2048
0.00.130.359 I llm_load_print_meta: n_layer          = 24
0.00.130.362 I llm_load_print_meta: n_head           = 16
0.00.130.363 I llm_load_print_meta: n_head_kv        = 16
0.00.130.363 I llm_load_print_meta: n_rot            = 32
0.00.130.364 I llm_load_print_meta: n_swa            = 0
0.00.130.364 I llm_load_print_meta: n_embd_head_k    = 128
0.00.130.364 I llm_load_print_meta: n_embd_head_v    = 128
0.00.130.365 I llm_load_print_meta: n_gqa            = 1
0.00.130.367 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.130.368 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.130.369 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.130.369 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.130.369 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.130.369 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.130.370 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.130.371 I llm_load_print_meta: n_ff             = 8192
0.00.130.371 I llm_load_print_meta: n_expert         = 0
0.00.130.371 I llm_load_print_meta: n_expert_used    = 0
0.00.130.373 I llm_load_print_meta: causal attn      = 1
0.00.130.373 I llm_load_print_meta: pooling type     = 0
0.00.130.373 I llm_load_print_meta: rope type        = 2
0.00.130.373 I llm_load_print_meta: rope scaling     = linear
0.00.130.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.130.374 I llm_load_print_meta: freq_scale_train = 1
0.00.130.374 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.130.374 I llm_load_print_meta: rope_finetuned   = unknown
0.00.130.374 I llm_load_print_meta: ssm_d_conv       = 0
0.00.130.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.130.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.130.375 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.130.375 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.130.375 I llm_load_print_meta: model type       = 1.4B
0.00.130.376 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.130.379 I llm_load_print_meta: model params     = 1.41 B
0.00.130.380 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.130.380 I llm_load_print_meta: general.name     = 1.4B
0.00.130.380 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.130.381 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.130.381 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.130.381 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.130.381 I llm_load_print_meta: LF token         = 128 ''
0.00.130.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.130.382 I llm_load_print_meta: max token length = 1024
0.00.133.053 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.133.053 I llm_load_tensors: offloading output layer to GPU
0.00.133.053 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.133.072 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.133.073 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.134.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.134.060 I llama_new_context_with_model: n_ctx         = 2048
0.00.134.060 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.134.061 I llama_new_context_with_model: n_batch       = 2048
0.00.134.061 I llama_new_context_with_model: n_ubatch      = 512
0.00.134.061 I llama_new_context_with_model: flash_attn    = 0
0.00.134.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.134.062 I llama_new_context_with_model: freq_scale    = 1
0.00.134.062 I ggml_metal_init: allocating
0.00.134.065 I ggml_metal_init: found device: Apple M4
0.00.134.067 I ggml_metal_init: picking default device: Apple M4
0.00.134.751 I ggml_metal_init: using embedded metal library
0.00.149.810 I ggml_metal_init: GPU name:   Apple M4
0.00.149.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.149.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.149.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.149.812 I ggml_metal_init: simdgroup reduction   = true
0.00.149.813 I ggml_metal_init: simdgroup matrix mul. = true
0.00.149.813 I ggml_metal_init: has bfloat            = true
0.00.149.813 I ggml_metal_init: use bfloat            = true
0.00.149.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.149.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.246.464 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.266.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.266.708 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.266.729 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.267.651 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.267.652 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.267.652 I llama_new_context_with_model: graph nodes  = 967
0.00.267.652 I llama_new_context_with_model: graph splits = 2
0.00.267.656 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.267.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.267.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.346.144 I main: llama threadpool init, n_threads = 4
0.00.346.187 I 
0.00.346.208 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.346.208 I 
0.00.346.287 I sampler seed: 1234
0.00.346.292 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.346.316 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.346.318 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.346.318 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.174.632 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.174.633 I llama_perf_context_print:        load time =     287.01 ms
0.02.174.633 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.08 tokens per second)
0.02.174.634 I llama_perf_context_print:        eval time =    1781.78 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.174.634 I llama_perf_context_print:       total time =    1828.49 ms /    70 tokens
0.02.174.880 I ggml_metal_free: deallocating

real	0m2.513s
user	0m0.150s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.904 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.140 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.603 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.624 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.624 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.486 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.707 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.707 I llama_model_loader: - type  f32:  194 tensors
0.00.054.708 I llama_model_loader: - type  f16:   98 tensors
0.00.084.505 I llm_load_vocab: special tokens cache size = 25
0.00.090.917 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.919 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.920 I llm_load_print_meta: arch             = gptneox
0.00.090.920 I llm_load_print_meta: vocab type       = BPE
0.00.090.920 I llm_load_print_meta: n_vocab          = 50304
0.00.090.920 I llm_load_print_meta: n_merges         = 50009
0.00.090.920 I llm_load_print_meta: vocab_only       = 0
0.00.090.921 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.921 I llm_load_print_meta: n_embd           = 2048
0.00.090.921 I llm_load_print_meta: n_layer          = 24
0.00.090.923 I llm_load_print_meta: n_head           = 16
0.00.090.924 I llm_load_print_meta: n_head_kv        = 16
0.00.090.925 I llm_load_print_meta: n_rot            = 32
0.00.090.925 I llm_load_print_meta: n_swa            = 0
0.00.090.925 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.925 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.926 I llm_load_print_meta: n_gqa            = 1
0.00.090.926 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.927 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.927 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.927 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.928 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.928 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.928 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.928 I llm_load_print_meta: n_ff             = 8192
0.00.090.928 I llm_load_print_meta: n_expert         = 0
0.00.090.929 I llm_load_print_meta: n_expert_used    = 0
0.00.090.929 I llm_load_print_meta: causal attn      = 1
0.00.090.929 I llm_load_print_meta: pooling type     = 0
0.00.090.929 I llm_load_print_meta: rope type        = 2
0.00.090.929 I llm_load_print_meta: rope scaling     = linear
0.00.090.929 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.931 I llm_load_print_meta: freq_scale_train = 1
0.00.090.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.931 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.931 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.932 I llm_load_print_meta: model type       = 1.4B
0.00.090.933 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.933 I llm_load_print_meta: model params     = 1.41 B
0.00.090.933 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.935 I llm_load_print_meta: general.name     = 1.4B
0.00.090.935 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.935 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.936 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.936 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.936 I llm_load_print_meta: LF token         = 128 ''
0.00.090.936 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.936 I llm_load_print_meta: max token length = 1024
0.00.093.479 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.480 I llm_load_tensors: offloading output layer to GPU
0.00.093.480 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.490 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.492 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.455 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.456 I llama_new_context_with_model: n_ctx         = 128
0.00.094.457 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.457 I llama_new_context_with_model: n_batch       = 128
0.00.094.457 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.457 I llama_new_context_with_model: flash_attn    = 0
0.00.094.458 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.458 I llama_new_context_with_model: freq_scale    = 1
0.00.094.458 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.459 I ggml_metal_init: allocating
0.00.094.467 I ggml_metal_init: found device: Apple M4
0.00.094.472 I ggml_metal_init: picking default device: Apple M4
0.00.095.093 I ggml_metal_init: using embedded metal library
0.00.097.644 I ggml_metal_init: GPU name:   Apple M4
0.00.097.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.647 I ggml_metal_init: simdgroup reduction   = true
0.00.097.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.647 I ggml_metal_init: has bfloat            = true
0.00.097.647 I ggml_metal_init: use bfloat            = true
0.00.097.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.469 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.471 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.484 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.328 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.329 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.329 I llama_new_context_with_model: graph nodes  = 967
0.00.110.329 I llama_new_context_with_model: graph splits = 2
0.00.110.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.360.727 I 
0.01.360.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.360.818 I perplexity: tokenizing the input ..
0.01.372.944 I perplexity: tokenization took 12.124 ms
0.01.372.949 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.493.739 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.495.724 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.495.743 I llama_perf_context_print:        load time =    1335.57 ms
0.01.495.745 I llama_perf_context_print: prompt eval time =     120.39 ms /   128 tokens (    0.94 ms per token,  1063.24 tokens per second)
0.01.495.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.495.748 I llama_perf_context_print:       total time =     135.02 ms /   129 tokens
0.01.496.406 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.123s
sys	0m0.245s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.352 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.420 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.421 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.422 I llama_model_loader: - type  f32:  194 tensors
0.00.038.422 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.584 I llm_load_vocab: special tokens cache size = 25
0.00.071.266 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.270 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.271 I llm_load_print_meta: arch             = gptneox
0.00.071.271 I llm_load_print_meta: vocab type       = BPE
0.00.071.271 I llm_load_print_meta: n_vocab          = 50304
0.00.071.271 I llm_load_print_meta: n_merges         = 50009
0.00.071.274 I llm_load_print_meta: vocab_only       = 0
0.00.071.274 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.275 I llm_load_print_meta: n_embd           = 2048
0.00.071.275 I llm_load_print_meta: n_layer          = 24
0.00.071.280 I llm_load_print_meta: n_head           = 16
0.00.071.281 I llm_load_print_meta: n_head_kv        = 16
0.00.071.281 I llm_load_print_meta: n_rot            = 32
0.00.071.281 I llm_load_print_meta: n_swa            = 0
0.00.071.281 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.281 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.282 I llm_load_print_meta: n_gqa            = 1
0.00.071.283 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.284 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.284 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.285 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.285 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.286 I llm_load_print_meta: n_ff             = 8192
0.00.071.286 I llm_load_print_meta: n_expert         = 0
0.00.071.286 I llm_load_print_meta: n_expert_used    = 0
0.00.071.286 I llm_load_print_meta: causal attn      = 1
0.00.071.286 I llm_load_print_meta: pooling type     = 0
0.00.071.287 I llm_load_print_meta: rope type        = 2
0.00.071.287 I llm_load_print_meta: rope scaling     = linear
0.00.071.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.288 I llm_load_print_meta: freq_scale_train = 1
0.00.071.288 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.290 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.290 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.290 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.290 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.290 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.291 I llm_load_print_meta: model type       = 1.4B
0.00.071.291 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.291 I llm_load_print_meta: model params     = 1.41 B
0.00.071.292 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.292 I llm_load_print_meta: general.name     = 1.4B
0.00.071.292 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.292 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.293 I llm_load_print_meta: LF token         = 128 ''
0.00.071.293 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.294 I llm_load_print_meta: max token length = 1024
0.00.073.936 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.936 I llm_load_tensors: offloading output layer to GPU
0.00.073.937 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.948 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.950 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.124 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.125 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.126 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.126 I llama_new_context_with_model: n_batch       = 2048
0.00.075.126 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.126 I llama_new_context_with_model: flash_attn    = 0
0.00.075.127 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.127 I llama_new_context_with_model: freq_scale    = 1
0.00.075.128 I ggml_metal_init: allocating
0.00.075.136 I ggml_metal_init: found device: Apple M4
0.00.075.138 I ggml_metal_init: picking default device: Apple M4
0.00.075.971 I ggml_metal_init: using embedded metal library
0.00.079.061 I ggml_metal_init: GPU name:   Apple M4
0.00.079.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.065 I ggml_metal_init: simdgroup reduction   = true
0.00.079.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.065 I ggml_metal_init: has bfloat            = true
0.00.079.065 I ggml_metal_init: use bfloat            = true
0.00.079.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.525 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.624 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.635 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.823 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.825 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.825 I llama_new_context_with_model: graph nodes  = 967
0.00.119.826 I llama_new_context_with_model: graph splits = 2
0.00.119.829 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.972 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.339.288 I main: llama threadpool init, n_threads = 4
0.01.339.323 I 
0.01.339.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.339.347 I 
0.01.339.566 I sampler seed: 1234
0.01.339.571 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.339.605 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.339.616 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.339.617 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.420.863 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.02.420.864 I llama_perf_context_print:        load time =    1328.93 ms
0.02.420.865 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.80 tokens per second)
0.02.420.865 I llama_perf_context_print:        eval time =    1038.57 ms /    63 runs   (   16.49 ms per token,    60.66 tokens per second)
0.02.420.866 I llama_perf_context_print:       total time =    1081.58 ms /    70 tokens
0.02.421.104 I ggml_metal_free: deallocating

real	0m2.439s
user	0m0.119s
sys	0m0.246s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.317 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.811 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.814 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.816 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.085 I llama_model_loader: - type  f32:  194 tensors
0.00.036.085 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.193 I llm_load_vocab: special tokens cache size = 25
0.00.069.485 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.488 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.488 I llm_load_print_meta: arch             = gptneox
0.00.069.489 I llm_load_print_meta: vocab type       = BPE
0.00.069.489 I llm_load_print_meta: n_vocab          = 50304
0.00.069.489 I llm_load_print_meta: n_merges         = 50009
0.00.069.489 I llm_load_print_meta: vocab_only       = 0
0.00.069.489 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.489 I llm_load_print_meta: n_embd           = 2048
0.00.069.489 I llm_load_print_meta: n_layer          = 24
0.00.069.493 I llm_load_print_meta: n_head           = 16
0.00.069.494 I llm_load_print_meta: n_head_kv        = 16
0.00.069.494 I llm_load_print_meta: n_rot            = 32
0.00.069.494 I llm_load_print_meta: n_swa            = 0
0.00.069.494 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.494 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.495 I llm_load_print_meta: n_gqa            = 1
0.00.069.496 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.497 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.497 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.498 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.498 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.498 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.499 I llm_load_print_meta: n_ff             = 8192
0.00.069.499 I llm_load_print_meta: n_expert         = 0
0.00.069.503 I llm_load_print_meta: n_expert_used    = 0
0.00.069.503 I llm_load_print_meta: causal attn      = 1
0.00.069.503 I llm_load_print_meta: pooling type     = 0
0.00.069.503 I llm_load_print_meta: rope type        = 2
0.00.069.503 I llm_load_print_meta: rope scaling     = linear
0.00.069.504 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.504 I llm_load_print_meta: freq_scale_train = 1
0.00.069.504 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.505 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.505 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.505 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.505 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.505 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.506 I llm_load_print_meta: model type       = 1.4B
0.00.069.507 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.507 I llm_load_print_meta: model params     = 1.41 B
0.00.069.507 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.508 I llm_load_print_meta: general.name     = 1.4B
0.00.069.508 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.508 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.508 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.508 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.509 I llm_load_print_meta: LF token         = 128 ''
0.00.069.509 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.509 I llm_load_print_meta: max token length = 1024
0.00.071.488 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.488 I llm_load_tensors: offloading output layer to GPU
0.00.071.488 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.494 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.495 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.415 I llama_new_context_with_model: n_ctx         = 128
0.00.072.415 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.415 I llama_new_context_with_model: n_batch       = 128
0.00.072.416 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.416 I llama_new_context_with_model: flash_attn    = 0
0.00.072.416 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.417 I llama_new_context_with_model: freq_scale    = 1
0.00.072.417 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.417 I ggml_metal_init: allocating
0.00.072.421 I ggml_metal_init: found device: Apple M4
0.00.072.423 I ggml_metal_init: picking default device: Apple M4
0.00.073.053 I ggml_metal_init: using embedded metal library
0.00.075.651 I ggml_metal_init: GPU name:   Apple M4
0.00.075.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.654 I ggml_metal_init: simdgroup reduction   = true
0.00.075.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.654 I ggml_metal_init: has bfloat            = true
0.00.075.654 I ggml_metal_init: use bfloat            = true
0.00.075.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.473 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.488 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.397 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.399 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.399 I llama_new_context_with_model: graph nodes  = 967
0.00.086.399 I llama_new_context_with_model: graph splits = 2
0.00.086.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.993.627 I 
0.00.993.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.993.660 I perplexity: tokenizing the input ..
0.01.001.406 I perplexity: tokenization took 7.744 ms
0.01.001.409 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.125.677 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.126.841 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.126.854 I llama_perf_context_print:        load time =     979.87 ms
0.01.126.855 I llama_perf_context_print: prompt eval time =     124.04 ms /   128 tokens (    0.97 ms per token,  1031.90 tokens per second)
0.01.126.855 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.126.856 I llama_perf_context_print:       total time =     133.23 ms /   129 tokens
0.01.127.252 I ggml_metal_free: deallocating

real	0m1.149s
user	0m0.095s
sys	0m0.166s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.023.557 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.620 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.050.851 I llama_model_loader: - type  f32:  194 tensors
0.00.050.851 I llama_model_loader: - type q4_0:   97 tensors
0.00.050.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.081.575 I llm_load_vocab: special tokens cache size = 25
0.00.092.903 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.909 I llm_load_print_meta: arch             = gptneox
0.00.092.909 I llm_load_print_meta: vocab type       = BPE
0.00.092.909 I llm_load_print_meta: n_vocab          = 50304
0.00.092.910 I llm_load_print_meta: n_merges         = 50009
0.00.092.910 I llm_load_print_meta: vocab_only       = 0
0.00.092.913 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.913 I llm_load_print_meta: n_embd           = 2048
0.00.092.913 I llm_load_print_meta: n_layer          = 24
0.00.092.918 I llm_load_print_meta: n_head           = 16
0.00.092.919 I llm_load_print_meta: n_head_kv        = 16
0.00.092.920 I llm_load_print_meta: n_rot            = 32
0.00.092.920 I llm_load_print_meta: n_swa            = 0
0.00.092.920 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.920 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.921 I llm_load_print_meta: n_gqa            = 1
0.00.092.922 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.923 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.924 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.924 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.924 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.925 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.925 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.926 I llm_load_print_meta: n_ff             = 8192
0.00.092.926 I llm_load_print_meta: n_expert         = 0
0.00.092.926 I llm_load_print_meta: n_expert_used    = 0
0.00.092.926 I llm_load_print_meta: causal attn      = 1
0.00.092.927 I llm_load_print_meta: pooling type     = 0
0.00.092.927 I llm_load_print_meta: rope type        = 2
0.00.092.930 I llm_load_print_meta: rope scaling     = linear
0.00.092.930 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.931 I llm_load_print_meta: freq_scale_train = 1
0.00.092.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.932 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.932 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.933 I llm_load_print_meta: model type       = 1.4B
0.00.092.933 I llm_load_print_meta: model ftype      = Q4_0
0.00.092.934 I llm_load_print_meta: model params     = 1.41 B
0.00.092.940 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.092.940 I llm_load_print_meta: general.name     = 1.4B
0.00.092.941 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.941 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.941 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.942 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.942 I llm_load_print_meta: LF token         = 128 ''
0.00.092.943 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.943 I llm_load_print_meta: max token length = 1024
0.00.096.022 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.023 I llm_load_tensors: offloading output layer to GPU
0.00.096.023 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.035 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.096.037 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.097.530 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.532 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.532 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.532 I llama_new_context_with_model: n_batch       = 2048
0.00.097.533 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.533 I llama_new_context_with_model: flash_attn    = 0
0.00.097.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.534 I llama_new_context_with_model: freq_scale    = 1
0.00.097.535 I ggml_metal_init: allocating
0.00.097.542 I ggml_metal_init: found device: Apple M4
0.00.097.546 I ggml_metal_init: picking default device: Apple M4
0.00.098.565 I ggml_metal_init: using embedded metal library
0.00.102.413 I ggml_metal_init: GPU name:   Apple M4
0.00.102.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.417 I ggml_metal_init: simdgroup reduction   = true
0.00.102.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.417 I ggml_metal_init: has bfloat            = true
0.00.102.417 I ggml_metal_init: use bfloat            = true
0.00.102.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.579 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.141.808 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.141.820 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.141.857 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.142.972 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.142.974 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.142.974 I llama_new_context_with_model: graph nodes  = 967
0.00.142.974 I llama_new_context_with_model: graph splits = 2
0.00.142.980 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.143.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.731 I main: llama threadpool init, n_threads = 4
0.00.847.831 I 
0.00.847.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.886 I 
0.00.848.407 I sampler seed: 1234
0.00.848.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.445 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.447 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.447 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.526.241 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.526.242 I llama_perf_context_print:        load time =     824.16 ms
0.01.526.243 I llama_perf_context_print: prompt eval time =      45.51 ms /     7 tokens (    6.50 ms per token,   153.82 tokens per second)
0.01.526.245 I llama_perf_context_print:        eval time =     629.32 ms /    63 runs   (    9.99 ms per token,   100.11 tokens per second)
0.01.526.245 I llama_perf_context_print:       total time =     678.52 ms /    70 tokens
0.01.526.453 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.137s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.267 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.249 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.610 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.616 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.054 I llama_model_loader: - type  f32:  194 tensors
0.00.024.055 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.808 I llm_load_vocab: special tokens cache size = 25
0.00.050.758 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.761 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.761 I llm_load_print_meta: arch             = gptneox
0.00.050.761 I llm_load_print_meta: vocab type       = BPE
0.00.050.762 I llm_load_print_meta: n_vocab          = 50304
0.00.050.762 I llm_load_print_meta: n_merges         = 50009
0.00.050.762 I llm_load_print_meta: vocab_only       = 0
0.00.050.762 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.762 I llm_load_print_meta: n_embd           = 2048
0.00.050.762 I llm_load_print_meta: n_layer          = 24
0.00.050.765 I llm_load_print_meta: n_head           = 16
0.00.050.766 I llm_load_print_meta: n_head_kv        = 16
0.00.050.766 I llm_load_print_meta: n_rot            = 32
0.00.050.766 I llm_load_print_meta: n_swa            = 0
0.00.050.767 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.767 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.768 I llm_load_print_meta: n_gqa            = 1
0.00.050.769 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.773 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.773 I llm_load_print_meta: n_ff             = 8192
0.00.050.774 I llm_load_print_meta: n_expert         = 0
0.00.050.774 I llm_load_print_meta: n_expert_used    = 0
0.00.050.774 I llm_load_print_meta: causal attn      = 1
0.00.050.774 I llm_load_print_meta: pooling type     = 0
0.00.050.774 I llm_load_print_meta: rope type        = 2
0.00.050.775 I llm_load_print_meta: rope scaling     = linear
0.00.050.775 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.775 I llm_load_print_meta: freq_scale_train = 1
0.00.050.776 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.776 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.777 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.777 I llm_load_print_meta: model type       = 1.4B
0.00.050.777 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.778 I llm_load_print_meta: model params     = 1.41 B
0.00.050.778 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.779 I llm_load_print_meta: general.name     = 1.4B
0.00.050.779 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.780 I llm_load_print_meta: LF token         = 128 ''
0.00.050.781 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.781 I llm_load_print_meta: max token length = 1024
0.00.052.755 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.755 I llm_load_tensors: offloading output layer to GPU
0.00.052.755 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.766 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.767 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.694 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.695 I llama_new_context_with_model: n_ctx         = 128
0.00.053.695 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.695 I llama_new_context_with_model: n_batch       = 128
0.00.053.695 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.696 I llama_new_context_with_model: flash_attn    = 0
0.00.053.696 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.696 I llama_new_context_with_model: freq_scale    = 1
0.00.053.696 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.697 I ggml_metal_init: allocating
0.00.053.700 I ggml_metal_init: found device: Apple M4
0.00.053.702 I ggml_metal_init: picking default device: Apple M4
0.00.054.268 I ggml_metal_init: using embedded metal library
0.00.056.566 I ggml_metal_init: GPU name:   Apple M4
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.569 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.569 I ggml_metal_init: simdgroup reduction   = true
0.00.056.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.569 I ggml_metal_init: has bfloat            = true
0.00.056.569 I ggml_metal_init: use bfloat            = true
0.00.056.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.864 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.211 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.233 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.175 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.176 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.177 I llama_new_context_with_model: graph nodes  = 967
0.00.071.177 I llama_new_context_with_model: graph splits = 2
0.00.071.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.035 I 
0.00.686.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.103 I perplexity: tokenizing the input ..
0.00.693.710 I perplexity: tokenization took 7.606 ms
0.00.693.713 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.326 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.817.484 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.817.509 I llama_perf_context_print:        load time =     675.78 ms
0.00.817.510 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.01 tokens per second)
0.00.817.511 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.512 I llama_perf_context_print:       total time =     131.48 ms /   129 tokens
0.00.818.041 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.078s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.622 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.489 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.346 I llama_model_loader: - type  f32:  194 tensors
0.00.034.347 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.475 I llm_load_vocab: special tokens cache size = 25
0.00.064.983 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.985 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.986 I llm_load_print_meta: arch             = gptneox
0.00.064.986 I llm_load_print_meta: vocab type       = BPE
0.00.064.986 I llm_load_print_meta: n_vocab          = 50304
0.00.064.986 I llm_load_print_meta: n_merges         = 50009
0.00.064.987 I llm_load_print_meta: vocab_only       = 0
0.00.064.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.987 I llm_load_print_meta: n_embd           = 2048
0.00.064.987 I llm_load_print_meta: n_layer          = 24
0.00.064.990 I llm_load_print_meta: n_head           = 16
0.00.064.991 I llm_load_print_meta: n_head_kv        = 16
0.00.064.991 I llm_load_print_meta: n_rot            = 32
0.00.064.993 I llm_load_print_meta: n_swa            = 0
0.00.064.993 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.994 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.994 I llm_load_print_meta: n_gqa            = 1
0.00.064.995 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.995 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.996 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.997 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.997 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.997 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.997 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.998 I llm_load_print_meta: n_ff             = 8192
0.00.064.998 I llm_load_print_meta: n_expert         = 0
0.00.064.998 I llm_load_print_meta: n_expert_used    = 0
0.00.065.000 I llm_load_print_meta: causal attn      = 1
0.00.065.001 I llm_load_print_meta: pooling type     = 0
0.00.065.001 I llm_load_print_meta: rope type        = 2
0.00.065.001 I llm_load_print_meta: rope scaling     = linear
0.00.065.002 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.002 I llm_load_print_meta: freq_scale_train = 1
0.00.065.002 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.002 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.003 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.003 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.003 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.003 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.003 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.007 I llm_load_print_meta: model type       = 1.4B
0.00.065.008 I llm_load_print_meta: model ftype      = Q4_1
0.00.065.009 I llm_load_print_meta: model params     = 1.41 B
0.00.065.009 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.065.009 I llm_load_print_meta: general.name     = 1.4B
0.00.065.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.010 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.010 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.010 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.010 I llm_load_print_meta: LF token         = 128 ''
0.00.065.011 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.011 I llm_load_print_meta: max token length = 1024
0.00.067.184 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.185 I llm_load_tensors: offloading output layer to GPU
0.00.067.185 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.196 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.067.197 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.068.212 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.213 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.214 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.214 I llama_new_context_with_model: n_batch       = 2048
0.00.068.214 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.214 I llama_new_context_with_model: flash_attn    = 0
0.00.068.215 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.215 I llama_new_context_with_model: freq_scale    = 1
0.00.068.215 I ggml_metal_init: allocating
0.00.068.218 I ggml_metal_init: found device: Apple M4
0.00.068.220 I ggml_metal_init: picking default device: Apple M4
0.00.068.858 I ggml_metal_init: using embedded metal library
0.00.071.491 I ggml_metal_init: GPU name:   Apple M4
0.00.071.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.494 I ggml_metal_init: simdgroup reduction   = true
0.00.071.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.494 I ggml_metal_init: has bfloat            = true
0.00.071.496 I ggml_metal_init: use bfloat            = true
0.00.071.496 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.656 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.368 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.373 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.420 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.422 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.422 I llama_new_context_with_model: graph nodes  = 967
0.00.102.422 I llama_new_context_with_model: graph splits = 2
0.00.102.424 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.755 I main: llama threadpool init, n_threads = 4
0.00.843.800 I 
0.00.843.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.822 I 
0.00.844.050 I sampler seed: 1234
0.00.844.056 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.844.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.844.102 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.844.102 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.584.553 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64369.90 tokens per second)
0.01.584.553 I llama_perf_context_print:        load time =     835.13 ms
0.01.584.554 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.86 tokens per second)
0.01.584.555 I llama_perf_context_print:        eval time =     694.07 ms /    63 runs   (   11.02 ms per token,    90.77 tokens per second)
0.01.584.555 I llama_perf_context_print:       total time =     740.80 ms /    70 tokens
0.01.584.757 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.114s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.791 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.317 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.330 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.330 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.330 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.034 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.825 I llama_model_loader: - type  f32:  194 tensors
0.00.022.825 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.577 I llm_load_vocab: special tokens cache size = 25
0.00.048.396 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.399 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.399 I llm_load_print_meta: arch             = gptneox
0.00.048.400 I llm_load_print_meta: vocab type       = BPE
0.00.048.400 I llm_load_print_meta: n_vocab          = 50304
0.00.048.400 I llm_load_print_meta: n_merges         = 50009
0.00.048.400 I llm_load_print_meta: vocab_only       = 0
0.00.048.400 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.400 I llm_load_print_meta: n_embd           = 2048
0.00.048.401 I llm_load_print_meta: n_layer          = 24
0.00.048.404 I llm_load_print_meta: n_head           = 16
0.00.048.407 I llm_load_print_meta: n_head_kv        = 16
0.00.048.407 I llm_load_print_meta: n_rot            = 32
0.00.048.407 I llm_load_print_meta: n_swa            = 0
0.00.048.407 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.407 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.408 I llm_load_print_meta: n_gqa            = 1
0.00.048.409 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.410 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.410 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.410 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.411 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.411 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.411 I llm_load_print_meta: n_ff             = 8192
0.00.048.412 I llm_load_print_meta: n_expert         = 0
0.00.048.412 I llm_load_print_meta: n_expert_used    = 0
0.00.048.412 I llm_load_print_meta: causal attn      = 1
0.00.048.413 I llm_load_print_meta: pooling type     = 0
0.00.048.417 I llm_load_print_meta: rope type        = 2
0.00.048.417 I llm_load_print_meta: rope scaling     = linear
0.00.048.417 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.418 I llm_load_print_meta: freq_scale_train = 1
0.00.048.418 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.418 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.418 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.418 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.419 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.419 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.419 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.419 I llm_load_print_meta: model type       = 1.4B
0.00.048.420 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.420 I llm_load_print_meta: model params     = 1.41 B
0.00.048.421 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.421 I llm_load_print_meta: general.name     = 1.4B
0.00.048.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.424 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.424 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.426 I llm_load_print_meta: LF token         = 128 ''
0.00.048.426 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.427 I llm_load_print_meta: max token length = 1024
0.00.050.347 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.347 I llm_load_tensors: offloading output layer to GPU
0.00.050.347 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.358 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.359 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.252 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.253 I llama_new_context_with_model: n_ctx         = 128
0.00.051.253 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.254 I llama_new_context_with_model: n_batch       = 128
0.00.051.254 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.254 I llama_new_context_with_model: flash_attn    = 0
0.00.051.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.255 I llama_new_context_with_model: freq_scale    = 1
0.00.051.255 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.256 I ggml_metal_init: allocating
0.00.051.258 I ggml_metal_init: found device: Apple M4
0.00.051.260 I ggml_metal_init: picking default device: Apple M4
0.00.051.827 I ggml_metal_init: using embedded metal library
0.00.054.105 I ggml_metal_init: GPU name:   Apple M4
0.00.054.107 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.108 I ggml_metal_init: simdgroup reduction   = true
0.00.054.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.108 I ggml_metal_init: has bfloat            = true
0.00.054.108 I ggml_metal_init: use bfloat            = true
0.00.054.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.437 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.786 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.789 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.786 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.788 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.788 I llama_new_context_with_model: graph nodes  = 967
0.00.065.788 I llama_new_context_with_model: graph splits = 2
0.00.065.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.646 I 
0.00.722.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.701 I perplexity: tokenizing the input ..
0.00.730.793 I perplexity: tokenization took 8.09 ms
0.00.730.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.007 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.855.172 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.855.188 I llama_perf_context_print:        load time =     713.85 ms
0.00.855.189 I llama_perf_context_print: prompt eval time =     122.99 ms /   128 tokens (    0.96 ms per token,  1040.76 tokens per second)
0.00.855.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.191 I llama_perf_context_print:       total time =     132.54 ms /   129 tokens
0.00.855.675 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.077s
sys	0m0.102s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.942 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.936 I llama_model_loader: - type  f32:  194 tensors
0.00.024.936 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.936 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.712 I llm_load_vocab: special tokens cache size = 25
0.00.050.560 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.563 I llm_load_print_meta: arch             = gptneox
0.00.050.564 I llm_load_print_meta: vocab type       = BPE
0.00.050.564 I llm_load_print_meta: n_vocab          = 50304
0.00.050.564 I llm_load_print_meta: n_merges         = 50009
0.00.050.564 I llm_load_print_meta: vocab_only       = 0
0.00.050.564 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.564 I llm_load_print_meta: n_embd           = 2048
0.00.050.565 I llm_load_print_meta: n_layer          = 24
0.00.050.568 I llm_load_print_meta: n_head           = 16
0.00.050.568 I llm_load_print_meta: n_head_kv        = 16
0.00.050.569 I llm_load_print_meta: n_rot            = 32
0.00.050.569 I llm_load_print_meta: n_swa            = 0
0.00.050.569 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.569 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.570 I llm_load_print_meta: n_gqa            = 1
0.00.050.571 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.572 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.572 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.575 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.575 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.575 I llm_load_print_meta: n_ff             = 8192
0.00.050.576 I llm_load_print_meta: n_expert         = 0
0.00.050.576 I llm_load_print_meta: n_expert_used    = 0
0.00.050.576 I llm_load_print_meta: causal attn      = 1
0.00.050.578 I llm_load_print_meta: pooling type     = 0
0.00.050.578 I llm_load_print_meta: rope type        = 2
0.00.050.578 I llm_load_print_meta: rope scaling     = linear
0.00.050.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.579 I llm_load_print_meta: freq_scale_train = 1
0.00.050.579 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.579 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.580 I llm_load_print_meta: model type       = 1.4B
0.00.050.581 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.581 I llm_load_print_meta: model params     = 1.41 B
0.00.050.582 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.582 I llm_load_print_meta: general.name     = 1.4B
0.00.050.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.582 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.582 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: LF token         = 128 ''
0.00.050.583 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: max token length = 1024
0.00.052.188 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.188 I llm_load_tensors: offloading output layer to GPU
0.00.052.188 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.199 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.200 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.056 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.056 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.057 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.057 I llama_new_context_with_model: n_batch       = 2048
0.00.053.057 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.057 I llama_new_context_with_model: flash_attn    = 0
0.00.053.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.058 I llama_new_context_with_model: freq_scale    = 1
0.00.053.058 I ggml_metal_init: allocating
0.00.053.061 I ggml_metal_init: found device: Apple M4
0.00.053.063 I ggml_metal_init: picking default device: Apple M4
0.00.053.643 I ggml_metal_init: using embedded metal library
0.00.055.921 I ggml_metal_init: GPU name:   Apple M4
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.924 I ggml_metal_init: simdgroup reduction   = true
0.00.055.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.924 I ggml_metal_init: has bfloat            = true
0.00.055.924 I ggml_metal_init: use bfloat            = true
0.00.055.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.195 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.221 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.306 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.308 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.308 I llama_new_context_with_model: graph nodes  = 967
0.00.085.308 I llama_new_context_with_model: graph splits = 2
0.00.085.311 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.355 I main: llama threadpool init, n_threads = 4
0.00.766.396 I 
0.00.766.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.418 I 
0.00.766.588 I sampler seed: 1234
0.00.766.592 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.621 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.622 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.622 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.592.797 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.592.798 I llama_perf_context_print:        load time =     756.41 ms
0.01.592.798 I llama_perf_context_print: prompt eval time =      43.21 ms /     7 tokens (    6.17 ms per token,   162.00 tokens per second)
0.01.592.799 I llama_perf_context_print:        eval time =     779.97 ms /    63 runs   (   12.38 ms per token,    80.77 tokens per second)
0.01.592.799 I llama_perf_context_print:       total time =     826.45 ms /    70 tokens
0.01.593.005 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.108s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.227 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.842 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.843 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.843 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.845 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.846 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.849 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.415 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.416 I llama_model_loader: - type  f32:  194 tensors
0.00.023.416 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.416 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.076 I llm_load_vocab: special tokens cache size = 25
0.00.050.008 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.011 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.011 I llm_load_print_meta: arch             = gptneox
0.00.050.012 I llm_load_print_meta: vocab type       = BPE
0.00.050.012 I llm_load_print_meta: n_vocab          = 50304
0.00.050.012 I llm_load_print_meta: n_merges         = 50009
0.00.050.012 I llm_load_print_meta: vocab_only       = 0
0.00.050.012 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.013 I llm_load_print_meta: n_embd           = 2048
0.00.050.013 I llm_load_print_meta: n_layer          = 24
0.00.050.015 I llm_load_print_meta: n_head           = 16
0.00.050.016 I llm_load_print_meta: n_head_kv        = 16
0.00.050.016 I llm_load_print_meta: n_rot            = 32
0.00.050.016 I llm_load_print_meta: n_swa            = 0
0.00.050.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.017 I llm_load_print_meta: n_gqa            = 1
0.00.050.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.022 I llm_load_print_meta: n_ff             = 8192
0.00.050.022 I llm_load_print_meta: n_expert         = 0
0.00.050.022 I llm_load_print_meta: n_expert_used    = 0
0.00.050.022 I llm_load_print_meta: causal attn      = 1
0.00.050.022 I llm_load_print_meta: pooling type     = 0
0.00.050.022 I llm_load_print_meta: rope type        = 2
0.00.050.023 I llm_load_print_meta: rope scaling     = linear
0.00.050.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.023 I llm_load_print_meta: freq_scale_train = 1
0.00.050.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.024 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.024 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.026 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.026 I llm_load_print_meta: model type       = 1.4B
0.00.050.026 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.027 I llm_load_print_meta: model params     = 1.41 B
0.00.050.027 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.028 I llm_load_print_meta: general.name     = 1.4B
0.00.050.028 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.029 I llm_load_print_meta: LF token         = 128 ''
0.00.050.029 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.029 I llm_load_print_meta: max token length = 1024
0.00.052.062 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.062 I llm_load_tensors: offloading output layer to GPU
0.00.052.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.072 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.074 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.025 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.026 I llama_new_context_with_model: n_ctx         = 128
0.00.053.026 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.026 I llama_new_context_with_model: n_batch       = 128
0.00.053.026 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.026 I llama_new_context_with_model: flash_attn    = 0
0.00.053.027 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.027 I llama_new_context_with_model: freq_scale    = 1
0.00.053.027 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.028 I ggml_metal_init: allocating
0.00.053.031 I ggml_metal_init: found device: Apple M4
0.00.053.033 I ggml_metal_init: picking default device: Apple M4
0.00.053.596 I ggml_metal_init: using embedded metal library
0.00.055.904 I ggml_metal_init: GPU name:   Apple M4
0.00.055.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.906 I ggml_metal_init: simdgroup reduction   = true
0.00.055.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.906 I ggml_metal_init: has bfloat            = true
0.00.055.907 I ggml_metal_init: use bfloat            = true
0.00.055.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.570 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.819 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.822 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.839 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.713 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.714 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.714 I llama_new_context_with_model: graph nodes  = 967
0.00.067.714 I llama_new_context_with_model: graph splits = 2
0.00.067.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.288 I 
0.00.752.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.344 I perplexity: tokenizing the input ..
0.00.759.692 I perplexity: tokenization took 7.347 ms
0.00.759.695 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.894.800 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.895.949 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.895.967 I llama_perf_context_print:        load time =     743.05 ms
0.00.895.968 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   948.98 tokens per second)
0.00.895.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.969 I llama_perf_context_print:       total time =     143.68 ms /   129 tokens
0.00.896.483 I ggml_metal_free: deallocating

real	0m0.911s
user	0m0.078s
sys	0m0.124s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.868 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.536 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.253 I llama_model_loader: - type  f32:  194 tensors
0.00.024.253 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.254 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.096 I llm_load_vocab: special tokens cache size = 25
0.00.050.017 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.021 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.021 I llm_load_print_meta: arch             = gptneox
0.00.050.021 I llm_load_print_meta: vocab type       = BPE
0.00.050.022 I llm_load_print_meta: n_vocab          = 50304
0.00.050.022 I llm_load_print_meta: n_merges         = 50009
0.00.050.022 I llm_load_print_meta: vocab_only       = 0
0.00.050.022 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.022 I llm_load_print_meta: n_embd           = 2048
0.00.050.023 I llm_load_print_meta: n_layer          = 24
0.00.050.025 I llm_load_print_meta: n_head           = 16
0.00.050.028 I llm_load_print_meta: n_head_kv        = 16
0.00.050.028 I llm_load_print_meta: n_rot            = 32
0.00.050.028 I llm_load_print_meta: n_swa            = 0
0.00.050.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.029 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.030 I llm_load_print_meta: n_gqa            = 1
0.00.050.030 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.031 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.032 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.032 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.032 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.033 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.033 I llm_load_print_meta: n_ff             = 8192
0.00.050.033 I llm_load_print_meta: n_expert         = 0
0.00.050.034 I llm_load_print_meta: n_expert_used    = 0
0.00.050.035 I llm_load_print_meta: causal attn      = 1
0.00.050.036 I llm_load_print_meta: pooling type     = 0
0.00.050.036 I llm_load_print_meta: rope type        = 2
0.00.050.037 I llm_load_print_meta: rope scaling     = linear
0.00.050.037 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.037 I llm_load_print_meta: freq_scale_train = 1
0.00.050.037 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.041 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.042 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.042 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.042 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.047 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.049 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.049 I llm_load_print_meta: model type       = 1.4B
0.00.050.050 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.050 I llm_load_print_meta: model params     = 1.41 B
0.00.050.051 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.052 I llm_load_print_meta: general.name     = 1.4B
0.00.050.053 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.053 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.053 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.053 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: LF token         = 128 ''
0.00.050.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: max token length = 1024
0.00.051.649 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.649 I llm_load_tensors: offloading output layer to GPU
0.00.051.649 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.659 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.660 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.511 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.512 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.512 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.512 I llama_new_context_with_model: n_batch       = 2048
0.00.052.512 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.512 I llama_new_context_with_model: flash_attn    = 0
0.00.052.513 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.513 I llama_new_context_with_model: freq_scale    = 1
0.00.052.514 I ggml_metal_init: allocating
0.00.052.520 I ggml_metal_init: found device: Apple M4
0.00.052.523 I ggml_metal_init: picking default device: Apple M4
0.00.053.107 I ggml_metal_init: using embedded metal library
0.00.055.448 I ggml_metal_init: GPU name:   Apple M4
0.00.055.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.450 I ggml_metal_init: simdgroup reduction   = true
0.00.055.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.451 I ggml_metal_init: has bfloat            = true
0.00.055.451 I ggml_metal_init: use bfloat            = true
0.00.055.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.047 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.300 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.312 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.263 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.264 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.264 I llama_new_context_with_model: graph nodes  = 967
0.00.085.265 I llama_new_context_with_model: graph splits = 2
0.00.085.267 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.235 I main: llama threadpool init, n_threads = 4
0.00.701.289 I 
0.00.701.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.314 I 
0.00.701.479 I sampler seed: 1234
0.00.701.483 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.514 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.515 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.515 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.556.039 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.556.040 I llama_perf_context_print:        load time =     692.36 ms
0.01.556.040 I llama_perf_context_print: prompt eval time =      42.31 ms /     7 tokens (    6.04 ms per token,   165.46 tokens per second)
0.01.556.041 I llama_perf_context_print:        eval time =     809.16 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.556.041 I llama_perf_context_print:       total time =     854.81 ms /    70 tokens
0.01.556.243 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.702 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.250 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.251 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.689 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.689 I llama_model_loader: - type  f32:  194 tensors
0.00.022.690 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.382 I llm_load_vocab: special tokens cache size = 25
0.00.048.266 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.268 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.269 I llm_load_print_meta: arch             = gptneox
0.00.048.269 I llm_load_print_meta: vocab type       = BPE
0.00.048.269 I llm_load_print_meta: n_vocab          = 50304
0.00.048.269 I llm_load_print_meta: n_merges         = 50009
0.00.048.270 I llm_load_print_meta: vocab_only       = 0
0.00.048.270 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.270 I llm_load_print_meta: n_embd           = 2048
0.00.048.270 I llm_load_print_meta: n_layer          = 24
0.00.048.273 I llm_load_print_meta: n_head           = 16
0.00.048.274 I llm_load_print_meta: n_head_kv        = 16
0.00.048.274 I llm_load_print_meta: n_rot            = 32
0.00.048.274 I llm_load_print_meta: n_swa            = 0
0.00.048.274 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.275 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.275 I llm_load_print_meta: n_gqa            = 1
0.00.048.276 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.277 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.277 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.278 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.278 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.279 I llm_load_print_meta: n_ff             = 8192
0.00.048.279 I llm_load_print_meta: n_expert         = 0
0.00.048.279 I llm_load_print_meta: n_expert_used    = 0
0.00.048.279 I llm_load_print_meta: causal attn      = 1
0.00.048.280 I llm_load_print_meta: pooling type     = 0
0.00.048.280 I llm_load_print_meta: rope type        = 2
0.00.048.280 I llm_load_print_meta: rope scaling     = linear
0.00.048.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.281 I llm_load_print_meta: freq_scale_train = 1
0.00.048.281 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.281 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.281 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.281 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.282 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.282 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.282 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.284 I llm_load_print_meta: model type       = 1.4B
0.00.048.284 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.284 I llm_load_print_meta: model params     = 1.41 B
0.00.048.285 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.285 I llm_load_print_meta: general.name     = 1.4B
0.00.048.285 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.285 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.286 I llm_load_print_meta: LF token         = 128 ''
0.00.048.286 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.287 I llm_load_print_meta: max token length = 1024
0.00.050.223 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.223 I llm_load_tensors: offloading output layer to GPU
0.00.050.223 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.234 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.235 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.104 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.104 I llama_new_context_with_model: n_ctx         = 128
0.00.051.105 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.105 I llama_new_context_with_model: n_batch       = 128
0.00.051.105 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.105 I llama_new_context_with_model: flash_attn    = 0
0.00.051.106 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.106 I llama_new_context_with_model: freq_scale    = 1
0.00.051.106 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.106 I ggml_metal_init: allocating
0.00.051.109 I ggml_metal_init: found device: Apple M4
0.00.051.111 I ggml_metal_init: picking default device: Apple M4
0.00.051.674 I ggml_metal_init: using embedded metal library
0.00.053.992 I ggml_metal_init: GPU name:   Apple M4
0.00.053.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.994 I ggml_metal_init: simdgroup reduction   = true
0.00.053.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.994 I ggml_metal_init: has bfloat            = true
0.00.053.994 I ggml_metal_init: use bfloat            = true
0.00.053.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.306 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.546 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.549 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.565 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.434 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.435 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.436 I llama_new_context_with_model: graph nodes  = 967
0.00.065.436 I llama_new_context_with_model: graph splits = 2
0.00.065.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.323 I 
0.00.508.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.370 I perplexity: tokenizing the input ..
0.00.515.879 I perplexity: tokenization took 7.508 ms
0.00.515.883 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.651.097 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.652.319 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.652.333 I llama_perf_context_print:        load time =     499.62 ms
0.00.652.334 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.25 tokens per second)
0.00.652.335 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.652.335 I llama_perf_context_print:       total time =     144.01 ms /   129 tokens
0.00.652.840 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.076s
sys	0m0.092s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.539 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.909 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.910 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.651 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.426 I llama_model_loader: - type  f32:  194 tensors
0.00.024.426 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.426 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.257 I llm_load_vocab: special tokens cache size = 25
0.00.050.185 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.188 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.188 I llm_load_print_meta: arch             = gptneox
0.00.050.188 I llm_load_print_meta: vocab type       = BPE
0.00.050.189 I llm_load_print_meta: n_vocab          = 50304
0.00.050.189 I llm_load_print_meta: n_merges         = 50009
0.00.050.189 I llm_load_print_meta: vocab_only       = 0
0.00.050.189 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.189 I llm_load_print_meta: n_embd           = 2048
0.00.050.189 I llm_load_print_meta: n_layer          = 24
0.00.050.192 I llm_load_print_meta: n_head           = 16
0.00.050.193 I llm_load_print_meta: n_head_kv        = 16
0.00.050.193 I llm_load_print_meta: n_rot            = 32
0.00.050.193 I llm_load_print_meta: n_swa            = 0
0.00.050.193 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.193 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.194 I llm_load_print_meta: n_gqa            = 1
0.00.050.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.195 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.196 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.196 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.199 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.199 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.200 I llm_load_print_meta: n_ff             = 8192
0.00.050.200 I llm_load_print_meta: n_expert         = 0
0.00.050.202 I llm_load_print_meta: n_expert_used    = 0
0.00.050.203 I llm_load_print_meta: causal attn      = 1
0.00.050.203 I llm_load_print_meta: pooling type     = 0
0.00.050.203 I llm_load_print_meta: rope type        = 2
0.00.050.204 I llm_load_print_meta: rope scaling     = linear
0.00.050.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.204 I llm_load_print_meta: freq_scale_train = 1
0.00.050.204 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.205 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.205 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.205 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.205 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.206 I llm_load_print_meta: model type       = 1.4B
0.00.050.210 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.210 I llm_load_print_meta: model params     = 1.41 B
0.00.050.210 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.211 I llm_load_print_meta: general.name     = 1.4B
0.00.050.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.212 I llm_load_print_meta: LF token         = 128 ''
0.00.050.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.212 I llm_load_print_meta: max token length = 1024
0.00.052.127 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.127 I llm_load_tensors: offloading output layer to GPU
0.00.052.127 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.138 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.139 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.204 I llama_new_context_with_model: n_batch       = 2048
0.00.053.204 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.205 I llama_new_context_with_model: flash_attn    = 0
0.00.053.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.205 I llama_new_context_with_model: freq_scale    = 1
0.00.053.206 I ggml_metal_init: allocating
0.00.053.208 I ggml_metal_init: found device: Apple M4
0.00.053.210 I ggml_metal_init: picking default device: Apple M4
0.00.053.797 I ggml_metal_init: using embedded metal library
0.00.056.091 I ggml_metal_init: GPU name:   Apple M4
0.00.056.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.094 I ggml_metal_init: simdgroup reduction   = true
0.00.056.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.095 I ggml_metal_init: has bfloat            = true
0.00.056.096 I ggml_metal_init: use bfloat            = true
0.00.056.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.177 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.183 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.275 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.276 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.276 I llama_new_context_with_model: graph nodes  = 967
0.00.087.277 I llama_new_context_with_model: graph splits = 2
0.00.087.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.977 I main: llama threadpool init, n_threads = 4
0.00.441.011 I 
0.00.441.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.034 I 
0.00.441.265 I sampler seed: 1234
0.00.441.269 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.281 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.281 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.281 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.457 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.01.121.457 I llama_perf_context_print:        load time =     430.43 ms
0.01.121.458 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.59 tokens per second)
0.01.121.459 I llama_perf_context_print:        eval time =     641.51 ms /    63 runs   (   10.18 ms per token,    98.21 tokens per second)
0.01.121.459 I llama_perf_context_print:       total time =     680.48 ms /    70 tokens
0.01.121.716 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.108s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.838 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.255 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.616 I llama_model_loader: - type  f32:  194 tensors
0.00.024.616 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.617 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.617 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.284 I llm_load_vocab: special tokens cache size = 25
0.00.050.167 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.169 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.170 I llm_load_print_meta: arch             = gptneox
0.00.050.170 I llm_load_print_meta: vocab type       = BPE
0.00.050.170 I llm_load_print_meta: n_vocab          = 50304
0.00.050.170 I llm_load_print_meta: n_merges         = 50009
0.00.050.171 I llm_load_print_meta: vocab_only       = 0
0.00.050.171 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.171 I llm_load_print_meta: n_embd           = 2048
0.00.050.171 I llm_load_print_meta: n_layer          = 24
0.00.050.174 I llm_load_print_meta: n_head           = 16
0.00.050.175 I llm_load_print_meta: n_head_kv        = 16
0.00.050.175 I llm_load_print_meta: n_rot            = 32
0.00.050.175 I llm_load_print_meta: n_swa            = 0
0.00.050.175 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.176 I llm_load_print_meta: n_gqa            = 1
0.00.050.177 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.177 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.178 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.179 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.179 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.179 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.180 I llm_load_print_meta: n_ff             = 8192
0.00.050.180 I llm_load_print_meta: n_expert         = 0
0.00.050.180 I llm_load_print_meta: n_expert_used    = 0
0.00.050.180 I llm_load_print_meta: causal attn      = 1
0.00.050.180 I llm_load_print_meta: pooling type     = 0
0.00.050.180 I llm_load_print_meta: rope type        = 2
0.00.050.181 I llm_load_print_meta: rope scaling     = linear
0.00.050.181 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.181 I llm_load_print_meta: freq_scale_train = 1
0.00.050.182 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.182 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.182 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.182 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.182 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.183 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.183 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.183 I llm_load_print_meta: model type       = 1.4B
0.00.050.184 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.184 I llm_load_print_meta: model params     = 1.41 B
0.00.050.185 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.185 I llm_load_print_meta: general.name     = 1.4B
0.00.050.185 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.185 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.186 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.186 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.186 I llm_load_print_meta: LF token         = 128 ''
0.00.050.186 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.187 I llm_load_print_meta: max token length = 1024
0.00.052.066 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.066 I llm_load_tensors: offloading output layer to GPU
0.00.052.067 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.077 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.078 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.031 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.032 I llama_new_context_with_model: n_ctx         = 128
0.00.053.032 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.033 I llama_new_context_with_model: n_batch       = 128
0.00.053.033 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.033 I llama_new_context_with_model: flash_attn    = 0
0.00.053.033 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.034 I llama_new_context_with_model: freq_scale    = 1
0.00.053.034 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.034 I ggml_metal_init: allocating
0.00.053.040 I ggml_metal_init: found device: Apple M4
0.00.053.042 I ggml_metal_init: picking default device: Apple M4
0.00.053.606 I ggml_metal_init: using embedded metal library
0.00.055.975 I ggml_metal_init: GPU name:   Apple M4
0.00.055.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.977 I ggml_metal_init: simdgroup reduction   = true
0.00.055.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.977 I ggml_metal_init: has bfloat            = true
0.00.055.978 I ggml_metal_init: use bfloat            = true
0.00.055.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.547 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.872 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.735 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.736 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.737 I llama_new_context_with_model: graph nodes  = 967
0.00.067.737 I llama_new_context_with_model: graph splits = 2
0.00.067.738 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.882 I 
0.00.388.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.936 I perplexity: tokenizing the input ..
0.00.396.707 I perplexity: tokenization took 7.768 ms
0.00.396.710 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.529.419 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.530.608 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.530.625 I llama_perf_context_print:        load time =     378.04 ms
0.00.530.627 I llama_perf_context_print: prompt eval time =     132.47 ms /   128 tokens (    1.03 ms per token,   966.29 tokens per second)
0.00.530.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.530.629 I llama_perf_context_print:       total time =     141.75 ms /   129 tokens
0.00.531.163 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.077s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.503 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.911 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.912 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.723 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.498 I llama_model_loader: - type  f32:  194 tensors
0.00.024.498 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.498 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.498 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.499 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.369 I llm_load_vocab: special tokens cache size = 25
0.00.050.039 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.042 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.043 I llm_load_print_meta: arch             = gptneox
0.00.050.043 I llm_load_print_meta: vocab type       = BPE
0.00.050.043 I llm_load_print_meta: n_vocab          = 50304
0.00.050.043 I llm_load_print_meta: n_merges         = 50009
0.00.050.044 I llm_load_print_meta: vocab_only       = 0
0.00.050.044 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.044 I llm_load_print_meta: n_embd           = 2048
0.00.050.044 I llm_load_print_meta: n_layer          = 24
0.00.050.047 I llm_load_print_meta: n_head           = 16
0.00.050.047 I llm_load_print_meta: n_head_kv        = 16
0.00.050.048 I llm_load_print_meta: n_rot            = 32
0.00.050.048 I llm_load_print_meta: n_swa            = 0
0.00.050.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.049 I llm_load_print_meta: n_gqa            = 1
0.00.050.050 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.050 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.051 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.051 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.052 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.054 I llm_load_print_meta: n_ff             = 8192
0.00.050.056 I llm_load_print_meta: n_expert         = 0
0.00.050.057 I llm_load_print_meta: n_expert_used    = 0
0.00.050.058 I llm_load_print_meta: causal attn      = 1
0.00.050.058 I llm_load_print_meta: pooling type     = 0
0.00.050.058 I llm_load_print_meta: rope type        = 2
0.00.050.058 I llm_load_print_meta: rope scaling     = linear
0.00.050.058 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.059 I llm_load_print_meta: freq_scale_train = 1
0.00.050.059 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.059 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.059 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.059 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.060 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.060 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.060 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.060 I llm_load_print_meta: model type       = 1.4B
0.00.050.061 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.061 I llm_load_print_meta: model params     = 1.41 B
0.00.050.062 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.062 I llm_load_print_meta: general.name     = 1.4B
0.00.050.065 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.065 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.065 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.066 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.066 I llm_load_print_meta: LF token         = 128 ''
0.00.050.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: max token length = 1024
0.00.051.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.962 I llm_load_tensors: offloading output layer to GPU
0.00.051.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.973 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.974 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.868 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.869 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.869 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.870 I llama_new_context_with_model: n_batch       = 2048
0.00.052.870 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.870 I llama_new_context_with_model: flash_attn    = 0
0.00.052.870 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.871 I llama_new_context_with_model: freq_scale    = 1
0.00.052.871 I ggml_metal_init: allocating
0.00.052.875 I ggml_metal_init: found device: Apple M4
0.00.052.876 I ggml_metal_init: picking default device: Apple M4
0.00.053.485 I ggml_metal_init: using embedded metal library
0.00.055.778 I ggml_metal_init: GPU name:   Apple M4
0.00.055.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.781 I ggml_metal_init: simdgroup reduction   = true
0.00.055.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.781 I ggml_metal_init: has bfloat            = true
0.00.055.782 I ggml_metal_init: use bfloat            = true
0.00.055.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.161 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.180 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.099 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.101 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.101 I llama_new_context_with_model: graph nodes  = 967
0.00.086.101 I llama_new_context_with_model: graph splits = 2
0.00.086.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.334 I main: llama threadpool init, n_threads = 4
0.00.516.372 I 
0.00.516.400 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.401 I 
0.00.516.551 I sampler seed: 1234
0.00.516.558 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.516.595 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.516.598 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.516.598 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.261.786 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.261.787 I llama_perf_context_print:        load time =     506.83 ms
0.01.261.787 I llama_perf_context_print: prompt eval time =      40.44 ms /     7 tokens (    5.78 ms per token,   173.09 tokens per second)
0.01.261.788 I llama_perf_context_print:        eval time =     701.88 ms /    63 runs   (   11.14 ms per token,    89.76 tokens per second)
0.01.261.788 I llama_perf_context_print:       total time =     745.45 ms /    70 tokens
0.01.262.012 I ggml_metal_free: deallocating

real	0m1.278s
user	0m0.108s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.039 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.040 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.041 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.042 I llama_model_loader: - type  f32:  194 tensors
0.00.023.042 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.042 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.043 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.681 I llm_load_vocab: special tokens cache size = 25
0.00.049.722 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.724 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.725 I llm_load_print_meta: arch             = gptneox
0.00.049.725 I llm_load_print_meta: vocab type       = BPE
0.00.049.725 I llm_load_print_meta: n_vocab          = 50304
0.00.049.726 I llm_load_print_meta: n_merges         = 50009
0.00.049.726 I llm_load_print_meta: vocab_only       = 0
0.00.049.726 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.726 I llm_load_print_meta: n_embd           = 2048
0.00.049.726 I llm_load_print_meta: n_layer          = 24
0.00.049.729 I llm_load_print_meta: n_head           = 16
0.00.049.730 I llm_load_print_meta: n_head_kv        = 16
0.00.049.730 I llm_load_print_meta: n_rot            = 32
0.00.049.730 I llm_load_print_meta: n_swa            = 0
0.00.049.730 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.731 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.731 I llm_load_print_meta: n_gqa            = 1
0.00.049.732 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.735 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.736 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.738 I llm_load_print_meta: n_ff             = 8192
0.00.049.738 I llm_load_print_meta: n_expert         = 0
0.00.049.738 I llm_load_print_meta: n_expert_used    = 0
0.00.049.739 I llm_load_print_meta: causal attn      = 1
0.00.049.739 I llm_load_print_meta: pooling type     = 0
0.00.049.739 I llm_load_print_meta: rope type        = 2
0.00.049.739 I llm_load_print_meta: rope scaling     = linear
0.00.049.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.740 I llm_load_print_meta: freq_scale_train = 1
0.00.049.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.740 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.740 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.742 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.743 I llm_load_print_meta: model type       = 1.4B
0.00.049.743 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.744 I llm_load_print_meta: model params     = 1.41 B
0.00.049.744 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.744 I llm_load_print_meta: general.name     = 1.4B
0.00.049.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: LF token         = 128 ''
0.00.049.747 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.747 I llm_load_print_meta: max token length = 1024
0.00.051.624 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.624 I llm_load_tensors: offloading output layer to GPU
0.00.051.624 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.635 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.636 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.498 I llama_new_context_with_model: n_ctx         = 128
0.00.052.499 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.499 I llama_new_context_with_model: n_batch       = 128
0.00.052.499 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.499 I llama_new_context_with_model: flash_attn    = 0
0.00.052.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.500 I llama_new_context_with_model: freq_scale    = 1
0.00.052.500 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.501 I ggml_metal_init: allocating
0.00.052.507 I ggml_metal_init: found device: Apple M4
0.00.052.509 I ggml_metal_init: picking default device: Apple M4
0.00.053.069 I ggml_metal_init: using embedded metal library
0.00.055.397 I ggml_metal_init: GPU name:   Apple M4
0.00.055.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.399 I ggml_metal_init: simdgroup reduction   = true
0.00.055.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.400 I ggml_metal_init: has bfloat            = true
0.00.055.400 I ggml_metal_init: use bfloat            = true
0.00.055.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.650 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.044 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.058 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.889 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.890 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.890 I llama_new_context_with_model: graph nodes  = 967
0.00.066.891 I llama_new_context_with_model: graph splits = 2
0.00.066.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.117 I 
0.00.473.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.206 I perplexity: tokenizing the input ..
0.00.480.840 I perplexity: tokenization took 7.632 ms
0.00.480.844 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.052 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.331 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.345 I llama_perf_context_print:        load time =     464.31 ms
0.00.614.347 I llama_perf_context_print: prompt eval time =     131.97 ms /   128 tokens (    1.03 ms per token,   969.88 tokens per second)
0.00.614.348 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.349 I llama_perf_context_print:       total time =     141.23 ms /   129 tokens
0.00.614.904 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.077s
sys	0m0.079s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.570 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.852 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.853 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.854 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.854 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.855 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.856 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.861 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.649 I llama_model_loader: - type  f32:  194 tensors
0.00.023.650 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.650 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.650 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.802 I llm_load_vocab: special tokens cache size = 25
0.00.050.767 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.771 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.771 I llm_load_print_meta: arch             = gptneox
0.00.050.771 I llm_load_print_meta: vocab type       = BPE
0.00.050.772 I llm_load_print_meta: n_vocab          = 50304
0.00.050.772 I llm_load_print_meta: n_merges         = 50009
0.00.050.772 I llm_load_print_meta: vocab_only       = 0
0.00.050.772 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.772 I llm_load_print_meta: n_embd           = 2048
0.00.050.773 I llm_load_print_meta: n_layer          = 24
0.00.050.776 I llm_load_print_meta: n_head           = 16
0.00.050.777 I llm_load_print_meta: n_head_kv        = 16
0.00.050.777 I llm_load_print_meta: n_rot            = 32
0.00.050.777 I llm_load_print_meta: n_swa            = 0
0.00.050.777 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.778 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.778 I llm_load_print_meta: n_gqa            = 1
0.00.050.779 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.780 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.780 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.781 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.781 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.781 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.781 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.782 I llm_load_print_meta: n_ff             = 8192
0.00.050.782 I llm_load_print_meta: n_expert         = 0
0.00.050.782 I llm_load_print_meta: n_expert_used    = 0
0.00.050.783 I llm_load_print_meta: causal attn      = 1
0.00.050.783 I llm_load_print_meta: pooling type     = 0
0.00.050.783 I llm_load_print_meta: rope type        = 2
0.00.050.783 I llm_load_print_meta: rope scaling     = linear
0.00.050.783 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.784 I llm_load_print_meta: freq_scale_train = 1
0.00.050.784 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.784 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.784 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.785 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.785 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.785 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.785 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.785 I llm_load_print_meta: model type       = 1.4B
0.00.050.786 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.786 I llm_load_print_meta: model params     = 1.41 B
0.00.050.787 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.787 I llm_load_print_meta: general.name     = 1.4B
0.00.050.787 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.788 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.788 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.788 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.788 I llm_load_print_meta: LF token         = 128 ''
0.00.050.789 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.789 I llm_load_print_meta: max token length = 1024
0.00.052.796 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.796 I llm_load_tensors: offloading output layer to GPU
0.00.052.796 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.807 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.808 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.707 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.708 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.708 I llama_new_context_with_model: n_batch       = 2048
0.00.053.708 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.708 I llama_new_context_with_model: flash_attn    = 0
0.00.053.709 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.709 I llama_new_context_with_model: freq_scale    = 1
0.00.053.710 I ggml_metal_init: allocating
0.00.053.717 I ggml_metal_init: found device: Apple M4
0.00.053.720 I ggml_metal_init: picking default device: Apple M4
0.00.054.328 I ggml_metal_init: using embedded metal library
0.00.056.700 I ggml_metal_init: GPU name:   Apple M4
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.703 I ggml_metal_init: simdgroup reduction   = true
0.00.056.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.703 I ggml_metal_init: has bfloat            = true
0.00.056.703 I ggml_metal_init: use bfloat            = true
0.00.056.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.864 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.022 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.988 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.990 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.990 I llama_new_context_with_model: graph nodes  = 967
0.00.086.990 I llama_new_context_with_model: graph splits = 2
0.00.086.993 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.836 I main: llama threadpool init, n_threads = 4
0.00.619.877 I 
0.00.619.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.925 I 
0.00.620.168 I sampler seed: 1234
0.00.620.174 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.221 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.223 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.223 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.635 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.375.636 I llama_perf_context_print:        load time =     611.26 ms
0.01.375.636 I llama_perf_context_print: prompt eval time =      51.08 ms /     7 tokens (    7.30 ms per token,   137.05 tokens per second)
0.01.375.637 I llama_perf_context_print:        eval time =     701.30 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.375.637 I llama_perf_context_print:       total time =     755.80 ms /    70 tokens
0.01.375.811 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.111s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.904 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.597 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.990 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.990 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.991 I llama_model_loader: - type  f32:  194 tensors
0.00.022.991 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.991 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.991 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.571 I llm_load_vocab: special tokens cache size = 25
0.00.048.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.542 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.543 I llm_load_print_meta: arch             = gptneox
0.00.048.543 I llm_load_print_meta: vocab type       = BPE
0.00.048.543 I llm_load_print_meta: n_vocab          = 50304
0.00.048.543 I llm_load_print_meta: n_merges         = 50009
0.00.048.544 I llm_load_print_meta: vocab_only       = 0
0.00.048.544 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.544 I llm_load_print_meta: n_embd           = 2048
0.00.048.544 I llm_load_print_meta: n_layer          = 24
0.00.048.547 I llm_load_print_meta: n_head           = 16
0.00.048.547 I llm_load_print_meta: n_head_kv        = 16
0.00.048.548 I llm_load_print_meta: n_rot            = 32
0.00.048.548 I llm_load_print_meta: n_swa            = 0
0.00.048.548 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.548 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.549 I llm_load_print_meta: n_gqa            = 1
0.00.048.550 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.550 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.551 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.552 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.553 I llm_load_print_meta: n_ff             = 8192
0.00.048.553 I llm_load_print_meta: n_expert         = 0
0.00.048.553 I llm_load_print_meta: n_expert_used    = 0
0.00.048.553 I llm_load_print_meta: causal attn      = 1
0.00.048.553 I llm_load_print_meta: pooling type     = 0
0.00.048.553 I llm_load_print_meta: rope type        = 2
0.00.048.556 I llm_load_print_meta: rope scaling     = linear
0.00.048.557 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.557 I llm_load_print_meta: freq_scale_train = 1
0.00.048.557 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.557 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.557 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.558 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.558 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.558 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.558 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.558 I llm_load_print_meta: model type       = 1.4B
0.00.048.559 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.559 I llm_load_print_meta: model params     = 1.41 B
0.00.048.560 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.560 I llm_load_print_meta: general.name     = 1.4B
0.00.048.560 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.560 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: LF token         = 128 ''
0.00.048.561 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: max token length = 1024
0.00.050.257 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.257 I llm_load_tensors: offloading output layer to GPU
0.00.050.257 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.263 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.263 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.123 I llama_new_context_with_model: n_ctx         = 128
0.00.051.124 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.124 I llama_new_context_with_model: n_batch       = 128
0.00.051.124 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.124 I llama_new_context_with_model: flash_attn    = 0
0.00.051.125 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.125 I llama_new_context_with_model: freq_scale    = 1
0.00.051.125 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.126 I ggml_metal_init: allocating
0.00.051.129 I ggml_metal_init: found device: Apple M4
0.00.051.131 I ggml_metal_init: picking default device: Apple M4
0.00.051.663 I ggml_metal_init: using embedded metal library
0.00.053.991 I ggml_metal_init: GPU name:   Apple M4
0.00.053.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.994 I ggml_metal_init: simdgroup reduction   = true
0.00.053.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.994 I ggml_metal_init: has bfloat            = true
0.00.053.994 I ggml_metal_init: use bfloat            = true
0.00.053.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.303 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.513 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.516 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.531 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.406 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.407 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.407 I llama_new_context_with_model: graph nodes  = 967
0.00.065.407 I llama_new_context_with_model: graph splits = 2
0.00.065.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.658 I 
0.00.543.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.721 I perplexity: tokenizing the input ..
0.00.551.378 I perplexity: tokenization took 7.654 ms
0.00.551.382 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.685.736 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.686.890 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.686.904 I llama_perf_context_print:        load time =     534.75 ms
0.00.686.905 I llama_perf_context_print: prompt eval time =     134.13 ms /   128 tokens (    1.05 ms per token,   954.31 tokens per second)
0.00.686.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.686.908 I llama_perf_context_print:       total time =     143.25 ms /   129 tokens
0.00.687.375 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.076s
sys	0m0.092s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.917 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.980 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.770 I llama_model_loader: - type  f32:  194 tensors
0.00.024.770 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.771 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.682 I llm_load_vocab: special tokens cache size = 25
0.00.050.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.520 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.520 I llm_load_print_meta: arch             = gptneox
0.00.050.520 I llm_load_print_meta: vocab type       = BPE
0.00.050.521 I llm_load_print_meta: n_vocab          = 50304
0.00.050.521 I llm_load_print_meta: n_merges         = 50009
0.00.050.521 I llm_load_print_meta: vocab_only       = 0
0.00.050.521 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.521 I llm_load_print_meta: n_embd           = 2048
0.00.050.521 I llm_load_print_meta: n_layer          = 24
0.00.050.524 I llm_load_print_meta: n_head           = 16
0.00.050.525 I llm_load_print_meta: n_head_kv        = 16
0.00.050.525 I llm_load_print_meta: n_rot            = 32
0.00.050.525 I llm_load_print_meta: n_swa            = 0
0.00.050.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.526 I llm_load_print_meta: n_gqa            = 1
0.00.050.527 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.528 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.528 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.531 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.532 I llm_load_print_meta: n_ff             = 8192
0.00.050.532 I llm_load_print_meta: n_expert         = 0
0.00.050.533 I llm_load_print_meta: n_expert_used    = 0
0.00.050.533 I llm_load_print_meta: causal attn      = 1
0.00.050.533 I llm_load_print_meta: pooling type     = 0
0.00.050.533 I llm_load_print_meta: rope type        = 2
0.00.050.533 I llm_load_print_meta: rope scaling     = linear
0.00.050.534 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.534 I llm_load_print_meta: freq_scale_train = 1
0.00.050.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.534 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.536 I llm_load_print_meta: model type       = 1.4B
0.00.050.536 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.537 I llm_load_print_meta: model params     = 1.41 B
0.00.050.537 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.538 I llm_load_print_meta: general.name     = 1.4B
0.00.050.538 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.538 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.538 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.538 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.539 I llm_load_print_meta: LF token         = 128 ''
0.00.050.539 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.539 I llm_load_print_meta: max token length = 1024
0.00.052.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.292 I llm_load_tensors: offloading output layer to GPU
0.00.052.293 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.298 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.299 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.149 I llama_new_context_with_model: n_batch       = 2048
0.00.053.149 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.149 I llama_new_context_with_model: flash_attn    = 0
0.00.053.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.150 I llama_new_context_with_model: freq_scale    = 1
0.00.053.150 I ggml_metal_init: allocating
0.00.053.153 I ggml_metal_init: found device: Apple M4
0.00.053.155 I ggml_metal_init: picking default device: Apple M4
0.00.053.716 I ggml_metal_init: using embedded metal library
0.00.056.048 I ggml_metal_init: GPU name:   Apple M4
0.00.056.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.050 I ggml_metal_init: simdgroup reduction   = true
0.00.056.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.051 I ggml_metal_init: has bfloat            = true
0.00.056.051 I ggml_metal_init: use bfloat            = true
0.00.056.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.052 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.513 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.205 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.213 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.235 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.306 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.307 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.307 I llama_new_context_with_model: graph nodes  = 967
0.00.086.308 I llama_new_context_with_model: graph splits = 2
0.00.086.314 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.175 I main: llama threadpool init, n_threads = 4
0.00.714.212 I 
0.00.714.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.233 I 
0.00.714.461 I sampler seed: 1234
0.00.714.465 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.476 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.476 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.476 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.563.818 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.563.819 I llama_perf_context_print:        load time =     704.25 ms
0.01.563.820 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.64 tokens per second)
0.01.563.822 I llama_perf_context_print:        eval time =     794.77 ms /    63 runs   (   12.62 ms per token,    79.27 tokens per second)
0.01.563.822 I llama_perf_context_print:       total time =     849.65 ms /    70 tokens
0.01.564.078 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.858 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.559 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.559 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.562 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.562 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.178 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.178 I llama_model_loader: - type  f32:  194 tensors
0.00.024.179 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.179 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.736 I llm_load_vocab: special tokens cache size = 25
0.00.050.834 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.837 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.837 I llm_load_print_meta: arch             = gptneox
0.00.050.837 I llm_load_print_meta: vocab type       = BPE
0.00.050.838 I llm_load_print_meta: n_vocab          = 50304
0.00.050.838 I llm_load_print_meta: n_merges         = 50009
0.00.050.838 I llm_load_print_meta: vocab_only       = 0
0.00.050.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.838 I llm_load_print_meta: n_embd           = 2048
0.00.050.838 I llm_load_print_meta: n_layer          = 24
0.00.050.841 I llm_load_print_meta: n_head           = 16
0.00.050.842 I llm_load_print_meta: n_head_kv        = 16
0.00.050.842 I llm_load_print_meta: n_rot            = 32
0.00.050.843 I llm_load_print_meta: n_swa            = 0
0.00.050.843 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.843 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.844 I llm_load_print_meta: n_gqa            = 1
0.00.050.844 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.845 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.846 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.846 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.846 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.846 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.850 I llm_load_print_meta: n_ff             = 8192
0.00.050.850 I llm_load_print_meta: n_expert         = 0
0.00.050.850 I llm_load_print_meta: n_expert_used    = 0
0.00.050.850 I llm_load_print_meta: causal attn      = 1
0.00.050.850 I llm_load_print_meta: pooling type     = 0
0.00.050.851 I llm_load_print_meta: rope type        = 2
0.00.050.851 I llm_load_print_meta: rope scaling     = linear
0.00.050.851 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.852 I llm_load_print_meta: freq_scale_train = 1
0.00.050.852 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.852 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.852 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.852 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.853 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.853 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.853 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.853 I llm_load_print_meta: model type       = 1.4B
0.00.050.854 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.854 I llm_load_print_meta: model params     = 1.41 B
0.00.050.855 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.855 I llm_load_print_meta: general.name     = 1.4B
0.00.050.855 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: LF token         = 128 ''
0.00.050.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: max token length = 1024
0.00.052.884 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.884 I llm_load_tensors: offloading output layer to GPU
0.00.052.884 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.895 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.896 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.797 I llama_new_context_with_model: n_ctx         = 128
0.00.053.798 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.798 I llama_new_context_with_model: n_batch       = 128
0.00.053.798 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.798 I llama_new_context_with_model: flash_attn    = 0
0.00.053.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.799 I llama_new_context_with_model: freq_scale    = 1
0.00.053.799 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.800 I ggml_metal_init: allocating
0.00.053.805 I ggml_metal_init: found device: Apple M4
0.00.053.808 I ggml_metal_init: picking default device: Apple M4
0.00.054.376 I ggml_metal_init: using embedded metal library
0.00.056.717 I ggml_metal_init: GPU name:   Apple M4
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.719 I ggml_metal_init: simdgroup reduction   = true
0.00.056.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.720 I ggml_metal_init: has bfloat            = true
0.00.056.720 I ggml_metal_init: use bfloat            = true
0.00.056.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.132 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.403 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.418 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.293 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.294 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.294 I llama_new_context_with_model: graph nodes  = 967
0.00.068.295 I llama_new_context_with_model: graph splits = 2
0.00.068.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.587 I 
0.00.638.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.644 I perplexity: tokenizing the input ..
0.00.646.504 I perplexity: tokenization took 7.859 ms
0.00.646.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.301 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.787.652 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.787.666 I llama_perf_context_print:        load time =     628.72 ms
0.00.787.667 I llama_perf_context_print: prompt eval time =     139.55 ms /   128 tokens (    1.09 ms per token,   917.26 tokens per second)
0.00.787.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.668 I llama_perf_context_print:       total time =     149.08 ms /   129 tokens
0.00.788.055 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.078s
sys	0m0.110s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.206 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.455 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.228 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.062 I llama_model_loader: - type  f32:  194 tensors
0.00.024.062 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.384 I llm_load_vocab: special tokens cache size = 25
0.00.051.477 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.481 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.482 I llm_load_print_meta: arch             = gptneox
0.00.051.482 I llm_load_print_meta: vocab type       = BPE
0.00.051.482 I llm_load_print_meta: n_vocab          = 50304
0.00.051.482 I llm_load_print_meta: n_merges         = 50009
0.00.051.485 I llm_load_print_meta: vocab_only       = 0
0.00.051.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.486 I llm_load_print_meta: n_embd           = 2048
0.00.051.486 I llm_load_print_meta: n_layer          = 24
0.00.051.490 I llm_load_print_meta: n_head           = 16
0.00.051.491 I llm_load_print_meta: n_head_kv        = 16
0.00.051.491 I llm_load_print_meta: n_rot            = 32
0.00.051.491 I llm_load_print_meta: n_swa            = 0
0.00.051.491 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.491 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.493 I llm_load_print_meta: n_gqa            = 1
0.00.051.494 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.494 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.495 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.495 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.495 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.496 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.496 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.496 I llm_load_print_meta: n_ff             = 8192
0.00.051.497 I llm_load_print_meta: n_expert         = 0
0.00.051.497 I llm_load_print_meta: n_expert_used    = 0
0.00.051.497 I llm_load_print_meta: causal attn      = 1
0.00.051.499 I llm_load_print_meta: pooling type     = 0
0.00.051.500 I llm_load_print_meta: rope type        = 2
0.00.051.500 I llm_load_print_meta: rope scaling     = linear
0.00.051.500 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.500 I llm_load_print_meta: freq_scale_train = 1
0.00.051.501 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.501 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.501 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.501 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.502 I llm_load_print_meta: model type       = 1.4B
0.00.051.502 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.502 I llm_load_print_meta: model params     = 1.41 B
0.00.051.503 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.503 I llm_load_print_meta: general.name     = 1.4B
0.00.051.503 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.503 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.503 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.503 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.504 I llm_load_print_meta: LF token         = 128 ''
0.00.051.504 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.504 I llm_load_print_meta: max token length = 1024
0.00.053.653 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.653 I llm_load_tensors: offloading output layer to GPU
0.00.053.653 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.664 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.666 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.632 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.633 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.633 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.633 I llama_new_context_with_model: n_batch       = 2048
0.00.054.633 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.634 I llama_new_context_with_model: flash_attn    = 0
0.00.054.634 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.634 I llama_new_context_with_model: freq_scale    = 1
0.00.054.635 I ggml_metal_init: allocating
0.00.054.639 I ggml_metal_init: found device: Apple M4
0.00.054.641 I ggml_metal_init: picking default device: Apple M4
0.00.055.308 I ggml_metal_init: using embedded metal library
0.00.057.840 I ggml_metal_init: GPU name:   Apple M4
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.843 I ggml_metal_init: simdgroup reduction   = true
0.00.057.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.845 I ggml_metal_init: has bfloat            = true
0.00.057.846 I ggml_metal_init: use bfloat            = true
0.00.057.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.147 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.647 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.660 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.678 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.698 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.699 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.699 I llama_new_context_with_model: graph nodes  = 967
0.00.088.699 I llama_new_context_with_model: graph splits = 2
0.00.088.702 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.092 I main: llama threadpool init, n_threads = 4
0.00.806.129 I 
0.00.806.154 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.154 I 
0.00.806.393 I sampler seed: 1234
0.00.806.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.409 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.409 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.680.015 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.01.680.015 I llama_perf_context_print:        load time =     796.88 ms
0.01.680.016 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.86 tokens per second)
0.01.680.017 I llama_perf_context_print:        eval time =     816.55 ms /    63 runs   (   12.96 ms per token,    77.15 tokens per second)
0.01.680.017 I llama_perf_context_print:       total time =     873.93 ms /    70 tokens
0.01.680.260 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4414 (78c67851) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.180 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.964 I llama_model_loader: - type  f32:  194 tensors
0.00.024.965 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.574 I llm_load_vocab: special tokens cache size = 25
0.00.051.580 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.584 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.584 I llm_load_print_meta: arch             = gptneox
0.00.051.584 I llm_load_print_meta: vocab type       = BPE
0.00.051.585 I llm_load_print_meta: n_vocab          = 50304
0.00.051.585 I llm_load_print_meta: n_merges         = 50009
0.00.051.585 I llm_load_print_meta: vocab_only       = 0
0.00.051.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.585 I llm_load_print_meta: n_embd           = 2048
0.00.051.587 I llm_load_print_meta: n_layer          = 24
0.00.051.591 I llm_load_print_meta: n_head           = 16
0.00.051.592 I llm_load_print_meta: n_head_kv        = 16
0.00.051.592 I llm_load_print_meta: n_rot            = 32
0.00.051.592 I llm_load_print_meta: n_swa            = 0
0.00.051.592 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.594 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.595 I llm_load_print_meta: n_gqa            = 1
0.00.051.596 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.596 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.597 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.597 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.597 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.597 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.598 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.598 I llm_load_print_meta: n_ff             = 8192
0.00.051.598 I llm_load_print_meta: n_expert         = 0
0.00.051.598 I llm_load_print_meta: n_expert_used    = 0
0.00.051.598 I llm_load_print_meta: causal attn      = 1
0.00.051.599 I llm_load_print_meta: pooling type     = 0
0.00.051.599 I llm_load_print_meta: rope type        = 2
0.00.051.617 I llm_load_print_meta: rope scaling     = linear
0.00.051.619 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.620 I llm_load_print_meta: freq_scale_train = 1
0.00.051.621 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.621 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.621 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.621 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.621 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.624 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.624 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.624 I llm_load_print_meta: model type       = 1.4B
0.00.051.624 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.625 I llm_load_print_meta: model params     = 1.41 B
0.00.051.625 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.625 I llm_load_print_meta: general.name     = 1.4B
0.00.051.627 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.627 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.627 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.627 I llm_load_print_meta: LF token         = 128 ''
0.00.051.629 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.629 I llm_load_print_meta: max token length = 1024
0.00.053.523 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.523 I llm_load_tensors: offloading output layer to GPU
0.00.053.524 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.535 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.536 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.535 I llama_new_context_with_model: n_ctx         = 128
0.00.054.535 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.535 I llama_new_context_with_model: n_batch       = 128
0.00.054.535 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.536 I llama_new_context_with_model: flash_attn    = 0
0.00.054.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.537 I llama_new_context_with_model: freq_scale    = 1
0.00.054.537 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.538 I ggml_metal_init: allocating
0.00.054.544 I ggml_metal_init: found device: Apple M4
0.00.054.547 I ggml_metal_init: picking default device: Apple M4
0.00.055.152 I ggml_metal_init: using embedded metal library
0.00.057.570 I ggml_metal_init: GPU name:   Apple M4
0.00.057.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.573 I ggml_metal_init: simdgroup reduction   = true
0.00.057.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.573 I ggml_metal_init: has bfloat            = true
0.00.057.573 I ggml_metal_init: use bfloat            = true
0.00.057.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.751 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.019 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.028 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.935 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.936 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.936 I llama_new_context_with_model: graph nodes  = 967
0.00.069.937 I llama_new_context_with_model: graph splits = 2
0.00.069.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.321 I 
0.00.543.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.361 I perplexity: tokenizing the input ..
0.00.550.979 I perplexity: tokenization took 7.616 ms
0.00.550.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.691.270 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.692.567 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.692.581 I llama_perf_context_print:        load time =     534.35 ms
0.00.692.582 I llama_perf_context_print: prompt eval time =     140.06 ms /   128 tokens (    1.09 ms per token,   913.89 tokens per second)
0.00.692.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.583 I llama_perf_context_print:       total time =     149.26 ms /   129 tokens
0.00.693.073 I ggml_metal_free: deallocating

real	0m0.708s
user	0m0.079s
sys	0m0.096s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4414 (78c67851)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127907740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127907e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127908400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1279089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127908f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127909510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127909ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12790a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12790a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12790ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12790b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12790b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12790c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12790c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12790d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12790d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12790de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12790e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12790ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12790f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12790fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127910290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1279109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127911250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127911c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127912240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127912eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1279133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1279136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127913b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127913e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1279146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127914be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127914ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127915340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1279157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127915c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127916120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1279165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127916a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127916f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1279173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127917840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127917b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127918110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127918720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127919040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127919650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127919c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12791a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12791a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12791ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12791b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12791bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12791c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12791c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12791c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12791cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12791d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12791d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12791ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12791e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12791e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12791ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12791f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12791f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12791f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12791fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1279202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127920790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127920c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1279210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127921620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127921b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1279220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127922610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127922b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1279230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127923600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127923b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1279240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1279245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127924b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127925090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1279255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127925b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127926080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1279265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127926b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127927070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1279275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127927b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127928060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1279285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127928b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127929050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127918d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1279294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127929c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12792a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12792a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12792ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12792b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12792b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12792bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12792c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12792c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12792cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12792d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12792d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12792dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12792e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12792e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12792eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12792ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12792f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12792f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12792fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1279301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127930680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127930b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127930fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127931460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127931900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127931da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127932240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1279326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127932b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127933020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1279334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127933960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127933e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1279342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127934740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127934be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127935080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127935520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1279359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127935e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127936300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1279367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127936c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1279370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127937580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127937a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127937ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127938360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127938800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127938ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127939140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1279395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127939a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127939f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12793a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12793a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12793ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12793b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12793b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12793bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12793bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12793c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12793c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12793cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12793d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12793d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12793db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12793dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12793e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12793e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12793edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12793f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12793f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12793fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127940040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1279404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127940980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127940e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1279412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127941760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127941c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1279420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127942540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1279429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127942e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127943320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1279437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127943c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127944100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1279445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127944a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127944ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127945380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1279458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127945e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127946370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1279468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127946b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127947190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1279477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127947db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1279485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127948a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127948d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127949310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127949920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12794a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12794a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12794aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12794aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12794b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12794bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12794c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12794c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12794cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12794d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12794d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12794dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12794e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12794e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12794ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12794f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12794f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12794fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127950100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127950650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127950ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1279510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127951640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127951b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1279520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127952630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127952b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1279530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127953620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127953b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1279540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127954610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127954b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1279550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127955600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127955b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1279560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1279565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127956b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127957090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1279575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127957b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127958080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1279585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127958b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127959070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1279595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127959b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12795a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12795a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12795ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12795b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12795b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12795baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12795c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12795c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12795cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12795d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12795d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12795dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12795e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12795e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12795e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12795ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12795f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12795f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12795fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127960080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127960520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1279609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127960e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127961300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1279617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127961c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1279620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127962580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127962ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1279631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127963910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127964030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127964750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127964a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127965200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1279654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127965ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.137.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127b08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127b08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127b09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127b094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127b09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127b09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127b0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127b0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127b0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127b0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127b0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127b0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127b0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127b0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127b0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127b0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127b0e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127b0eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127b0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127b0f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127b100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127b10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127b10f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127b11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127b11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127b12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127b122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127b12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127b12bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127b13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127b13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127b13a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127b13ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127b14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127b145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127b14a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127b14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127b154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127b159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127b15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127b163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127b168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127b16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127b172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127b177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127b17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127b180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127b18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127b18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127b18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127b19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127b196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127b19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127b19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127b1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127b1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127b1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127b1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127b1b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127b1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127b1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127b1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127b1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127b1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127b1d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127b1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127b1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127b1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127b1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127b1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127b1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127b1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127b1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127b202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127b20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127b20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127b212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127b21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127b21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127b222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127b227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127b22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127b23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127b237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127b23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127b24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127b247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127b24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127b25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127b257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127b25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127b26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127b267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127b26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127b27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127b277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127b27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127b28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127b28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127b28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127b29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127b29780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127b29cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127b2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127b2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127b2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127b2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127b2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127b2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127b2c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127b2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127b2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127b2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127b2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127b2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127b2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127b2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127b2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127b2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127b2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127b2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127b2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127b30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127b304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127b30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127b30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127b312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127b31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127b31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127b32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127b32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127b329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127b32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127b33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127b337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127b33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127b340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127b34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127b34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127b34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127b35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127b35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127b35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127b36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127b365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127b36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127b36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127b373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127b37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127b37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127b381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127b38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127b38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127b38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127b39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127b398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127b39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127b3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127b3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127b3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127b3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127b3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127b3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127b3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127b3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127b3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127b3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127b3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127b3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127b3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127b3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127b3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127b3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127b3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127b3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127b3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127b3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127b3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127b40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127b407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127b40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127b41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127b415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127b41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127b41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127b42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127b42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127b42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127b43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127b43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127b43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127b43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127b443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127b44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127b44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127b453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127b45930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127b45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127b46200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127b46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127b46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127b47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127b47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127b47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127b48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127b48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127b49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127b49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127b49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127b49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127b4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127b4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127b4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127b4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127b4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127b4c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127b4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127b4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127b4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127b4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127b4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127b4e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127b4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127b4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127b4f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127b4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127b4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127b50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127b506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127b50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127b51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127b516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127b51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127b52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127b52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127b52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127b53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127b53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127b53bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127b54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127b54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127b54bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127b55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127b55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127b55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127b56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127b56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127b56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127b570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127b57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127b57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127b580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127b58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127b58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127b590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127b59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127b59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127b5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127b5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127b5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127b5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127b5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127b5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127b5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127b5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127b5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127b5d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127b5d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127b5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127b5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127b5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127b5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127b5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127b5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127b5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127b5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127b5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127b60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127b60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127b60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127b61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127b615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127b61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127b62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127b62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127b630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127b637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127b63a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127b64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127b64530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127b64b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127b647f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127b464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127b45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127b46ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127b1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127b48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127b0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127b08320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127b1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127b63d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127b1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127b48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127b0b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127b652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127b658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127b65ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127b65e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127b66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127b663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127b666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127b66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127b66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127b66ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127b671a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127b67460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127b67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127b679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127b67ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127b67f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127b68220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127b684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127b687a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127b68a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127b68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127b68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127b692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127b69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127b69820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127b69ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127b69da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127b6a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127b6a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127b6a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127b6a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127b6ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127b6ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127b6b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127b6b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127b6b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127b6b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127b6bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127b6bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127b6c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127b6c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127b6c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127b6c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127b6cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127b6cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127b6d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127b6d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127b6d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127b6da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127b6dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127b6dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127b6e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127b6e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127b6e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127b6eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127b6ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127b6f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127b6f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127b6f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127b6f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127b6fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127b6fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127b700a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127b70360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127b70620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127b708e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127b70ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127b70e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127b71120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127b713e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127b716a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127b71960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127b71c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127b71ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127b721a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127b72460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127b72720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127b729e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127b72ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127b72f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127b73220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127b734e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127b737a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127b73a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127b73d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127b73fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127b742a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127b74560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127b74820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127b74ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127b74da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127b75060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127b75320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127b755e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127b758a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127b75b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127b75e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127b760e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127b763a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127b76660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127b76920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127b76be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127b76ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127b77160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127b77420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127b776e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127b779a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127b77c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127b77f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127b781e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127b784a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127b78760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127b78a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127b78ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127b78fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127b79260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127b79520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127b797e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127b79aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127b79d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127b7a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127b7a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127b7a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127b7a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127b7ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127b7ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127b7b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127b7b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127b7b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127b7b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127b7bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127b7be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127b7c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127b7c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127b7c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127b7c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127b7cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127b7cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127b7d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127b7d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127b7d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127b7d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127b7dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127b7df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127b7e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127b7e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127b7e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127b7ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127b7ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127b7efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127b7f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127b7f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127b7f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127b7fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127b7fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127b80060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127b80320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127b805e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127b808a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127b80b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127b80e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127b810e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127b813a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127b81660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127b81920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127b81be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127b81ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127b82160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127b82420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127b826e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127b829a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127b82c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127b82f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127b831e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127b834a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127b83760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127b83a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127b83ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127b83fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127b84260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127b84520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127b847e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127b84aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127b84d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127b85020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127b852e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127b855a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127b85860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127b85b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127b85de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127b860a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127b86360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127b86620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127b868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127b86ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127b86e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127b87120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127b876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127b879b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127b87c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127b87f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127b881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127b884b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127b88770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127b88a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127b88cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127b88fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127b89270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127b89530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127b897f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127b89ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127b89d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127b8a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127b8a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127b8a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127b8a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127b8ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127b8adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127b8b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127b8b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127b8b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127b8b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127b8bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127b8be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127b8c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127b8c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127b8c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127b8c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127b8cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127b8cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127b8d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127b8d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127b8d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127b8d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127b8dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127b8df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127b8e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127b8e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127b8e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127b8ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127b8ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127b8eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127b8f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127b8f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127b8f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127b8faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127b90040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127b90590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127b90ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127b91030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127b91580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127b91ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127b92020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127b92570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127b92830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127b92af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127b92ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127b934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127b939f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127b93ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127b943f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127b948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127b94df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127b952f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127b957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127b95cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127b961f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127b966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127b96bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127b970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127b97b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127b98220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127b98940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127b99060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127b99320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127b99b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127b99dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127b9a3e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.801s
user	0m0.291s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4414 (78c67851)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13af10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13af10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13af10f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13af114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13af11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13af12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13af125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13af12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13af13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13af13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13af13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13af14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13af14b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13af15320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13af15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13af16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13af16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13af17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13af177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13af17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13af186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13af18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13af194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13af19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13af1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13af1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13af1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13af1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13af1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13af1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13af1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13af1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13af1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13af1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13af1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13af1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13af1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13af1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13af1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13af1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13af1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13af1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13af1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13af20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13af20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13af20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13af21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13af21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13af22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13af22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13af22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13af233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13af239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13af23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13af247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13af24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13af25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13af253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13af259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13af261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13af26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13af26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13af26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13af27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13af27700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13af27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13af28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13af284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13af28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13af28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13af292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13af29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13af29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13af2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13af2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13af2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13af2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13af2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13af2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13af2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13af2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13af2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13af2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13af2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13af2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13af2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13af2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13af2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13af2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13af2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13af2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13af300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13af30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13af30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13af310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13af31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13af31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13af21860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13af31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13af327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13af32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13af33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13af33790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13af33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13af34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13af34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13af34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13af35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13af35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13af35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13af36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13af36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13af36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13af37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13af375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13af37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13af37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13af383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13af38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13af38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13af391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13af39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13af39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13af39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13af3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13af3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13af3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13af3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13af3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13af3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13af3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13af3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13af3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13af3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13af3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13af3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13af3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13af3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13af3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13af3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13af3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13af3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13af3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13af3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13af400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13af40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13af409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13af40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13af41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13af417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13af41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13af42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13af425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13af42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13af42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13af43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13af43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13af43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13af44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13af44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13af44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13af44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13af453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13af45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13af45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13af461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13af46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13af46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13af46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13af47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13af478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13af47d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13af48230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13af486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13af48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13af49010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13af494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13af49950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13af49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13af4a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13af4a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13af4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13af4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13af4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13af4b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13af4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13af4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13af4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13af4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13af4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13af4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13af4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13af4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13af4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13af4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13af4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13af4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13af4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13af4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13af502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13af508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13af510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13af51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13af51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13af51e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13af52450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13af52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13af530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13af53580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13af53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13af541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13af54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13af54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13af551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13af55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13af55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13af561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13af56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13af56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13af571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13af576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13af57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13af58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13af586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13af58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13af59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13af596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13af59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13af5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13af5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13af5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13af5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13af5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13af5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13af5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13af5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13af5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13af5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13af5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13af5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13af5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13af5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13af5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13af5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13af5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13af5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13af60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13af60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13af60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13af61100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13af61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13af61ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13af620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13af62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13af62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13af630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13af63630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13af63b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13af640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13af64620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13af64b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13af650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13af65610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13af65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13af660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13af66600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13af66b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13af66ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13af67490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13af67930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13af67dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13af68270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13af68710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13af68bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13af69050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13af694f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13af69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13af69e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13af6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13af6a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13af6ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13af6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13af6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13af6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13af6c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13af6cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13af6d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13af6d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13af6dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13af6dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13af6e600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c0090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c00a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c00a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c00ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c00b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c00bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c00c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c00cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c00d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c00d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c00e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c00e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c00e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c00eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c00ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c00f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c00f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c00fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c0101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c0123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c0139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c0142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c0161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c0170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c0186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c01a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c01a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c01aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c01aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c01b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c01b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c01bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c01c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c01c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c01c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c01cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c01d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c01d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c01db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c01df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c01e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c01e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c01ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c01f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c01f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c01fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c01fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c0214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c0233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c0245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c0252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c0264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c0283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c0299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c02a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c02a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c02abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c02b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c02b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c02b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c02bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c02c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c02c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c02cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c02cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c02d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c02d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c02dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c02e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c02e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c02e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c02ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c02f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c02f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c02fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c0308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c0311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c0327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c0330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c0339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c0377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c0380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c0396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c03a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c03a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c03ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c03b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c03b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c03ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c03bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c03c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c03c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c03cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c03d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c03d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c03d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c03ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c03e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c03e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c03eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c03ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c03f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c03f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c03fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c0402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c0425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c0439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c0450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c0467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c0495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c04a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c04a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c04acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c04b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c04b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c04be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c04c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c04c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c04cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c04d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c04dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c04e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c04e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c04ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c04f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c04f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c04fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c0508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c0536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c0564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c05a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c05a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c05ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c05b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c05b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c05bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c05c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c05cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c05d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c05d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c05dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c05e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c05e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c05b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c04c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c04b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c048190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c045950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c055090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c0505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c04e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c0464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c043c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c048d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c049e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c04f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c04c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c053f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c046a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c04eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c042b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c04d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c048750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c052e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c04dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c0436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c045390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c055c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c04af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c0533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c0492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c04bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c04fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c04a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c047050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c051710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c045f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c054510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c051cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c04d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c056790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c044dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c0561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c044250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c054ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c04e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c050b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c053990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c052290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c04a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c041d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c004680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c05da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c007a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c05ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c05f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c05f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c05f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c05fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c05fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c05ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c0602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c060560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c060820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c060ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c060da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c061060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c061320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c0615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c0618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c061b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c061e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c0620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c0623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c062660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c062920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c062be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c062ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c063160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c063420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c0636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c0639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c063c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c063f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c0641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c0644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c064760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c064a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c064ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c064fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c065260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c065520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c0657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c065aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c065d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c066020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c0662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c0665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c066860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c066b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c066de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c0670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c067360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c067620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c0678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c067ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c067e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c068120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c0683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c0686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c068960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c068c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c068ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c0691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c069460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c069720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c0699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c069ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c069f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c06a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c06a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c06a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c06aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c06ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c06afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c06b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c06b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c06b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c06bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c06bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c06c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c06c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c06c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c06c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c06cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c06ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c06d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c06d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c06d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c06d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c06dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c06dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c06e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c06e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c06e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c06e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c06ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c06ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c06f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c06f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c06f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c06fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c06fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c06ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c070260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c070520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c0707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c070aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c070d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c071020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c0712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c0715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c071860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c071b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c071de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c0720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c072360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c072620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c0728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c072ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c072e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c073120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c0733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c0736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c073960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c073c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c073ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c0741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c074460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c074720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c0749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c074ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c074f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c075220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c0754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c0757a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c075a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c075d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c075fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c0762a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c076560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c076820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c076ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c076da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c077060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c077320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c0775e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c0778a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c077b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c077e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c0780e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c0783a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c078660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c078920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c078be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c078ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c079160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c079420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c0796e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c0799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c079c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c07a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c07a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c07a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c07aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c07ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c07aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c07b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c07b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c07bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c07c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c07c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c07cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c07d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c07d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c07daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c07dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c07e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c07ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c07efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c07f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c07fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c07ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c080520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c080a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c080fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c081510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c081a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c081fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c082500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c082a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c082fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c0834f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c083a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c083f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c0844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c084a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c084f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c0854d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c085a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c085f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c0864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c086a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c086f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c0874b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c087a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c087f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c0884a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c0889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c088f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c089490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c0899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c089f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c08a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c08a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c08af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c08b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c08b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c08bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c08bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c08c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c08c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c08ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c08d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c08d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c08dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c08e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c08e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c08ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c08f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c08f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c08fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c090040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c090540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c090f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c091670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c091d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c0924b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c092770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c092f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c093220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c093830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.924s
user	0m0.242s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.62 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.16 sec*proc (2 tests)

Total Test time (real) =   1.17 sec
        1.19 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
