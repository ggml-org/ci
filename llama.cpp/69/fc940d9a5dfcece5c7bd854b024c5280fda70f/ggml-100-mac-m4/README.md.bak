### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.69 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.65 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.34 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.95 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.00 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.30 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    1.12 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.47 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.33 sec*proc (28 tests)

Total Test time (real) = 224.34 sec

real	3m44.369s
user	7m39.063s
sys	0m6.353s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.38 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.38 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.51 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.21 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.95 sec*proc (28 tests)

Total Test time (real) =  51.97 sec

real	0m51.978s
user	1m12.338s
sys	0m5.646s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.178 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.393 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.701 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.711 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.031.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.712 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.031.713 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.031.713 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.031.715 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.031.716 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.031.716 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.031.717 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.031.718 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.031.725 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.031.726 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.031.727 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.031.728 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.031.728 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.031.729 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.031.730 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.036.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.038.192 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.195 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.038.195 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.038.196 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.038.196 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.038.197 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.038.198 I llama_model_loader: - type  f32:  124 tensors
0.00.038.198 I llama_model_loader: - type  f16:   73 tensors
0.00.038.199 I print_info: file format = GGUF V3 (latest)
0.00.038.200 I print_info: file type   = F16
0.00.038.202 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.042.905 I load: special tokens cache size = 5
0.00.045.288 I load: token to piece cache size = 0.2032 MB
0.00.045.294 I print_info: arch             = bert
0.00.045.294 I print_info: vocab_only       = 0
0.00.045.294 I print_info: n_ctx_train      = 512
0.00.045.294 I print_info: n_embd           = 384
0.00.045.295 I print_info: n_layer          = 12
0.00.045.298 I print_info: n_head           = 12
0.00.045.300 I print_info: n_head_kv        = 12
0.00.045.300 I print_info: n_rot            = 32
0.00.045.300 I print_info: n_swa            = 0
0.00.045.303 I print_info: n_embd_head_k    = 32
0.00.045.303 I print_info: n_embd_head_v    = 32
0.00.045.304 I print_info: n_gqa            = 1
0.00.045.305 I print_info: n_embd_k_gqa     = 384
0.00.045.306 I print_info: n_embd_v_gqa     = 384
0.00.045.307 I print_info: f_norm_eps       = 1.0e-12
0.00.045.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.317 I print_info: f_logit_scale    = 0.0e+00
0.00.045.318 I print_info: n_ff             = 1536
0.00.045.319 I print_info: n_expert         = 0
0.00.045.319 I print_info: n_expert_used    = 0
0.00.045.319 I print_info: causal attn      = 0
0.00.045.319 I print_info: pooling type     = 2
0.00.045.320 I print_info: rope type        = 2
0.00.045.320 I print_info: rope scaling     = linear
0.00.045.321 I print_info: freq_base_train  = 10000.0
0.00.045.321 I print_info: freq_scale_train = 1
0.00.045.322 I print_info: n_ctx_orig_yarn  = 512
0.00.045.328 I print_info: rope_finetuned   = unknown
0.00.045.328 I print_info: ssm_d_conv       = 0
0.00.045.328 I print_info: ssm_d_inner      = 0
0.00.045.328 I print_info: ssm_d_state      = 0
0.00.045.329 I print_info: ssm_dt_rank      = 0
0.00.045.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.330 I print_info: model type       = 33M
0.00.045.330 I print_info: model params     = 33.21 M
0.00.045.330 I print_info: general.name     = Bge Small
0.00.045.332 I print_info: vocab type       = WPM
0.00.045.332 I print_info: n_vocab          = 30522
0.00.045.333 I print_info: n_merges         = 0
0.00.045.333 I print_info: BOS token        = 101 '[CLS]'
0.00.045.333 I print_info: UNK token        = 100 '[UNK]'
0.00.045.334 I print_info: SEP token        = 102 '[SEP]'
0.00.045.334 I print_info: PAD token        = 0 '[PAD]'
0.00.045.334 I print_info: MASK token       = 103 '[MASK]'
0.00.045.335 I print_info: LF token         = 0 '[PAD]'
0.00.045.335 I print_info: max token length = 21
0.00.047.552 I load_tensors: offloading 12 repeating layers to GPU
0.00.047.553 I load_tensors: offloading output layer to GPU
0.00.047.554 I load_tensors: offloaded 13/13 layers to GPU
0.00.047.583 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.047.585 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.047.884 I llama_init_from_model: n_seq_max     = 1
0.00.047.886 I llama_init_from_model: n_ctx         = 512
0.00.047.886 I llama_init_from_model: n_ctx_per_seq = 512
0.00.047.886 I llama_init_from_model: n_batch       = 2048
0.00.047.887 I llama_init_from_model: n_ubatch      = 2048
0.00.047.887 I llama_init_from_model: flash_attn    = 0
0.00.047.888 I llama_init_from_model: freq_base     = 10000.0
0.00.047.888 I llama_init_from_model: freq_scale    = 1
0.00.047.889 I ggml_metal_init: allocating
0.00.047.893 I ggml_metal_init: found device: Apple M4
0.00.047.896 I ggml_metal_init: picking default device: Apple M4
0.00.048.828 I ggml_metal_init: using embedded metal library
0.00.053.207 I ggml_metal_init: GPU name:   Apple M4
0.00.053.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.211 I ggml_metal_init: simdgroup reduction   = true
0.00.053.211 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.211 I ggml_metal_init: has bfloat            = true
0.00.053.211 I ggml_metal_init: use bfloat            = true
0.00.053.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.058 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.066.792 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.066.796 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.066.797 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.067.570 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.067.572 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.067.573 I llama_init_from_model: graph nodes  = 429
0.00.067.573 I llama_init_from_model: graph splits = 2
0.00.067.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.067.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.078.093 I 
0.00.078.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.078.762 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.083.476 I llama_perf_context_print:        load time =      52.69 ms
0.00.083.477 I llama_perf_context_print: prompt eval time =       4.57 ms /     9 tokens (    0.51 ms per token,  1970.23 tokens per second)
0.00.083.478 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.083.478 I llama_perf_context_print:       total time =       5.38 ms /    10 tokens
0.00.083.628 I ggml_metal_free: deallocating

real	0m0.270s
user	0m0.067s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.340 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.106 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.111 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.116 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.117 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.117 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.119 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.119 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.120 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.120 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.120 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.122 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.123 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.123 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.124 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.124 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.124 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.564 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.198 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.199 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.199 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.199 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.200 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.200 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.200 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.201 I llama_model_loader: - type  f32:  124 tensors
0.00.015.201 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.202 I print_info: file format = GGUF V3 (latest)
0.00.015.202 I print_info: file type   = Q8_0
0.00.015.203 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.780 I load: special tokens cache size = 5
0.00.019.095 I load: token to piece cache size = 0.2032 MB
0.00.019.098 I print_info: arch             = bert
0.00.019.099 I print_info: vocab_only       = 0
0.00.019.099 I print_info: n_ctx_train      = 512
0.00.019.099 I print_info: n_embd           = 384
0.00.019.099 I print_info: n_layer          = 12
0.00.019.102 I print_info: n_head           = 12
0.00.019.102 I print_info: n_head_kv        = 12
0.00.019.102 I print_info: n_rot            = 32
0.00.019.103 I print_info: n_swa            = 0
0.00.019.103 I print_info: n_embd_head_k    = 32
0.00.019.103 I print_info: n_embd_head_v    = 32
0.00.019.103 I print_info: n_gqa            = 1
0.00.019.104 I print_info: n_embd_k_gqa     = 384
0.00.019.104 I print_info: n_embd_v_gqa     = 384
0.00.019.105 I print_info: f_norm_eps       = 1.0e-12
0.00.019.105 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.106 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.106 I print_info: f_logit_scale    = 0.0e+00
0.00.019.106 I print_info: n_ff             = 1536
0.00.019.107 I print_info: n_expert         = 0
0.00.019.107 I print_info: n_expert_used    = 0
0.00.019.107 I print_info: causal attn      = 0
0.00.019.107 I print_info: pooling type     = 2
0.00.019.107 I print_info: rope type        = 2
0.00.019.107 I print_info: rope scaling     = linear
0.00.019.108 I print_info: freq_base_train  = 10000.0
0.00.019.108 I print_info: freq_scale_train = 1
0.00.019.108 I print_info: n_ctx_orig_yarn  = 512
0.00.019.108 I print_info: rope_finetuned   = unknown
0.00.019.108 I print_info: ssm_d_conv       = 0
0.00.019.108 I print_info: ssm_d_inner      = 0
0.00.019.108 I print_info: ssm_d_state      = 0
0.00.019.110 I print_info: ssm_dt_rank      = 0
0.00.019.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.111 I print_info: model type       = 33M
0.00.019.111 I print_info: model params     = 33.21 M
0.00.019.111 I print_info: general.name     = Bge Small
0.00.019.112 I print_info: vocab type       = WPM
0.00.019.114 I print_info: n_vocab          = 30522
0.00.019.114 I print_info: n_merges         = 0
0.00.019.114 I print_info: BOS token        = 101 '[CLS]'
0.00.019.114 I print_info: UNK token        = 100 '[UNK]'
0.00.019.114 I print_info: SEP token        = 102 '[SEP]'
0.00.019.114 I print_info: PAD token        = 0 '[PAD]'
0.00.019.115 I print_info: MASK token       = 103 '[MASK]'
0.00.019.115 I print_info: LF token         = 0 '[PAD]'
0.00.019.115 I print_info: max token length = 21
0.00.020.397 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.398 I load_tensors: offloading output layer to GPU
0.00.020.398 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.405 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.406 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.556 I llama_init_from_model: n_seq_max     = 1
0.00.020.556 I llama_init_from_model: n_ctx         = 512
0.00.020.557 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.557 I llama_init_from_model: n_batch       = 2048
0.00.020.557 I llama_init_from_model: n_ubatch      = 2048
0.00.020.557 I llama_init_from_model: flash_attn    = 0
0.00.020.558 I llama_init_from_model: freq_base     = 10000.0
0.00.020.558 I llama_init_from_model: freq_scale    = 1
0.00.020.558 I ggml_metal_init: allocating
0.00.020.561 I ggml_metal_init: found device: Apple M4
0.00.020.563 I ggml_metal_init: picking default device: Apple M4
0.00.021.191 I ggml_metal_init: using embedded metal library
0.00.023.707 I ggml_metal_init: GPU name:   Apple M4
0.00.023.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.710 I ggml_metal_init: simdgroup reduction   = true
0.00.023.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.710 I ggml_metal_init: has bfloat            = true
0.00.023.710 I ggml_metal_init: use bfloat            = true
0.00.023.711 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.206 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.670 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.672 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.675 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.322 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.324 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.324 I llama_init_from_model: graph nodes  = 429
0.00.035.324 I llama_init_from_model: graph splits = 2
0.00.035.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.112 I 
0.00.040.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.656 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.914 I llama_perf_context_print:        load time =      30.76 ms
0.00.044.914 I llama_perf_context_print: prompt eval time =       4.13 ms /     9 tokens (    0.46 ms per token,  2180.23 tokens per second)
0.00.044.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.915 I llama_perf_context_print:       total time =       4.80 ms /    10 tokens
0.00.045.088 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.198 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.089 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.644 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.652 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.654 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.655 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.655 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.657 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.657 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.658 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.659 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.659 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.662 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.663 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.664 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.963 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.963 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.964 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.964 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.965 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.965 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.965 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.968 I llama_model_loader: - type  f32:   40 tensors
0.00.049.968 I llama_model_loader: - type  f16:   30 tensors
0.00.049.969 I print_info: file format = GGUF V3 (latest)
0.00.049.970 I print_info: file type   = F16
0.00.049.972 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.067.354 W load: empty token at index 5
0.00.072.013 W load: model vocab missing newline token, using special_pad_id instead
0.00.073.375 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.407 I load: special tokens cache size = 5
0.00.330.618 I load: token to piece cache size = 1.5060 MB
0.00.330.624 I print_info: arch             = jina-bert-v2
0.00.330.624 I print_info: vocab_only       = 0
0.00.330.624 I print_info: n_ctx_train      = 8192
0.00.330.625 I print_info: n_embd           = 384
0.00.330.625 I print_info: n_layer          = 4
0.00.330.631 I print_info: n_head           = 12
0.00.330.634 I print_info: n_head_kv        = 12
0.00.330.634 I print_info: n_rot            = 32
0.00.330.634 I print_info: n_swa            = 0
0.00.330.634 I print_info: n_embd_head_k    = 32
0.00.330.634 I print_info: n_embd_head_v    = 32
0.00.330.635 I print_info: n_gqa            = 1
0.00.330.636 I print_info: n_embd_k_gqa     = 384
0.00.330.636 I print_info: n_embd_v_gqa     = 384
0.00.330.637 I print_info: f_norm_eps       = 1.0e-12
0.00.330.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.330.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.330.641 I print_info: f_max_alibi_bias = 8.0e+00
0.00.330.641 I print_info: f_logit_scale    = 0.0e+00
0.00.330.642 I print_info: n_ff             = 1536
0.00.330.642 I print_info: n_expert         = 0
0.00.330.642 I print_info: n_expert_used    = 0
0.00.330.643 I print_info: causal attn      = 0
0.00.330.643 I print_info: pooling type     = -1
0.00.330.643 I print_info: rope type        = -1
0.00.330.643 I print_info: rope scaling     = linear
0.00.330.643 I print_info: freq_base_train  = 10000.0
0.00.330.644 I print_info: freq_scale_train = 1
0.00.330.644 I print_info: n_ctx_orig_yarn  = 8192
0.00.330.644 I print_info: rope_finetuned   = unknown
0.00.330.644 I print_info: ssm_d_conv       = 0
0.00.330.644 I print_info: ssm_d_inner      = 0
0.00.330.644 I print_info: ssm_d_state      = 0
0.00.330.646 I print_info: ssm_dt_rank      = 0
0.00.330.646 I print_info: ssm_dt_b_c_rms   = 0
0.00.330.646 I print_info: model type       = 33M
0.00.330.646 I print_info: model params     = 32.90 M
0.00.330.647 I print_info: general.name     = Jina Bert Implementation
0.00.330.647 I print_info: vocab type       = BPE
0.00.330.648 I print_info: n_vocab          = 61056
0.00.330.648 I print_info: n_merges         = 39382
0.00.330.648 I print_info: BOS token        = 0 '<s>'
0.00.330.648 I print_info: EOS token        = 2 '</s>'
0.00.330.648 I print_info: UNK token        = 3 '<unk>'
0.00.330.649 I print_info: SEP token        = 2 '</s>'
0.00.330.649 I print_info: PAD token        = 1 '<pad>'
0.00.330.649 I print_info: MASK token       = 4 '<mask>'
0.00.330.653 I print_info: EOG token        = 2 '</s>'
0.00.330.654 I print_info: max token length = 45
0.00.331.718 I load_tensors: offloading 4 repeating layers to GPU
0.00.331.718 I load_tensors: offloading output layer to GPU
0.00.331.718 I load_tensors: offloaded 5/5 layers to GPU
0.00.331.743 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.744 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.331.964 I llama_init_from_model: n_seq_max     = 1
0.00.331.965 I llama_init_from_model: n_ctx         = 8192
0.00.331.965 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.331.965 I llama_init_from_model: n_batch       = 2048
0.00.331.965 I llama_init_from_model: n_ubatch      = 2048
0.00.331.965 I llama_init_from_model: flash_attn    = 0
0.00.331.966 I llama_init_from_model: freq_base     = 10000.0
0.00.331.966 I llama_init_from_model: freq_scale    = 1
0.00.331.967 I ggml_metal_init: allocating
0.00.331.969 I ggml_metal_init: found device: Apple M4
0.00.331.971 I ggml_metal_init: picking default device: Apple M4
0.00.332.764 I ggml_metal_init: using embedded metal library
0.00.335.415 I ggml_metal_init: GPU name:   Apple M4
0.00.335.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.417 I ggml_metal_init: simdgroup reduction   = true
0.00.335.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.417 I ggml_metal_init: has bfloat            = true
0.00.335.418 I ggml_metal_init: use bfloat            = true
0.00.335.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.046 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.560 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.562 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.564 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.238 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.239 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.239 I llama_init_from_model: graph nodes  = 154
0.00.348.239 I llama_init_from_model: graph splits = 2
0.00.348.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.120 I 
0.00.360.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.287 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.288 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.290 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.291 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.293 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.295 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.787 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.473 I llama_perf_context_print:        load time =     338.03 ms
0.00.364.474 I llama_perf_context_print: prompt eval time =       3.68 ms /    62 tokens (    0.06 ms per token, 16861.57 tokens per second)
0.00.364.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.475 I llama_perf_context_print:       total time =       4.35 ms /    63 tokens
0.00.364.718 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.338s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.175 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.291 I main: llama backend init
0.00.000.299 I main: load the model and apply lora adapter, if any
0.00.036.523 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.024 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.048 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.067.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.611 I llama_model_loader: - type  f32:  194 tensors
0.00.067.611 I llama_model_loader: - type  f16:   98 tensors
0.00.067.613 I print_info: file format = GGUF V3 (latest)
0.00.067.614 I print_info: file type   = all F32 (guessed)
0.00.067.616 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.570 I load: special tokens cache size = 25
0.00.108.177 I load: token to piece cache size = 0.2984 MB
0.00.108.180 I print_info: arch             = gptneox
0.00.108.180 I print_info: vocab_only       = 0
0.00.108.181 I print_info: n_ctx_train      = 2048
0.00.108.181 I print_info: n_embd           = 2048
0.00.108.181 I print_info: n_layer          = 24
0.00.108.184 I print_info: n_head           = 16
0.00.108.185 I print_info: n_head_kv        = 16
0.00.108.185 I print_info: n_rot            = 32
0.00.108.186 I print_info: n_swa            = 0
0.00.108.186 I print_info: n_embd_head_k    = 128
0.00.108.186 I print_info: n_embd_head_v    = 128
0.00.108.187 I print_info: n_gqa            = 1
0.00.108.187 I print_info: n_embd_k_gqa     = 2048
0.00.108.188 I print_info: n_embd_v_gqa     = 2048
0.00.108.189 I print_info: f_norm_eps       = 1.0e-05
0.00.108.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.189 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.190 I print_info: f_logit_scale    = 0.0e+00
0.00.108.190 I print_info: n_ff             = 8192
0.00.108.191 I print_info: n_expert         = 0
0.00.108.191 I print_info: n_expert_used    = 0
0.00.108.191 I print_info: causal attn      = 1
0.00.108.191 I print_info: pooling type     = 0
0.00.108.191 I print_info: rope type        = 2
0.00.108.191 I print_info: rope scaling     = linear
0.00.108.192 I print_info: freq_base_train  = 10000.0
0.00.108.192 I print_info: freq_scale_train = 1
0.00.108.192 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.192 I print_info: rope_finetuned   = unknown
0.00.108.193 I print_info: ssm_d_conv       = 0
0.00.108.193 I print_info: ssm_d_inner      = 0
0.00.108.193 I print_info: ssm_d_state      = 0
0.00.108.193 I print_info: ssm_dt_rank      = 0
0.00.108.193 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.193 I print_info: model type       = 1.4B
0.00.108.194 I print_info: model params     = 1.41 B
0.00.108.194 I print_info: general.name     = 1.4B
0.00.108.194 I print_info: vocab type       = BPE
0.00.108.195 I print_info: n_vocab          = 50304
0.00.108.195 I print_info: n_merges         = 50009
0.00.108.195 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.195 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.195 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.196 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.196 I print_info: LF token         = 128 'Ä'
0.00.108.196 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.196 I print_info: max token length = 1024
0.00.110.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.110.875 I load_tensors: offloading output layer to GPU
0.00.110.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.110.894 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.895 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.111.219 I llama_init_from_model: n_seq_max     = 1
0.00.111.220 I llama_init_from_model: n_ctx         = 2048
0.00.111.220 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.111.220 I llama_init_from_model: n_batch       = 2048
0.00.111.220 I llama_init_from_model: n_ubatch      = 512
0.00.111.220 I llama_init_from_model: flash_attn    = 0
0.00.111.221 I llama_init_from_model: freq_base     = 10000.0
0.00.111.221 I llama_init_from_model: freq_scale    = 1
0.00.111.221 I ggml_metal_init: allocating
0.00.111.224 I ggml_metal_init: found device: Apple M4
0.00.111.227 I ggml_metal_init: picking default device: Apple M4
0.00.111.904 I ggml_metal_init: using embedded metal library
0.00.124.016 I ggml_metal_init: GPU name:   Apple M4
0.00.124.018 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.124.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.124.019 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.124.019 I ggml_metal_init: simdgroup reduction   = true
0.00.124.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.124.020 I ggml_metal_init: has bfloat            = true
0.00.124.020 I ggml_metal_init: use bfloat            = true
0.00.124.020 I ggml_metal_init: hasUnifiedMemory      = true
0.00.124.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.149.588 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.171.050 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.171.059 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.171.083 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.172.124 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.172.127 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.172.128 I llama_init_from_model: graph nodes  = 967
0.00.172.128 I llama_init_from_model: graph splits = 2
0.00.172.131 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.172.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.172.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.254.236 I main: llama threadpool init, n_threads = 4
0.00.254.280 I 
0.00.254.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.254.306 I 
0.00.254.375 I sampler seed: 1234
0.00.254.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.254.406 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.254.408 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.254.408 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.088.087 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.02.088.088 I llama_perf_context_print:        load time =     217.70 ms
0.02.088.089 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.85 tokens per second)
0.02.088.089 I llama_perf_context_print:        eval time =    1787.23 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.088.090 I llama_perf_context_print:       total time =    1833.85 ms /    70 tokens
0.02.088.296 I ggml_metal_free: deallocating

real	0m2.380s
user	0m0.148s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.448 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.366 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.576 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.591 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.148 I llama_model_loader: - type  f32:  194 tensors
0.00.054.149 I llama_model_loader: - type  f16:   98 tensors
0.00.054.150 I print_info: file format = GGUF V3 (latest)
0.00.054.151 I print_info: file type   = all F32 (guessed)
0.00.054.153 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.681 I load: special tokens cache size = 25
0.00.090.771 I load: token to piece cache size = 0.2984 MB
0.00.090.775 I print_info: arch             = gptneox
0.00.090.775 I print_info: vocab_only       = 0
0.00.090.775 I print_info: n_ctx_train      = 2048
0.00.090.776 I print_info: n_embd           = 2048
0.00.090.776 I print_info: n_layer          = 24
0.00.090.779 I print_info: n_head           = 16
0.00.090.780 I print_info: n_head_kv        = 16
0.00.090.780 I print_info: n_rot            = 32
0.00.090.780 I print_info: n_swa            = 0
0.00.090.782 I print_info: n_embd_head_k    = 128
0.00.090.782 I print_info: n_embd_head_v    = 128
0.00.090.783 I print_info: n_gqa            = 1
0.00.090.784 I print_info: n_embd_k_gqa     = 2048
0.00.090.784 I print_info: n_embd_v_gqa     = 2048
0.00.090.785 I print_info: f_norm_eps       = 1.0e-05
0.00.090.785 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.786 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.786 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.786 I print_info: f_logit_scale    = 0.0e+00
0.00.090.787 I print_info: n_ff             = 8192
0.00.090.787 I print_info: n_expert         = 0
0.00.090.787 I print_info: n_expert_used    = 0
0.00.090.787 I print_info: causal attn      = 1
0.00.090.787 I print_info: pooling type     = 0
0.00.090.787 I print_info: rope type        = 2
0.00.090.787 I print_info: rope scaling     = linear
0.00.090.788 I print_info: freq_base_train  = 10000.0
0.00.090.788 I print_info: freq_scale_train = 1
0.00.090.788 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.788 I print_info: rope_finetuned   = unknown
0.00.090.789 I print_info: ssm_d_conv       = 0
0.00.090.790 I print_info: ssm_d_inner      = 0
0.00.090.790 I print_info: ssm_d_state      = 0
0.00.090.790 I print_info: ssm_dt_rank      = 0
0.00.090.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.791 I print_info: model type       = 1.4B
0.00.090.791 I print_info: model params     = 1.41 B
0.00.090.791 I print_info: general.name     = 1.4B
0.00.090.792 I print_info: vocab type       = BPE
0.00.090.792 I print_info: n_vocab          = 50304
0.00.090.792 I print_info: n_merges         = 50009
0.00.090.792 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.792 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.793 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.793 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.793 I print_info: LF token         = 128 'Ä'
0.00.090.794 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.794 I print_info: max token length = 1024
0.00.093.389 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.389 I load_tensors: offloading output layer to GPU
0.00.093.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.401 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.402 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.706 I llama_init_from_model: n_seq_max     = 1
0.00.093.707 I llama_init_from_model: n_ctx         = 128
0.00.093.707 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.707 I llama_init_from_model: n_batch       = 128
0.00.093.707 I llama_init_from_model: n_ubatch      = 128
0.00.093.708 I llama_init_from_model: flash_attn    = 0
0.00.093.708 I llama_init_from_model: freq_base     = 10000.0
0.00.093.708 I llama_init_from_model: freq_scale    = 1
0.00.093.708 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.709 I ggml_metal_init: allocating
0.00.093.712 I ggml_metal_init: found device: Apple M4
0.00.093.714 I ggml_metal_init: picking default device: Apple M4
0.00.094.355 I ggml_metal_init: using embedded metal library
0.00.097.074 I ggml_metal_init: GPU name:   Apple M4
0.00.097.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.077 I ggml_metal_init: simdgroup reduction   = true
0.00.097.077 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.077 I ggml_metal_init: has bfloat            = true
0.00.097.077 I ggml_metal_init: use bfloat            = true
0.00.097.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.433 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.772 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.774 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.788 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.623 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.624 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.624 I llama_init_from_model: graph nodes  = 967
0.00.108.624 I llama_init_from_model: graph splits = 2
0.00.108.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.047.681 I 
0.01.047.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.047.731 I perplexity: tokenizing the input ..
0.01.061.055 I perplexity: tokenization took 13.313 ms
0.01.061.062 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.185.074 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.187.019 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.187.081 I llama_perf_context_print:        load time =    1025.30 ms
0.01.187.082 I llama_perf_context_print: prompt eval time =     123.12 ms /   128 tokens (    0.96 ms per token,  1039.66 tokens per second)
0.01.187.084 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.187.084 I llama_perf_context_print:       total time =     139.40 ms /   129 tokens
0.01.187.921 I ggml_metal_free: deallocating

real	0m1.380s
user	0m0.127s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.774 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.415 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.416 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.418 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.420 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.420 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.420 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.188 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.190 I llama_model_loader: - type  f32:  194 tensors
0.00.037.190 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.191 I print_info: file format = GGUF V3 (latest)
0.00.037.192 I print_info: file type   = Q8_0
0.00.037.193 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.059.304 I load: special tokens cache size = 25
0.00.066.263 I load: token to piece cache size = 0.2984 MB
0.00.066.267 I print_info: arch             = gptneox
0.00.066.268 I print_info: vocab_only       = 0
0.00.066.268 I print_info: n_ctx_train      = 2048
0.00.066.268 I print_info: n_embd           = 2048
0.00.066.268 I print_info: n_layer          = 24
0.00.066.275 I print_info: n_head           = 16
0.00.066.277 I print_info: n_head_kv        = 16
0.00.066.277 I print_info: n_rot            = 32
0.00.066.277 I print_info: n_swa            = 0
0.00.066.278 I print_info: n_embd_head_k    = 128
0.00.066.278 I print_info: n_embd_head_v    = 128
0.00.066.278 I print_info: n_gqa            = 1
0.00.066.279 I print_info: n_embd_k_gqa     = 2048
0.00.066.280 I print_info: n_embd_v_gqa     = 2048
0.00.066.280 I print_info: f_norm_eps       = 1.0e-05
0.00.066.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.281 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.281 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.281 I print_info: f_logit_scale    = 0.0e+00
0.00.066.282 I print_info: n_ff             = 8192
0.00.066.282 I print_info: n_expert         = 0
0.00.066.282 I print_info: n_expert_used    = 0
0.00.066.282 I print_info: causal attn      = 1
0.00.066.282 I print_info: pooling type     = 0
0.00.066.282 I print_info: rope type        = 2
0.00.066.283 I print_info: rope scaling     = linear
0.00.066.284 I print_info: freq_base_train  = 10000.0
0.00.066.284 I print_info: freq_scale_train = 1
0.00.066.284 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.285 I print_info: rope_finetuned   = unknown
0.00.066.285 I print_info: ssm_d_conv       = 0
0.00.066.285 I print_info: ssm_d_inner      = 0
0.00.066.285 I print_info: ssm_d_state      = 0
0.00.066.286 I print_info: ssm_dt_rank      = 0
0.00.066.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.286 I print_info: model type       = 1.4B
0.00.066.287 I print_info: model params     = 1.41 B
0.00.066.287 I print_info: general.name     = 1.4B
0.00.066.288 I print_info: vocab type       = BPE
0.00.066.289 I print_info: n_vocab          = 50304
0.00.066.289 I print_info: n_merges         = 50009
0.00.066.289 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.290 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.290 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.290 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.290 I print_info: LF token         = 128 'Ä'
0.00.066.290 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.290 I print_info: max token length = 1024
0.00.068.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.490 I load_tensors: offloading output layer to GPU
0.00.068.490 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.497 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.498 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.868 I llama_init_from_model: n_seq_max     = 1
0.00.068.869 I llama_init_from_model: n_ctx         = 2048
0.00.068.869 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.068.870 I llama_init_from_model: n_batch       = 2048
0.00.068.870 I llama_init_from_model: n_ubatch      = 512
0.00.068.870 I llama_init_from_model: flash_attn    = 0
0.00.068.871 I llama_init_from_model: freq_base     = 10000.0
0.00.068.871 I llama_init_from_model: freq_scale    = 1
0.00.068.871 I ggml_metal_init: allocating
0.00.068.874 I ggml_metal_init: found device: Apple M4
0.00.068.876 I ggml_metal_init: picking default device: Apple M4
0.00.069.630 I ggml_metal_init: using embedded metal library
0.00.072.560 I ggml_metal_init: GPU name:   Apple M4
0.00.072.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.563 I ggml_metal_init: simdgroup reduction   = true
0.00.072.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.564 I ggml_metal_init: has bfloat            = true
0.00.072.564 I ggml_metal_init: use bfloat            = true
0.00.072.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.782 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.540 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.552 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.576 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.111.849 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.111.851 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.111.852 I llama_init_from_model: graph nodes  = 967
0.00.111.852 I llama_init_from_model: graph splits = 2
0.00.111.857 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.336.646 I main: llama threadpool init, n_threads = 4
0.01.336.697 I 
0.01.336.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.336.726 I 
0.01.337.058 I sampler seed: 1234
0.01.337.066 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.337.129 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.337.130 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.337.130 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.435.136 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.02.435.137 I llama_perf_context_print:        load time =    1326.87 ms
0.02.435.137 I llama_perf_context_print: prompt eval time =      49.58 ms /     7 tokens (    7.08 ms per token,   141.18 tokens per second)
0.02.435.139 I llama_perf_context_print:        eval time =    1045.35 ms /    63 runs   (   16.59 ms per token,    60.27 tokens per second)
0.02.435.140 I llama_perf_context_print:       total time =    1098.49 ms /    70 tokens
0.02.435.384 I ggml_metal_free: deallocating

real	0m2.453s
user	0m0.124s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.874 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.726 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.727 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.041 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.045 I llama_model_loader: - type  f32:  194 tensors
0.00.035.045 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.046 I print_info: file format = GGUF V3 (latest)
0.00.035.047 I print_info: file type   = Q8_0
0.00.035.048 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.227 I load: special tokens cache size = 25
0.00.066.858 I load: token to piece cache size = 0.2984 MB
0.00.066.861 I print_info: arch             = gptneox
0.00.066.861 I print_info: vocab_only       = 0
0.00.066.862 I print_info: n_ctx_train      = 2048
0.00.066.862 I print_info: n_embd           = 2048
0.00.066.862 I print_info: n_layer          = 24
0.00.066.866 I print_info: n_head           = 16
0.00.066.867 I print_info: n_head_kv        = 16
0.00.066.867 I print_info: n_rot            = 32
0.00.066.867 I print_info: n_swa            = 0
0.00.066.867 I print_info: n_embd_head_k    = 128
0.00.066.867 I print_info: n_embd_head_v    = 128
0.00.066.868 I print_info: n_gqa            = 1
0.00.066.868 I print_info: n_embd_k_gqa     = 2048
0.00.066.869 I print_info: n_embd_v_gqa     = 2048
0.00.066.870 I print_info: f_norm_eps       = 1.0e-05
0.00.066.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.870 I print_info: f_logit_scale    = 0.0e+00
0.00.066.871 I print_info: n_ff             = 8192
0.00.066.871 I print_info: n_expert         = 0
0.00.066.871 I print_info: n_expert_used    = 0
0.00.066.872 I print_info: causal attn      = 1
0.00.066.872 I print_info: pooling type     = 0
0.00.066.872 I print_info: rope type        = 2
0.00.066.872 I print_info: rope scaling     = linear
0.00.066.872 I print_info: freq_base_train  = 10000.0
0.00.066.873 I print_info: freq_scale_train = 1
0.00.066.873 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.873 I print_info: rope_finetuned   = unknown
0.00.066.873 I print_info: ssm_d_conv       = 0
0.00.066.874 I print_info: ssm_d_inner      = 0
0.00.066.874 I print_info: ssm_d_state      = 0
0.00.066.874 I print_info: ssm_dt_rank      = 0
0.00.066.874 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.874 I print_info: model type       = 1.4B
0.00.066.875 I print_info: model params     = 1.41 B
0.00.066.876 I print_info: general.name     = 1.4B
0.00.066.876 I print_info: vocab type       = BPE
0.00.066.876 I print_info: n_vocab          = 50304
0.00.066.876 I print_info: n_merges         = 50009
0.00.066.877 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.877 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.877 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.879 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.879 I print_info: LF token         = 128 'Ä'
0.00.066.880 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.880 I print_info: max token length = 1024
0.00.069.280 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.280 I load_tensors: offloading output layer to GPU
0.00.069.280 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.291 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.292 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.605 I llama_init_from_model: n_seq_max     = 1
0.00.069.606 I llama_init_from_model: n_ctx         = 128
0.00.069.606 I llama_init_from_model: n_ctx_per_seq = 128
0.00.069.606 I llama_init_from_model: n_batch       = 128
0.00.069.606 I llama_init_from_model: n_ubatch      = 128
0.00.069.606 I llama_init_from_model: flash_attn    = 0
0.00.069.607 I llama_init_from_model: freq_base     = 10000.0
0.00.069.607 I llama_init_from_model: freq_scale    = 1
0.00.069.607 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.608 I ggml_metal_init: allocating
0.00.069.610 I ggml_metal_init: found device: Apple M4
0.00.069.612 I ggml_metal_init: picking default device: Apple M4
0.00.070.284 I ggml_metal_init: using embedded metal library
0.00.072.947 I ggml_metal_init: GPU name:   Apple M4
0.00.072.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.950 I ggml_metal_init: simdgroup reduction   = true
0.00.072.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.950 I ggml_metal_init: has bfloat            = true
0.00.072.950 I ggml_metal_init: use bfloat            = true
0.00.072.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.599 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.953 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.958 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.977 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.921 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.923 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.923 I llama_init_from_model: graph nodes  = 967
0.00.084.923 I llama_init_from_model: graph splits = 2
0.00.084.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.846.846 I 
0.00.846.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.846.874 I perplexity: tokenizing the input ..
0.00.854.942 I perplexity: tokenization took 8.067 ms
0.00.854.945 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.979.539 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.980.705 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.980.725 I llama_perf_context_print:        load time =     834.97 ms
0.00.980.729 I llama_perf_context_print: prompt eval time =     124.37 ms /   128 tokens (    0.97 ms per token,  1029.20 tokens per second)
0.00.980.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.980.731 I llama_perf_context_print:       total time =     133.88 ms /   129 tokens
0.00.981.094 I ggml_metal_free: deallocating

real	0m0.999s
user	0m0.095s
sys	0m0.141s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.016.858 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.046 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.198 I llama_model_loader: - type  f32:  194 tensors
0.00.040.198 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.199 I print_info: file format = GGUF V3 (latest)
0.00.040.200 I print_info: file type   = Q4_0
0.00.040.201 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.865 I load: special tokens cache size = 25
0.00.069.343 I load: token to piece cache size = 0.2984 MB
0.00.069.347 I print_info: arch             = gptneox
0.00.069.347 I print_info: vocab_only       = 0
0.00.069.348 I print_info: n_ctx_train      = 2048
0.00.069.348 I print_info: n_embd           = 2048
0.00.069.348 I print_info: n_layer          = 24
0.00.069.352 I print_info: n_head           = 16
0.00.069.353 I print_info: n_head_kv        = 16
0.00.069.355 I print_info: n_rot            = 32
0.00.069.355 I print_info: n_swa            = 0
0.00.069.355 I print_info: n_embd_head_k    = 128
0.00.069.355 I print_info: n_embd_head_v    = 128
0.00.069.356 I print_info: n_gqa            = 1
0.00.069.356 I print_info: n_embd_k_gqa     = 2048
0.00.069.357 I print_info: n_embd_v_gqa     = 2048
0.00.069.358 I print_info: f_norm_eps       = 1.0e-05
0.00.069.358 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.358 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.358 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.358 I print_info: f_logit_scale    = 0.0e+00
0.00.069.359 I print_info: n_ff             = 8192
0.00.069.359 I print_info: n_expert         = 0
0.00.069.359 I print_info: n_expert_used    = 0
0.00.069.361 I print_info: causal attn      = 1
0.00.069.362 I print_info: pooling type     = 0
0.00.069.362 I print_info: rope type        = 2
0.00.069.362 I print_info: rope scaling     = linear
0.00.069.363 I print_info: freq_base_train  = 10000.0
0.00.069.363 I print_info: freq_scale_train = 1
0.00.069.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.364 I print_info: rope_finetuned   = unknown
0.00.069.364 I print_info: ssm_d_conv       = 0
0.00.069.367 I print_info: ssm_d_inner      = 0
0.00.069.367 I print_info: ssm_d_state      = 0
0.00.069.367 I print_info: ssm_dt_rank      = 0
0.00.069.367 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.368 I print_info: model type       = 1.4B
0.00.069.368 I print_info: model params     = 1.41 B
0.00.069.368 I print_info: general.name     = 1.4B
0.00.069.369 I print_info: vocab type       = BPE
0.00.069.369 I print_info: n_vocab          = 50304
0.00.069.369 I print_info: n_merges         = 50009
0.00.069.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.370 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.370 I print_info: LF token         = 128 'Ä'
0.00.069.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.371 I print_info: max token length = 1024
0.00.071.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.689 I load_tensors: offloading output layer to GPU
0.00.071.689 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.700 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.071.701 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.040 I llama_init_from_model: n_seq_max     = 1
0.00.072.041 I llama_init_from_model: n_ctx         = 2048
0.00.072.041 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.041 I llama_init_from_model: n_batch       = 2048
0.00.072.041 I llama_init_from_model: n_ubatch      = 512
0.00.072.041 I llama_init_from_model: flash_attn    = 0
0.00.072.042 I llama_init_from_model: freq_base     = 10000.0
0.00.072.042 I llama_init_from_model: freq_scale    = 1
0.00.072.042 I ggml_metal_init: allocating
0.00.072.045 I ggml_metal_init: found device: Apple M4
0.00.072.047 I ggml_metal_init: picking default device: Apple M4
0.00.072.829 I ggml_metal_init: using embedded metal library
0.00.075.679 I ggml_metal_init: GPU name:   Apple M4
0.00.075.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.682 I ggml_metal_init: simdgroup reduction   = true
0.00.075.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.682 I ggml_metal_init: has bfloat            = true
0.00.075.683 I ggml_metal_init: use bfloat            = true
0.00.075.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.136 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.672 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.680 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.706 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.116.016 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.116.018 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.116.019 I llama_init_from_model: graph nodes  = 967
0.00.116.019 I llama_init_from_model: graph splits = 2
0.00.116.023 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.504 I main: llama threadpool init, n_threads = 4
0.00.692.542 I 
0.00.692.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.566 I 
0.00.692.724 I sampler seed: 1234
0.00.692.728 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.738 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.738 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.738 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.381.120 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.381.121 I llama_perf_context_print:        load time =     675.64 ms
0.01.381.121 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.44 tokens per second)
0.01.381.122 I llama_perf_context_print:        eval time =     641.63 ms /    63 runs   (   10.18 ms per token,    98.19 tokens per second)
0.01.381.125 I llama_perf_context_print:       total time =     688.62 ms /    70 tokens
0.01.381.368 I ggml_metal_free: deallocating

real	0m1.399s
user	0m0.118s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.216 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.352 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.353 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.357 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.796 I llama_model_loader: - type  f32:  194 tensors
0.00.025.796 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.796 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.797 I print_info: file format = GGUF V3 (latest)
0.00.025.798 I print_info: file type   = Q4_0
0.00.025.798 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.712 I load: special tokens cache size = 25
0.00.050.649 I load: token to piece cache size = 0.2984 MB
0.00.050.652 I print_info: arch             = gptneox
0.00.050.653 I print_info: vocab_only       = 0
0.00.050.653 I print_info: n_ctx_train      = 2048
0.00.050.653 I print_info: n_embd           = 2048
0.00.050.653 I print_info: n_layer          = 24
0.00.050.656 I print_info: n_head           = 16
0.00.050.656 I print_info: n_head_kv        = 16
0.00.050.657 I print_info: n_rot            = 32
0.00.050.657 I print_info: n_swa            = 0
0.00.050.658 I print_info: n_embd_head_k    = 128
0.00.050.658 I print_info: n_embd_head_v    = 128
0.00.050.660 I print_info: n_gqa            = 1
0.00.050.661 I print_info: n_embd_k_gqa     = 2048
0.00.050.662 I print_info: n_embd_v_gqa     = 2048
0.00.050.662 I print_info: f_norm_eps       = 1.0e-05
0.00.050.663 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.663 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.663 I print_info: f_logit_scale    = 0.0e+00
0.00.050.664 I print_info: n_ff             = 8192
0.00.050.664 I print_info: n_expert         = 0
0.00.050.664 I print_info: n_expert_used    = 0
0.00.050.664 I print_info: causal attn      = 1
0.00.050.665 I print_info: pooling type     = 0
0.00.050.665 I print_info: rope type        = 2
0.00.050.665 I print_info: rope scaling     = linear
0.00.050.669 I print_info: freq_base_train  = 10000.0
0.00.050.670 I print_info: freq_scale_train = 1
0.00.050.670 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.670 I print_info: rope_finetuned   = unknown
0.00.050.670 I print_info: ssm_d_conv       = 0
0.00.050.671 I print_info: ssm_d_inner      = 0
0.00.050.671 I print_info: ssm_d_state      = 0
0.00.050.671 I print_info: ssm_dt_rank      = 0
0.00.050.671 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.672 I print_info: model type       = 1.4B
0.00.050.673 I print_info: model params     = 1.41 B
0.00.050.673 I print_info: general.name     = 1.4B
0.00.050.674 I print_info: vocab type       = BPE
0.00.050.674 I print_info: n_vocab          = 50304
0.00.050.674 I print_info: n_merges         = 50009
0.00.050.674 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.676 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.676 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.676 I print_info: LF token         = 128 'Ä'
0.00.050.676 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.676 I print_info: max token length = 1024
0.00.052.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.594 I load_tensors: offloading output layer to GPU
0.00.052.595 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.605 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.606 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.905 I llama_init_from_model: n_seq_max     = 1
0.00.052.906 I llama_init_from_model: n_ctx         = 128
0.00.052.906 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.906 I llama_init_from_model: n_batch       = 128
0.00.052.906 I llama_init_from_model: n_ubatch      = 128
0.00.052.906 I llama_init_from_model: flash_attn    = 0
0.00.052.907 I llama_init_from_model: freq_base     = 10000.0
0.00.052.907 I llama_init_from_model: freq_scale    = 1
0.00.052.907 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.908 I ggml_metal_init: allocating
0.00.052.911 I ggml_metal_init: found device: Apple M4
0.00.052.913 I ggml_metal_init: picking default device: Apple M4
0.00.053.478 I ggml_metal_init: using embedded metal library
0.00.055.828 I ggml_metal_init: GPU name:   Apple M4
0.00.055.829 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.830 I ggml_metal_init: simdgroup reduction   = true
0.00.055.830 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.831 I ggml_metal_init: has bfloat            = true
0.00.055.831 I ggml_metal_init: use bfloat            = true
0.00.055.831 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.569 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.793 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.795 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.809 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.765 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.765 I llama_init_from_model: graph nodes  = 967
0.00.067.765 I llama_init_from_model: graph splits = 2
0.00.067.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.579.760 I 
0.00.579.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.579.792 I perplexity: tokenizing the input ..
0.00.587.417 I perplexity: tokenization took 7.624 ms
0.00.587.426 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.710.203 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.711.349 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.711.379 I llama_perf_context_print:        load time =     569.54 ms
0.00.711.380 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.59 tokens per second)
0.00.711.381 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.381 I llama_perf_context_print:       total time =     131.62 ms /   129 tokens
0.00.711.847 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.077s
sys	0m0.097s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.649 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.832 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.554 I llama_model_loader: - type  f32:  194 tensors
0.00.027.554 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.554 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.555 I print_info: file format = GGUF V3 (latest)
0.00.027.556 I print_info: file type   = Q4_1
0.00.027.556 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.565 I load: special tokens cache size = 25
0.00.052.652 I load: token to piece cache size = 0.2984 MB
0.00.052.655 I print_info: arch             = gptneox
0.00.052.655 I print_info: vocab_only       = 0
0.00.052.656 I print_info: n_ctx_train      = 2048
0.00.052.656 I print_info: n_embd           = 2048
0.00.052.656 I print_info: n_layer          = 24
0.00.052.659 I print_info: n_head           = 16
0.00.052.660 I print_info: n_head_kv        = 16
0.00.052.660 I print_info: n_rot            = 32
0.00.052.661 I print_info: n_swa            = 0
0.00.052.661 I print_info: n_embd_head_k    = 128
0.00.052.661 I print_info: n_embd_head_v    = 128
0.00.052.662 I print_info: n_gqa            = 1
0.00.052.662 I print_info: n_embd_k_gqa     = 2048
0.00.052.663 I print_info: n_embd_v_gqa     = 2048
0.00.052.664 I print_info: f_norm_eps       = 1.0e-05
0.00.052.664 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.664 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.664 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.664 I print_info: f_logit_scale    = 0.0e+00
0.00.052.665 I print_info: n_ff             = 8192
0.00.052.665 I print_info: n_expert         = 0
0.00.052.665 I print_info: n_expert_used    = 0
0.00.052.667 I print_info: causal attn      = 1
0.00.052.668 I print_info: pooling type     = 0
0.00.052.669 I print_info: rope type        = 2
0.00.052.669 I print_info: rope scaling     = linear
0.00.052.669 I print_info: freq_base_train  = 10000.0
0.00.052.670 I print_info: freq_scale_train = 1
0.00.052.670 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.671 I print_info: rope_finetuned   = unknown
0.00.052.671 I print_info: ssm_d_conv       = 0
0.00.052.671 I print_info: ssm_d_inner      = 0
0.00.052.671 I print_info: ssm_d_state      = 0
0.00.052.672 I print_info: ssm_dt_rank      = 0
0.00.052.672 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.672 I print_info: model type       = 1.4B
0.00.052.672 I print_info: model params     = 1.41 B
0.00.052.672 I print_info: general.name     = 1.4B
0.00.052.673 I print_info: vocab type       = BPE
0.00.052.673 I print_info: n_vocab          = 50304
0.00.052.675 I print_info: n_merges         = 50009
0.00.052.675 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.675 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.676 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.676 I print_info: LF token         = 128 'Ä'
0.00.052.676 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.676 I print_info: max token length = 1024
0.00.054.606 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.606 I load_tensors: offloading output layer to GPU
0.00.054.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.617 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.618 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.906 I llama_init_from_model: n_seq_max     = 1
0.00.054.907 I llama_init_from_model: n_ctx         = 2048
0.00.054.907 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.907 I llama_init_from_model: n_batch       = 2048
0.00.054.907 I llama_init_from_model: n_ubatch      = 512
0.00.054.908 I llama_init_from_model: flash_attn    = 0
0.00.054.908 I llama_init_from_model: freq_base     = 10000.0
0.00.054.908 I llama_init_from_model: freq_scale    = 1
0.00.054.909 I ggml_metal_init: allocating
0.00.054.912 I ggml_metal_init: found device: Apple M4
0.00.054.914 I ggml_metal_init: picking default device: Apple M4
0.00.055.516 I ggml_metal_init: using embedded metal library
0.00.057.854 I ggml_metal_init: GPU name:   Apple M4
0.00.057.856 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.857 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.857 I ggml_metal_init: simdgroup reduction   = true
0.00.057.857 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.857 I ggml_metal_init: has bfloat            = true
0.00.057.857 I ggml_metal_init: use bfloat            = true
0.00.057.858 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.388 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.394 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.412 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.553 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.554 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.554 I llama_init_from_model: graph nodes  = 967
0.00.087.555 I llama_init_from_model: graph splits = 2
0.00.087.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.227 I main: llama threadpool init, n_threads = 4
0.00.769.266 I 
0.00.769.289 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.290 I 
0.00.769.531 I sampler seed: 1234
0.00.769.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.556 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.557 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.557 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.187 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63563.12 tokens per second)
0.01.497.187 I llama_perf_context_print:        load time =     760.57 ms
0.01.497.188 I llama_perf_context_print: prompt eval time =      43.53 ms /     7 tokens (    6.22 ms per token,   160.79 tokens per second)
0.01.497.189 I llama_perf_context_print:        eval time =     681.13 ms /    63 runs   (   10.81 ms per token,    92.49 tokens per second)
0.01.497.189 I llama_perf_context_print:       total time =     727.96 ms /    70 tokens
0.01.497.446 I ggml_metal_free: deallocating

real	0m1.513s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.401 I llama_model_loader: - type  f32:  194 tensors
0.00.024.401 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.401 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.401 I print_info: file format = GGUF V3 (latest)
0.00.024.402 I print_info: file type   = Q4_1
0.00.024.403 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.281 I load: special tokens cache size = 25
0.00.049.161 I load: token to piece cache size = 0.2984 MB
0.00.049.164 I print_info: arch             = gptneox
0.00.049.164 I print_info: vocab_only       = 0
0.00.049.164 I print_info: n_ctx_train      = 2048
0.00.049.164 I print_info: n_embd           = 2048
0.00.049.164 I print_info: n_layer          = 24
0.00.049.168 I print_info: n_head           = 16
0.00.049.169 I print_info: n_head_kv        = 16
0.00.049.169 I print_info: n_rot            = 32
0.00.049.169 I print_info: n_swa            = 0
0.00.049.169 I print_info: n_embd_head_k    = 128
0.00.049.169 I print_info: n_embd_head_v    = 128
0.00.049.170 I print_info: n_gqa            = 1
0.00.049.171 I print_info: n_embd_k_gqa     = 2048
0.00.049.171 I print_info: n_embd_v_gqa     = 2048
0.00.049.172 I print_info: f_norm_eps       = 1.0e-05
0.00.049.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.174 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.174 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.174 I print_info: f_logit_scale    = 0.0e+00
0.00.049.175 I print_info: n_ff             = 8192
0.00.049.175 I print_info: n_expert         = 0
0.00.049.175 I print_info: n_expert_used    = 0
0.00.049.175 I print_info: causal attn      = 1
0.00.049.175 I print_info: pooling type     = 0
0.00.049.176 I print_info: rope type        = 2
0.00.049.176 I print_info: rope scaling     = linear
0.00.049.178 I print_info: freq_base_train  = 10000.0
0.00.049.179 I print_info: freq_scale_train = 1
0.00.049.179 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.179 I print_info: rope_finetuned   = unknown
0.00.049.179 I print_info: ssm_d_conv       = 0
0.00.049.179 I print_info: ssm_d_inner      = 0
0.00.049.179 I print_info: ssm_d_state      = 0
0.00.049.180 I print_info: ssm_dt_rank      = 0
0.00.049.180 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.180 I print_info: model type       = 1.4B
0.00.049.180 I print_info: model params     = 1.41 B
0.00.049.180 I print_info: general.name     = 1.4B
0.00.049.181 I print_info: vocab type       = BPE
0.00.049.181 I print_info: n_vocab          = 50304
0.00.049.182 I print_info: n_merges         = 50009
0.00.049.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: LF token         = 128 'Ä'
0.00.049.187 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.189 I print_info: max token length = 1024
0.00.051.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.133 I load_tensors: offloading output layer to GPU
0.00.051.134 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.144 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.145 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.437 I llama_init_from_model: n_seq_max     = 1
0.00.051.437 I llama_init_from_model: n_ctx         = 128
0.00.051.437 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.437 I llama_init_from_model: n_batch       = 128
0.00.051.438 I llama_init_from_model: n_ubatch      = 128
0.00.051.438 I llama_init_from_model: flash_attn    = 0
0.00.051.438 I llama_init_from_model: freq_base     = 10000.0
0.00.051.438 I llama_init_from_model: freq_scale    = 1
0.00.051.439 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.439 I ggml_metal_init: allocating
0.00.051.442 I ggml_metal_init: found device: Apple M4
0.00.051.444 I ggml_metal_init: picking default device: Apple M4
0.00.052.036 I ggml_metal_init: using embedded metal library
0.00.054.400 I ggml_metal_init: GPU name:   Apple M4
0.00.054.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.402 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.403 I ggml_metal_init: simdgroup reduction   = true
0.00.054.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.403 I ggml_metal_init: has bfloat            = true
0.00.054.403 I ggml_metal_init: use bfloat            = true
0.00.054.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.071 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.301 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.303 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.268 I llama_init_from_model: graph nodes  = 967
0.00.066.268 I llama_init_from_model: graph splits = 2
0.00.066.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.971 I 
0.00.613.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.020 I perplexity: tokenizing the input ..
0.00.620.605 I perplexity: tokenization took 7.583 ms
0.00.620.609 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.358 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.744.534 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.744.565 I llama_perf_context_print:        load time =     604.08 ms
0.00.744.566 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.70 tokens per second)
0.00.744.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.568 I llama_perf_context_print:       total time =     131.59 ms /   129 tokens
0.00.745.095 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.077s
sys	0m0.089s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.404 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.033 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.034 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.034 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.041 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.042 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.473 I llama_model_loader: - type  f32:  194 tensors
0.00.026.474 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.475 I print_info: file format = GGUF V3 (latest)
0.00.026.475 I print_info: file type   = Q5_0
0.00.026.476 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.187 I load: special tokens cache size = 25
0.00.052.274 I load: token to piece cache size = 0.2984 MB
0.00.052.277 I print_info: arch             = gptneox
0.00.052.277 I print_info: vocab_only       = 0
0.00.052.278 I print_info: n_ctx_train      = 2048
0.00.052.278 I print_info: n_embd           = 2048
0.00.052.278 I print_info: n_layer          = 24
0.00.052.281 I print_info: n_head           = 16
0.00.052.281 I print_info: n_head_kv        = 16
0.00.052.282 I print_info: n_rot            = 32
0.00.052.282 I print_info: n_swa            = 0
0.00.052.282 I print_info: n_embd_head_k    = 128
0.00.052.282 I print_info: n_embd_head_v    = 128
0.00.052.284 I print_info: n_gqa            = 1
0.00.052.284 I print_info: n_embd_k_gqa     = 2048
0.00.052.285 I print_info: n_embd_v_gqa     = 2048
0.00.052.286 I print_info: f_norm_eps       = 1.0e-05
0.00.052.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.286 I print_info: f_logit_scale    = 0.0e+00
0.00.052.287 I print_info: n_ff             = 8192
0.00.052.287 I print_info: n_expert         = 0
0.00.052.287 I print_info: n_expert_used    = 0
0.00.052.289 I print_info: causal attn      = 1
0.00.052.291 I print_info: pooling type     = 0
0.00.052.291 I print_info: rope type        = 2
0.00.052.291 I print_info: rope scaling     = linear
0.00.052.292 I print_info: freq_base_train  = 10000.0
0.00.052.292 I print_info: freq_scale_train = 1
0.00.052.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.294 I print_info: rope_finetuned   = unknown
0.00.052.294 I print_info: ssm_d_conv       = 0
0.00.052.294 I print_info: ssm_d_inner      = 0
0.00.052.294 I print_info: ssm_d_state      = 0
0.00.052.294 I print_info: ssm_dt_rank      = 0
0.00.052.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.295 I print_info: model type       = 1.4B
0.00.052.295 I print_info: model params     = 1.41 B
0.00.052.295 I print_info: general.name     = 1.4B
0.00.052.296 I print_info: vocab type       = BPE
0.00.052.296 I print_info: n_vocab          = 50304
0.00.052.296 I print_info: n_merges         = 50009
0.00.052.297 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.297 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.298 I print_info: LF token         = 128 'Ä'
0.00.052.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.298 I print_info: max token length = 1024
0.00.054.341 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.341 I load_tensors: offloading output layer to GPU
0.00.054.341 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.352 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.354 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.638 I llama_init_from_model: n_seq_max     = 1
0.00.054.639 I llama_init_from_model: n_ctx         = 2048
0.00.054.639 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.639 I llama_init_from_model: n_batch       = 2048
0.00.054.639 I llama_init_from_model: n_ubatch      = 512
0.00.054.639 I llama_init_from_model: flash_attn    = 0
0.00.054.640 I llama_init_from_model: freq_base     = 10000.0
0.00.054.640 I llama_init_from_model: freq_scale    = 1
0.00.054.641 I ggml_metal_init: allocating
0.00.054.644 I ggml_metal_init: found device: Apple M4
0.00.054.646 I ggml_metal_init: picking default device: Apple M4
0.00.055.267 I ggml_metal_init: using embedded metal library
0.00.057.652 I ggml_metal_init: GPU name:   Apple M4
0.00.057.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.655 I ggml_metal_init: simdgroup reduction   = true
0.00.057.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.655 I ggml_metal_init: has bfloat            = true
0.00.057.655 I ggml_metal_init: use bfloat            = true
0.00.057.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.722 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.977 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.998 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.058 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.059 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.059 I llama_init_from_model: graph nodes  = 967
0.00.088.060 I llama_init_from_model: graph splits = 2
0.00.088.063 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.180 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.528 I main: llama threadpool init, n_threads = 4
0.00.741.568 I 
0.00.741.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.588 I 
0.00.741.813 I sampler seed: 1234
0.00.741.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.840 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.521.270 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.521.271 I llama_perf_context_print:        load time =     731.12 ms
0.01.521.272 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.04 tokens per second)
0.01.521.272 I llama_perf_context_print:        eval time =     733.34 ms /    63 runs   (   11.64 ms per token,    85.91 tokens per second)
0.01.521.273 I llama_perf_context_print:       total time =     779.75 ms /    70 tokens
0.01.521.529 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.109s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.155 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.640 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.640 I print_info: file format = GGUF V3 (latest)
0.00.025.641 I print_info: file type   = Q5_0
0.00.025.642 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.587 I load: special tokens cache size = 25
0.00.050.444 I load: token to piece cache size = 0.2984 MB
0.00.050.447 I print_info: arch             = gptneox
0.00.050.447 I print_info: vocab_only       = 0
0.00.050.447 I print_info: n_ctx_train      = 2048
0.00.050.448 I print_info: n_embd           = 2048
0.00.050.448 I print_info: n_layer          = 24
0.00.050.451 I print_info: n_head           = 16
0.00.050.452 I print_info: n_head_kv        = 16
0.00.050.452 I print_info: n_rot            = 32
0.00.050.454 I print_info: n_swa            = 0
0.00.050.454 I print_info: n_embd_head_k    = 128
0.00.050.454 I print_info: n_embd_head_v    = 128
0.00.050.455 I print_info: n_gqa            = 1
0.00.050.455 I print_info: n_embd_k_gqa     = 2048
0.00.050.456 I print_info: n_embd_v_gqa     = 2048
0.00.050.456 I print_info: f_norm_eps       = 1.0e-05
0.00.050.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.457 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.457 I print_info: f_logit_scale    = 0.0e+00
0.00.050.458 I print_info: n_ff             = 8192
0.00.050.458 I print_info: n_expert         = 0
0.00.050.458 I print_info: n_expert_used    = 0
0.00.050.459 I print_info: causal attn      = 1
0.00.050.459 I print_info: pooling type     = 0
0.00.050.459 I print_info: rope type        = 2
0.00.050.459 I print_info: rope scaling     = linear
0.00.050.459 I print_info: freq_base_train  = 10000.0
0.00.050.460 I print_info: freq_scale_train = 1
0.00.050.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.460 I print_info: rope_finetuned   = unknown
0.00.050.460 I print_info: ssm_d_conv       = 0
0.00.050.460 I print_info: ssm_d_inner      = 0
0.00.050.461 I print_info: ssm_d_state      = 0
0.00.050.461 I print_info: ssm_dt_rank      = 0
0.00.050.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.461 I print_info: model type       = 1.4B
0.00.050.462 I print_info: model params     = 1.41 B
0.00.050.462 I print_info: general.name     = 1.4B
0.00.050.462 I print_info: vocab type       = BPE
0.00.050.467 I print_info: n_vocab          = 50304
0.00.050.467 I print_info: n_merges         = 50009
0.00.050.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: LF token         = 128 'Ä'
0.00.050.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: max token length = 1024
0.00.052.442 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.442 I load_tensors: offloading output layer to GPU
0.00.052.442 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.453 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.454 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.734 I llama_init_from_model: n_seq_max     = 1
0.00.052.734 I llama_init_from_model: n_ctx         = 128
0.00.052.734 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.735 I llama_init_from_model: n_batch       = 128
0.00.052.735 I llama_init_from_model: n_ubatch      = 128
0.00.052.735 I llama_init_from_model: flash_attn    = 0
0.00.052.735 I llama_init_from_model: freq_base     = 10000.0
0.00.052.736 I llama_init_from_model: freq_scale    = 1
0.00.052.736 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.736 I ggml_metal_init: allocating
0.00.052.739 I ggml_metal_init: found device: Apple M4
0.00.052.741 I ggml_metal_init: picking default device: Apple M4
0.00.053.305 I ggml_metal_init: using embedded metal library
0.00.055.641 I ggml_metal_init: GPU name:   Apple M4
0.00.055.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.643 I ggml_metal_init: simdgroup reduction   = true
0.00.055.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.643 I ggml_metal_init: has bfloat            = true
0.00.055.643 I ggml_metal_init: use bfloat            = true
0.00.055.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.279 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.542 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.420 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.421 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.421 I llama_init_from_model: graph nodes  = 967
0.00.067.422 I llama_init_from_model: graph splits = 2
0.00.067.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.939 I 
0.00.679.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.976 I perplexity: tokenizing the input ..
0.00.687.728 I perplexity: tokenization took 7.75 ms
0.00.687.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.598 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.823.775 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.823.821 I llama_perf_context_print:        load time =     669.78 ms
0.00.823.821 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.70 tokens per second)
0.00.823.826 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.826 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.824.291 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.077s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.640 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.074 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.547 I llama_model_loader: - type  f32:  194 tensors
0.00.025.548 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.548 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.548 I print_info: file format = GGUF V3 (latest)
0.00.025.549 I print_info: file type   = Q5_1
0.00.025.550 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.507 I load: special tokens cache size = 25
0.00.050.537 I load: token to piece cache size = 0.2984 MB
0.00.050.540 I print_info: arch             = gptneox
0.00.050.540 I print_info: vocab_only       = 0
0.00.050.541 I print_info: n_ctx_train      = 2048
0.00.050.541 I print_info: n_embd           = 2048
0.00.050.541 I print_info: n_layer          = 24
0.00.050.544 I print_info: n_head           = 16
0.00.050.545 I print_info: n_head_kv        = 16
0.00.050.545 I print_info: n_rot            = 32
0.00.050.545 I print_info: n_swa            = 0
0.00.050.546 I print_info: n_embd_head_k    = 128
0.00.050.546 I print_info: n_embd_head_v    = 128
0.00.050.547 I print_info: n_gqa            = 1
0.00.050.547 I print_info: n_embd_k_gqa     = 2048
0.00.050.548 I print_info: n_embd_v_gqa     = 2048
0.00.050.549 I print_info: f_norm_eps       = 1.0e-05
0.00.050.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.550 I print_info: f_logit_scale    = 0.0e+00
0.00.050.551 I print_info: n_ff             = 8192
0.00.050.551 I print_info: n_expert         = 0
0.00.050.551 I print_info: n_expert_used    = 0
0.00.050.552 I print_info: causal attn      = 1
0.00.050.554 I print_info: pooling type     = 0
0.00.050.554 I print_info: rope type        = 2
0.00.050.554 I print_info: rope scaling     = linear
0.00.050.554 I print_info: freq_base_train  = 10000.0
0.00.050.555 I print_info: freq_scale_train = 1
0.00.050.555 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.555 I print_info: rope_finetuned   = unknown
0.00.050.556 I print_info: ssm_d_conv       = 0
0.00.050.556 I print_info: ssm_d_inner      = 0
0.00.050.556 I print_info: ssm_d_state      = 0
0.00.050.556 I print_info: ssm_dt_rank      = 0
0.00.050.556 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.556 I print_info: model type       = 1.4B
0.00.050.557 I print_info: model params     = 1.41 B
0.00.050.557 I print_info: general.name     = 1.4B
0.00.050.558 I print_info: vocab type       = BPE
0.00.050.558 I print_info: n_vocab          = 50304
0.00.050.558 I print_info: n_merges         = 50009
0.00.050.558 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.559 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.559 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.559 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.559 I print_info: LF token         = 128 'Ä'
0.00.050.560 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.560 I print_info: max token length = 1024
0.00.052.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.165 I load_tensors: offloading output layer to GPU
0.00.052.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.175 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.176 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.455 I llama_init_from_model: n_seq_max     = 1
0.00.052.456 I llama_init_from_model: n_ctx         = 2048
0.00.052.456 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.456 I llama_init_from_model: n_batch       = 2048
0.00.052.456 I llama_init_from_model: n_ubatch      = 512
0.00.052.457 I llama_init_from_model: flash_attn    = 0
0.00.052.457 I llama_init_from_model: freq_base     = 10000.0
0.00.052.457 I llama_init_from_model: freq_scale    = 1
0.00.052.458 I ggml_metal_init: allocating
0.00.052.461 I ggml_metal_init: found device: Apple M4
0.00.052.463 I ggml_metal_init: picking default device: Apple M4
0.00.053.061 I ggml_metal_init: using embedded metal library
0.00.055.419 I ggml_metal_init: GPU name:   Apple M4
0.00.055.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.421 I ggml_metal_init: simdgroup reduction   = true
0.00.055.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.421 I ggml_metal_init: has bfloat            = true
0.00.055.422 I ggml_metal_init: use bfloat            = true
0.00.055.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.222 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.054 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.060 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.080 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.046 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.047 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.048 I llama_init_from_model: graph nodes  = 967
0.00.085.048 I llama_init_from_model: graph splits = 2
0.00.085.051 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.215 I main: llama threadpool init, n_threads = 4
0.00.805.250 I 
0.00.805.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.297 I 
0.00.805.525 I sampler seed: 1234
0.00.805.530 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.540 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.541 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.541 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.646.877 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.646.877 I llama_perf_context_print:        load time =     796.57 ms
0.01.646.878 I llama_perf_context_print: prompt eval time =      48.15 ms /     7 tokens (    6.88 ms per token,   145.38 tokens per second)
0.01.646.879 I llama_perf_context_print:        eval time =     790.30 ms /    63 runs   (   12.54 ms per token,    79.72 tokens per second)
0.01.646.879 I llama_perf_context_print:       total time =     841.66 ms /    70 tokens
0.01.647.078 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.786 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.914 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.917 I llama_model_loader: - type  f32:  194 tensors
0.00.024.917 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.918 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.918 I print_info: file format = GGUF V3 (latest)
0.00.024.919 I print_info: file type   = Q5_1
0.00.024.924 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.525 I load: special tokens cache size = 25
0.00.050.524 I load: token to piece cache size = 0.2984 MB
0.00.050.527 I print_info: arch             = gptneox
0.00.050.527 I print_info: vocab_only       = 0
0.00.050.527 I print_info: n_ctx_train      = 2048
0.00.050.527 I print_info: n_embd           = 2048
0.00.050.527 I print_info: n_layer          = 24
0.00.050.531 I print_info: n_head           = 16
0.00.050.532 I print_info: n_head_kv        = 16
0.00.050.532 I print_info: n_rot            = 32
0.00.050.532 I print_info: n_swa            = 0
0.00.050.532 I print_info: n_embd_head_k    = 128
0.00.050.532 I print_info: n_embd_head_v    = 128
0.00.050.533 I print_info: n_gqa            = 1
0.00.050.534 I print_info: n_embd_k_gqa     = 2048
0.00.050.535 I print_info: n_embd_v_gqa     = 2048
0.00.050.535 I print_info: f_norm_eps       = 1.0e-05
0.00.050.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.536 I print_info: f_logit_scale    = 0.0e+00
0.00.050.537 I print_info: n_ff             = 8192
0.00.050.537 I print_info: n_expert         = 0
0.00.050.537 I print_info: n_expert_used    = 0
0.00.050.537 I print_info: causal attn      = 1
0.00.050.537 I print_info: pooling type     = 0
0.00.050.537 I print_info: rope type        = 2
0.00.050.538 I print_info: rope scaling     = linear
0.00.050.538 I print_info: freq_base_train  = 10000.0
0.00.050.538 I print_info: freq_scale_train = 1
0.00.050.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.539 I print_info: rope_finetuned   = unknown
0.00.050.539 I print_info: ssm_d_conv       = 0
0.00.050.539 I print_info: ssm_d_inner      = 0
0.00.050.541 I print_info: ssm_d_state      = 0
0.00.050.541 I print_info: ssm_dt_rank      = 0
0.00.050.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.541 I print_info: model type       = 1.4B
0.00.050.542 I print_info: model params     = 1.41 B
0.00.050.542 I print_info: general.name     = 1.4B
0.00.050.542 I print_info: vocab type       = BPE
0.00.050.543 I print_info: n_vocab          = 50304
0.00.050.543 I print_info: n_merges         = 50009
0.00.050.543 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.544 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.546 I print_info: LF token         = 128 'Ä'
0.00.050.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.546 I print_info: max token length = 1024
0.00.052.560 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.560 I load_tensors: offloading output layer to GPU
0.00.052.560 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.571 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.572 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.854 I llama_init_from_model: n_seq_max     = 1
0.00.052.855 I llama_init_from_model: n_ctx         = 128
0.00.052.855 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.855 I llama_init_from_model: n_batch       = 128
0.00.052.855 I llama_init_from_model: n_ubatch      = 128
0.00.052.856 I llama_init_from_model: flash_attn    = 0
0.00.052.856 I llama_init_from_model: freq_base     = 10000.0
0.00.052.856 I llama_init_from_model: freq_scale    = 1
0.00.052.857 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.857 I ggml_metal_init: allocating
0.00.052.860 I ggml_metal_init: found device: Apple M4
0.00.052.863 I ggml_metal_init: picking default device: Apple M4
0.00.053.436 I ggml_metal_init: using embedded metal library
0.00.055.794 I ggml_metal_init: GPU name:   Apple M4
0.00.055.795 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.796 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.796 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.796 I ggml_metal_init: simdgroup reduction   = true
0.00.055.797 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.797 I ggml_metal_init: has bfloat            = true
0.00.055.797 I ggml_metal_init: use bfloat            = true
0.00.055.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.613 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.886 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.888 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.816 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.817 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.818 I llama_init_from_model: graph nodes  = 967
0.00.067.818 I llama_init_from_model: graph splits = 2
0.00.067.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.819 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.871 I 
0.00.720.902 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.906 I perplexity: tokenizing the input ..
0.00.728.650 I perplexity: tokenization took 7.743 ms
0.00.728.659 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.589 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.864.777 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.864.826 I llama_perf_context_print:        load time =     712.08 ms
0.00.864.827 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.23 tokens per second)
0.00.864.828 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.828 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.865.184 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.078s
sys	0m0.106s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.004 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.480 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.483 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.060 I llama_model_loader: - type  f32:  194 tensors
0.00.027.060 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.060 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.060 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.061 I print_info: file format = GGUF V3 (latest)
0.00.027.061 I print_info: file type   = Q2_K - Medium
0.00.027.062 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.788 I load: special tokens cache size = 25
0.00.052.737 I load: token to piece cache size = 0.2984 MB
0.00.052.740 I print_info: arch             = gptneox
0.00.052.740 I print_info: vocab_only       = 0
0.00.052.740 I print_info: n_ctx_train      = 2048
0.00.052.740 I print_info: n_embd           = 2048
0.00.052.740 I print_info: n_layer          = 24
0.00.052.743 I print_info: n_head           = 16
0.00.052.744 I print_info: n_head_kv        = 16
0.00.052.744 I print_info: n_rot            = 32
0.00.052.744 I print_info: n_swa            = 0
0.00.052.744 I print_info: n_embd_head_k    = 128
0.00.052.745 I print_info: n_embd_head_v    = 128
0.00.052.746 I print_info: n_gqa            = 1
0.00.052.747 I print_info: n_embd_k_gqa     = 2048
0.00.052.748 I print_info: n_embd_v_gqa     = 2048
0.00.052.748 I print_info: f_norm_eps       = 1.0e-05
0.00.052.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.749 I print_info: f_logit_scale    = 0.0e+00
0.00.052.750 I print_info: n_ff             = 8192
0.00.052.750 I print_info: n_expert         = 0
0.00.052.752 I print_info: n_expert_used    = 0
0.00.052.754 I print_info: causal attn      = 1
0.00.052.754 I print_info: pooling type     = 0
0.00.052.754 I print_info: rope type        = 2
0.00.052.754 I print_info: rope scaling     = linear
0.00.052.755 I print_info: freq_base_train  = 10000.0
0.00.052.755 I print_info: freq_scale_train = 1
0.00.052.755 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.755 I print_info: rope_finetuned   = unknown
0.00.052.755 I print_info: ssm_d_conv       = 0
0.00.052.755 I print_info: ssm_d_inner      = 0
0.00.052.756 I print_info: ssm_d_state      = 0
0.00.052.756 I print_info: ssm_dt_rank      = 0
0.00.052.756 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.756 I print_info: model type       = 1.4B
0.00.052.756 I print_info: model params     = 1.41 B
0.00.052.757 I print_info: general.name     = 1.4B
0.00.052.757 I print_info: vocab type       = BPE
0.00.052.757 I print_info: n_vocab          = 50304
0.00.052.757 I print_info: n_merges         = 50009
0.00.052.758 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.762 I print_info: LF token         = 128 'Ä'
0.00.052.763 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.763 I print_info: max token length = 1024
0.00.054.712 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.713 I load_tensors: offloading output layer to GPU
0.00.054.713 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.724 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.725 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.055.028 I llama_init_from_model: n_seq_max     = 1
0.00.055.028 I llama_init_from_model: n_ctx         = 2048
0.00.055.029 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.029 I llama_init_from_model: n_batch       = 2048
0.00.055.029 I llama_init_from_model: n_ubatch      = 512
0.00.055.029 I llama_init_from_model: flash_attn    = 0
0.00.055.029 I llama_init_from_model: freq_base     = 10000.0
0.00.055.030 I llama_init_from_model: freq_scale    = 1
0.00.055.030 I ggml_metal_init: allocating
0.00.055.033 I ggml_metal_init: found device: Apple M4
0.00.055.035 I ggml_metal_init: picking default device: Apple M4
0.00.055.655 I ggml_metal_init: using embedded metal library
0.00.058.039 I ggml_metal_init: GPU name:   Apple M4
0.00.058.041 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.041 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.042 I ggml_metal_init: simdgroup reduction   = true
0.00.058.042 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.042 I ggml_metal_init: has bfloat            = true
0.00.058.042 I ggml_metal_init: use bfloat            = true
0.00.058.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.081 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.002 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.007 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.097 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.098 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.098 I llama_init_from_model: graph nodes  = 967
0.00.089.099 I llama_init_from_model: graph splits = 2
0.00.089.102 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.030 I main: llama threadpool init, n_threads = 4
0.00.475.070 I 
0.00.475.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.094 I 
0.00.475.322 I sampler seed: 1234
0.00.475.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.475.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.475.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.475.348 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.154.221 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.154.222 I llama_perf_context_print:        load time =     465.02 ms
0.01.154.223 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.54 tokens per second)
0.01.154.223 I llama_perf_context_print:        eval time =     640.08 ms /    63 runs   (   10.16 ms per token,    98.42 tokens per second)
0.01.154.224 I llama_perf_context_print:       total time =     679.19 ms /    70 tokens
0.01.154.461 I ggml_metal_free: deallocating

real	0m1.172s
user	0m0.111s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.664 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.014 I llama_model_loader: - type  f32:  194 tensors
0.00.025.014 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.014 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.014 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.015 I print_info: file format = GGUF V3 (latest)
0.00.025.015 I print_info: file type   = Q2_K - Medium
0.00.025.016 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.869 I load: special tokens cache size = 25
0.00.049.881 I load: token to piece cache size = 0.2984 MB
0.00.049.885 I print_info: arch             = gptneox
0.00.049.885 I print_info: vocab_only       = 0
0.00.049.885 I print_info: n_ctx_train      = 2048
0.00.049.886 I print_info: n_embd           = 2048
0.00.049.886 I print_info: n_layer          = 24
0.00.049.889 I print_info: n_head           = 16
0.00.049.889 I print_info: n_head_kv        = 16
0.00.049.890 I print_info: n_rot            = 32
0.00.049.891 I print_info: n_swa            = 0
0.00.049.891 I print_info: n_embd_head_k    = 128
0.00.049.892 I print_info: n_embd_head_v    = 128
0.00.049.893 I print_info: n_gqa            = 1
0.00.049.894 I print_info: n_embd_k_gqa     = 2048
0.00.049.895 I print_info: n_embd_v_gqa     = 2048
0.00.049.895 I print_info: f_norm_eps       = 1.0e-05
0.00.049.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.896 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.896 I print_info: f_logit_scale    = 0.0e+00
0.00.049.897 I print_info: n_ff             = 8192
0.00.049.897 I print_info: n_expert         = 0
0.00.049.897 I print_info: n_expert_used    = 0
0.00.049.897 I print_info: causal attn      = 1
0.00.049.897 I print_info: pooling type     = 0
0.00.049.897 I print_info: rope type        = 2
0.00.049.898 I print_info: rope scaling     = linear
0.00.049.898 I print_info: freq_base_train  = 10000.0
0.00.049.898 I print_info: freq_scale_train = 1
0.00.049.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.899 I print_info: rope_finetuned   = unknown
0.00.049.899 I print_info: ssm_d_conv       = 0
0.00.049.899 I print_info: ssm_d_inner      = 0
0.00.049.899 I print_info: ssm_d_state      = 0
0.00.049.899 I print_info: ssm_dt_rank      = 0
0.00.049.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.899 I print_info: model type       = 1.4B
0.00.049.900 I print_info: model params     = 1.41 B
0.00.049.900 I print_info: general.name     = 1.4B
0.00.049.900 I print_info: vocab type       = BPE
0.00.049.901 I print_info: n_vocab          = 50304
0.00.049.901 I print_info: n_merges         = 50009
0.00.049.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.902 I print_info: LF token         = 128 'Ä'
0.00.049.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.902 I print_info: max token length = 1024
0.00.051.813 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.813 I load_tensors: offloading output layer to GPU
0.00.051.814 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.824 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.825 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.132 I llama_init_from_model: n_seq_max     = 1
0.00.052.133 I llama_init_from_model: n_ctx         = 128
0.00.052.133 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.133 I llama_init_from_model: n_batch       = 128
0.00.052.134 I llama_init_from_model: n_ubatch      = 128
0.00.052.134 I llama_init_from_model: flash_attn    = 0
0.00.052.134 I llama_init_from_model: freq_base     = 10000.0
0.00.052.134 I llama_init_from_model: freq_scale    = 1
0.00.052.135 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.135 I ggml_metal_init: allocating
0.00.052.138 I ggml_metal_init: found device: Apple M4
0.00.052.140 I ggml_metal_init: picking default device: Apple M4
0.00.052.709 I ggml_metal_init: using embedded metal library
0.00.055.035 I ggml_metal_init: GPU name:   Apple M4
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.038 I ggml_metal_init: simdgroup reduction   = true
0.00.055.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.038 I ggml_metal_init: has bfloat            = true
0.00.055.038 I ggml_metal_init: use bfloat            = true
0.00.055.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.721 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.984 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.988 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.004 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.841 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.842 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.842 I llama_init_from_model: graph nodes  = 967
0.00.066.843 I llama_init_from_model: graph splits = 2
0.00.066.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.425.519 I 
0.00.425.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.425.554 I perplexity: tokenizing the input ..
0.00.433.251 I perplexity: tokenization took 7.695 ms
0.00.433.255 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.565.750 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.566.904 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.566.931 I llama_perf_context_print:        load time =     415.85 ms
0.00.566.932 I llama_perf_context_print: prompt eval time =     132.27 ms /   128 tokens (    1.03 ms per token,   967.73 tokens per second)
0.00.566.933 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.566.933 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.567.398 I ggml_metal_free: deallocating

real	0m0.583s
user	0m0.077s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.173 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.802 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.803 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.803 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.804 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.805 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.805 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.805 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.806 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.806 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.806 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.807 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.809 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.810 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.296 I llama_model_loader: - type  f32:  194 tensors
0.00.025.297 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.297 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.297 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.298 I print_info: file format = GGUF V3 (latest)
0.00.025.298 I print_info: file type   = Q3_K - Medium
0.00.025.299 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.393 I load: special tokens cache size = 25
0.00.050.385 I load: token to piece cache size = 0.2984 MB
0.00.050.388 I print_info: arch             = gptneox
0.00.050.388 I print_info: vocab_only       = 0
0.00.050.388 I print_info: n_ctx_train      = 2048
0.00.050.388 I print_info: n_embd           = 2048
0.00.050.389 I print_info: n_layer          = 24
0.00.050.391 I print_info: n_head           = 16
0.00.050.392 I print_info: n_head_kv        = 16
0.00.050.392 I print_info: n_rot            = 32
0.00.050.393 I print_info: n_swa            = 0
0.00.050.393 I print_info: n_embd_head_k    = 128
0.00.050.395 I print_info: n_embd_head_v    = 128
0.00.050.395 I print_info: n_gqa            = 1
0.00.050.396 I print_info: n_embd_k_gqa     = 2048
0.00.050.397 I print_info: n_embd_v_gqa     = 2048
0.00.050.406 I print_info: f_norm_eps       = 1.0e-05
0.00.050.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.407 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.408 I print_info: f_logit_scale    = 0.0e+00
0.00.050.414 I print_info: n_ff             = 8192
0.00.050.414 I print_info: n_expert         = 0
0.00.050.414 I print_info: n_expert_used    = 0
0.00.050.414 I print_info: causal attn      = 1
0.00.050.414 I print_info: pooling type     = 0
0.00.050.415 I print_info: rope type        = 2
0.00.050.415 I print_info: rope scaling     = linear
0.00.050.415 I print_info: freq_base_train  = 10000.0
0.00.050.415 I print_info: freq_scale_train = 1
0.00.050.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.416 I print_info: rope_finetuned   = unknown
0.00.050.416 I print_info: ssm_d_conv       = 0
0.00.050.416 I print_info: ssm_d_inner      = 0
0.00.050.416 I print_info: ssm_d_state      = 0
0.00.050.416 I print_info: ssm_dt_rank      = 0
0.00.050.418 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.418 I print_info: model type       = 1.4B
0.00.050.418 I print_info: model params     = 1.41 B
0.00.050.418 I print_info: general.name     = 1.4B
0.00.050.419 I print_info: vocab type       = BPE
0.00.050.420 I print_info: n_vocab          = 50304
0.00.050.420 I print_info: n_merges         = 50009
0.00.050.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: LF token         = 128 'Ä'
0.00.050.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.422 I print_info: max token length = 1024
0.00.052.389 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.389 I load_tensors: offloading output layer to GPU
0.00.052.389 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.400 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.401 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.677 I llama_init_from_model: n_seq_max     = 1
0.00.052.678 I llama_init_from_model: n_ctx         = 2048
0.00.052.678 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.678 I llama_init_from_model: n_batch       = 2048
0.00.052.678 I llama_init_from_model: n_ubatch      = 512
0.00.052.678 I llama_init_from_model: flash_attn    = 0
0.00.052.679 I llama_init_from_model: freq_base     = 10000.0
0.00.052.679 I llama_init_from_model: freq_scale    = 1
0.00.052.679 I ggml_metal_init: allocating
0.00.052.682 I ggml_metal_init: found device: Apple M4
0.00.052.684 I ggml_metal_init: picking default device: Apple M4
0.00.053.268 I ggml_metal_init: using embedded metal library
0.00.055.599 I ggml_metal_init: GPU name:   Apple M4
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.602 I ggml_metal_init: simdgroup reduction   = true
0.00.055.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.602 I ggml_metal_init: has bfloat            = true
0.00.055.602 I ggml_metal_init: use bfloat            = true
0.00.055.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.239 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.152 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.160 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.181 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.090 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.092 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.092 I llama_init_from_model: graph nodes  = 967
0.00.085.092 I llama_init_from_model: graph splits = 2
0.00.085.095 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.329 I main: llama threadpool init, n_threads = 4
0.00.519.377 I 
0.00.519.412 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.414 I 
0.00.519.645 I sampler seed: 1234
0.00.519.649 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.711 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.712 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.266.711 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.266.712 I llama_perf_context_print:        load time =     509.15 ms
0.01.266.713 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.77 tokens per second)
0.01.266.713 I llama_perf_context_print:        eval time =     703.49 ms /    63 runs   (   11.17 ms per token,    89.55 tokens per second)
0.01.266.714 I llama_perf_context_print:       total time =     747.39 ms /    70 tokens
0.01.266.920 I ggml_metal_free: deallocating

real	0m1.284s
user	0m0.109s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.536 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.537 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.538 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.539 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.539 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.540 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.029 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.030 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.031 I llama_model_loader: - type  f32:  194 tensors
0.00.024.031 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.032 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.032 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.032 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.033 I print_info: file format = GGUF V3 (latest)
0.00.024.033 I print_info: file type   = Q3_K - Medium
0.00.024.034 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.920 I load: special tokens cache size = 25
0.00.048.930 I load: token to piece cache size = 0.2984 MB
0.00.048.933 I print_info: arch             = gptneox
0.00.048.933 I print_info: vocab_only       = 0
0.00.048.933 I print_info: n_ctx_train      = 2048
0.00.048.933 I print_info: n_embd           = 2048
0.00.048.933 I print_info: n_layer          = 24
0.00.048.936 I print_info: n_head           = 16
0.00.048.937 I print_info: n_head_kv        = 16
0.00.048.937 I print_info: n_rot            = 32
0.00.048.937 I print_info: n_swa            = 0
0.00.048.937 I print_info: n_embd_head_k    = 128
0.00.048.944 I print_info: n_embd_head_v    = 128
0.00.048.952 I print_info: n_gqa            = 1
0.00.048.953 I print_info: n_embd_k_gqa     = 2048
0.00.048.954 I print_info: n_embd_v_gqa     = 2048
0.00.048.955 I print_info: f_norm_eps       = 1.0e-05
0.00.048.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.956 I print_info: f_logit_scale    = 0.0e+00
0.00.048.957 I print_info: n_ff             = 8192
0.00.048.957 I print_info: n_expert         = 0
0.00.048.957 I print_info: n_expert_used    = 0
0.00.048.957 I print_info: causal attn      = 1
0.00.048.957 I print_info: pooling type     = 0
0.00.048.957 I print_info: rope type        = 2
0.00.048.959 I print_info: rope scaling     = linear
0.00.048.959 I print_info: freq_base_train  = 10000.0
0.00.048.959 I print_info: freq_scale_train = 1
0.00.048.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.960 I print_info: rope_finetuned   = unknown
0.00.048.960 I print_info: ssm_d_conv       = 0
0.00.048.960 I print_info: ssm_d_inner      = 0
0.00.048.960 I print_info: ssm_d_state      = 0
0.00.048.961 I print_info: ssm_dt_rank      = 0
0.00.048.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.961 I print_info: model type       = 1.4B
0.00.048.962 I print_info: model params     = 1.41 B
0.00.048.962 I print_info: general.name     = 1.4B
0.00.048.962 I print_info: vocab type       = BPE
0.00.048.963 I print_info: n_vocab          = 50304
0.00.048.963 I print_info: n_merges         = 50009
0.00.048.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.966 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.966 I print_info: LF token         = 128 'Ä'
0.00.048.967 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.967 I print_info: max token length = 1024
0.00.050.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.890 I load_tensors: offloading output layer to GPU
0.00.050.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.901 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.902 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.189 I llama_init_from_model: n_seq_max     = 1
0.00.051.189 I llama_init_from_model: n_ctx         = 128
0.00.051.190 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.190 I llama_init_from_model: n_batch       = 128
0.00.051.190 I llama_init_from_model: n_ubatch      = 128
0.00.051.190 I llama_init_from_model: flash_attn    = 0
0.00.051.190 I llama_init_from_model: freq_base     = 10000.0
0.00.051.191 I llama_init_from_model: freq_scale    = 1
0.00.051.191 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.191 I ggml_metal_init: allocating
0.00.051.194 I ggml_metal_init: found device: Apple M4
0.00.051.196 I ggml_metal_init: picking default device: Apple M4
0.00.051.755 I ggml_metal_init: using embedded metal library
0.00.054.095 I ggml_metal_init: GPU name:   Apple M4
0.00.054.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.097 I ggml_metal_init: simdgroup reduction   = true
0.00.054.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.097 I ggml_metal_init: has bfloat            = true
0.00.054.098 I ggml_metal_init: use bfloat            = true
0.00.054.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.606 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.129 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.030 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.031 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.031 I llama_init_from_model: graph nodes  = 967
0.00.066.031 I llama_init_from_model: graph splits = 2
0.00.066.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.460.613 I 
0.00.460.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.460.664 I perplexity: tokenizing the input ..
0.00.468.382 I perplexity: tokenization took 7.715 ms
0.00.468.385 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.600.399 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.601.569 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.601.601 I llama_perf_context_print:        load time =     451.88 ms
0.00.601.601 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.27 tokens per second)
0.00.601.602 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.601.603 I llama_perf_context_print:       total time =     140.99 ms /   129 tokens
0.00.602.086 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.077s
sys	0m0.076s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.384 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.940 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.940 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.942 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.942 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.541 I llama_model_loader: - type  f32:  194 tensors
0.00.025.542 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.542 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.542 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.543 I print_info: file format = GGUF V3 (latest)
0.00.025.543 I print_info: file type   = Q4_K - Medium
0.00.025.544 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.543 I load: special tokens cache size = 25
0.00.050.467 I load: token to piece cache size = 0.2984 MB
0.00.050.470 I print_info: arch             = gptneox
0.00.050.470 I print_info: vocab_only       = 0
0.00.050.470 I print_info: n_ctx_train      = 2048
0.00.050.470 I print_info: n_embd           = 2048
0.00.050.470 I print_info: n_layer          = 24
0.00.050.473 I print_info: n_head           = 16
0.00.050.474 I print_info: n_head_kv        = 16
0.00.050.474 I print_info: n_rot            = 32
0.00.050.474 I print_info: n_swa            = 0
0.00.050.475 I print_info: n_embd_head_k    = 128
0.00.050.475 I print_info: n_embd_head_v    = 128
0.00.050.475 I print_info: n_gqa            = 1
0.00.050.476 I print_info: n_embd_k_gqa     = 2048
0.00.050.479 I print_info: n_embd_v_gqa     = 2048
0.00.050.479 I print_info: f_norm_eps       = 1.0e-05
0.00.050.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.480 I print_info: f_logit_scale    = 0.0e+00
0.00.050.481 I print_info: n_ff             = 8192
0.00.050.481 I print_info: n_expert         = 0
0.00.050.482 I print_info: n_expert_used    = 0
0.00.050.482 I print_info: causal attn      = 1
0.00.050.482 I print_info: pooling type     = 0
0.00.050.482 I print_info: rope type        = 2
0.00.050.482 I print_info: rope scaling     = linear
0.00.050.483 I print_info: freq_base_train  = 10000.0
0.00.050.483 I print_info: freq_scale_train = 1
0.00.050.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.483 I print_info: rope_finetuned   = unknown
0.00.050.483 I print_info: ssm_d_conv       = 0
0.00.050.484 I print_info: ssm_d_inner      = 0
0.00.050.484 I print_info: ssm_d_state      = 0
0.00.050.484 I print_info: ssm_dt_rank      = 0
0.00.050.484 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.484 I print_info: model type       = 1.4B
0.00.050.485 I print_info: model params     = 1.41 B
0.00.050.485 I print_info: general.name     = 1.4B
0.00.050.485 I print_info: vocab type       = BPE
0.00.050.486 I print_info: n_vocab          = 50304
0.00.050.486 I print_info: n_merges         = 50009
0.00.050.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.487 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.487 I print_info: LF token         = 128 'Ä'
0.00.050.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.487 I print_info: max token length = 1024
0.00.052.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.401 I load_tensors: offloading output layer to GPU
0.00.052.402 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.412 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.414 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.696 I llama_init_from_model: n_seq_max     = 1
0.00.052.697 I llama_init_from_model: n_ctx         = 2048
0.00.052.697 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.697 I llama_init_from_model: n_batch       = 2048
0.00.052.697 I llama_init_from_model: n_ubatch      = 512
0.00.052.697 I llama_init_from_model: flash_attn    = 0
0.00.052.698 I llama_init_from_model: freq_base     = 10000.0
0.00.052.698 I llama_init_from_model: freq_scale    = 1
0.00.052.699 I ggml_metal_init: allocating
0.00.052.702 I ggml_metal_init: found device: Apple M4
0.00.052.704 I ggml_metal_init: picking default device: Apple M4
0.00.053.283 I ggml_metal_init: using embedded metal library
0.00.055.620 I ggml_metal_init: GPU name:   Apple M4
0.00.055.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.623 I ggml_metal_init: simdgroup reduction   = true
0.00.055.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.623 I ggml_metal_init: has bfloat            = true
0.00.055.623 I ggml_metal_init: use bfloat            = true
0.00.055.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.407 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.082 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.100 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.152 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.153 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.154 I llama_init_from_model: graph nodes  = 967
0.00.085.154 I llama_init_from_model: graph splits = 2
0.00.085.157 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.094 I main: llama threadpool init, n_threads = 4
0.00.602.138 I 
0.00.602.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.165 I 
0.00.602.399 I sampler seed: 1234
0.00.602.405 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.443 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.443 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.363.991 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.363.991 I llama_perf_context_print:        load time =     591.70 ms
0.01.363.992 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.58 tokens per second)
0.01.363.993 I llama_perf_context_print:        eval time =     711.26 ms /    63 runs   (   11.29 ms per token,    88.57 tokens per second)
0.01.363.993 I llama_perf_context_print:       total time =     761.90 ms /    70 tokens
0.01.364.221 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.109s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.965 I llama_model_loader: - type  f32:  194 tensors
0.00.023.965 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.965 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.966 I llama_model_loader: - type q6_K:   13 tensors
0.00.023.966 I print_info: file format = GGUF V3 (latest)
0.00.023.967 I print_info: file type   = Q4_K - Medium
0.00.023.968 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.519 I load: special tokens cache size = 25
0.00.049.492 I load: token to piece cache size = 0.2984 MB
0.00.049.496 I print_info: arch             = gptneox
0.00.049.496 I print_info: vocab_only       = 0
0.00.049.496 I print_info: n_ctx_train      = 2048
0.00.049.497 I print_info: n_embd           = 2048
0.00.049.497 I print_info: n_layer          = 24
0.00.049.500 I print_info: n_head           = 16
0.00.049.500 I print_info: n_head_kv        = 16
0.00.049.501 I print_info: n_rot            = 32
0.00.049.501 I print_info: n_swa            = 0
0.00.049.501 I print_info: n_embd_head_k    = 128
0.00.049.501 I print_info: n_embd_head_v    = 128
0.00.049.502 I print_info: n_gqa            = 1
0.00.049.503 I print_info: n_embd_k_gqa     = 2048
0.00.049.505 I print_info: n_embd_v_gqa     = 2048
0.00.049.506 I print_info: f_norm_eps       = 1.0e-05
0.00.049.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.508 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.509 I print_info: f_logit_scale    = 0.0e+00
0.00.049.509 I print_info: n_ff             = 8192
0.00.049.509 I print_info: n_expert         = 0
0.00.049.510 I print_info: n_expert_used    = 0
0.00.049.510 I print_info: causal attn      = 1
0.00.049.510 I print_info: pooling type     = 0
0.00.049.510 I print_info: rope type        = 2
0.00.049.510 I print_info: rope scaling     = linear
0.00.049.515 I print_info: freq_base_train  = 10000.0
0.00.049.516 I print_info: freq_scale_train = 1
0.00.049.516 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.516 I print_info: rope_finetuned   = unknown
0.00.049.518 I print_info: ssm_d_conv       = 0
0.00.049.518 I print_info: ssm_d_inner      = 0
0.00.049.518 I print_info: ssm_d_state      = 0
0.00.049.518 I print_info: ssm_dt_rank      = 0
0.00.049.518 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.518 I print_info: model type       = 1.4B
0.00.049.519 I print_info: model params     = 1.41 B
0.00.049.519 I print_info: general.name     = 1.4B
0.00.049.519 I print_info: vocab type       = BPE
0.00.049.519 I print_info: n_vocab          = 50304
0.00.049.520 I print_info: n_merges         = 50009
0.00.049.520 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.520 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.520 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.525 I print_info: LF token         = 128 'Ä'
0.00.049.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.528 I print_info: max token length = 1024
0.00.051.124 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.124 I load_tensors: offloading output layer to GPU
0.00.051.124 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.135 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.136 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.404 I llama_init_from_model: n_seq_max     = 1
0.00.051.405 I llama_init_from_model: n_ctx         = 128
0.00.051.405 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.405 I llama_init_from_model: n_batch       = 128
0.00.051.405 I llama_init_from_model: n_ubatch      = 128
0.00.051.405 I llama_init_from_model: flash_attn    = 0
0.00.051.406 I llama_init_from_model: freq_base     = 10000.0
0.00.051.406 I llama_init_from_model: freq_scale    = 1
0.00.051.406 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.407 I ggml_metal_init: allocating
0.00.051.410 I ggml_metal_init: found device: Apple M4
0.00.051.412 I ggml_metal_init: picking default device: Apple M4
0.00.051.968 I ggml_metal_init: using embedded metal library
0.00.054.279 I ggml_metal_init: GPU name:   Apple M4
0.00.054.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.281 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.281 I ggml_metal_init: simdgroup reduction   = true
0.00.054.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.282 I ggml_metal_init: has bfloat            = true
0.00.054.282 I ggml_metal_init: use bfloat            = true
0.00.054.282 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.760 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.048 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.053 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.077 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.952 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.953 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.953 I llama_init_from_model: graph nodes  = 967
0.00.065.954 I llama_init_from_model: graph splits = 2
0.00.065.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.357 I 
0.00.570.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.397 I perplexity: tokenizing the input ..
0.00.578.189 I perplexity: tokenization took 7.791 ms
0.00.578.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.240 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.712.613 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.712.632 I llama_perf_context_print:        load time =     561.64 ms
0.00.712.638 I llama_perf_context_print: prompt eval time =     132.81 ms /   128 tokens (    1.04 ms per token,   963.80 tokens per second)
0.00.712.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.639 I llama_perf_context_print:       total time =     142.28 ms /   129 tokens
0.00.713.019 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.078s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.481 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.027 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.032 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.034 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.497 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.499 I llama_model_loader: - type  f32:  194 tensors
0.00.025.499 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.499 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.500 I print_info: file format = GGUF V3 (latest)
0.00.025.500 I print_info: file type   = Q5_K - Medium
0.00.025.501 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.466 I load: special tokens cache size = 25
0.00.050.435 I load: token to piece cache size = 0.2984 MB
0.00.050.438 I print_info: arch             = gptneox
0.00.050.439 I print_info: vocab_only       = 0
0.00.050.439 I print_info: n_ctx_train      = 2048
0.00.050.439 I print_info: n_embd           = 2048
0.00.050.439 I print_info: n_layer          = 24
0.00.050.442 I print_info: n_head           = 16
0.00.050.443 I print_info: n_head_kv        = 16
0.00.050.443 I print_info: n_rot            = 32
0.00.050.443 I print_info: n_swa            = 0
0.00.050.443 I print_info: n_embd_head_k    = 128
0.00.050.444 I print_info: n_embd_head_v    = 128
0.00.050.444 I print_info: n_gqa            = 1
0.00.050.445 I print_info: n_embd_k_gqa     = 2048
0.00.050.446 I print_info: n_embd_v_gqa     = 2048
0.00.050.446 I print_info: f_norm_eps       = 1.0e-05
0.00.050.447 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.447 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.447 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.447 I print_info: f_logit_scale    = 0.0e+00
0.00.050.448 I print_info: n_ff             = 8192
0.00.050.449 I print_info: n_expert         = 0
0.00.050.449 I print_info: n_expert_used    = 0
0.00.050.449 I print_info: causal attn      = 1
0.00.050.449 I print_info: pooling type     = 0
0.00.050.450 I print_info: rope type        = 2
0.00.050.450 I print_info: rope scaling     = linear
0.00.050.450 I print_info: freq_base_train  = 10000.0
0.00.050.450 I print_info: freq_scale_train = 1
0.00.050.452 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.452 I print_info: rope_finetuned   = unknown
0.00.050.452 I print_info: ssm_d_conv       = 0
0.00.050.452 I print_info: ssm_d_inner      = 0
0.00.050.453 I print_info: ssm_d_state      = 0
0.00.050.453 I print_info: ssm_dt_rank      = 0
0.00.050.453 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.453 I print_info: model type       = 1.4B
0.00.050.453 I print_info: model params     = 1.41 B
0.00.050.454 I print_info: general.name     = 1.4B
0.00.050.454 I print_info: vocab type       = BPE
0.00.050.454 I print_info: n_vocab          = 50304
0.00.050.455 I print_info: n_merges         = 50009
0.00.050.455 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.456 I print_info: LF token         = 128 'Ä'
0.00.050.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.456 I print_info: max token length = 1024
0.00.052.426 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.426 I load_tensors: offloading output layer to GPU
0.00.052.427 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.437 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.438 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.726 I llama_init_from_model: n_seq_max     = 1
0.00.052.727 I llama_init_from_model: n_ctx         = 2048
0.00.052.727 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.727 I llama_init_from_model: n_batch       = 2048
0.00.052.727 I llama_init_from_model: n_ubatch      = 512
0.00.052.727 I llama_init_from_model: flash_attn    = 0
0.00.052.728 I llama_init_from_model: freq_base     = 10000.0
0.00.052.728 I llama_init_from_model: freq_scale    = 1
0.00.052.729 I ggml_metal_init: allocating
0.00.052.731 I ggml_metal_init: found device: Apple M4
0.00.052.733 I ggml_metal_init: picking default device: Apple M4
0.00.053.307 I ggml_metal_init: using embedded metal library
0.00.055.622 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.625 I ggml_metal_init: simdgroup reduction   = true
0.00.055.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.625 I ggml_metal_init: has bfloat            = true
0.00.055.625 I ggml_metal_init: use bfloat            = true
0.00.055.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.380 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.513 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.546 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.517 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.519 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.519 I llama_init_from_model: graph nodes  = 967
0.00.085.519 I llama_init_from_model: graph splits = 2
0.00.085.522 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.657 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.020 I main: llama threadpool init, n_threads = 4
0.00.679.059 I 
0.00.679.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.101 I 
0.00.679.340 I sampler seed: 1234
0.00.679.344 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.386 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.386 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.387 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.535.200 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.535.201 I llama_perf_context_print:        load time =     668.53 ms
0.01.535.206 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.91 tokens per second)
0.01.535.206 I llama_perf_context_print:        eval time =     801.57 ms /    63 runs   (   12.72 ms per token,    78.60 tokens per second)
0.01.535.207 I llama_perf_context_print:       total time =     856.19 ms /    70 tokens
0.01.535.453 I ggml_metal_free: deallocating

real	0m1.555s
user	0m0.109s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.185 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.055 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.668 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.669 I llama_model_loader: - type  f32:  194 tensors
0.00.027.669 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.669 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.670 I print_info: file format = GGUF V3 (latest)
0.00.027.670 I print_info: file type   = Q5_K - Medium
0.00.027.672 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.467 I load: special tokens cache size = 25
0.00.053.619 I load: token to piece cache size = 0.2984 MB
0.00.053.623 I print_info: arch             = gptneox
0.00.053.623 I print_info: vocab_only       = 0
0.00.053.623 I print_info: n_ctx_train      = 2048
0.00.053.623 I print_info: n_embd           = 2048
0.00.053.624 I print_info: n_layer          = 24
0.00.053.628 I print_info: n_head           = 16
0.00.053.628 I print_info: n_head_kv        = 16
0.00.053.630 I print_info: n_rot            = 32
0.00.053.630 I print_info: n_swa            = 0
0.00.053.630 I print_info: n_embd_head_k    = 128
0.00.053.633 I print_info: n_embd_head_v    = 128
0.00.053.633 I print_info: n_gqa            = 1
0.00.053.634 I print_info: n_embd_k_gqa     = 2048
0.00.053.635 I print_info: n_embd_v_gqa     = 2048
0.00.053.635 I print_info: f_norm_eps       = 1.0e-05
0.00.053.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.636 I print_info: f_logit_scale    = 0.0e+00
0.00.053.637 I print_info: n_ff             = 8192
0.00.053.637 I print_info: n_expert         = 0
0.00.053.637 I print_info: n_expert_used    = 0
0.00.053.638 I print_info: causal attn      = 1
0.00.053.638 I print_info: pooling type     = 0
0.00.053.638 I print_info: rope type        = 2
0.00.053.638 I print_info: rope scaling     = linear
0.00.053.638 I print_info: freq_base_train  = 10000.0
0.00.053.639 I print_info: freq_scale_train = 1
0.00.053.639 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.639 I print_info: rope_finetuned   = unknown
0.00.053.639 I print_info: ssm_d_conv       = 0
0.00.053.639 I print_info: ssm_d_inner      = 0
0.00.053.639 I print_info: ssm_d_state      = 0
0.00.053.640 I print_info: ssm_dt_rank      = 0
0.00.053.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.640 I print_info: model type       = 1.4B
0.00.053.640 I print_info: model params     = 1.41 B
0.00.053.641 I print_info: general.name     = 1.4B
0.00.053.641 I print_info: vocab type       = BPE
0.00.053.641 I print_info: n_vocab          = 50304
0.00.053.641 I print_info: n_merges         = 50009
0.00.053.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: LF token         = 128 'Ä'
0.00.053.642 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: max token length = 1024
0.00.055.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.726 I load_tensors: offloading output layer to GPU
0.00.055.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.738 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.739 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.035 I llama_init_from_model: n_seq_max     = 1
0.00.056.036 I llama_init_from_model: n_ctx         = 128
0.00.056.036 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.036 I llama_init_from_model: n_batch       = 128
0.00.056.036 I llama_init_from_model: n_ubatch      = 128
0.00.056.036 I llama_init_from_model: flash_attn    = 0
0.00.056.037 I llama_init_from_model: freq_base     = 10000.0
0.00.056.037 I llama_init_from_model: freq_scale    = 1
0.00.056.037 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.038 I ggml_metal_init: allocating
0.00.056.041 I ggml_metal_init: found device: Apple M4
0.00.056.043 I ggml_metal_init: picking default device: Apple M4
0.00.056.654 I ggml_metal_init: using embedded metal library
0.00.059.154 I ggml_metal_init: GPU name:   Apple M4
0.00.059.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.157 I ggml_metal_init: simdgroup reduction   = true
0.00.059.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.157 I ggml_metal_init: has bfloat            = true
0.00.059.157 I ggml_metal_init: use bfloat            = true
0.00.059.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.159 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.247 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.502 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.504 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.520 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.389 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.390 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.391 I llama_init_from_model: graph nodes  = 967
0.00.071.391 I llama_init_from_model: graph splits = 2
0.00.071.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.961 I 
0.00.629.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.992 I perplexity: tokenizing the input ..
0.00.636.938 I perplexity: tokenization took 6.944 ms
0.00.636.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.018 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.779.310 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.779.337 I llama_perf_context_print:        load time =     617.77 ms
0.00.779.338 I llama_perf_context_print: prompt eval time =     140.84 ms /   128 tokens (    1.10 ms per token,   908.81 tokens per second)
0.00.779.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.339 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.779.788 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.078s
sys	0m0.106s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.813 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.133 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.057 I llama_model_loader: - type  f32:  194 tensors
0.00.025.057 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.058 I print_info: file format = GGUF V3 (latest)
0.00.025.059 I print_info: file type   = Q6_K
0.00.025.060 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.082 I load: special tokens cache size = 25
0.00.052.251 I load: token to piece cache size = 0.2984 MB
0.00.052.256 I print_info: arch             = gptneox
0.00.052.256 I print_info: vocab_only       = 0
0.00.052.256 I print_info: n_ctx_train      = 2048
0.00.052.256 I print_info: n_embd           = 2048
0.00.052.256 I print_info: n_layer          = 24
0.00.052.261 I print_info: n_head           = 16
0.00.052.263 I print_info: n_head_kv        = 16
0.00.052.263 I print_info: n_rot            = 32
0.00.052.263 I print_info: n_swa            = 0
0.00.052.263 I print_info: n_embd_head_k    = 128
0.00.052.263 I print_info: n_embd_head_v    = 128
0.00.052.264 I print_info: n_gqa            = 1
0.00.052.265 I print_info: n_embd_k_gqa     = 2048
0.00.052.265 I print_info: n_embd_v_gqa     = 2048
0.00.052.266 I print_info: f_norm_eps       = 1.0e-05
0.00.052.266 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.267 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.267 I print_info: f_logit_scale    = 0.0e+00
0.00.052.267 I print_info: n_ff             = 8192
0.00.052.268 I print_info: n_expert         = 0
0.00.052.268 I print_info: n_expert_used    = 0
0.00.052.268 I print_info: causal attn      = 1
0.00.052.269 I print_info: pooling type     = 0
0.00.052.271 I print_info: rope type        = 2
0.00.052.271 I print_info: rope scaling     = linear
0.00.052.271 I print_info: freq_base_train  = 10000.0
0.00.052.272 I print_info: freq_scale_train = 1
0.00.052.272 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.272 I print_info: rope_finetuned   = unknown
0.00.052.272 I print_info: ssm_d_conv       = 0
0.00.052.272 I print_info: ssm_d_inner      = 0
0.00.052.272 I print_info: ssm_d_state      = 0
0.00.052.273 I print_info: ssm_dt_rank      = 0
0.00.052.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.273 I print_info: model type       = 1.4B
0.00.052.273 I print_info: model params     = 1.41 B
0.00.052.273 I print_info: general.name     = 1.4B
0.00.052.274 I print_info: vocab type       = BPE
0.00.052.275 I print_info: n_vocab          = 50304
0.00.052.275 I print_info: n_merges         = 50009
0.00.052.276 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.276 I print_info: LF token         = 128 'Ä'
0.00.052.277 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.277 I print_info: max token length = 1024
0.00.054.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.394 I load_tensors: offloading output layer to GPU
0.00.054.394 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.405 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.407 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.701 I llama_init_from_model: n_seq_max     = 1
0.00.054.702 I llama_init_from_model: n_ctx         = 2048
0.00.054.702 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.702 I llama_init_from_model: n_batch       = 2048
0.00.054.703 I llama_init_from_model: n_ubatch      = 512
0.00.054.703 I llama_init_from_model: flash_attn    = 0
0.00.054.703 I llama_init_from_model: freq_base     = 10000.0
0.00.054.704 I llama_init_from_model: freq_scale    = 1
0.00.054.704 I ggml_metal_init: allocating
0.00.054.708 I ggml_metal_init: found device: Apple M4
0.00.054.710 I ggml_metal_init: picking default device: Apple M4
0.00.055.369 I ggml_metal_init: using embedded metal library
0.00.057.935 I ggml_metal_init: GPU name:   Apple M4
0.00.057.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.938 I ggml_metal_init: simdgroup reduction   = true
0.00.057.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.939 I ggml_metal_init: has bfloat            = true
0.00.057.939 I ggml_metal_init: use bfloat            = true
0.00.057.939 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.243 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.039 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.045 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.065 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.095 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.096 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.096 I llama_init_from_model: graph nodes  = 967
0.00.091.097 I llama_init_from_model: graph splits = 2
0.00.091.100 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.247.144 I main: llama threadpool init, n_threads = 4
0.01.247.193 I 
0.01.247.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.247.215 I 
0.01.247.442 I sampler seed: 1234
0.01.247.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.247.492 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.247.495 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.247.495 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.02.132.794 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.02.132.794 I llama_perf_context_print:        load time =    1238.33 ms
0.02.132.795 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.51 tokens per second)
0.02.132.796 I llama_perf_context_print:        eval time =     827.90 ms /    63 runs   (   13.14 ms per token,    76.10 tokens per second)
0.02.132.796 I llama_perf_context_print:       total time =     885.65 ms /    70 tokens
0.02.133.047 I ggml_metal_free: deallocating

real	0m2.157s
user	0m0.112s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4480 (69fc940d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.850 I llama_model_loader: - type  f32:  194 tensors
0.00.023.851 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.851 I print_info: file format = GGUF V3 (latest)
0.00.023.852 I print_info: file type   = Q6_K
0.00.023.852 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.819 I load: special tokens cache size = 25
0.00.049.035 I load: token to piece cache size = 0.2984 MB
0.00.049.037 I print_info: arch             = gptneox
0.00.049.038 I print_info: vocab_only       = 0
0.00.049.038 I print_info: n_ctx_train      = 2048
0.00.049.038 I print_info: n_embd           = 2048
0.00.049.038 I print_info: n_layer          = 24
0.00.049.041 I print_info: n_head           = 16
0.00.049.042 I print_info: n_head_kv        = 16
0.00.049.042 I print_info: n_rot            = 32
0.00.049.044 I print_info: n_swa            = 0
0.00.049.044 I print_info: n_embd_head_k    = 128
0.00.049.044 I print_info: n_embd_head_v    = 128
0.00.049.045 I print_info: n_gqa            = 1
0.00.049.046 I print_info: n_embd_k_gqa     = 2048
0.00.049.046 I print_info: n_embd_v_gqa     = 2048
0.00.049.051 I print_info: f_norm_eps       = 1.0e-05
0.00.049.052 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.052 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.052 I print_info: f_logit_scale    = 0.0e+00
0.00.049.053 I print_info: n_ff             = 8192
0.00.049.053 I print_info: n_expert         = 0
0.00.049.053 I print_info: n_expert_used    = 0
0.00.049.054 I print_info: causal attn      = 1
0.00.049.054 I print_info: pooling type     = 0
0.00.049.054 I print_info: rope type        = 2
0.00.049.054 I print_info: rope scaling     = linear
0.00.049.055 I print_info: freq_base_train  = 10000.0
0.00.049.056 I print_info: freq_scale_train = 1
0.00.049.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.058 I print_info: rope_finetuned   = unknown
0.00.049.058 I print_info: ssm_d_conv       = 0
0.00.049.058 I print_info: ssm_d_inner      = 0
0.00.049.058 I print_info: ssm_d_state      = 0
0.00.049.059 I print_info: ssm_dt_rank      = 0
0.00.049.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.059 I print_info: model type       = 1.4B
0.00.049.060 I print_info: model params     = 1.41 B
0.00.049.060 I print_info: general.name     = 1.4B
0.00.049.061 I print_info: vocab type       = BPE
0.00.049.061 I print_info: n_vocab          = 50304
0.00.049.061 I print_info: n_merges         = 50009
0.00.049.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: LF token         = 128 'Ä'
0.00.049.064 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.064 I print_info: max token length = 1024
0.00.051.101 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.101 I load_tensors: offloading output layer to GPU
0.00.051.101 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.112 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.113 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.481 I llama_init_from_model: n_seq_max     = 1
0.00.051.481 I llama_init_from_model: n_ctx         = 128
0.00.051.482 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.482 I llama_init_from_model: n_batch       = 128
0.00.051.482 I llama_init_from_model: n_ubatch      = 128
0.00.051.482 I llama_init_from_model: flash_attn    = 0
0.00.051.482 I llama_init_from_model: freq_base     = 10000.0
0.00.051.483 I llama_init_from_model: freq_scale    = 1
0.00.051.483 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.483 I ggml_metal_init: allocating
0.00.051.486 I ggml_metal_init: found device: Apple M4
0.00.051.488 I ggml_metal_init: picking default device: Apple M4
0.00.052.081 I ggml_metal_init: using embedded metal library
0.00.054.489 I ggml_metal_init: GPU name:   Apple M4
0.00.054.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.491 I ggml_metal_init: simdgroup reduction   = true
0.00.054.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.491 I ggml_metal_init: has bfloat            = true
0.00.054.492 I ggml_metal_init: use bfloat            = true
0.00.054.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.141 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.388 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.391 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.352 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.352 I llama_init_from_model: graph nodes  = 967
0.00.066.352 I llama_init_from_model: graph splits = 2
0.00.066.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.322.411 I 
0.00.322.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.322.460 I perplexity: tokenizing the input ..
0.00.329.898 I perplexity: tokenization took 7.436 ms
0.00.329.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.469.816 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.470.982 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.471.001 I llama_perf_context_print:        load time =     313.66 ms
0.00.471.002 I llama_perf_context_print: prompt eval time =     139.69 ms /   128 tokens (    1.09 ms per token,   916.33 tokens per second)
0.00.471.002 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.471.003 I llama_perf_context_print:       total time =     148.59 ms /   129 tokens
0.00.471.341 I ggml_metal_free: deallocating

real	0m0.484s
user	0m0.077s
sys	0m0.057s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4480 (69fc940d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12660a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12660ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12660b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12660b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12660bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12660c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12660c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12660cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12660d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12660d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12660dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12660e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12660ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12660f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12660fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1266103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126610b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126611230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126612120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126612840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126612f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126613680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126613f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126614640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126614900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126614f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1266160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126616380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126616820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126616ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126617370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1266178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126617b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126618010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1266184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126618df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126619290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126619bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12661a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12661a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12661a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12661ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12661b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12661bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12661c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12661c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12661cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12661d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12661db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12661e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12661e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12661ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12661f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12661f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12661fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126620360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126620620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126620ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126620f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1266218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126621d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1266221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126622680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126622b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126623460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1266242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126624d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1266252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126625d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1266262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1266272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1266282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126628800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1266292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1266297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126629d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12662a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12662a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12662ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12662b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12662b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12662bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12661ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12662c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12662c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12662ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12662d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12662d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12662de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12662e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12662e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12662ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12662f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12662f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12662fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1266303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126630900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126630e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1266312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126631790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126631c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1266320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126632a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126633350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1266337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126633c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126634130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1266345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126634a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126634f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1266353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126635850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126635cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126636190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126636630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126636ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126636f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126637410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1266378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126637d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1266381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126638690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126638fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126639470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126639910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12663a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12663a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12663ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12663b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12663b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12663b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12663be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12663c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12663c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12663cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12663d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12663d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12663d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12663de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12663e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12663e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12663ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12663f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12663f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12663fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12663fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126640370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126640810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126640cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126641150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1266415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126641f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1266423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126642870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126642d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1266431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126643650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126643af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1266448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126645210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1266456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126645b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126645ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126646dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126647270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1266485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126648af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126649040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126649850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126649e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12664a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12664aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12664b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12664b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12664b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12664bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12664c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12664cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12664d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12664d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12664dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12664e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12664e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12664ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12664f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12664f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12664fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1266508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1266562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1266572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1266582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1266592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12665a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12665a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12665ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12665b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12665b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12665bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12665c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12665c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12665cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12665d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12665d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12665dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12665e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12665e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12665ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12665f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12665f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12665fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126660250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1266607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126660cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126661190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126661630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126661ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126661f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126662410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1266628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126662d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1266631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126663690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126663b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126663fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126664470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126664910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126664db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126665250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1266657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1266665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126666d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126667420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1266676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126667ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126668190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1266687a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.142.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1270056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1270063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1270075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127007a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1270080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127008c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1270093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127009bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12700a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12700aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12700b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12700b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12700c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12700c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12700ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12700d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12700dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12700e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12700e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12700e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12700eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12700f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12700f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12700faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127010750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127010bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127011030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1270114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1270121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1270133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1270149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1270152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1270168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127016e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1270177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127017c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1270180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127018520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127018990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127018e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127019270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1270196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127019b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127019fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12701a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12701a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12701ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12701b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12701b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12701ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12701bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12701c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12701c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12701cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12701d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12701d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12701d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12701dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12701e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12701e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12701eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12701efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12701f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12701f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12701fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127020160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1270205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127020a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127020eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127021320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127021790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127021c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127022070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1270224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127022950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127022dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127023230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1270236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127023b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127023f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1270243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127024860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127024cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127025140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1270255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127025a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127025e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127026300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127026770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127026be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127027050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1270274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127027930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127027da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127028210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127028680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127028af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127028f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1270293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127029cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12702a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12702a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12702aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12702ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12702b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12702b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12702bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12702c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12702c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12702c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12702cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12702d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12702d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12702dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12702df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12702e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12702e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12702ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12702f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12702f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12702f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12702fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1270302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127030730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127030ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127031010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127031480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1270318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127031d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1270321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127032640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127032ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127032f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127033390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127033800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127033c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1270340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127034550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1270349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127034e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1270352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127036450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1270368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1270371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127037610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127037a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127037ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127038360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1270387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127038c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1270390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127039520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127039990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127039e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12703a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12703a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12703ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12703afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12703b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12703b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12703bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12703c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12703c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12703ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12703ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12703d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12703d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12703dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12703e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12703e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12703e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12703ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12703f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12703f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12703fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127040130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1270405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127040a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127040e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1270412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127041810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127041d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127042b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127043110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1270436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127043c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127044250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127044810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127044dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127045390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127045950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127045f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1270464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127046a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127047050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127047610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127047bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127048190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127048750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127048d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1270492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127049e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12704a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12704a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12704af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12704b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12704bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12704c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12704c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12704cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12704d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12704d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12704dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12704e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12704e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12704eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12704f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12704fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127050010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1270505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127050b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127051150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127051710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127051cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127052290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127052e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1270533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127053990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127053f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127054510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127054ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127055090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127055650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127055c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1270561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127056790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127056d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127057250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127057750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127057c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127058150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127058650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127058b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127059050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127059550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127059a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127059f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12705a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12705a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12705ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12705b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12705b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12705c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12705c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12705d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12705d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12705da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12705e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12705e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12705eb40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12705bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12704c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12704b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127048450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127045c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127055350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127052b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127050890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12704e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127046790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127043f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127048fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12704a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12704f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12704c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127054210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127046d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12704f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127049b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127042e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12704d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127048a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1270530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12704e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127043990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127045650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127055ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12704b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127053690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127049590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12704bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12704fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12704ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127047310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1270519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1270461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1270547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127051f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12704da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127056a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127056490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127044510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127054d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12704ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127050e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127053c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127052550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12704a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127041fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12705dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127007ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12705efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12705f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12705f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12705f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12705faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12705fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127060020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1270602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1270605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127060860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127060b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127060de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1270610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127061360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127061620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1270618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127061ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127061e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127062120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1270623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127062930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127062bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127062eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127063170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127063430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1270636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1270639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127063c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127063f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1270641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1270644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127064770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127064a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127064cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127064fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127065270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127065530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1270657f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127065ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127065d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127066030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1270662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1270665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127066870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127066b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127066df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1270670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127067370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127067630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1270678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127067bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127067e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127068130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1270683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1270686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127068970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127068c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127068ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1270691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127069470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127069730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1270699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127069cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127069f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12706a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12706a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12706a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12706aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12706ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12706aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12706b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12706b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12706b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12706baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12706bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12706c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12706c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12706c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12706c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12706cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12706ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12706d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12706d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12706d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12706d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12706dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12706deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12706e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12706e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12706e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12706e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12706ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12706ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12706f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12706f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12706f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12706fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12706fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12706ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127070270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127070530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1270707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127070ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127070d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127071030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1270712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1270715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127071870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127071b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127071df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1270720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127072370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127072630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1270728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127072bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127072e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127073130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1270733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1270736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127073970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127073c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127073ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1270741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127074470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127074730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1270749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127074cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127074f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127075230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1270754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1270757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127075a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127075d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127075ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1270762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127076570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127076830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127076af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127076db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127077070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127077330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1270775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1270778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127077b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127077e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1270780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1270783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127078670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127078930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127078bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127078eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127079170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127079430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1277044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127705950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127705c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127705ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127706340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1277067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127706c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127707090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127707500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127707970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127707de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127708250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1277086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127708fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127709410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127709880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127709cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12770a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12770a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12770aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12770aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12770b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12770b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12770bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12770c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12770c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12770c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12770cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12770d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12770d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12770db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12770df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12770e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12770e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12770ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12770f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12770f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12770fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12770fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127710300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127710770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127710be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127711050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1277114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127711930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127712210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127712680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127712af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127712f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1277133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127713840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127713cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127714120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127714590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127714a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127714e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1277152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127715750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127715bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127716030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1277164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127716910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1277171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127717ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127717f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1277183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127718820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127718c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127719100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127719570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127719fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12771a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12771ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12771b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12771b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12771bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12771c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12771c880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.777s
user	0m0.294s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4480 (69fc940d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12310a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12310aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12310b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12310b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12310bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12310c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12310c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12310ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12310d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12310d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12310dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12310e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12310ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12310f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12310fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123110370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123110a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1231111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1231118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1231120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1231127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123112ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123113600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123113ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1231145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123114880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123114e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123115b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123116040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123116300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1231167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123116a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1231172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123117830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123117af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123117f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123118430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1231188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123118d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123119210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1231196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123119b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123119ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12311a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12311a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12311ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12311b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12311bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12311c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12311c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12311cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12311d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12311dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12311e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12311e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12311ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12311f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12311f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12311faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1231202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1231205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123120a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123120ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123121380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123121820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123121cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123122160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123122600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123122aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123122f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1231233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123123880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123123d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123124270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1231247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123124d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123125260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1231257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123125d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123126250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1231267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123126cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123127240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123127790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123127ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123128230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123128780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123128cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123129220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123129770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123129cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12312a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12312a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12312acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12312b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12312b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12312bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12311b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12312c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12312c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12312ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12312d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12312d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12312de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12312e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12312e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12312edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12312f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12312f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12312fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123130330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123130880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123130dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123131270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123131710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123131bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123132050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1231324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123132990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123132e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1231332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123133770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123133c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1231340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123134550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1231349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123134e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123135330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1231357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123135c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123136110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1231365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123136a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123136ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123137390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123137830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123137cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123138170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123138610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123138ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123138f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1231393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123139890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123139d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12313a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12313a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12313ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12313afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12313b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12313b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12313bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12313c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12313c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12313cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12313d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12313d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12313d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12313ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12313e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12313e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12313ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12313f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12313f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12313f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12313fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1231402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123140790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123140c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1231410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123141570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123141a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123141eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123142350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1231427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123142c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123143130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1231435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123143a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123143f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1231443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123144850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123144cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123145190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123145630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123145ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123145f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123146410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1231468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123146d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1231471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123147690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123147b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123147fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123148520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123148a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123148fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123149510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1231497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123149de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12314a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12314aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12314b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12314b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12314b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12314bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12314c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12314cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12314d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12314d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12314db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12314e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12314e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12314ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12314f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12314f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12314fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1231502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123150820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123150d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1231512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123151810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123151d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1231522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123152800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123152d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1231532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1231537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123153d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123154290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1231547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123154d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123155280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1231557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123155d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123156270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1231567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123156d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123157260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1231577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123157d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123158250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1231587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123158cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123159240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123159790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123159ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12315a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12315a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12315acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12315b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12315b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12315bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12315c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12315c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12315ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12315d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12315d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12315dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12315e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12315e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12315ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12315f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12315f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12315fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1231601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123160720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123160c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123161110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1231615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123161a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123161ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123162390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123162830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123162cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123163170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123163610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123163ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123163f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1231643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123164890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123164d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1231651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123165720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123165e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123166560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123166c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1231673a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123167660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123167e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123168110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123168720 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.886 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f07d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f08640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f08ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f08f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f09390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f09800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f09c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f0bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f0d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f0e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f0f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f0f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f0feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f11410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f11990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f12270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f12b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f13080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f13c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f14970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f1a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f1a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f1b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f1b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f1be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f1cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f1dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f1e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f1eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f1ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f1f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f21b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f22000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f22470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f23630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f23f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f25540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f26700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f26fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f28a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f29360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f29c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f2a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f2a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f2b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f2b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f2bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f2bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f2ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f2e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f2e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f2f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f2fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f31410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f31880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f32a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f32eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f33790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f33c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f34070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f34950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f43190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f4f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f56fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f59db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f5d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f5deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f5e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f5e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f60100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f60ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f61ba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1230046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1230058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1230065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123007a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123008530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123008ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1230094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123009c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12300a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12300aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12300b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12300b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12300c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12300c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12300cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12300d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12300dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12300dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12300e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12300e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12300eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12300efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12300f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12300f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12300fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123010080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1230104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123010960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123010dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123011240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1230116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123011f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123012400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123012870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123012ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1230135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123013a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123013ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123014780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123014bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1230154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123016220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123016c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123017100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123017570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1230179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1230182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123018730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123018ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123019010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123019480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1230198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123019d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12301a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12301a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12301aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12301af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12301b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12301b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12301bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12301c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12301c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12301c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12301ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12301d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12301d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12301db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12301dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12301e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12301e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12301ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12301f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12301f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12301fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12301ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123020370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1230207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123020c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1230210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123021530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1230219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123022280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1230226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123022b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123022fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123023cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1230255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123025ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123026780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123026bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1230274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123028690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123028f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1230293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123029850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12302a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12302a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12302aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12302ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12302b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12302b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12302bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12302c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12302c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12302c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12302cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12302d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12302d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12302dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12302df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12302e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12302e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12302eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12302f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12302f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12302f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12302fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1230302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123030740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123031020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123031900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123031d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1230321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123032650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123032ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1230333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123033810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1230340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123034560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1230349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1230352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123035b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1230368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123036d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1230371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123037630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123037aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123037f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123038380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1230387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123038c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1230390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123039540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1230399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12303a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12303a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12303ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12303afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12303b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12303b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12303bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12303c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12303c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12303ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12303cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12303d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12303d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12303dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12303e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12303e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12303e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12303ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12303f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12303f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12303fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12303ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1230408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123040d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123041180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123041d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123042280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1230426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123042b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123042fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1230438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123043d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123044ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123045350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1230457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1230460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123046510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123046980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123046df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1230476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123047b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123049170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1230495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123049a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12304a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12304a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12304ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12304b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12304b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12304b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12304bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12304c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12304c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12304cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12304cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12304d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12304d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12304dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12304e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12304e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12304ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12304eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12304f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12304f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12304fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123050060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1230504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123050940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123050db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123051220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123051690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123051b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123051f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1230523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123052cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123053130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1230535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123053a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123053e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1230542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123054760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123054bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123055040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1230554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123055920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123056390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123056ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1230571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1230578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123057bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123058020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123058620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123058c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.244s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.68 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.14 user         0.04 sys
```
