### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.44 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.23 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   24.82 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.32 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.16 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.20 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.18 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.23 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.20 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.79 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  173.30 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.33 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.48 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 214.84 sec*proc (27 tests)

Total Test time (real) = 214.86 sec

real	3m34.892s
user	7m25.425s
sys	0m5.133s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.17 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.05 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.21 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    0.90 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.17 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.17 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.21 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.17 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.29 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   28.04 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.10 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.74 sec*proc (27 tests)

Total Test time (real) =  49.75 sec

real	0m49.758s
user	1m10.649s
sys	0m4.449s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.121 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.472 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.436 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.447 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.449 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.450 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.451 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.453 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.453 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.454 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.455 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.456 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.460 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.460 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.461 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.462 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.463 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.463 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.464 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.593 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.004 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.006 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.007 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.007 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.008 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.008 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.009 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.009 I llama_model_loader: - type  f32:  124 tensors
0.00.034.010 I llama_model_loader: - type  f16:   73 tensors
0.00.039.037 I llm_load_vocab: special tokens cache size = 5
0.00.041.478 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.482 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.482 I llm_load_print_meta: arch             = bert
0.00.041.483 I llm_load_print_meta: vocab type       = WPM
0.00.041.483 I llm_load_print_meta: n_vocab          = 30522
0.00.041.484 I llm_load_print_meta: n_merges         = 0
0.00.041.484 I llm_load_print_meta: vocab_only       = 0
0.00.041.484 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.484 I llm_load_print_meta: n_embd           = 384
0.00.041.484 I llm_load_print_meta: n_layer          = 12
0.00.041.488 I llm_load_print_meta: n_head           = 12
0.00.041.489 I llm_load_print_meta: n_head_kv        = 12
0.00.041.489 I llm_load_print_meta: n_rot            = 32
0.00.041.490 I llm_load_print_meta: n_swa            = 0
0.00.041.490 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.490 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.491 I llm_load_print_meta: n_gqa            = 1
0.00.041.492 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.495 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.496 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.496 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.497 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.497 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.497 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.498 I llm_load_print_meta: n_ff             = 1536
0.00.041.498 I llm_load_print_meta: n_expert         = 0
0.00.041.500 I llm_load_print_meta: n_expert_used    = 0
0.00.041.501 I llm_load_print_meta: causal attn      = 0
0.00.041.501 I llm_load_print_meta: pooling type     = 2
0.00.041.501 I llm_load_print_meta: rope type        = 2
0.00.041.501 I llm_load_print_meta: rope scaling     = linear
0.00.041.502 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.502 I llm_load_print_meta: freq_scale_train = 1
0.00.041.502 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.503 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.503 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.503 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.503 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.504 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.517 I llm_load_print_meta: model type       = 33M
0.00.041.517 I llm_load_print_meta: model ftype      = F16
0.00.041.518 I llm_load_print_meta: model params     = 33.21 M
0.00.041.518 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.519 I llm_load_print_meta: general.name     = Bge Small
0.00.041.519 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.520 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.521 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.521 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.521 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.522 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.522 I llm_load_print_meta: max token length = 21
0.00.043.365 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.366 I llm_load_tensors: offloading output layer to GPU
0.00.043.367 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.391 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.393 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.043.967 I llama_new_context_with_model: n_ctx         = 512
0.00.043.967 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.043.968 I llama_new_context_with_model: n_batch       = 2048
0.00.043.968 I llama_new_context_with_model: n_ubatch      = 2048
0.00.043.968 I llama_new_context_with_model: flash_attn    = 0
0.00.043.969 I llama_new_context_with_model: freq_base     = 10000.0
0.00.043.969 I llama_new_context_with_model: freq_scale    = 1
0.00.043.970 I ggml_metal_init: allocating
0.00.043.974 I ggml_metal_init: found device: Apple M4
0.00.043.977 I ggml_metal_init: picking default device: Apple M4
0.00.044.844 I ggml_metal_init: using embedded metal library
0.00.048.529 I ggml_metal_init: GPU name:   Apple M4
0.00.048.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.534 I ggml_metal_init: simdgroup reduction   = true
0.00.048.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.534 I ggml_metal_init: has bfloat            = true
0.00.048.534 I ggml_metal_init: use bfloat            = true
0.00.048.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.104 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.106 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.108 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.993 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.994 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.995 I llama_new_context_with_model: graph nodes  = 429
0.00.060.995 I llama_new_context_with_model: graph splits = 2
0.00.061.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.471 I 
0.00.071.489 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.072.133 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.918 I llama_perf_context_print:        load time =      48.99 ms
0.00.076.919 I llama_perf_context_print: prompt eval time =       4.62 ms /     9 tokens (    0.51 ms per token,  1945.95 tokens per second)
0.00.076.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.925 I llama_perf_context_print:       total time =       5.45 ms /    10 tokens
0.00.077.043 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.062s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.418 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.452 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.456 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.457 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.457 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.458 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.458 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.459 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.459 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.459 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.460 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.462 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.462 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.462 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.463 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.463 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.463 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.463 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.547 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.548 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.549 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.549 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.549 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.549 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.550 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.550 I llama_model_loader: - type  f32:  124 tensors
0.00.013.550 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.904 I llm_load_vocab: special tokens cache size = 5
0.00.017.213 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.216 I llm_load_print_meta: arch             = bert
0.00.017.216 I llm_load_print_meta: vocab type       = WPM
0.00.017.216 I llm_load_print_meta: n_vocab          = 30522
0.00.017.216 I llm_load_print_meta: n_merges         = 0
0.00.017.217 I llm_load_print_meta: vocab_only       = 0
0.00.017.217 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.217 I llm_load_print_meta: n_embd           = 384
0.00.017.217 I llm_load_print_meta: n_layer          = 12
0.00.017.218 I llm_load_print_meta: n_head           = 12
0.00.017.219 I llm_load_print_meta: n_head_kv        = 12
0.00.017.219 I llm_load_print_meta: n_rot            = 32
0.00.017.219 I llm_load_print_meta: n_swa            = 0
0.00.017.219 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.220 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.220 I llm_load_print_meta: n_gqa            = 1
0.00.017.221 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.221 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.222 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.222 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.223 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.223 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.223 I llm_load_print_meta: n_ff             = 1536
0.00.017.223 I llm_load_print_meta: n_expert         = 0
0.00.017.224 I llm_load_print_meta: n_expert_used    = 0
0.00.017.224 I llm_load_print_meta: causal attn      = 0
0.00.017.224 I llm_load_print_meta: pooling type     = 2
0.00.017.224 I llm_load_print_meta: rope type        = 2
0.00.017.224 I llm_load_print_meta: rope scaling     = linear
0.00.017.225 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.225 I llm_load_print_meta: freq_scale_train = 1
0.00.017.225 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.225 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.225 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.226 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.226 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.231 I llm_load_print_meta: model type       = 33M
0.00.017.232 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.232 I llm_load_print_meta: model params     = 33.21 M
0.00.017.233 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.233 I llm_load_print_meta: general.name     = Bge Small
0.00.017.233 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.233 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.233 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.234 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.234 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.234 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.234 I llm_load_print_meta: max token length = 21
0.00.018.363 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.364 I llm_load_tensors: offloading output layer to GPU
0.00.018.364 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.371 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.371 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.720 I llama_new_context_with_model: n_ctx         = 512
0.00.018.720 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.720 I llama_new_context_with_model: n_batch       = 2048
0.00.018.720 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.721 I llama_new_context_with_model: flash_attn    = 0
0.00.018.721 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.721 I llama_new_context_with_model: freq_scale    = 1
0.00.018.722 I ggml_metal_init: allocating
0.00.018.726 I ggml_metal_init: found device: Apple M4
0.00.018.728 I ggml_metal_init: picking default device: Apple M4
0.00.019.194 I ggml_metal_init: using embedded metal library
0.00.021.170 I ggml_metal_init: GPU name:   Apple M4
0.00.021.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.173 I ggml_metal_init: simdgroup reduction   = true
0.00.021.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.173 I ggml_metal_init: has bfloat            = true
0.00.021.174 I ggml_metal_init: use bfloat            = true
0.00.021.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.028.702 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.028.704 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.028.705 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.029.258 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.029.259 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.029.260 I llama_new_context_with_model: graph nodes  = 429
0.00.029.260 I llama_new_context_with_model: graph splits = 2
0.00.029.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.034.492 I 
0.00.034.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.035.016 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.039.676 I llama_perf_context_print:        load time =      26.07 ms
0.00.039.677 I llama_perf_context_print: prompt eval time =       4.52 ms /     9 tokens (    0.50 ms per token,  1989.83 tokens per second)
0.00.039.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.039.678 I llama_perf_context_print:       total time =       5.19 ms /    10 tokens
0.00.039.783 I ggml_metal_free: deallocating

real	0m0.049s
user	0m0.026s
sys	0m0.012s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.192 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.050 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.550 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.558 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.560 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.561 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.561 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.563 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.564 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.564 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.565 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.566 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.569 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.570 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.571 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.032 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.033 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.033 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.034 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.034 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.034 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.035 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.035 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.036 I llama_model_loader: - type  f32:   41 tensors
0.00.050.036 I llama_model_loader: - type  f16:   29 tensors
0.00.068.474 W llm_load_vocab: empty token at index 5
0.00.073.137 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.423 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.467 I llm_load_vocab: special tokens cache size = 5
0.00.317.089 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.317.095 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.317.095 I llm_load_print_meta: arch             = jina-bert-v2
0.00.317.095 I llm_load_print_meta: vocab type       = BPE
0.00.317.096 I llm_load_print_meta: n_vocab          = 61056
0.00.317.096 I llm_load_print_meta: n_merges         = 39382
0.00.317.096 I llm_load_print_meta: vocab_only       = 0
0.00.317.096 I llm_load_print_meta: n_ctx_train      = 8192
0.00.317.096 I llm_load_print_meta: n_embd           = 384
0.00.317.096 I llm_load_print_meta: n_layer          = 4
0.00.317.102 I llm_load_print_meta: n_head           = 12
0.00.317.102 I llm_load_print_meta: n_head_kv        = 12
0.00.317.103 I llm_load_print_meta: n_rot            = 32
0.00.317.103 I llm_load_print_meta: n_swa            = 0
0.00.317.103 I llm_load_print_meta: n_embd_head_k    = 32
0.00.317.103 I llm_load_print_meta: n_embd_head_v    = 32
0.00.317.104 I llm_load_print_meta: n_gqa            = 1
0.00.317.104 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.317.105 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.317.105 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.317.106 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.317.106 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.317.107 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.317.107 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.317.107 I llm_load_print_meta: n_ff             = 1536
0.00.317.108 I llm_load_print_meta: n_expert         = 0
0.00.317.108 I llm_load_print_meta: n_expert_used    = 0
0.00.317.108 I llm_load_print_meta: causal attn      = 0
0.00.317.108 I llm_load_print_meta: pooling type     = -1
0.00.317.108 I llm_load_print_meta: rope type        = -1
0.00.317.108 I llm_load_print_meta: rope scaling     = linear
0.00.317.109 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.317.109 I llm_load_print_meta: freq_scale_train = 1
0.00.317.109 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.317.110 I llm_load_print_meta: rope_finetuned   = unknown
0.00.317.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.317.113 I llm_load_print_meta: ssm_d_inner      = 0
0.00.317.113 I llm_load_print_meta: ssm_d_state      = 0
0.00.317.113 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.317.114 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.317.141 I llm_load_print_meta: model type       = 33M
0.00.317.141 I llm_load_print_meta: model ftype      = F16
0.00.317.141 I llm_load_print_meta: model params     = 32.90 M
0.00.317.142 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.317.142 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.317.142 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.317.143 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.317.143 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.317.143 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.317.144 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.317.145 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.317.145 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.317.145 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.317.145 I llm_load_print_meta: max token length = 45
0.00.318.279 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.318.280 I llm_load_tensors: offloading output layer to GPU
0.00.318.280 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.318.304 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.318.305 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.319.221 I llama_new_context_with_model: n_seq_max     = 1
0.00.319.223 I llama_new_context_with_model: n_ctx         = 8192
0.00.319.223 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.319.223 I llama_new_context_with_model: n_batch       = 2048
0.00.319.224 I llama_new_context_with_model: n_ubatch      = 2048
0.00.319.224 I llama_new_context_with_model: flash_attn    = 0
0.00.319.224 I llama_new_context_with_model: freq_base     = 10000.0
0.00.319.225 I llama_new_context_with_model: freq_scale    = 1
0.00.319.226 I ggml_metal_init: allocating
0.00.319.230 I ggml_metal_init: found device: Apple M4
0.00.319.232 I ggml_metal_init: picking default device: Apple M4
0.00.320.160 I ggml_metal_init: using embedded metal library
0.00.322.247 I ggml_metal_init: GPU name:   Apple M4
0.00.322.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.322.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.322.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.322.249 I ggml_metal_init: simdgroup reduction   = true
0.00.322.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.322.249 I ggml_metal_init: has bfloat            = true
0.00.322.249 I ggml_metal_init: use bfloat            = true
0.00.322.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.322.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.683 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.332.685 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.332.686 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.333.263 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.333.263 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.333.264 I llama_new_context_with_model: graph nodes  = 154
0.00.333.264 I llama_new_context_with_model: graph splits = 2
0.00.333.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.347.435 I 
0.00.347.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.347.613 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.347.614 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.347.617 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.347.617 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.347.620 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.347.620 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.348.171 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.350.901 I llama_perf_context_print:        load time =     323.38 ms
0.00.350.902 I llama_perf_context_print: prompt eval time =       2.72 ms /    62 tokens (    0.04 ms per token, 22777.37 tokens per second)
0.00.350.902 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.350.903 I llama_perf_context_print:       total time =       3.47 ms /    63 tokens
0.00.351.097 I ggml_metal_free: deallocating

real	0m1.027s
user	0m0.331s
sys	0m0.039s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.228 I main: llama backend init
0.00.000.235 I main: load the model and apply lora adapter, if any
0.00.055.063 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.067.535 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.564 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.898 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.086.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.186 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.189 I llama_model_loader: - type  f32:  194 tensors
0.00.086.190 I llama_model_loader: - type  f16:   98 tensors
0.00.122.483 I llm_load_vocab: special tokens cache size = 25
0.00.130.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.130.015 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.130.015 I llm_load_print_meta: arch             = gptneox
0.00.130.016 I llm_load_print_meta: vocab type       = BPE
0.00.130.016 I llm_load_print_meta: n_vocab          = 50304
0.00.130.016 I llm_load_print_meta: n_merges         = 50009
0.00.130.016 I llm_load_print_meta: vocab_only       = 0
0.00.130.017 I llm_load_print_meta: n_ctx_train      = 2048
0.00.130.017 I llm_load_print_meta: n_embd           = 2048
0.00.130.017 I llm_load_print_meta: n_layer          = 24
0.00.130.021 I llm_load_print_meta: n_head           = 16
0.00.130.022 I llm_load_print_meta: n_head_kv        = 16
0.00.130.022 I llm_load_print_meta: n_rot            = 32
0.00.130.022 I llm_load_print_meta: n_swa            = 0
0.00.130.022 I llm_load_print_meta: n_embd_head_k    = 128
0.00.130.022 I llm_load_print_meta: n_embd_head_v    = 128
0.00.130.023 I llm_load_print_meta: n_gqa            = 1
0.00.130.024 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.130.025 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.130.026 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.130.026 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.130.029 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.130.029 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.130.029 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.130.030 I llm_load_print_meta: n_ff             = 8192
0.00.130.030 I llm_load_print_meta: n_expert         = 0
0.00.130.031 I llm_load_print_meta: n_expert_used    = 0
0.00.130.031 I llm_load_print_meta: causal attn      = 1
0.00.130.032 I llm_load_print_meta: pooling type     = 0
0.00.130.032 I llm_load_print_meta: rope type        = 2
0.00.130.032 I llm_load_print_meta: rope scaling     = linear
0.00.130.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.130.032 I llm_load_print_meta: freq_scale_train = 1
0.00.130.033 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.130.033 I llm_load_print_meta: rope_finetuned   = unknown
0.00.130.033 I llm_load_print_meta: ssm_d_conv       = 0
0.00.130.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.130.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.130.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.130.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.130.045 I llm_load_print_meta: model type       = 1.4B
0.00.130.045 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.130.046 I llm_load_print_meta: model params     = 1.41 B
0.00.130.046 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.130.046 I llm_load_print_meta: general.name     = 1.4B
0.00.130.047 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.130.047 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.130.047 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.130.047 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.130.047 I llm_load_print_meta: LF token         = 128 ''
0.00.130.048 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.130.048 I llm_load_print_meta: max token length = 1024
0.00.131.911 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.131.911 I llm_load_tensors: offloading output layer to GPU
0.00.131.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.131.929 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.131.930 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.132.884 I llama_new_context_with_model: n_seq_max     = 1
0.00.132.885 I llama_new_context_with_model: n_ctx         = 2048
0.00.132.885 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.132.885 I llama_new_context_with_model: n_batch       = 2048
0.00.132.886 I llama_new_context_with_model: n_ubatch      = 512
0.00.132.886 I llama_new_context_with_model: flash_attn    = 0
0.00.132.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.132.887 I llama_new_context_with_model: freq_scale    = 1
0.00.132.887 I ggml_metal_init: allocating
0.00.132.895 I ggml_metal_init: found device: Apple M4
0.00.132.898 I ggml_metal_init: picking default device: Apple M4
0.00.133.580 I ggml_metal_init: using embedded metal library
0.00.149.429 I ggml_metal_init: GPU name:   Apple M4
0.00.149.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.149.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.149.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.149.432 I ggml_metal_init: simdgroup reduction   = true
0.00.149.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.149.433 I ggml_metal_init: has bfloat            = true
0.00.149.433 I ggml_metal_init: use bfloat            = true
0.00.149.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.149.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.186.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.566 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.585 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.187.505 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.187.506 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.187.506 I llama_new_context_with_model: graph nodes  = 967
0.00.187.507 I llama_new_context_with_model: graph splits = 2
0.00.187.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.313.640 I main: llama threadpool init, n_threads = 4
0.00.313.676 I 
0.00.313.702 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.313.702 I 
0.00.313.952 I sampler seed: 1234
0.00.313.958 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.313.982 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.313.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.313.984 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.147.948 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.147.949 I llama_perf_context_print:        load time =     258.56 ms
0.02.147.950 I llama_perf_context_print: prompt eval time =      37.89 ms /     7 tokens (    5.41 ms per token,   184.75 tokens per second)
0.02.147.950 I llama_perf_context_print:        eval time =    1793.16 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.147.951 I llama_perf_context_print:       total time =    1834.31 ms /    70 tokens
0.02.148.135 I ggml_metal_free: deallocating

real	0m2.482s
user	0m0.150s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.791 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.151 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.028 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.815 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.817 I llama_model_loader: - type  f32:  194 tensors
0.00.056.817 I llama_model_loader: - type  f16:   98 tensors
0.00.087.423 I llm_load_vocab: special tokens cache size = 25
0.00.094.097 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.099 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.100 I llm_load_print_meta: arch             = gptneox
0.00.094.100 I llm_load_print_meta: vocab type       = BPE
0.00.094.100 I llm_load_print_meta: n_vocab          = 50304
0.00.094.100 I llm_load_print_meta: n_merges         = 50009
0.00.094.100 I llm_load_print_meta: vocab_only       = 0
0.00.094.101 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.101 I llm_load_print_meta: n_embd           = 2048
0.00.094.101 I llm_load_print_meta: n_layer          = 24
0.00.094.104 I llm_load_print_meta: n_head           = 16
0.00.094.105 I llm_load_print_meta: n_head_kv        = 16
0.00.094.105 I llm_load_print_meta: n_rot            = 32
0.00.094.107 I llm_load_print_meta: n_swa            = 0
0.00.094.107 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.107 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.108 I llm_load_print_meta: n_gqa            = 1
0.00.094.109 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.110 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.112 I llm_load_print_meta: n_ff             = 8192
0.00.094.112 I llm_load_print_meta: n_expert         = 0
0.00.094.113 I llm_load_print_meta: n_expert_used    = 0
0.00.094.113 I llm_load_print_meta: causal attn      = 1
0.00.094.113 I llm_load_print_meta: pooling type     = 0
0.00.094.113 I llm_load_print_meta: rope type        = 2
0.00.094.113 I llm_load_print_meta: rope scaling     = linear
0.00.094.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.114 I llm_load_print_meta: freq_scale_train = 1
0.00.094.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.118 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.118 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.130 I llm_load_print_meta: model type       = 1.4B
0.00.094.131 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.131 I llm_load_print_meta: model params     = 1.41 B
0.00.094.132 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.132 I llm_load_print_meta: general.name     = 1.4B
0.00.094.132 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.132 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.132 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.132 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.133 I llm_load_print_meta: LF token         = 128 ''
0.00.094.133 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.133 I llm_load_print_meta: max token length = 1024
0.00.096.741 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.741 I llm_load_tensors: offloading output layer to GPU
0.00.096.742 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.752 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.753 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.765 I llama_new_context_with_model: n_ctx         = 128
0.00.097.765 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.765 I llama_new_context_with_model: n_batch       = 128
0.00.097.766 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.766 I llama_new_context_with_model: flash_attn    = 0
0.00.097.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.767 I llama_new_context_with_model: freq_scale    = 1
0.00.097.767 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.768 I ggml_metal_init: allocating
0.00.097.778 I ggml_metal_init: found device: Apple M4
0.00.097.780 I ggml_metal_init: picking default device: Apple M4
0.00.098.431 I ggml_metal_init: using embedded metal library
0.00.101.430 I ggml_metal_init: GPU name:   Apple M4
0.00.101.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.433 I ggml_metal_init: simdgroup reduction   = true
0.00.101.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.433 I ggml_metal_init: has bfloat            = true
0.00.101.433 I ggml_metal_init: use bfloat            = true
0.00.101.434 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.401 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.418 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.364 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.366 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.366 I llama_new_context_with_model: graph nodes  = 967
0.00.111.366 I llama_new_context_with_model: graph splits = 2
0.00.111.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.511.475 I 
0.01.511.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.511.606 I perplexity: tokenizing the input ..
0.01.525.832 I perplexity: tokenization took 14.224 ms
0.01.525.838 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.647.640 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.649.122 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.649.146 I llama_perf_context_print:        load time =    1486.31 ms
0.01.649.148 I llama_perf_context_print: prompt eval time =     120.86 ms /   128 tokens (    0.94 ms per token,  1059.06 tokens per second)
0.01.649.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.649.149 I llama_perf_context_print:       total time =     137.67 ms /   129 tokens
0.01.649.491 I ggml_metal_free: deallocating

real	0m1.862s
user	0m0.121s
sys	0m0.258s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.662 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.487 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.490 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.490 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.491 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.493 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.494 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.494 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.978 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.978 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.979 I llama_model_loader: - type  f32:  194 tensors
0.00.024.979 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.126 I llm_load_vocab: special tokens cache size = 25
0.00.052.069 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.073 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.074 I llm_load_print_meta: arch             = gptneox
0.00.052.074 I llm_load_print_meta: vocab type       = BPE
0.00.052.075 I llm_load_print_meta: n_vocab          = 50304
0.00.052.075 I llm_load_print_meta: n_merges         = 50009
0.00.052.075 I llm_load_print_meta: vocab_only       = 0
0.00.052.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.077 I llm_load_print_meta: n_embd           = 2048
0.00.052.077 I llm_load_print_meta: n_layer          = 24
0.00.052.080 I llm_load_print_meta: n_head           = 16
0.00.052.081 I llm_load_print_meta: n_head_kv        = 16
0.00.052.081 I llm_load_print_meta: n_rot            = 32
0.00.052.082 I llm_load_print_meta: n_swa            = 0
0.00.052.082 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.082 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.083 I llm_load_print_meta: n_gqa            = 1
0.00.052.083 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.084 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.085 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.087 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.088 I llm_load_print_meta: n_ff             = 8192
0.00.052.088 I llm_load_print_meta: n_expert         = 0
0.00.052.088 I llm_load_print_meta: n_expert_used    = 0
0.00.052.088 I llm_load_print_meta: causal attn      = 1
0.00.052.089 I llm_load_print_meta: pooling type     = 0
0.00.052.089 I llm_load_print_meta: rope type        = 2
0.00.052.089 I llm_load_print_meta: rope scaling     = linear
0.00.052.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.090 I llm_load_print_meta: freq_scale_train = 1
0.00.052.090 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.093 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.093 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.093 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.105 I llm_load_print_meta: model type       = 1.4B
0.00.052.106 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.106 I llm_load_print_meta: model params     = 1.41 B
0.00.052.107 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.107 I llm_load_print_meta: general.name     = 1.4B
0.00.052.107 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.107 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.108 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.108 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.108 I llm_load_print_meta: LF token         = 128 ''
0.00.052.109 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.109 I llm_load_print_meta: max token length = 1024
0.00.053.869 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.870 I llm_load_tensors: offloading output layer to GPU
0.00.053.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.883 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.053.885 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.054.809 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.810 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.810 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.810 I llama_new_context_with_model: n_batch       = 2048
0.00.054.811 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.811 I llama_new_context_with_model: flash_attn    = 0
0.00.054.811 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.811 I llama_new_context_with_model: freq_scale    = 1
0.00.054.812 I ggml_metal_init: allocating
0.00.054.818 I ggml_metal_init: found device: Apple M4
0.00.054.820 I ggml_metal_init: picking default device: Apple M4
0.00.055.511 I ggml_metal_init: using embedded metal library
0.00.057.638 I ggml_metal_init: GPU name:   Apple M4
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.641 I ggml_metal_init: simdgroup reduction   = true
0.00.057.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.641 I ggml_metal_init: has bfloat            = true
0.00.057.641 I ggml_metal_init: use bfloat            = true
0.00.057.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.099 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.107 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.130 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.123 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.125 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.126 I llama_new_context_with_model: graph nodes  = 967
0.00.090.126 I llama_new_context_with_model: graph splits = 2
0.00.090.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.181.191 I main: llama threadpool init, n_threads = 4
0.01.181.227 I 
0.01.181.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.181.255 I 
0.01.181.495 I sampler seed: 1234
0.01.181.501 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.181.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.181.552 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.181.552 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.257.191 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.02.257.192 I llama_perf_context_print:        load time =    1171.52 ms
0.02.257.192 I llama_perf_context_print: prompt eval time =      33.49 ms /     7 tokens (    4.78 ms per token,   209.04 tokens per second)
0.02.257.193 I llama_perf_context_print:        eval time =    1039.04 ms /    63 runs   (   16.49 ms per token,    60.63 tokens per second)
0.02.257.193 I llama_perf_context_print:       total time =    1076.00 ms /    70 tokens
0.02.257.362 I ggml_metal_free: deallocating

real	0m2.275s
user	0m0.110s
sys	0m0.252s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.787 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.628 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.646 I llama_model_loader: - type  f32:  194 tensors
0.00.029.646 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.132 I llm_load_vocab: special tokens cache size = 25
0.00.060.117 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.120 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.120 I llm_load_print_meta: arch             = gptneox
0.00.060.121 I llm_load_print_meta: vocab type       = BPE
0.00.060.121 I llm_load_print_meta: n_vocab          = 50304
0.00.060.121 I llm_load_print_meta: n_merges         = 50009
0.00.060.121 I llm_load_print_meta: vocab_only       = 0
0.00.060.121 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.122 I llm_load_print_meta: n_embd           = 2048
0.00.060.122 I llm_load_print_meta: n_layer          = 24
0.00.060.125 I llm_load_print_meta: n_head           = 16
0.00.060.126 I llm_load_print_meta: n_head_kv        = 16
0.00.060.126 I llm_load_print_meta: n_rot            = 32
0.00.060.126 I llm_load_print_meta: n_swa            = 0
0.00.060.126 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.126 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.127 I llm_load_print_meta: n_gqa            = 1
0.00.060.128 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.129 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.131 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.131 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.131 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.131 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.132 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.133 I llm_load_print_meta: n_ff             = 8192
0.00.060.133 I llm_load_print_meta: n_expert         = 0
0.00.060.133 I llm_load_print_meta: n_expert_used    = 0
0.00.060.133 I llm_load_print_meta: causal attn      = 1
0.00.060.133 I llm_load_print_meta: pooling type     = 0
0.00.060.134 I llm_load_print_meta: rope type        = 2
0.00.060.134 I llm_load_print_meta: rope scaling     = linear
0.00.060.134 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.135 I llm_load_print_meta: freq_scale_train = 1
0.00.060.135 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.135 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.135 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.135 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.136 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.136 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.136 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.147 I llm_load_print_meta: model type       = 1.4B
0.00.060.147 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.147 I llm_load_print_meta: model params     = 1.41 B
0.00.060.148 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.148 I llm_load_print_meta: general.name     = 1.4B
0.00.060.148 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.148 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.148 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.149 I llm_load_print_meta: LF token         = 128 ''
0.00.060.150 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.150 I llm_load_print_meta: max token length = 1024
0.00.061.764 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.765 I llm_load_tensors: offloading output layer to GPU
0.00.061.765 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.774 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.776 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.630 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.631 I llama_new_context_with_model: n_ctx         = 128
0.00.062.631 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.632 I llama_new_context_with_model: n_batch       = 128
0.00.062.632 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.632 I llama_new_context_with_model: flash_attn    = 0
0.00.062.632 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.633 I llama_new_context_with_model: freq_scale    = 1
0.00.062.633 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.634 I ggml_metal_init: allocating
0.00.062.640 I ggml_metal_init: found device: Apple M4
0.00.062.642 I ggml_metal_init: picking default device: Apple M4
0.00.063.181 I ggml_metal_init: using embedded metal library
0.00.065.249 I ggml_metal_init: GPU name:   Apple M4
0.00.065.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.252 I ggml_metal_init: simdgroup reduction   = true
0.00.065.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.252 I ggml_metal_init: has bfloat            = true
0.00.065.252 I ggml_metal_init: use bfloat            = true
0.00.065.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.480 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.493 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.362 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.364 I llama_new_context_with_model: graph nodes  = 967
0.00.076.364 I llama_new_context_with_model: graph splits = 2
0.00.076.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.002.729 I 
0.01.002.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.002.765 I perplexity: tokenizing the input ..
0.01.010.450 I perplexity: tokenization took 7.682 ms
0.01.010.453 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.132.629 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.133.874 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.133.885 I llama_perf_context_print:        load time =     991.94 ms
0.01.133.887 I llama_perf_context_print: prompt eval time =     121.95 ms /   128 tokens (    0.95 ms per token,  1049.65 tokens per second)
0.01.133.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.133.889 I llama_perf_context_print:       total time =     131.16 ms /   129 tokens
0.01.134.326 I ggml_metal_free: deallocating

real	0m1.150s
user	0m0.086s
sys	0m0.175s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.444 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.456 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.459 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.023 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.027 I llama_model_loader: - type  f32:  194 tensors
0.00.027.027 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.027 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.389 I llm_load_vocab: special tokens cache size = 25
0.00.053.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.192 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.193 I llm_load_print_meta: arch             = gptneox
0.00.053.193 I llm_load_print_meta: vocab type       = BPE
0.00.053.194 I llm_load_print_meta: n_vocab          = 50304
0.00.053.194 I llm_load_print_meta: n_merges         = 50009
0.00.053.194 I llm_load_print_meta: vocab_only       = 0
0.00.053.194 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.194 I llm_load_print_meta: n_embd           = 2048
0.00.053.194 I llm_load_print_meta: n_layer          = 24
0.00.053.199 I llm_load_print_meta: n_head           = 16
0.00.053.200 I llm_load_print_meta: n_head_kv        = 16
0.00.053.200 I llm_load_print_meta: n_rot            = 32
0.00.053.200 I llm_load_print_meta: n_swa            = 0
0.00.053.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.201 I llm_load_print_meta: n_gqa            = 1
0.00.053.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.204 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.204 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.204 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.204 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.207 I llm_load_print_meta: n_ff             = 8192
0.00.053.207 I llm_load_print_meta: n_expert         = 0
0.00.053.207 I llm_load_print_meta: n_expert_used    = 0
0.00.053.207 I llm_load_print_meta: causal attn      = 1
0.00.053.207 I llm_load_print_meta: pooling type     = 0
0.00.053.207 I llm_load_print_meta: rope type        = 2
0.00.053.214 I llm_load_print_meta: rope scaling     = linear
0.00.053.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.216 I llm_load_print_meta: freq_scale_train = 1
0.00.053.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.218 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.219 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.228 I llm_load_print_meta: model type       = 1.4B
0.00.053.228 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.228 I llm_load_print_meta: model params     = 1.41 B
0.00.053.229 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.229 I llm_load_print_meta: general.name     = 1.4B
0.00.053.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: LF token         = 128 ''
0.00.053.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.231 I llm_load_print_meta: max token length = 1024
0.00.055.241 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.241 I llm_load_tensors: offloading output layer to GPU
0.00.055.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.247 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.248 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.257 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.258 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.258 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.259 I llama_new_context_with_model: n_batch       = 2048
0.00.056.259 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.259 I llama_new_context_with_model: flash_attn    = 0
0.00.056.259 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.260 I llama_new_context_with_model: freq_scale    = 1
0.00.056.260 I ggml_metal_init: allocating
0.00.056.263 I ggml_metal_init: found device: Apple M4
0.00.056.265 I ggml_metal_init: picking default device: Apple M4
0.00.056.924 I ggml_metal_init: using embedded metal library
0.00.059.076 I ggml_metal_init: GPU name:   Apple M4
0.00.059.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.078 I ggml_metal_init: simdgroup reduction   = true
0.00.059.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.079 I ggml_metal_init: has bfloat            = true
0.00.059.079 I ggml_metal_init: use bfloat            = true
0.00.059.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.210 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.216 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.240 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.344 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.346 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.346 I llama_new_context_with_model: graph nodes  = 967
0.00.092.346 I llama_new_context_with_model: graph splits = 2
0.00.092.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.589 I main: llama threadpool init, n_threads = 4
0.00.733.624 I 
0.00.733.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.733.643 I 
0.00.733.864 I sampler seed: 1234
0.00.733.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.880 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.407.140 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.407.141 I llama_perf_context_print:        load time =     722.77 ms
0.01.407.142 I llama_perf_context_print: prompt eval time =      32.83 ms /     7 tokens (    4.69 ms per token,   213.19 tokens per second)
0.01.407.142 I llama_perf_context_print:        eval time =     637.46 ms /    63 runs   (   10.12 ms per token,    98.83 tokens per second)
0.01.407.143 I llama_perf_context_print:       total time =     673.55 ms /    70 tokens
0.01.407.319 I ggml_metal_free: deallocating

real	0m1.422s
user	0m0.109s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.275 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.032 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.411 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.967 I llama_model_loader: - type  f32:  194 tensors
0.00.023.968 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.968 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.859 I llm_load_vocab: special tokens cache size = 25
0.00.049.589 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.592 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.593 I llm_load_print_meta: arch             = gptneox
0.00.049.593 I llm_load_print_meta: vocab type       = BPE
0.00.049.593 I llm_load_print_meta: n_vocab          = 50304
0.00.049.593 I llm_load_print_meta: n_merges         = 50009
0.00.049.593 I llm_load_print_meta: vocab_only       = 0
0.00.049.594 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.594 I llm_load_print_meta: n_embd           = 2048
0.00.049.594 I llm_load_print_meta: n_layer          = 24
0.00.049.597 I llm_load_print_meta: n_head           = 16
0.00.049.598 I llm_load_print_meta: n_head_kv        = 16
0.00.049.599 I llm_load_print_meta: n_rot            = 32
0.00.049.599 I llm_load_print_meta: n_swa            = 0
0.00.049.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.600 I llm_load_print_meta: n_gqa            = 1
0.00.049.601 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.601 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.602 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.602 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.604 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.604 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.604 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.605 I llm_load_print_meta: n_ff             = 8192
0.00.049.605 I llm_load_print_meta: n_expert         = 0
0.00.049.605 I llm_load_print_meta: n_expert_used    = 0
0.00.049.606 I llm_load_print_meta: causal attn      = 1
0.00.049.607 I llm_load_print_meta: pooling type     = 0
0.00.049.607 I llm_load_print_meta: rope type        = 2
0.00.049.607 I llm_load_print_meta: rope scaling     = linear
0.00.049.607 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.608 I llm_load_print_meta: freq_scale_train = 1
0.00.049.608 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.608 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.608 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.608 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.609 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.609 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.609 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.621 I llm_load_print_meta: model type       = 1.4B
0.00.049.622 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.622 I llm_load_print_meta: model params     = 1.41 B
0.00.049.622 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.623 I llm_load_print_meta: general.name     = 1.4B
0.00.049.623 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.624 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.624 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.624 I llm_load_print_meta: LF token         = 128 ''
0.00.049.625 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.625 I llm_load_print_meta: max token length = 1024
0.00.051.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.527 I llm_load_tensors: offloading output layer to GPU
0.00.051.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.538 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.539 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.454 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.455 I llama_new_context_with_model: n_ctx         = 128
0.00.052.455 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.455 I llama_new_context_with_model: n_batch       = 128
0.00.052.455 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.456 I llama_new_context_with_model: flash_attn    = 0
0.00.052.456 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.456 I llama_new_context_with_model: freq_scale    = 1
0.00.052.457 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.457 I ggml_metal_init: allocating
0.00.052.463 I ggml_metal_init: found device: Apple M4
0.00.052.465 I ggml_metal_init: picking default device: Apple M4
0.00.052.989 I ggml_metal_init: using embedded metal library
0.00.054.935 I ggml_metal_init: GPU name:   Apple M4
0.00.054.936 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.936 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.937 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.937 I ggml_metal_init: simdgroup reduction   = true
0.00.054.937 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.937 I ggml_metal_init: has bfloat            = true
0.00.054.938 I ggml_metal_init: use bfloat            = true
0.00.054.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.010 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.013 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.026 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.876 I llama_new_context_with_model: graph nodes  = 967
0.00.064.876 I llama_new_context_with_model: graph splits = 2
0.00.064.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.297 I 
0.00.645.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.645.326 I perplexity: tokenizing the input ..
0.00.652.573 I perplexity: tokenization took 7.244 ms
0.00.652.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.925 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.776.073 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.776.094 I llama_perf_context_print:        load time =     635.26 ms
0.00.776.095 I llama_perf_context_print: prompt eval time =     122.11 ms /   128 tokens (    0.95 ms per token,  1048.24 tokens per second)
0.00.776.098 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.102 I llama_perf_context_print:       total time =     130.80 ms /   129 tokens
0.00.776.557 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.075s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.675 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.341 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.342 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.342 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.345 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.954 I llama_model_loader: - type  f32:  194 tensors
0.00.023.954 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.954 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.929 I llm_load_vocab: special tokens cache size = 25
0.00.049.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.893 I llm_load_print_meta: arch             = gptneox
0.00.049.893 I llm_load_print_meta: vocab type       = BPE
0.00.049.893 I llm_load_print_meta: n_vocab          = 50304
0.00.049.893 I llm_load_print_meta: n_merges         = 50009
0.00.049.893 I llm_load_print_meta: vocab_only       = 0
0.00.049.894 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.894 I llm_load_print_meta: n_embd           = 2048
0.00.049.894 I llm_load_print_meta: n_layer          = 24
0.00.049.897 I llm_load_print_meta: n_head           = 16
0.00.049.898 I llm_load_print_meta: n_head_kv        = 16
0.00.049.898 I llm_load_print_meta: n_rot            = 32
0.00.049.898 I llm_load_print_meta: n_swa            = 0
0.00.049.898 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.898 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.899 I llm_load_print_meta: n_gqa            = 1
0.00.049.900 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.900 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.901 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.901 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.901 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.902 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.903 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.903 I llm_load_print_meta: n_ff             = 8192
0.00.049.904 I llm_load_print_meta: n_expert         = 0
0.00.049.904 I llm_load_print_meta: n_expert_used    = 0
0.00.049.904 I llm_load_print_meta: causal attn      = 1
0.00.049.904 I llm_load_print_meta: pooling type     = 0
0.00.049.904 I llm_load_print_meta: rope type        = 2
0.00.049.904 I llm_load_print_meta: rope scaling     = linear
0.00.049.905 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.905 I llm_load_print_meta: freq_scale_train = 1
0.00.049.905 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.906 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.906 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.906 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.913 I llm_load_print_meta: model type       = 1.4B
0.00.049.913 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.914 I llm_load_print_meta: model params     = 1.41 B
0.00.049.914 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.914 I llm_load_print_meta: general.name     = 1.4B
0.00.049.915 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: LF token         = 128 ''
0.00.049.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.917 I llm_load_print_meta: max token length = 1024
0.00.051.466 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.467 I llm_load_tensors: offloading output layer to GPU
0.00.051.467 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.471 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.473 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.329 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.330 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.330 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.330 I llama_new_context_with_model: n_batch       = 2048
0.00.052.330 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.330 I llama_new_context_with_model: flash_attn    = 0
0.00.052.331 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.331 I llama_new_context_with_model: freq_scale    = 1
0.00.052.332 I ggml_metal_init: allocating
0.00.052.338 I ggml_metal_init: found device: Apple M4
0.00.052.340 I ggml_metal_init: picking default device: Apple M4
0.00.052.924 I ggml_metal_init: using embedded metal library
0.00.054.968 I ggml_metal_init: GPU name:   Apple M4
0.00.054.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.971 I ggml_metal_init: simdgroup reduction   = true
0.00.054.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.971 I ggml_metal_init: has bfloat            = true
0.00.054.971 I ggml_metal_init: use bfloat            = true
0.00.054.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.522 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.529 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.548 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.455 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.456 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.456 I llama_new_context_with_model: graph nodes  = 967
0.00.083.457 I llama_new_context_with_model: graph splits = 2
0.00.083.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.006 I main: llama threadpool init, n_threads = 4
0.00.785.045 I 
0.00.785.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.785.062 I 
0.00.785.216 I sampler seed: 1234
0.00.785.220 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.231 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.231 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.231 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.505.460 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64021.64 tokens per second)
0.01.505.461 I llama_perf_context_print:        load time =     776.33 ms
0.01.505.462 I llama_perf_context_print: prompt eval time =      32.74 ms /     7 tokens (    4.68 ms per token,   213.83 tokens per second)
0.01.505.462 I llama_perf_context_print:        eval time =     684.61 ms /    63 runs   (   10.87 ms per token,    92.02 tokens per second)
0.01.505.463 I llama_perf_context_print:       total time =     720.46 ms /    70 tokens
0.01.505.630 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.106s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.335 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.112 I llama_model_loader: - type  f32:  194 tensors
0.00.023.113 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.113 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.962 I llm_load_vocab: special tokens cache size = 25
0.00.048.919 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.922 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.922 I llm_load_print_meta: arch             = gptneox
0.00.048.922 I llm_load_print_meta: vocab type       = BPE
0.00.048.922 I llm_load_print_meta: n_vocab          = 50304
0.00.048.923 I llm_load_print_meta: n_merges         = 50009
0.00.048.923 I llm_load_print_meta: vocab_only       = 0
0.00.048.923 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.923 I llm_load_print_meta: n_embd           = 2048
0.00.048.923 I llm_load_print_meta: n_layer          = 24
0.00.048.926 I llm_load_print_meta: n_head           = 16
0.00.048.927 I llm_load_print_meta: n_head_kv        = 16
0.00.048.927 I llm_load_print_meta: n_rot            = 32
0.00.048.928 I llm_load_print_meta: n_swa            = 0
0.00.048.928 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.929 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.929 I llm_load_print_meta: n_gqa            = 1
0.00.048.930 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.931 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.931 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.932 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.932 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.932 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.932 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.933 I llm_load_print_meta: n_ff             = 8192
0.00.048.933 I llm_load_print_meta: n_expert         = 0
0.00.048.933 I llm_load_print_meta: n_expert_used    = 0
0.00.048.934 I llm_load_print_meta: causal attn      = 1
0.00.048.934 I llm_load_print_meta: pooling type     = 0
0.00.048.934 I llm_load_print_meta: rope type        = 2
0.00.048.934 I llm_load_print_meta: rope scaling     = linear
0.00.048.936 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.936 I llm_load_print_meta: freq_scale_train = 1
0.00.048.937 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.937 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.937 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.937 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.937 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.937 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.937 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.949 I llm_load_print_meta: model type       = 1.4B
0.00.048.949 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.949 I llm_load_print_meta: model params     = 1.41 B
0.00.048.950 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.950 I llm_load_print_meta: general.name     = 1.4B
0.00.048.950 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.951 I llm_load_print_meta: LF token         = 128 ''
0.00.048.951 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.952 I llm_load_print_meta: max token length = 1024
0.00.050.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.513 I llm_load_tensors: offloading output layer to GPU
0.00.050.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.522 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.523 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.596 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.597 I llama_new_context_with_model: n_ctx         = 128
0.00.051.597 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.597 I llama_new_context_with_model: n_batch       = 128
0.00.051.598 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.598 I llama_new_context_with_model: flash_attn    = 0
0.00.051.598 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.598 I llama_new_context_with_model: freq_scale    = 1
0.00.051.599 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.599 I ggml_metal_init: allocating
0.00.051.602 I ggml_metal_init: found device: Apple M4
0.00.051.604 I ggml_metal_init: picking default device: Apple M4
0.00.052.167 I ggml_metal_init: using embedded metal library
0.00.054.172 I ggml_metal_init: GPU name:   Apple M4
0.00.054.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.176 I ggml_metal_init: simdgroup reduction   = true
0.00.054.176 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.176 I ggml_metal_init: has bfloat            = true
0.00.054.176 I ggml_metal_init: use bfloat            = true
0.00.054.177 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.139 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.142 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.156 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.064 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.065 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.065 I llama_new_context_with_model: graph nodes  = 967
0.00.064.065 I llama_new_context_with_model: graph splits = 2
0.00.064.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.872 I 
0.00.705.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.705.932 I perplexity: tokenizing the input ..
0.00.713.995 I perplexity: tokenization took 8.062 ms
0.00.714.000 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.405 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.837.542 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.837.556 I llama_perf_context_print:        load time =     697.02 ms
0.00.837.557 I llama_perf_context_print: prompt eval time =     122.18 ms /   128 tokens (    0.95 ms per token,  1047.63 tokens per second)
0.00.837.558 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.558 I llama_perf_context_print:       total time =     131.68 ms /   129 tokens
0.00.837.927 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.075s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.309 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.323 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.769 I llama_model_loader: - type  f32:  194 tensors
0.00.023.769 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.769 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.790 I llm_load_vocab: special tokens cache size = 25
0.00.049.838 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.841 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.841 I llm_load_print_meta: arch             = gptneox
0.00.049.841 I llm_load_print_meta: vocab type       = BPE
0.00.049.842 I llm_load_print_meta: n_vocab          = 50304
0.00.049.842 I llm_load_print_meta: n_merges         = 50009
0.00.049.842 I llm_load_print_meta: vocab_only       = 0
0.00.049.842 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.842 I llm_load_print_meta: n_embd           = 2048
0.00.049.843 I llm_load_print_meta: n_layer          = 24
0.00.049.845 I llm_load_print_meta: n_head           = 16
0.00.049.846 I llm_load_print_meta: n_head_kv        = 16
0.00.049.846 I llm_load_print_meta: n_rot            = 32
0.00.049.847 I llm_load_print_meta: n_swa            = 0
0.00.049.847 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.847 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.848 I llm_load_print_meta: n_gqa            = 1
0.00.049.849 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.849 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.850 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.850 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.850 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.850 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.853 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.853 I llm_load_print_meta: n_ff             = 8192
0.00.049.853 I llm_load_print_meta: n_expert         = 0
0.00.049.854 I llm_load_print_meta: n_expert_used    = 0
0.00.049.854 I llm_load_print_meta: causal attn      = 1
0.00.049.854 I llm_load_print_meta: pooling type     = 0
0.00.049.854 I llm_load_print_meta: rope type        = 2
0.00.049.854 I llm_load_print_meta: rope scaling     = linear
0.00.049.855 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.855 I llm_load_print_meta: freq_scale_train = 1
0.00.049.855 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.855 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.856 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.856 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.856 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.868 I llm_load_print_meta: model type       = 1.4B
0.00.049.868 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.869 I llm_load_print_meta: model params     = 1.41 B
0.00.049.869 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.870 I llm_load_print_meta: general.name     = 1.4B
0.00.049.870 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: LF token         = 128 ''
0.00.049.871 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: max token length = 1024
0.00.051.856 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.856 I llm_load_tensors: offloading output layer to GPU
0.00.051.856 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.866 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.867 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.785 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.785 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.785 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.786 I llama_new_context_with_model: n_batch       = 2048
0.00.052.786 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.786 I llama_new_context_with_model: flash_attn    = 0
0.00.052.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.787 I llama_new_context_with_model: freq_scale    = 1
0.00.052.787 I ggml_metal_init: allocating
0.00.052.790 I ggml_metal_init: found device: Apple M4
0.00.052.792 I ggml_metal_init: picking default device: Apple M4
0.00.053.347 I ggml_metal_init: using embedded metal library
0.00.055.238 I ggml_metal_init: GPU name:   Apple M4
0.00.055.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.240 I ggml_metal_init: simdgroup reduction   = true
0.00.055.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.240 I ggml_metal_init: has bfloat            = true
0.00.055.241 I ggml_metal_init: use bfloat            = true
0.00.055.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.649 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.657 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.678 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.643 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.644 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.644 I llama_new_context_with_model: graph nodes  = 967
0.00.082.645 I llama_new_context_with_model: graph splits = 2
0.00.082.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.484 I main: llama threadpool init, n_threads = 4
0.00.792.521 I 
0.00.792.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.792.540 I 
0.00.792.759 I sampler seed: 1234
0.00.792.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.775 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.574.803 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61846.69 tokens per second)
0.01.574.804 I llama_perf_context_print:        load time =     783.17 ms
0.01.574.805 I llama_perf_context_print: prompt eval time =      36.64 ms /     7 tokens (    5.23 ms per token,   191.03 tokens per second)
0.01.574.805 I llama_perf_context_print:        eval time =     742.48 ms /    63 runs   (   11.79 ms per token,    84.85 tokens per second)
0.01.574.806 I llama_perf_context_print:       total time =     782.32 ms /    70 tokens
0.01.574.969 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.107s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.092 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.419 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.420 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.421 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.422 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.423 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.423 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.424 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.022.658 I llama_model_loader: - type  f32:  194 tensors
0.00.022.659 I llama_model_loader: - type q5_0:   97 tensors
0.00.022.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.380 I llm_load_vocab: special tokens cache size = 25
0.00.048.372 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.375 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.375 I llm_load_print_meta: arch             = gptneox
0.00.048.375 I llm_load_print_meta: vocab type       = BPE
0.00.048.375 I llm_load_print_meta: n_vocab          = 50304
0.00.048.376 I llm_load_print_meta: n_merges         = 50009
0.00.048.376 I llm_load_print_meta: vocab_only       = 0
0.00.048.376 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.376 I llm_load_print_meta: n_embd           = 2048
0.00.048.376 I llm_load_print_meta: n_layer          = 24
0.00.048.379 I llm_load_print_meta: n_head           = 16
0.00.048.380 I llm_load_print_meta: n_head_kv        = 16
0.00.048.380 I llm_load_print_meta: n_rot            = 32
0.00.048.380 I llm_load_print_meta: n_swa            = 0
0.00.048.381 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.381 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.382 I llm_load_print_meta: n_gqa            = 1
0.00.048.383 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.383 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.384 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.384 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.384 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.385 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.385 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.385 I llm_load_print_meta: n_ff             = 8192
0.00.048.386 I llm_load_print_meta: n_expert         = 0
0.00.048.386 I llm_load_print_meta: n_expert_used    = 0
0.00.048.386 I llm_load_print_meta: causal attn      = 1
0.00.048.386 I llm_load_print_meta: pooling type     = 0
0.00.048.386 I llm_load_print_meta: rope type        = 2
0.00.048.386 I llm_load_print_meta: rope scaling     = linear
0.00.048.387 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.387 I llm_load_print_meta: freq_scale_train = 1
0.00.048.387 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.387 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.388 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.388 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.388 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.388 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.399 I llm_load_print_meta: model type       = 1.4B
0.00.048.400 I llm_load_print_meta: model ftype      = Q5_0
0.00.048.400 I llm_load_print_meta: model params     = 1.41 B
0.00.048.402 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.048.402 I llm_load_print_meta: general.name     = 1.4B
0.00.048.402 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.402 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.402 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.402 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.403 I llm_load_print_meta: LF token         = 128 ''
0.00.048.403 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.403 I llm_load_print_meta: max token length = 1024
0.00.049.897 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.049.897 I llm_load_tensors: offloading output layer to GPU
0.00.049.897 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.049.906 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.049.908 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.050.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.779 I llama_new_context_with_model: n_ctx         = 128
0.00.050.779 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.779 I llama_new_context_with_model: n_batch       = 128
0.00.050.779 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.780 I llama_new_context_with_model: flash_attn    = 0
0.00.050.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.780 I llama_new_context_with_model: freq_scale    = 1
0.00.050.780 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.781 I ggml_metal_init: allocating
0.00.050.784 I ggml_metal_init: found device: Apple M4
0.00.050.786 I ggml_metal_init: picking default device: Apple M4
0.00.051.309 I ggml_metal_init: using embedded metal library
0.00.053.175 I ggml_metal_init: GPU name:   Apple M4
0.00.053.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.177 I ggml_metal_init: simdgroup reduction   = true
0.00.053.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.177 I ggml_metal_init: has bfloat            = true
0.00.053.178 I ggml_metal_init: use bfloat            = true
0.00.053.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.103 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.062.959 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.062.960 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.062.961 I llama_new_context_with_model: graph nodes  = 967
0.00.062.961 I llama_new_context_with_model: graph splits = 2
0.00.062.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.675 I 
0.00.708.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.708.698 I perplexity: tokenizing the input ..
0.00.716.250 I perplexity: tokenization took 7.55 ms
0.00.716.253 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.152 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.852.433 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.852.457 I llama_perf_context_print:        load time =     699.58 ms
0.00.852.459 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.44 tokens per second)
0.00.852.460 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.460 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.852.920 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.075s
sys	0m0.120s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.935 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.188 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.931 I llm_load_vocab: special tokens cache size = 25
0.00.051.032 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.035 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.035 I llm_load_print_meta: arch             = gptneox
0.00.051.036 I llm_load_print_meta: vocab type       = BPE
0.00.051.036 I llm_load_print_meta: n_vocab          = 50304
0.00.051.036 I llm_load_print_meta: n_merges         = 50009
0.00.051.036 I llm_load_print_meta: vocab_only       = 0
0.00.051.036 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.037 I llm_load_print_meta: n_embd           = 2048
0.00.051.037 I llm_load_print_meta: n_layer          = 24
0.00.051.040 I llm_load_print_meta: n_head           = 16
0.00.051.041 I llm_load_print_meta: n_head_kv        = 16
0.00.051.041 I llm_load_print_meta: n_rot            = 32
0.00.051.042 I llm_load_print_meta: n_swa            = 0
0.00.051.042 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.042 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.043 I llm_load_print_meta: n_gqa            = 1
0.00.051.043 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.044 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.045 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.045 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.045 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.046 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.046 I llm_load_print_meta: n_ff             = 8192
0.00.051.047 I llm_load_print_meta: n_expert         = 0
0.00.051.047 I llm_load_print_meta: n_expert_used    = 0
0.00.051.047 I llm_load_print_meta: causal attn      = 1
0.00.051.047 I llm_load_print_meta: pooling type     = 0
0.00.051.047 I llm_load_print_meta: rope type        = 2
0.00.051.047 I llm_load_print_meta: rope scaling     = linear
0.00.051.048 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.048 I llm_load_print_meta: freq_scale_train = 1
0.00.051.048 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.048 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.049 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.050 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.052 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.052 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.063 I llm_load_print_meta: model type       = 1.4B
0.00.051.063 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.064 I llm_load_print_meta: model params     = 1.41 B
0.00.051.064 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.064 I llm_load_print_meta: general.name     = 1.4B
0.00.051.065 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.065 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.065 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.065 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.065 I llm_load_print_meta: LF token         = 128 ''
0.00.051.066 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.066 I llm_load_print_meta: max token length = 1024
0.00.052.658 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.658 I llm_load_tensors: offloading output layer to GPU
0.00.052.658 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.668 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.669 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.557 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.557 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.557 I llama_new_context_with_model: n_batch       = 2048
0.00.053.558 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.558 I llama_new_context_with_model: flash_attn    = 0
0.00.053.558 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.558 I llama_new_context_with_model: freq_scale    = 1
0.00.053.559 I ggml_metal_init: allocating
0.00.053.562 I ggml_metal_init: found device: Apple M4
0.00.053.563 I ggml_metal_init: picking default device: Apple M4
0.00.054.112 I ggml_metal_init: using embedded metal library
0.00.056.074 I ggml_metal_init: GPU name:   Apple M4
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.076 I ggml_metal_init: simdgroup reduction   = true
0.00.056.077 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.077 I ggml_metal_init: has bfloat            = true
0.00.056.077 I ggml_metal_init: use bfloat            = true
0.00.056.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.536 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.555 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.442 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.443 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.443 I llama_new_context_with_model: graph nodes  = 967
0.00.084.444 I llama_new_context_with_model: graph splits = 2
0.00.084.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.439 I main: llama threadpool init, n_threads = 4
0.00.745.472 I 
0.00.745.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.745.499 I 
0.00.745.642 I sampler seed: 1234
0.00.745.647 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.664 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.574.254 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.574.255 I llama_perf_context_print:        load time =     736.50 ms
0.01.574.256 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.53 tokens per second)
0.01.574.256 I llama_perf_context_print:        eval time =     789.11 ms /    63 runs   (   12.53 ms per token,    79.84 tokens per second)
0.01.574.257 I llama_perf_context_print:       total time =     828.82 ms /    70 tokens
0.01.574.425 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.108s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.600 I llama_model_loader: - type  f32:  194 tensors
0.00.023.601 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.601 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.295 I llm_load_vocab: special tokens cache size = 25
0.00.050.426 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.429 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.430 I llm_load_print_meta: arch             = gptneox
0.00.050.430 I llm_load_print_meta: vocab type       = BPE
0.00.050.430 I llm_load_print_meta: n_vocab          = 50304
0.00.050.430 I llm_load_print_meta: n_merges         = 50009
0.00.050.431 I llm_load_print_meta: vocab_only       = 0
0.00.050.431 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.431 I llm_load_print_meta: n_embd           = 2048
0.00.050.431 I llm_load_print_meta: n_layer          = 24
0.00.050.433 I llm_load_print_meta: n_head           = 16
0.00.050.436 I llm_load_print_meta: n_head_kv        = 16
0.00.050.436 I llm_load_print_meta: n_rot            = 32
0.00.050.436 I llm_load_print_meta: n_swa            = 0
0.00.050.437 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.437 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.437 I llm_load_print_meta: n_gqa            = 1
0.00.050.443 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.444 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.444 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.445 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.446 I llm_load_print_meta: n_ff             = 8192
0.00.050.446 I llm_load_print_meta: n_expert         = 0
0.00.050.446 I llm_load_print_meta: n_expert_used    = 0
0.00.050.446 I llm_load_print_meta: causal attn      = 1
0.00.050.447 I llm_load_print_meta: pooling type     = 0
0.00.050.447 I llm_load_print_meta: rope type        = 2
0.00.050.447 I llm_load_print_meta: rope scaling     = linear
0.00.050.451 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.451 I llm_load_print_meta: freq_scale_train = 1
0.00.050.451 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.451 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.451 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.451 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.452 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.463 I llm_load_print_meta: model type       = 1.4B
0.00.050.464 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.464 I llm_load_print_meta: model params     = 1.41 B
0.00.050.465 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.465 I llm_load_print_meta: general.name     = 1.4B
0.00.050.465 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.465 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.465 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.465 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.467 I llm_load_print_meta: LF token         = 128 ''
0.00.050.467 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.467 I llm_load_print_meta: max token length = 1024
0.00.052.483 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.483 I llm_load_tensors: offloading output layer to GPU
0.00.052.484 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.493 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.495 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.495 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.496 I llama_new_context_with_model: n_ctx         = 128
0.00.053.497 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.497 I llama_new_context_with_model: n_batch       = 128
0.00.053.497 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.497 I llama_new_context_with_model: flash_attn    = 0
0.00.053.498 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.498 I llama_new_context_with_model: freq_scale    = 1
0.00.053.498 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.499 I ggml_metal_init: allocating
0.00.053.509 I ggml_metal_init: found device: Apple M4
0.00.053.512 I ggml_metal_init: picking default device: Apple M4
0.00.054.067 I ggml_metal_init: using embedded metal library
0.00.056.014 I ggml_metal_init: GPU name:   Apple M4
0.00.056.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.016 I ggml_metal_init: simdgroup reduction   = true
0.00.056.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.017 I ggml_metal_init: has bfloat            = true
0.00.056.017 I ggml_metal_init: use bfloat            = true
0.00.056.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.082 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.013 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.014 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.015 I llama_new_context_with_model: graph nodes  = 967
0.00.066.015 I llama_new_context_with_model: graph splits = 2
0.00.066.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.416 I 
0.00.675.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.675.473 I perplexity: tokenizing the input ..
0.00.682.847 I perplexity: tokenization took 7.373 ms
0.00.682.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.296 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.818.432 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.818.448 I llama_perf_context_print:        load time =     666.52 ms
0.00.818.449 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.76 tokens per second)
0.00.818.450 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.451 I llama_perf_context_print:       total time =     143.03 ms /   129 tokens
0.00.818.821 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.076s
sys	0m0.120s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.548 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.629 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.385 I llama_model_loader: - type  f32:  194 tensors
0.00.023.386 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.386 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.523 I llm_load_vocab: special tokens cache size = 25
0.00.049.281 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.284 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.284 I llm_load_print_meta: arch             = gptneox
0.00.049.285 I llm_load_print_meta: vocab type       = BPE
0.00.049.285 I llm_load_print_meta: n_vocab          = 50304
0.00.049.285 I llm_load_print_meta: n_merges         = 50009
0.00.049.285 I llm_load_print_meta: vocab_only       = 0
0.00.049.285 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.285 I llm_load_print_meta: n_embd           = 2048
0.00.049.286 I llm_load_print_meta: n_layer          = 24
0.00.049.288 I llm_load_print_meta: n_head           = 16
0.00.049.289 I llm_load_print_meta: n_head_kv        = 16
0.00.049.289 I llm_load_print_meta: n_rot            = 32
0.00.049.290 I llm_load_print_meta: n_swa            = 0
0.00.049.291 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.291 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.292 I llm_load_print_meta: n_gqa            = 1
0.00.049.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.293 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.294 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.294 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.294 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.295 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.295 I llm_load_print_meta: n_ff             = 8192
0.00.049.295 I llm_load_print_meta: n_expert         = 0
0.00.049.296 I llm_load_print_meta: n_expert_used    = 0
0.00.049.296 I llm_load_print_meta: causal attn      = 1
0.00.049.296 I llm_load_print_meta: pooling type     = 0
0.00.049.296 I llm_load_print_meta: rope type        = 2
0.00.049.296 I llm_load_print_meta: rope scaling     = linear
0.00.049.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.298 I llm_load_print_meta: freq_scale_train = 1
0.00.049.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.300 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.300 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.312 I llm_load_print_meta: model type       = 1.4B
0.00.049.312 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.316 I llm_load_print_meta: model params     = 1.41 B
0.00.049.316 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.317 I llm_load_print_meta: general.name     = 1.4B
0.00.049.317 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.317 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.317 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.317 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.317 I llm_load_print_meta: LF token         = 128 ''
0.00.049.318 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.318 I llm_load_print_meta: max token length = 1024
0.00.051.162 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.162 I llm_load_tensors: offloading output layer to GPU
0.00.051.162 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.172 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.173 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.084 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.084 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.084 I llama_new_context_with_model: n_batch       = 2048
0.00.052.084 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.085 I llama_new_context_with_model: flash_attn    = 0
0.00.052.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.085 I llama_new_context_with_model: freq_scale    = 1
0.00.052.086 I ggml_metal_init: allocating
0.00.052.091 I ggml_metal_init: found device: Apple M4
0.00.052.093 I ggml_metal_init: picking default device: Apple M4
0.00.052.623 I ggml_metal_init: using embedded metal library
0.00.054.562 I ggml_metal_init: GPU name:   Apple M4
0.00.054.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.564 I ggml_metal_init: simdgroup reduction   = true
0.00.054.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.564 I ggml_metal_init: has bfloat            = true
0.00.054.564 I ggml_metal_init: use bfloat            = true
0.00.054.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.418 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.425 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.444 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.394 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.395 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.396 I llama_new_context_with_model: graph nodes  = 967
0.00.082.396 I llama_new_context_with_model: graph splits = 2
0.00.082.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.591 I main: llama threadpool init, n_threads = 4
0.00.504.623 I 
0.00.504.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.504.645 I 
0.00.504.876 I sampler seed: 1234
0.00.504.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.895 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.895 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.956 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65377.53 tokens per second)
0.01.184.957 I llama_perf_context_print:        load time =     495.04 ms
0.01.184.958 I llama_perf_context_print: prompt eval time =      35.89 ms /     7 tokens (    5.13 ms per token,   195.03 tokens per second)
0.01.184.958 I llama_perf_context_print:        eval time =     641.30 ms /    63 runs   (   10.18 ms per token,    98.24 tokens per second)
0.01.184.959 I llama_perf_context_print:       total time =     680.37 ms /    70 tokens
0.01.185.137 I ggml_metal_free: deallocating

real	0m1.201s
user	0m0.106s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.085 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.881 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.022.883 I llama_model_loader: - type  f32:  194 tensors
0.00.022.883 I llama_model_loader: - type q2_K:   49 tensors
0.00.022.883 I llama_model_loader: - type q3_K:   48 tensors
0.00.022.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.516 I llm_load_vocab: special tokens cache size = 25
0.00.048.166 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.168 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.169 I llm_load_print_meta: arch             = gptneox
0.00.048.169 I llm_load_print_meta: vocab type       = BPE
0.00.048.169 I llm_load_print_meta: n_vocab          = 50304
0.00.048.170 I llm_load_print_meta: n_merges         = 50009
0.00.048.170 I llm_load_print_meta: vocab_only       = 0
0.00.048.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.170 I llm_load_print_meta: n_embd           = 2048
0.00.048.170 I llm_load_print_meta: n_layer          = 24
0.00.048.173 I llm_load_print_meta: n_head           = 16
0.00.048.174 I llm_load_print_meta: n_head_kv        = 16
0.00.048.174 I llm_load_print_meta: n_rot            = 32
0.00.048.174 I llm_load_print_meta: n_swa            = 0
0.00.048.174 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.178 I llm_load_print_meta: n_gqa            = 1
0.00.048.179 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.180 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.181 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.183 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.184 I llm_load_print_meta: n_ff             = 8192
0.00.048.184 I llm_load_print_meta: n_expert         = 0
0.00.048.184 I llm_load_print_meta: n_expert_used    = 0
0.00.048.184 I llm_load_print_meta: causal attn      = 1
0.00.048.184 I llm_load_print_meta: pooling type     = 0
0.00.048.184 I llm_load_print_meta: rope type        = 2
0.00.048.185 I llm_load_print_meta: rope scaling     = linear
0.00.048.185 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.186 I llm_load_print_meta: freq_scale_train = 1
0.00.048.186 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.186 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.186 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.187 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.187 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.193 I llm_load_print_meta: model type       = 1.4B
0.00.048.193 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.048.194 I llm_load_print_meta: model params     = 1.41 B
0.00.048.194 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.048.195 I llm_load_print_meta: general.name     = 1.4B
0.00.048.195 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.195 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.196 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.196 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.196 I llm_load_print_meta: LF token         = 128 ''
0.00.048.196 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.196 I llm_load_print_meta: max token length = 1024
0.00.049.838 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.049.838 I llm_load_tensors: offloading output layer to GPU
0.00.049.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.049.846 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.049.847 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.050.952 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.952 I llama_new_context_with_model: n_ctx         = 128
0.00.050.953 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.953 I llama_new_context_with_model: n_batch       = 128
0.00.050.953 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.953 I llama_new_context_with_model: flash_attn    = 0
0.00.050.954 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.954 I llama_new_context_with_model: freq_scale    = 1
0.00.050.954 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.955 I ggml_metal_init: allocating
0.00.050.959 I ggml_metal_init: found device: Apple M4
0.00.050.961 I ggml_metal_init: picking default device: Apple M4
0.00.051.505 I ggml_metal_init: using embedded metal library
0.00.053.372 I ggml_metal_init: GPU name:   Apple M4
0.00.053.374 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.375 I ggml_metal_init: simdgroup reduction   = true
0.00.053.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.375 I ggml_metal_init: has bfloat            = true
0.00.053.375 I ggml_metal_init: use bfloat            = true
0.00.053.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.262 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.263 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.285 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.129 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.130 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.131 I llama_new_context_with_model: graph nodes  = 967
0.00.063.131 I llama_new_context_with_model: graph splits = 2
0.00.063.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.412.234 I 
0.00.412.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.412.306 I perplexity: tokenizing the input ..
0.00.419.979 I perplexity: tokenization took 7.673 ms
0.00.419.985 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.733 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.960 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.975 I llama_perf_context_print:        load time =     403.14 ms
0.00.552.976 I llama_perf_context_print: prompt eval time =     131.52 ms /   128 tokens (    1.03 ms per token,   973.25 tokens per second)
0.00.552.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.978 I llama_perf_context_print:       total time =     140.74 ms /   129 tokens
0.00.553.369 I ggml_metal_free: deallocating

real	0m0.566s
user	0m0.074s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.012.633 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.019.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.028.075 I llama_model_loader: - type  f32:  194 tensors
0.00.028.075 I llama_model_loader: - type q3_K:   25 tensors
0.00.028.075 I llama_model_loader: - type q4_K:   71 tensors
0.00.028.075 I llama_model_loader: - type q5_K:    1 tensors
0.00.028.076 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.865 I llm_load_vocab: special tokens cache size = 25
0.00.054.938 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.940 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.941 I llm_load_print_meta: arch             = gptneox
0.00.054.941 I llm_load_print_meta: vocab type       = BPE
0.00.054.941 I llm_load_print_meta: n_vocab          = 50304
0.00.054.942 I llm_load_print_meta: n_merges         = 50009
0.00.054.942 I llm_load_print_meta: vocab_only       = 0
0.00.054.942 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.942 I llm_load_print_meta: n_embd           = 2048
0.00.054.942 I llm_load_print_meta: n_layer          = 24
0.00.054.945 I llm_load_print_meta: n_head           = 16
0.00.054.946 I llm_load_print_meta: n_head_kv        = 16
0.00.054.946 I llm_load_print_meta: n_rot            = 32
0.00.054.946 I llm_load_print_meta: n_swa            = 0
0.00.054.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.947 I llm_load_print_meta: n_gqa            = 1
0.00.054.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.949 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.949 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.951 I llm_load_print_meta: n_ff             = 8192
0.00.054.952 I llm_load_print_meta: n_expert         = 0
0.00.054.954 I llm_load_print_meta: n_expert_used    = 0
0.00.054.954 I llm_load_print_meta: causal attn      = 1
0.00.054.954 I llm_load_print_meta: pooling type     = 0
0.00.054.954 I llm_load_print_meta: rope type        = 2
0.00.054.954 I llm_load_print_meta: rope scaling     = linear
0.00.054.955 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.955 I llm_load_print_meta: freq_scale_train = 1
0.00.054.955 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.955 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.956 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.956 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.956 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.956 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.956 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.963 I llm_load_print_meta: model type       = 1.4B
0.00.054.963 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.054.963 I llm_load_print_meta: model params     = 1.41 B
0.00.054.964 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.054.964 I llm_load_print_meta: general.name     = 1.4B
0.00.054.964 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.964 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.964 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.965 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.965 I llm_load_print_meta: LF token         = 128 ''
0.00.054.965 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.965 I llm_load_print_meta: max token length = 1024
0.00.056.501 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.501 I llm_load_tensors: offloading output layer to GPU
0.00.056.501 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.506 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.056.506 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.057.345 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.346 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.346 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.346 I llama_new_context_with_model: n_batch       = 2048
0.00.057.347 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.347 I llama_new_context_with_model: flash_attn    = 0
0.00.057.347 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.348 I llama_new_context_with_model: freq_scale    = 1
0.00.057.348 I ggml_metal_init: allocating
0.00.057.351 I ggml_metal_init: found device: Apple M4
0.00.057.353 I ggml_metal_init: picking default device: Apple M4
0.00.057.868 I ggml_metal_init: using embedded metal library
0.00.059.758 I ggml_metal_init: GPU name:   Apple M4
0.00.059.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.762 I ggml_metal_init: simdgroup reduction   = true
0.00.059.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.762 I ggml_metal_init: has bfloat            = true
0.00.059.763 I ggml_metal_init: use bfloat            = true
0.00.059.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.123 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.128 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.148 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.018 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.019 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.019 I llama_new_context_with_model: graph nodes  = 967
0.00.087.020 I llama_new_context_with_model: graph splits = 2
0.00.087.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.562 I main: llama threadpool init, n_threads = 4
0.00.594.593 I 
0.00.594.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.594.612 I 
0.00.594.843 I sampler seed: 1234
0.00.594.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.859 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.860 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.860 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.335.180 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.335.181 I llama_perf_context_print:        load time =     581.93 ms
0.01.335.182 I llama_perf_context_print: prompt eval time =      35.72 ms /     7 tokens (    5.10 ms per token,   195.95 tokens per second)
0.01.335.183 I llama_perf_context_print:        eval time =     701.46 ms /    63 runs   (   11.13 ms per token,    89.81 tokens per second)
0.01.335.183 I llama_perf_context_print:       total time =     740.62 ms /    70 tokens
0.01.335.345 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.107s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.599 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.391 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.238 I llama_model_loader: - type  f32:  194 tensors
0.00.023.238 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.239 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.239 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.935 I llm_load_vocab: special tokens cache size = 25
0.00.049.921 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.924 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.925 I llm_load_print_meta: arch             = gptneox
0.00.049.925 I llm_load_print_meta: vocab type       = BPE
0.00.049.925 I llm_load_print_meta: n_vocab          = 50304
0.00.049.925 I llm_load_print_meta: n_merges         = 50009
0.00.049.925 I llm_load_print_meta: vocab_only       = 0
0.00.049.926 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.926 I llm_load_print_meta: n_embd           = 2048
0.00.049.926 I llm_load_print_meta: n_layer          = 24
0.00.049.928 I llm_load_print_meta: n_head           = 16
0.00.049.929 I llm_load_print_meta: n_head_kv        = 16
0.00.049.929 I llm_load_print_meta: n_rot            = 32
0.00.049.930 I llm_load_print_meta: n_swa            = 0
0.00.049.930 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.930 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.931 I llm_load_print_meta: n_gqa            = 1
0.00.049.932 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.932 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.933 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.933 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.933 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.934 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.934 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.934 I llm_load_print_meta: n_ff             = 8192
0.00.049.934 I llm_load_print_meta: n_expert         = 0
0.00.049.935 I llm_load_print_meta: n_expert_used    = 0
0.00.049.935 I llm_load_print_meta: causal attn      = 1
0.00.049.936 I llm_load_print_meta: pooling type     = 0
0.00.049.937 I llm_load_print_meta: rope type        = 2
0.00.049.939 I llm_load_print_meta: rope scaling     = linear
0.00.049.939 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.939 I llm_load_print_meta: freq_scale_train = 1
0.00.049.940 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.940 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.940 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.940 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.940 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.941 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.951 I llm_load_print_meta: model type       = 1.4B
0.00.049.952 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.952 I llm_load_print_meta: model params     = 1.41 B
0.00.049.953 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.953 I llm_load_print_meta: general.name     = 1.4B
0.00.049.955 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: LF token         = 128 ''
0.00.049.956 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: max token length = 1024
0.00.051.471 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.471 I llm_load_tensors: offloading output layer to GPU
0.00.051.472 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.481 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.482 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.353 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.354 I llama_new_context_with_model: n_ctx         = 128
0.00.052.354 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.354 I llama_new_context_with_model: n_batch       = 128
0.00.052.354 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.354 I llama_new_context_with_model: flash_attn    = 0
0.00.052.355 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.355 I llama_new_context_with_model: freq_scale    = 1
0.00.052.355 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.356 I ggml_metal_init: allocating
0.00.052.361 I ggml_metal_init: found device: Apple M4
0.00.052.363 I ggml_metal_init: picking default device: Apple M4
0.00.052.905 I ggml_metal_init: using embedded metal library
0.00.054.870 I ggml_metal_init: GPU name:   Apple M4
0.00.054.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.873 I ggml_metal_init: simdgroup reduction   = true
0.00.054.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.873 I ggml_metal_init: has bfloat            = true
0.00.054.873 I ggml_metal_init: use bfloat            = true
0.00.054.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.859 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.861 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.875 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.771 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.772 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.772 I llama_new_context_with_model: graph nodes  = 967
0.00.064.772 I llama_new_context_with_model: graph splits = 2
0.00.064.784 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.954 I 
0.00.514.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.515.031 I perplexity: tokenizing the input ..
0.00.523.212 I perplexity: tokenization took 8.18 ms
0.00.523.215 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.005 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.656.135 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.656.146 I llama_perf_context_print:        load time =     506.35 ms
0.00.656.147 I llama_perf_context_print: prompt eval time =     131.56 ms /   128 tokens (    1.03 ms per token,   972.91 tokens per second)
0.00.656.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.656.149 I llama_perf_context_print:       total time =     141.20 ms /   129 tokens
0.00.656.554 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.076s
sys	0m0.094s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.462 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.570 I llama_model_loader: - type  f32:  194 tensors
0.00.023.570 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.570 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.570 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.864 I llm_load_vocab: special tokens cache size = 25
0.00.049.820 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.823 I llm_load_print_meta: arch             = gptneox
0.00.049.823 I llm_load_print_meta: vocab type       = BPE
0.00.049.824 I llm_load_print_meta: n_vocab          = 50304
0.00.049.824 I llm_load_print_meta: n_merges         = 50009
0.00.049.824 I llm_load_print_meta: vocab_only       = 0
0.00.049.824 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.824 I llm_load_print_meta: n_embd           = 2048
0.00.049.825 I llm_load_print_meta: n_layer          = 24
0.00.049.827 I llm_load_print_meta: n_head           = 16
0.00.049.828 I llm_load_print_meta: n_head_kv        = 16
0.00.049.828 I llm_load_print_meta: n_rot            = 32
0.00.049.828 I llm_load_print_meta: n_swa            = 0
0.00.049.829 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.829 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.829 I llm_load_print_meta: n_gqa            = 1
0.00.049.830 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.831 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.831 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.832 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.832 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.832 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.832 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.833 I llm_load_print_meta: n_ff             = 8192
0.00.049.833 I llm_load_print_meta: n_expert         = 0
0.00.049.833 I llm_load_print_meta: n_expert_used    = 0
0.00.049.834 I llm_load_print_meta: causal attn      = 1
0.00.049.834 I llm_load_print_meta: pooling type     = 0
0.00.049.834 I llm_load_print_meta: rope type        = 2
0.00.049.834 I llm_load_print_meta: rope scaling     = linear
0.00.049.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.837 I llm_load_print_meta: freq_scale_train = 1
0.00.049.837 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.837 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.837 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.837 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.849 I llm_load_print_meta: model type       = 1.4B
0.00.049.850 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.851 I llm_load_print_meta: model params     = 1.41 B
0.00.049.851 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.851 I llm_load_print_meta: general.name     = 1.4B
0.00.049.851 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: LF token         = 128 ''
0.00.049.852 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.853 I llm_load_print_meta: max token length = 1024
0.00.051.393 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.393 I llm_load_tensors: offloading output layer to GPU
0.00.051.393 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.403 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.404 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.227 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.228 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.228 I llama_new_context_with_model: n_batch       = 2048
0.00.052.228 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.228 I llama_new_context_with_model: flash_attn    = 0
0.00.052.228 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.229 I llama_new_context_with_model: freq_scale    = 1
0.00.052.229 I ggml_metal_init: allocating
0.00.052.232 I ggml_metal_init: found device: Apple M4
0.00.052.234 I ggml_metal_init: picking default device: Apple M4
0.00.052.781 I ggml_metal_init: using embedded metal library
0.00.054.656 I ggml_metal_init: GPU name:   Apple M4
0.00.054.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.658 I ggml_metal_init: simdgroup reduction   = true
0.00.054.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.658 I ggml_metal_init: has bfloat            = true
0.00.054.658 I ggml_metal_init: use bfloat            = true
0.00.054.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.659 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.600 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.604 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.621 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.506 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.508 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.508 I llama_new_context_with_model: graph nodes  = 967
0.00.082.508 I llama_new_context_with_model: graph splits = 2
0.00.082.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.306 I main: llama threadpool init, n_threads = 4
0.00.661.343 I 
0.00.661.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.661.360 I 
0.00.661.509 I sampler seed: 1234
0.00.661.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.661.522 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.661.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.661.522 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.410.144 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.410.145 I llama_perf_context_print:        load time =     652.84 ms
0.01.410.146 I llama_perf_context_print: prompt eval time =      36.34 ms /     7 tokens (    5.19 ms per token,   192.61 tokens per second)
0.01.410.148 I llama_perf_context_print:        eval time =     709.18 ms /    63 runs   (   11.26 ms per token,    88.84 tokens per second)
0.01.410.149 I llama_perf_context_print:       total time =     748.84 ms /    70 tokens
0.01.410.317 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.107s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.361 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.199 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.204 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.205 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.023 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.804 I llama_model_loader: - type  f32:  194 tensors
0.00.022.805 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.805 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.805 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.608 I llm_load_vocab: special tokens cache size = 25
0.00.048.299 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.301 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.302 I llm_load_print_meta: arch             = gptneox
0.00.048.302 I llm_load_print_meta: vocab type       = BPE
0.00.048.302 I llm_load_print_meta: n_vocab          = 50304
0.00.048.303 I llm_load_print_meta: n_merges         = 50009
0.00.048.303 I llm_load_print_meta: vocab_only       = 0
0.00.048.303 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.303 I llm_load_print_meta: n_embd           = 2048
0.00.048.303 I llm_load_print_meta: n_layer          = 24
0.00.048.306 I llm_load_print_meta: n_head           = 16
0.00.048.310 I llm_load_print_meta: n_head_kv        = 16
0.00.048.310 I llm_load_print_meta: n_rot            = 32
0.00.048.310 I llm_load_print_meta: n_swa            = 0
0.00.048.310 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.310 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.311 I llm_load_print_meta: n_gqa            = 1
0.00.048.312 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.312 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.313 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.313 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.313 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.313 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.314 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.314 I llm_load_print_meta: n_ff             = 8192
0.00.048.319 I llm_load_print_meta: n_expert         = 0
0.00.048.319 I llm_load_print_meta: n_expert_used    = 0
0.00.048.319 I llm_load_print_meta: causal attn      = 1
0.00.048.319 I llm_load_print_meta: pooling type     = 0
0.00.048.319 I llm_load_print_meta: rope type        = 2
0.00.048.319 I llm_load_print_meta: rope scaling     = linear
0.00.048.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.320 I llm_load_print_meta: freq_scale_train = 1
0.00.048.320 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.321 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.321 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.332 I llm_load_print_meta: model type       = 1.4B
0.00.048.333 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.333 I llm_load_print_meta: model params     = 1.41 B
0.00.048.334 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.334 I llm_load_print_meta: general.name     = 1.4B
0.00.048.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.335 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.335 I llm_load_print_meta: LF token         = 128 ''
0.00.048.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.335 I llm_load_print_meta: max token length = 1024
0.00.049.859 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.049.859 I llm_load_tensors: offloading output layer to GPU
0.00.049.860 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.049.869 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.049.870 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.050.724 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.725 I llama_new_context_with_model: n_ctx         = 128
0.00.050.725 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.725 I llama_new_context_with_model: n_batch       = 128
0.00.050.726 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.726 I llama_new_context_with_model: flash_attn    = 0
0.00.050.726 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.726 I llama_new_context_with_model: freq_scale    = 1
0.00.050.727 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.727 I ggml_metal_init: allocating
0.00.050.732 I ggml_metal_init: found device: Apple M4
0.00.050.734 I ggml_metal_init: picking default device: Apple M4
0.00.051.254 I ggml_metal_init: using embedded metal library
0.00.053.191 I ggml_metal_init: GPU name:   Apple M4
0.00.053.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.193 I ggml_metal_init: simdgroup reduction   = true
0.00.053.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.193 I ggml_metal_init: has bfloat            = true
0.00.053.193 I ggml_metal_init: use bfloat            = true
0.00.053.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.010 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.023 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.062.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.062.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.062.876 I llama_new_context_with_model: graph nodes  = 967
0.00.062.876 I llama_new_context_with_model: graph splits = 2
0.00.062.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.011 I 
0.00.608.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.608.089 I perplexity: tokenizing the input ..
0.00.615.152 I perplexity: tokenization took 7.062 ms
0.00.615.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.136 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.750.270 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.750.282 I llama_perf_context_print:        load time =     599.64 ms
0.00.750.284 I llama_perf_context_print: prompt eval time =     133.74 ms /   128 tokens (    1.04 ms per token,   957.12 tokens per second)
0.00.750.285 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.285 I llama_perf_context_print:       total time =     142.27 ms /   129 tokens
0.00.750.556 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.074s
sys	0m0.118s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.711 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.255 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.698 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.699 I llama_model_loader: - type  f32:  194 tensors
0.00.024.700 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.700 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.701 I llm_load_vocab: special tokens cache size = 25
0.00.050.622 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.625 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.625 I llm_load_print_meta: arch             = gptneox
0.00.050.625 I llm_load_print_meta: vocab type       = BPE
0.00.050.626 I llm_load_print_meta: n_vocab          = 50304
0.00.050.626 I llm_load_print_meta: n_merges         = 50009
0.00.050.626 I llm_load_print_meta: vocab_only       = 0
0.00.050.626 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.626 I llm_load_print_meta: n_embd           = 2048
0.00.050.627 I llm_load_print_meta: n_layer          = 24
0.00.050.630 I llm_load_print_meta: n_head           = 16
0.00.050.630 I llm_load_print_meta: n_head_kv        = 16
0.00.050.630 I llm_load_print_meta: n_rot            = 32
0.00.050.631 I llm_load_print_meta: n_swa            = 0
0.00.050.631 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.631 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.632 I llm_load_print_meta: n_gqa            = 1
0.00.050.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.634 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.635 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.635 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.636 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.636 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.636 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.637 I llm_load_print_meta: n_ff             = 8192
0.00.050.637 I llm_load_print_meta: n_expert         = 0
0.00.050.637 I llm_load_print_meta: n_expert_used    = 0
0.00.050.640 I llm_load_print_meta: causal attn      = 1
0.00.050.641 I llm_load_print_meta: pooling type     = 0
0.00.050.641 I llm_load_print_meta: rope type        = 2
0.00.050.641 I llm_load_print_meta: rope scaling     = linear
0.00.050.641 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.642 I llm_load_print_meta: freq_scale_train = 1
0.00.050.642 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.642 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.642 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.642 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.642 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.643 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.643 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.649 I llm_load_print_meta: model type       = 1.4B
0.00.050.650 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.650 I llm_load_print_meta: model params     = 1.41 B
0.00.050.650 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.650 I llm_load_print_meta: general.name     = 1.4B
0.00.050.651 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: LF token         = 128 ''
0.00.050.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: max token length = 1024
0.00.052.445 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.445 I llm_load_tensors: offloading output layer to GPU
0.00.052.445 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.450 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.451 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.458 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.459 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.459 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.460 I llama_new_context_with_model: n_batch       = 2048
0.00.053.460 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.460 I llama_new_context_with_model: flash_attn    = 0
0.00.053.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.461 I llama_new_context_with_model: freq_scale    = 1
0.00.053.461 I ggml_metal_init: allocating
0.00.053.466 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.054.000 I ggml_metal_init: using embedded metal library
0.00.055.925 I ggml_metal_init: GPU name:   Apple M4
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.929 I ggml_metal_init: simdgroup reduction   = true
0.00.055.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.929 I ggml_metal_init: has bfloat            = true
0.00.055.929 I ggml_metal_init: use bfloat            = true
0.00.055.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.850 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.868 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.845 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.846 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.846 I llama_new_context_with_model: graph nodes  = 967
0.00.084.847 I llama_new_context_with_model: graph splits = 2
0.00.084.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.690 I main: llama threadpool init, n_threads = 4
0.00.743.720 I 
0.00.743.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.743.743 I 
0.00.743.961 I sampler seed: 1234
0.00.743.966 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.988 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.989 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.578.682 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.578.683 I llama_perf_context_print:        load time =     733.98 ms
0.01.578.684 I llama_perf_context_print: prompt eval time =      38.70 ms /     7 tokens (    5.53 ms per token,   180.88 tokens per second)
0.01.578.685 I llama_perf_context_print:        eval time =     792.99 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.578.685 I llama_perf_context_print:       total time =     834.99 ms /    70 tokens
0.01.578.852 I ggml_metal_free: deallocating

real	0m1.594s
user	0m0.106s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.487 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.212 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.224 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.086 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.878 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.879 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.880 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.880 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.881 I llama_model_loader: - type  f32:  194 tensors
0.00.023.882 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.882 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.629 I llm_load_vocab: special tokens cache size = 25
0.00.050.600 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.603 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.603 I llm_load_print_meta: arch             = gptneox
0.00.050.603 I llm_load_print_meta: vocab type       = BPE
0.00.050.604 I llm_load_print_meta: n_vocab          = 50304
0.00.050.604 I llm_load_print_meta: n_merges         = 50009
0.00.050.604 I llm_load_print_meta: vocab_only       = 0
0.00.050.604 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.604 I llm_load_print_meta: n_embd           = 2048
0.00.050.605 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.608 I llm_load_print_meta: n_head_kv        = 16
0.00.050.609 I llm_load_print_meta: n_rot            = 32
0.00.050.609 I llm_load_print_meta: n_swa            = 0
0.00.050.609 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.609 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.612 I llm_load_print_meta: n_gqa            = 1
0.00.050.613 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.614 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.614 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.615 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.615 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.616 I llm_load_print_meta: n_ff             = 8192
0.00.050.616 I llm_load_print_meta: n_expert         = 0
0.00.050.617 I llm_load_print_meta: n_expert_used    = 0
0.00.050.617 I llm_load_print_meta: causal attn      = 1
0.00.050.618 I llm_load_print_meta: pooling type     = 0
0.00.050.618 I llm_load_print_meta: rope type        = 2
0.00.050.618 I llm_load_print_meta: rope scaling     = linear
0.00.050.618 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.619 I llm_load_print_meta: freq_scale_train = 1
0.00.050.619 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.619 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.619 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.620 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.620 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.631 I llm_load_print_meta: model type       = 1.4B
0.00.050.631 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.632 I llm_load_print_meta: model params     = 1.41 B
0.00.050.632 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.632 I llm_load_print_meta: general.name     = 1.4B
0.00.050.633 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: LF token         = 128 ''
0.00.050.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.634 I llm_load_print_meta: max token length = 1024
0.00.052.149 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.149 I llm_load_tensors: offloading output layer to GPU
0.00.052.149 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.159 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.160 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.024 I llama_new_context_with_model: n_ctx         = 128
0.00.053.025 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.025 I llama_new_context_with_model: n_batch       = 128
0.00.053.025 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.025 I llama_new_context_with_model: flash_attn    = 0
0.00.053.025 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.026 I llama_new_context_with_model: freq_scale    = 1
0.00.053.026 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.027 I ggml_metal_init: allocating
0.00.053.032 I ggml_metal_init: found device: Apple M4
0.00.053.035 I ggml_metal_init: picking default device: Apple M4
0.00.053.562 I ggml_metal_init: using embedded metal library
0.00.055.486 I ggml_metal_init: GPU name:   Apple M4
0.00.055.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.489 I ggml_metal_init: simdgroup reduction   = true
0.00.055.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.489 I ggml_metal_init: has bfloat            = true
0.00.055.489 I ggml_metal_init: use bfloat            = true
0.00.055.490 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.710 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.713 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.728 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.614 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.615 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.615 I llama_new_context_with_model: graph nodes  = 967
0.00.065.615 I llama_new_context_with_model: graph splits = 2
0.00.065.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.795 I 
0.00.681.810 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.681.819 I perplexity: tokenizing the input ..
0.00.689.071 I perplexity: tokenization took 7.251 ms
0.00.689.073 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.994 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.831.229 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.831.250 I llama_perf_context_print:        load time =     672.31 ms
0.00.831.251 I llama_perf_context_print: prompt eval time =     140.70 ms /   128 tokens (    1.10 ms per token,   909.72 tokens per second)
0.00.831.252 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.253 I llama_perf_context_print:       total time =     149.45 ms /   129 tokens
0.00.831.714 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.076s
sys	0m0.134s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.209 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.165 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.664 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.666 I llama_model_loader: - type  f32:  194 tensors
0.00.023.666 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.799 I llm_load_vocab: special tokens cache size = 25
0.00.049.759 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.762 I llm_load_print_meta: arch             = gptneox
0.00.049.763 I llm_load_print_meta: vocab type       = BPE
0.00.049.763 I llm_load_print_meta: n_vocab          = 50304
0.00.049.763 I llm_load_print_meta: n_merges         = 50009
0.00.049.763 I llm_load_print_meta: vocab_only       = 0
0.00.049.763 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.764 I llm_load_print_meta: n_embd           = 2048
0.00.049.764 I llm_load_print_meta: n_layer          = 24
0.00.049.766 I llm_load_print_meta: n_head           = 16
0.00.049.767 I llm_load_print_meta: n_head_kv        = 16
0.00.049.767 I llm_load_print_meta: n_rot            = 32
0.00.049.768 I llm_load_print_meta: n_swa            = 0
0.00.049.768 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.768 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.769 I llm_load_print_meta: n_gqa            = 1
0.00.049.769 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.771 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.771 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.771 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.774 I llm_load_print_meta: n_ff             = 8192
0.00.049.774 I llm_load_print_meta: n_expert         = 0
0.00.049.774 I llm_load_print_meta: n_expert_used    = 0
0.00.049.774 I llm_load_print_meta: causal attn      = 1
0.00.049.774 I llm_load_print_meta: pooling type     = 0
0.00.049.774 I llm_load_print_meta: rope type        = 2
0.00.049.775 I llm_load_print_meta: rope scaling     = linear
0.00.049.775 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.775 I llm_load_print_meta: freq_scale_train = 1
0.00.049.775 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.776 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.777 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.788 I llm_load_print_meta: model type       = 1.4B
0.00.049.788 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.789 I llm_load_print_meta: model params     = 1.41 B
0.00.049.789 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.789 I llm_load_print_meta: general.name     = 1.4B
0.00.049.789 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: LF token         = 128 ''
0.00.049.790 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.791 I llm_load_print_meta: max token length = 1024
0.00.051.300 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.300 I llm_load_tensors: offloading output layer to GPU
0.00.051.300 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.309 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.310 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.180 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.181 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.181 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.181 I llama_new_context_with_model: n_batch       = 2048
0.00.052.181 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.181 I llama_new_context_with_model: flash_attn    = 0
0.00.052.182 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.182 I llama_new_context_with_model: freq_scale    = 1
0.00.052.183 I ggml_metal_init: allocating
0.00.052.186 I ggml_metal_init: found device: Apple M4
0.00.052.188 I ggml_metal_init: picking default device: Apple M4
0.00.052.695 I ggml_metal_init: using embedded metal library
0.00.054.601 I ggml_metal_init: GPU name:   Apple M4
0.00.054.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.603 I ggml_metal_init: simdgroup reduction   = true
0.00.054.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.604 I ggml_metal_init: has bfloat            = true
0.00.054.604 I ggml_metal_init: use bfloat            = true
0.00.054.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.405 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.424 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.421 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.422 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.423 I llama_new_context_with_model: graph nodes  = 967
0.00.082.423 I llama_new_context_with_model: graph splits = 2
0.00.082.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.852 I main: llama threadpool init, n_threads = 4
0.00.800.886 I 
0.00.800.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.800.914 I 
0.00.801.163 I sampler seed: 1234
0.00.801.168 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.213 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.215 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.215 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.666.780 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.666.781 I llama_perf_context_print:        load time =     792.63 ms
0.01.666.782 I llama_perf_context_print: prompt eval time =      38.61 ms /     7 tokens (    5.52 ms per token,   181.31 tokens per second)
0.01.666.786 I llama_perf_context_print:        eval time =     823.95 ms /    63 runs   (   13.08 ms per token,    76.46 tokens per second)
0.01.666.786 I llama_perf_context_print:       total time =     865.93 ms /    70 tokens
0.01.666.937 I ggml_metal_free: deallocating

real	0m1.682s
user	0m0.106s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4161 (7f9cc205) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.010 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.787 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.787 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.790 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.366 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.367 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.368 I llama_model_loader: - type  f32:  194 tensors
0.00.023.368 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.900 I llm_load_vocab: special tokens cache size = 25
0.00.049.895 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.898 I llm_load_print_meta: arch             = gptneox
0.00.049.898 I llm_load_print_meta: vocab type       = BPE
0.00.049.898 I llm_load_print_meta: n_vocab          = 50304
0.00.049.899 I llm_load_print_meta: n_merges         = 50009
0.00.049.899 I llm_load_print_meta: vocab_only       = 0
0.00.049.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.899 I llm_load_print_meta: n_embd           = 2048
0.00.049.899 I llm_load_print_meta: n_layer          = 24
0.00.049.902 I llm_load_print_meta: n_head           = 16
0.00.049.902 I llm_load_print_meta: n_head_kv        = 16
0.00.049.903 I llm_load_print_meta: n_rot            = 32
0.00.049.903 I llm_load_print_meta: n_swa            = 0
0.00.049.903 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.903 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.904 I llm_load_print_meta: n_gqa            = 1
0.00.049.905 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.905 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.909 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.909 I llm_load_print_meta: n_ff             = 8192
0.00.049.911 I llm_load_print_meta: n_expert         = 0
0.00.049.912 I llm_load_print_meta: n_expert_used    = 0
0.00.049.912 I llm_load_print_meta: causal attn      = 1
0.00.049.912 I llm_load_print_meta: pooling type     = 0
0.00.049.912 I llm_load_print_meta: rope type        = 2
0.00.049.912 I llm_load_print_meta: rope scaling     = linear
0.00.049.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.914 I llm_load_print_meta: freq_scale_train = 1
0.00.049.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.916 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.916 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.916 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.916 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.916 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.928 I llm_load_print_meta: model type       = 1.4B
0.00.049.928 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.929 I llm_load_print_meta: model params     = 1.41 B
0.00.049.929 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.929 I llm_load_print_meta: general.name     = 1.4B
0.00.049.930 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.931 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.931 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.931 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.931 I llm_load_print_meta: LF token         = 128 ''
0.00.049.931 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.932 I llm_load_print_meta: max token length = 1024
0.00.051.977 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.977 I llm_load_tensors: offloading output layer to GPU
0.00.051.977 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.988 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.989 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.915 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.916 I llama_new_context_with_model: n_ctx         = 128
0.00.052.916 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.916 I llama_new_context_with_model: n_batch       = 128
0.00.052.917 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.917 I llama_new_context_with_model: flash_attn    = 0
0.00.052.917 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.917 I llama_new_context_with_model: freq_scale    = 1
0.00.052.918 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.918 I ggml_metal_init: allocating
0.00.052.921 I ggml_metal_init: found device: Apple M4
0.00.052.923 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.411 I ggml_metal_init: GPU name:   Apple M4
0.00.055.412 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.413 I ggml_metal_init: simdgroup reduction   = true
0.00.055.413 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.413 I ggml_metal_init: has bfloat            = true
0.00.055.413 I ggml_metal_init: use bfloat            = true
0.00.055.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.416 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.588 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.591 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.604 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.500 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.501 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.501 I llama_new_context_with_model: graph nodes  = 967
0.00.065.501 I llama_new_context_with_model: graph splits = 2
0.00.065.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.862 I 
0.00.681.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.681.926 I perplexity: tokenizing the input ..
0.00.689.691 I perplexity: tokenization took 7.763 ms
0.00.689.695 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.308 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.830.424 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.830.440 I llama_perf_context_print:        load time =     672.85 ms
0.00.830.441 I llama_perf_context_print: prompt eval time =     139.39 ms /   128 tokens (    1.09 ms per token,   918.29 tokens per second)
0.00.830.442 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.442 I llama_perf_context_print:       total time =     148.58 ms /   129 tokens
0.00.830.776 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.076s
sys	0m0.124s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4161 (7f9cc205)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144c0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144c0a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144c0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144c0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144c0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144c0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144c0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144c0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144c0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144c0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144c0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144c0de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144c0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144c0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144c0f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144c10070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144c10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144c10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144c115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144c11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144c124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144c12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144c13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144c13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144c14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144c14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144c15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144c15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144c16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144c164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144c16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144c16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144c17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144c177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144c17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144c18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144c185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144c18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144c18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144c193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144c19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144c19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144c1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144c1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144c1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144c1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144c1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144c1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144c1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144c1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144c1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144c1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144c1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144c1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144c1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144c1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144c1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144c1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144c202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144c20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144c20be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144c21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144c21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144c219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144c21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144c22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144c227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144c22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144c230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144c23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144c23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144c23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144c24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144c24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144c24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144c25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144c255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144c25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144c25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144c263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144c26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144c26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144c271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144c27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144c27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144c27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144c28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144c288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144c28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144c29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144c296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144c29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144c29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144c2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144c2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144c1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144c2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144c2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144c2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144c2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144c2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144c2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144c2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144c2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144c2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144c2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144c2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144c2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144c2e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144c2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144c2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144c2f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144c2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144c2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144c302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144c30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144c30bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144c31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144c31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144c319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144c31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144c32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144c327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144c32c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144c330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144c33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144c33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144c33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144c34370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144c34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144c34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144c35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144c355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144c35a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144c35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144c363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144c36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144c36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144c371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144c37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144c37af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144c37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144c38430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144c388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144c38d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144c39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144c396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144c39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144c39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144c3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144c3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144c3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144c3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144c3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144c3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144c3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144c3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144c3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144c3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144c3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144c3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144c3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144c3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144c3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144c3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144c3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144c40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144c407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144c40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144c41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144c41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144c41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144c42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144c42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144c43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144c43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144c43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144c44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144c44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144c44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144c45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144c45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144c45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144c461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144c46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144c46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144c471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144c47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144c47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144c481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144c48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144c48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144c491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144c49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144c49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144c4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144c4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144c4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144c4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144c4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144c4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144c4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144c4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144c4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144c4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144c4d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144c4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144c4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144c4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144c4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144c4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144c4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144c4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144c50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144c506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144c50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144c51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144c51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144c51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144c52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144c52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144c52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144c52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144c53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144c53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144c53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144c54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144c546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144c54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144c55020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144c554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144c55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144c55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144c562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144c567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144c56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144c57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144c57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144c58470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144c58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144c58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144c59350 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144f07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144f077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144f07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144f08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144f09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144f09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144f0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144f0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144f0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144f0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144f0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144f0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144f0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144f0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144f0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144f0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144f0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144f0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144f0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144f0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144f10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144f10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144f11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144f123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144f14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144f16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144f17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144f17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144f19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144f1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144f1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144b04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144b046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144b04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144b04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144b05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144b058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144b05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144b06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144b06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144b06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144b06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144b07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144b077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144b07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144b080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144b08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144b08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144b08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144b09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144b096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144b09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144b09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144b0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144b0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144b0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144b0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144b0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144b0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144b0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144b0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144b0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144b0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144b0d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144b0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144b0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144b0ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144b0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144b0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144b0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144b0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144b0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144b0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144b0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144b10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144b105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144b10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144b10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144b11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144b11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144b11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144b12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144b124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144b12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144b12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144b13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144b13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144b13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144b13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144b143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144b14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144b14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144b15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144b155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144b15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144b15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144b162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144b16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144b16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144b17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144b174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144b17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144b17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144b18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144b18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144b18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144b18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144b193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144b19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144b19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144b1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144b1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144b1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144b1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144b1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144b1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144b1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144b1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144b1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144b1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144b1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144b1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144b1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144b1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144b1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144b1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144b1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144b1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144b1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144b1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144b1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144b20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144b20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144b20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144b20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144b213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144b21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144b21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144b22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144b22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144b22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144b22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144b232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144b23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144b23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144b24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144b244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144b24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144b24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144b251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144b25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144b25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144b25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144b263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144b26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144b26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144b27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144b27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144b279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144b27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144b282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144b28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144b28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144b29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144b29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144b298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144b29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144b2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144b2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144b2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144b2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144b2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144b2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144b2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144b2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144b2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144b2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144b2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144b2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144b2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144b2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144b2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144b2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144b2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144b2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144b2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144b2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144b2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144b2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144b30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144b307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144b30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144b31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144b31eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144b325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144b32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144b32fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144b33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144b336e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144f072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144f07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144f07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144f08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144f08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144f0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144f0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144f0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144f10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144f11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144f119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144f11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144f122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144f12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144f13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144f13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144f138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144f14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144f14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144f14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144f15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144f160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144f16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144f17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144f17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144f1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144f1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144f1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144f1e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144f1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144f1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144f1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144f1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144f1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144f203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144f208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144f20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144f21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144f21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144f21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144f22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144f22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144f22c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144f23170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144f23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144f23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144f240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144f245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144f24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144f259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144f25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144f26410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144f26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144f26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144f27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144f27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144f28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144f286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144f28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144f295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144f29ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144f29ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144f2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144f2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144f2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144f2b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144f2b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144f2be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144f2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144f2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144f2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144f2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144f2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144f2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144f2e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144f2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144f2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144f2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144f2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144f2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144f30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144f30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144f30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144f30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144f31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144f31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144f31e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144f32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144f328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144f32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144f337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144f33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144f341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144f34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144f34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144f35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144f35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144f35c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144f36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144f36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144f37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144f37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144f38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144f38720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144f38bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144f39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144f39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144f39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144f3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144f3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144f3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144f3b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144f3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144f3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144f3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144f3cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144f3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144f3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144f3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144f3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144f3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144f3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144f3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144f3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144f3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144f40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144f407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144f40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144f41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144f41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144f42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144f42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144f42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144f43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144f43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144f43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144f44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144f44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144f45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144f45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144f45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144f461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144f46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144f46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144f471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144f47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144f481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144f48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144f491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144f49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144f4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144f4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144f4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144f4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144f4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144f4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144f4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144f4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144f4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144f4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144f4dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144f4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144f4e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144f4eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144f4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144f4f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144f4f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144f4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144f50300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144f50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144f51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144f52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144f52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144f52e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.819s
user	0m0.287s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4161 (7f9cc205)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123f0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123f0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123f0b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123f0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123f0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123f0caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123f0d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123f0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123f0dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123f0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123f0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123f0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123f0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123f105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123f10d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123f11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123f11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123f12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123f12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123f13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123f13870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123f13f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123f14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123f14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123f15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123f15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123f16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123f169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123f16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123f17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123f173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123f17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123f181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123f18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123f18920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123f18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123f19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123f19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123f19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123f1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123f1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123f1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123f1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123f1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123f1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123f1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123f1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123f1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123f1d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123f1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123f1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123f1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123f1f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123f1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123f1fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123f1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123f20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123f20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123f20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123f213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123f21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123f21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123f221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123f22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123f22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123f22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123f23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123f238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123f23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123f24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123f246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123f24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123f24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123f25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123f25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123f25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123f26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123f26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123f27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123f274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123f27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123f27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123f282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123f28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123f28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123f290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123f29550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123f299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123f2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123f2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123f2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123f2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123f2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123f1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123f2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123f2c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123f2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123f2c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123f2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123f2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123f2dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123f2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123f2e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123f2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123f2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123f2f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123f2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123f2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123f30600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123f30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123f313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123f31880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123f31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123f321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123f32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123f32b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123f32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123f33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123f338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123f33d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123f34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123f346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123f34b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123f35000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123f354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123f35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123f35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123f36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123f36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123f36bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123f37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123f37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123f379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123f37e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123f382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123f38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123f38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123f390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123f39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123f39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123f39ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123f3a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123f3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123f3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123f3b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123f3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123f3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123f3cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123f3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123f3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123f3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123f3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123f3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123f3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123f3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123f3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123f3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123f401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123f40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123f40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123f41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123f41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123f41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123f42420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123f42970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123f42ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123f43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123f43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123f43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123f44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123f44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123f44ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123f453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123f45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123f45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123f463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123f46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123f46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123f473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123f47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123f47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123f483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123f48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123f48e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123f493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123f49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123f49e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123f4a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123f4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123f4ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123f4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123f4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123f4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123f4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123f4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123f4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123f4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123f4e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123f4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123f4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123f4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123f50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123f50890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123f50de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123f51330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123f51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123f51dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123f52320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123f52870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123f52dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123f53310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123f537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123f53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123f540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123f54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123f54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123f54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123f55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123f55810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123f55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123f56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123f565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123f56a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123f56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123f57480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123f57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123f582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123f589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123f59100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123f593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123f599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123f59fe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125806050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1258064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125806930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125806da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125807210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125807680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125807af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125807f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1258083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1258088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125808d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1258093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125809ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12580a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12580aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12580b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12580bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12580c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12580cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12580d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12580da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12580e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12580e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12580ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12580f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12580f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12580fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125810090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125810500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125810970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125810de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125811310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125811780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125811a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125811eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125812320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125812790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125812c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125813070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1258134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125813950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125813dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125814230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1258146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125814b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125814f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1258153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125815860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125815cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1258165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125816a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125816e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125817300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125817770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125817be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125818150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125818650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125818ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125818f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1258193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125819810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125819c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12581a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12581a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12581a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12581ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12581b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12581b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12581bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12581c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12581c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12581c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12581cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12581d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12581d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12581daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12581df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12581e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12581e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12581ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12581f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12581f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12581f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12581fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125820290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125820700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125820b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125820fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125821450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1258218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125821d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1258221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125822610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125822a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125822ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125823360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1258237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125823c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1258240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125824520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125824990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125824e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125825270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1258256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125825b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125825fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125826430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1258268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125826d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125827180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1258275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125827a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125827ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125828340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1258287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125828c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125829090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125829500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125829970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125829de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12582a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12582a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12582ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12582afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12582b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12582b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12582bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12582c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12582c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12582ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12582ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12582d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12582d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12582dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12582e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12582e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12582e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12582edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12582f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12582f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12582fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12582ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1258303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125830860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125830cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125831140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1258315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125831a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125832300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125832770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125832be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125833050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1258334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125833930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125833da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125834210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125834680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125834af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125834f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1258353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125835840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125835cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125836120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125836590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125836a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125837590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125837850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125837f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1258383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125838860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125838cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125839140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1258395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125839a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125839e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12583a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12583a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12583abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12583b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12583b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12583b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12583bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12583c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12583c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12583caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12583cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12583d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12583d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12583dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12583e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12583e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12583ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12583ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12583f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12583f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12583fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125840030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1258404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125840910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125840d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1258411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125841660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125841ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125841f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1258423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125842820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125842c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125843100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125843570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1258439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125843e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1258442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125844730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125844ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125845010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125845480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1258458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125845d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1258461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125846640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125846ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125846f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125847390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125847800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125847c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1258480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125848550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1258489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125848e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1258492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125849710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125849b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125849ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12584a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12584a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12584b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12584bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12584c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12584c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12584cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12584cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12584d360 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125806050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1258064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125806930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125806da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125807210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125807680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125807af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125807f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1258083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125808840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125808cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125809290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125809b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12580a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12580aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12580b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12580b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12580bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12580c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12580d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12580d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12580de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12580e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12580ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12580f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12580f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12580fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125810020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1258111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125811910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125811d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1258121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125812660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125812ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125812f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1258133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125813820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125813c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125814100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125814570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1258149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125814e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1258152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125815730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125815ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125816010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125816480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1258168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125816d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1258171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125817640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125817ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125817f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125818390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125818800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125818c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1258190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125819550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1258199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12581a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12581a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12581ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12581aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12581b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12581b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12581bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12581c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12581c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12581ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12581cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12581d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12581d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12581dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12581e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12581e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12581e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12581ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12581f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12581f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12581fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12581ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125820440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1258208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125821190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125821600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125821a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125821ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125822350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1258227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125822c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1258230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125823510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125823980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125823df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125824260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1258246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125824b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125824fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125825420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125825890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125825d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125826170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1258265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125826a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125826ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125827330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1258277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125827c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125828080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1258284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125828960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125828dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125829240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1258296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125829b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125829f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12582a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12582a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12582ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12582b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12582b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12582ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12582bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12582c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12582c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12582cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12582d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12582d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12582d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12582ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12582e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12582e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12582eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12582ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12582f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12582f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12582fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125830130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1258305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125830a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125830e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1258312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125831760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125831bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125832040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1258324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125832920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125832d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125833200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125833670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125833ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125833f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1258343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125834ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125835110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125835580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1258359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125835e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1258362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125836740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125836ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125837330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1258377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125837c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125838080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1258384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125838960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125838dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125839240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1258396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125839b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125839f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12583a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12583a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12583ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12583b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12583b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12583ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12583bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12583c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12583c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12583cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12583d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12583d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12583d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12583ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12583e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12583e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12583eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12583ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12583f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12583f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12583fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1258405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125840a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125840e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1258412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125841760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125841bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125842040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1258424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125842920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125842d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125843200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125843670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125843ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125843f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1258443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125844830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125844ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125845110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125845580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1258459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1258462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125846740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125846bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125847020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125847490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125847900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125847d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1258481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125848650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125848ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125848f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1258493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125849810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125849c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12584a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12584a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12584ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12584b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12584ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12584c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12584c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12584ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12584ce70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.927s
user	0m0.236s
sys	0m0.117s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.54 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.04 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.25 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.25 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
