Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.673s
user	0m0.907s
sys	0m1.264s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-parallel
[ 81%] Built target llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-passkey
[ 83%] Built target llama-lookup-create
[ 83%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.100s
user	0m6.207s
sys	0m9.859s

main: quantize time =  5540.72 ms
main:    total time =  5540.72 ms

main: quantize time =  2928.41 ms
main:    total time =  2928.41 ms

main: quantize time =  3949.41 ms
main:    total time =  3949.41 ms

main: quantize time =  3668.20 ms
main:    total time =  3668.20 ms

main: quantize time =  2666.71 ms
main:    total time =  2666.71 ms

main: quantize time =  5140.83 ms
main:    total time =  5140.83 ms

main: quantize time =  5808.50 ms
main:    total time =  5808.50 ms

main: quantize time =  7231.66 ms
main:    total time =  7231.66 ms

main: quantize time =  6042.97 ms
main:    total time =  6042.97 ms

main: quantize time =  4497.42 ms
main:    total time =  4497.42 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.198 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.373 I main: llama backend init
0.00.000.380 I main: load the model and apply lora adapter, if any
0.00.051.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.176 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.225 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.226 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.071.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.081.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.924 I llama_model_loader: - type  f32:  194 tensors
0.00.081.925 I llama_model_loader: - type  f16:   98 tensors
0.00.081.929 I print_info: file format = GGUF V3 (latest)
0.00.081.931 I print_info: file type   = all F32 (guessed)
0.00.081.933 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.029 I load: special tokens cache size = 25
0.00.107.385 I load: token to piece cache size = 0.2984 MB
0.00.107.388 I print_info: arch             = gptneox
0.00.107.389 I print_info: vocab_only       = 0
0.00.107.389 I print_info: n_ctx_train      = 2048
0.00.107.389 I print_info: n_embd           = 2048
0.00.107.389 I print_info: n_layer          = 24
0.00.107.393 I print_info: n_head           = 16
0.00.107.395 I print_info: n_head_kv        = 16
0.00.107.395 I print_info: n_rot            = 32
0.00.107.395 I print_info: n_swa            = 0
0.00.107.395 I print_info: n_embd_head_k    = 128
0.00.107.396 I print_info: n_embd_head_v    = 128
0.00.107.396 I print_info: n_gqa            = 1
0.00.107.398 I print_info: n_embd_k_gqa     = 2048
0.00.107.398 I print_info: n_embd_v_gqa     = 2048
0.00.107.399 I print_info: f_norm_eps       = 1.0e-05
0.00.107.400 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.107.400 I print_info: f_clamp_kqv      = 0.0e+00
0.00.107.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.107.400 I print_info: f_logit_scale    = 0.0e+00
0.00.107.401 I print_info: n_ff             = 8192
0.00.107.402 I print_info: n_expert         = 0
0.00.107.402 I print_info: n_expert_used    = 0
0.00.107.402 I print_info: causal attn      = 1
0.00.107.402 I print_info: pooling type     = 0
0.00.107.402 I print_info: rope type        = 2
0.00.107.403 I print_info: rope scaling     = linear
0.00.107.403 I print_info: freq_base_train  = 10000.0
0.00.107.403 I print_info: freq_scale_train = 1
0.00.107.404 I print_info: n_ctx_orig_yarn  = 2048
0.00.107.407 I print_info: rope_finetuned   = unknown
0.00.107.407 I print_info: ssm_d_conv       = 0
0.00.107.407 I print_info: ssm_d_inner      = 0
0.00.107.407 I print_info: ssm_d_state      = 0
0.00.107.407 I print_info: ssm_dt_rank      = 0
0.00.107.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.107.408 I print_info: model type       = 1.4B
0.00.107.408 I print_info: model params     = 1.41 B
0.00.107.408 I print_info: general.name     = 1.4B
0.00.107.409 I print_info: vocab type       = BPE
0.00.107.409 I print_info: n_vocab          = 50304
0.00.107.409 I print_info: n_merges         = 50009
0.00.107.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.107.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.107.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.107.415 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.107.415 I print_info: LF token         = 128 'Ä'
0.00.107.416 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.107.416 I print_info: max token length = 1024
0.00.148.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.148.437 I load_tensors: offloading output layer to GPU
0.00.148.437 I load_tensors: offloaded 25/25 layers to GPU
0.00.148.462 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.148.464 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.148.781 I llama_init_from_model: n_seq_max     = 1
0.00.148.783 I llama_init_from_model: n_ctx         = 2048
0.00.148.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.148.783 I llama_init_from_model: n_batch       = 2048
0.00.148.784 I llama_init_from_model: n_ubatch      = 512
0.00.148.784 I llama_init_from_model: flash_attn    = 0
0.00.148.785 I llama_init_from_model: freq_base     = 10000.0
0.00.148.785 I llama_init_from_model: freq_scale    = 1
0.00.148.785 I ggml_metal_init: allocating
0.00.148.806 I ggml_metal_init: found device: Apple M4
0.00.148.814 I ggml_metal_init: picking default device: Apple M4
0.00.149.452 I ggml_metal_init: using embedded metal library
0.00.158.370 I ggml_metal_init: GPU name:   Apple M4
0.00.158.372 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.158.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.158.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.158.373 I ggml_metal_init: simdgroup reduction   = true
0.00.158.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.158.374 I ggml_metal_init: has residency sets    = true
0.00.158.374 I ggml_metal_init: has bfloat            = true
0.00.158.374 I ggml_metal_init: use bfloat            = true
0.00.158.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.158.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.186.384 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.215.706 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.215.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.215.735 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.219.309 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.219.311 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.219.312 I llama_init_from_model: graph nodes  = 967
0.00.219.312 I llama_init_from_model: graph splits = 2
0.00.219.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.219.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.219.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.287.305 I main: llama threadpool init, n_threads = 4
0.00.287.347 I 
0.00.287.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.287.380 I 
0.00.287.423 I sampler seed: 1234
0.00.287.428 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.287.456 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.287.456 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.287.456 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.114.914 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.114.915 I llama_perf_context_print:        load time =     234.36 ms
0.02.114.916 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.40 tokens per second)
0.02.114.917 I llama_perf_context_print:        eval time =    1780.85 ms /    63 runs   (   28.27 ms per token,    35.38 tokens per second)
0.02.114.917 I llama_perf_context_print:       total time =    1828.62 ms /    70 tokens
0.02.115.181 I ggml_metal_free: deallocating

real	0m2.414s
user	0m0.133s
sys	0m0.136s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.149 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.013 I llama_model_loader: - type  f32:  194 tensors
0.00.028.014 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.014 I print_info: file format = GGUF V3 (latest)
0.00.028.015 I print_info: file type   = Q8_0
0.00.028.016 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.120 I load: special tokens cache size = 25
0.00.042.113 I load: token to piece cache size = 0.2984 MB
0.00.042.119 I print_info: arch             = gptneox
0.00.042.119 I print_info: vocab_only       = 0
0.00.042.120 I print_info: n_ctx_train      = 2048
0.00.042.120 I print_info: n_embd           = 2048
0.00.042.120 I print_info: n_layer          = 24
0.00.042.127 I print_info: n_head           = 16
0.00.042.128 I print_info: n_head_kv        = 16
0.00.042.128 I print_info: n_rot            = 32
0.00.042.128 I print_info: n_swa            = 0
0.00.042.128 I print_info: n_embd_head_k    = 128
0.00.042.128 I print_info: n_embd_head_v    = 128
0.00.042.129 I print_info: n_gqa            = 1
0.00.042.130 I print_info: n_embd_k_gqa     = 2048
0.00.042.130 I print_info: n_embd_v_gqa     = 2048
0.00.042.131 I print_info: f_norm_eps       = 1.0e-05
0.00.042.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.136 I print_info: f_logit_scale    = 0.0e+00
0.00.042.138 I print_info: n_ff             = 8192
0.00.042.138 I print_info: n_expert         = 0
0.00.042.138 I print_info: n_expert_used    = 0
0.00.042.140 I print_info: causal attn      = 1
0.00.042.140 I print_info: pooling type     = 0
0.00.042.140 I print_info: rope type        = 2
0.00.042.140 I print_info: rope scaling     = linear
0.00.042.141 I print_info: freq_base_train  = 10000.0
0.00.042.141 I print_info: freq_scale_train = 1
0.00.042.141 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.142 I print_info: rope_finetuned   = unknown
0.00.042.142 I print_info: ssm_d_conv       = 0
0.00.042.142 I print_info: ssm_d_inner      = 0
0.00.042.142 I print_info: ssm_d_state      = 0
0.00.042.142 I print_info: ssm_dt_rank      = 0
0.00.042.142 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.143 I print_info: model type       = 1.4B
0.00.042.167 I print_info: model params     = 1.41 B
0.00.042.169 I print_info: general.name     = 1.4B
0.00.042.170 I print_info: vocab type       = BPE
0.00.042.170 I print_info: n_vocab          = 50304
0.00.042.170 I print_info: n_merges         = 50009
0.00.042.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.170 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.171 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.171 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.172 I print_info: LF token         = 128 'Ä'
0.00.042.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.173 I print_info: max token length = 1024
0.00.957.182 I load_tensors: offloading 24 repeating layers to GPU
0.00.957.187 I load_tensors: offloading output layer to GPU
0.00.957.188 I load_tensors: offloaded 25/25 layers to GPU
0.00.957.212 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.957.216 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.957.896 I llama_init_from_model: n_seq_max     = 1
0.00.957.898 I llama_init_from_model: n_ctx         = 2048
0.00.957.899 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.957.899 I llama_init_from_model: n_batch       = 2048
0.00.957.899 I llama_init_from_model: n_ubatch      = 512
0.00.957.900 I llama_init_from_model: flash_attn    = 0
0.00.957.901 I llama_init_from_model: freq_base     = 10000.0
0.00.957.901 I llama_init_from_model: freq_scale    = 1
0.00.957.902 I ggml_metal_init: allocating
0.00.957.917 I ggml_metal_init: found device: Apple M4
0.00.957.928 I ggml_metal_init: picking default device: Apple M4
0.00.959.175 I ggml_metal_init: using embedded metal library
0.00.964.968 I ggml_metal_init: GPU name:   Apple M4
0.00.964.972 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.964.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.964.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.964.974 I ggml_metal_init: simdgroup reduction   = true
0.00.964.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.964.975 I ggml_metal_init: has residency sets    = true
0.00.964.975 I ggml_metal_init: has bfloat            = true
0.00.964.975 I ggml_metal_init: use bfloat            = true
0.00.964.976 I ggml_metal_init: hasUnifiedMemory      = true
0.00.964.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.981.625 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.036.469 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.036.474 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.036.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.040.857 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.040.859 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.040.859 I llama_init_from_model: graph nodes  = 967
0.01.040.859 I llama_init_from_model: graph splits = 2
0.01.040.863 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.040.987 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.040.988 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.097.047 I main: llama threadpool init, n_threads = 4
0.01.097.090 I 
0.01.097.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.097.112 I 
0.01.097.200 I sampler seed: 1234
0.01.097.206 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.097.238 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.097.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.097.242 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.174.623 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.02.174.624 I llama_perf_context_print:        load time =    1086.29 ms
0.02.174.628 I llama_perf_context_print: prompt eval time =      39.85 ms /     7 tokens (    5.69 ms per token,   175.68 tokens per second)
0.02.174.629 I llama_perf_context_print:        eval time =    1034.58 ms /    63 runs   (   16.42 ms per token,    60.89 tokens per second)
0.02.174.629 I llama_perf_context_print:       total time =    1078.44 ms /    70 tokens
0.02.174.918 I ggml_metal_free: deallocating

real	0m2.193s
user	0m0.108s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.586 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.589 I llama_model_loader: - type  f32:  194 tensors
0.00.028.589 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.591 I print_info: file format = GGUF V3 (latest)
0.00.028.591 I print_info: file type   = Q4_0
0.00.028.593 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.933 I load: special tokens cache size = 25
0.00.042.954 I load: token to piece cache size = 0.2984 MB
0.00.042.957 I print_info: arch             = gptneox
0.00.042.957 I print_info: vocab_only       = 0
0.00.042.957 I print_info: n_ctx_train      = 2048
0.00.042.958 I print_info: n_embd           = 2048
0.00.042.958 I print_info: n_layer          = 24
0.00.042.961 I print_info: n_head           = 16
0.00.042.962 I print_info: n_head_kv        = 16
0.00.042.963 I print_info: n_rot            = 32
0.00.042.964 I print_info: n_swa            = 0
0.00.042.964 I print_info: n_embd_head_k    = 128
0.00.042.964 I print_info: n_embd_head_v    = 128
0.00.042.965 I print_info: n_gqa            = 1
0.00.042.966 I print_info: n_embd_k_gqa     = 2048
0.00.042.966 I print_info: n_embd_v_gqa     = 2048
0.00.042.967 I print_info: f_norm_eps       = 1.0e-05
0.00.042.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.970 I print_info: f_logit_scale    = 0.0e+00
0.00.042.971 I print_info: n_ff             = 8192
0.00.042.971 I print_info: n_expert         = 0
0.00.042.971 I print_info: n_expert_used    = 0
0.00.042.971 I print_info: causal attn      = 1
0.00.042.971 I print_info: pooling type     = 0
0.00.042.971 I print_info: rope type        = 2
0.00.042.972 I print_info: rope scaling     = linear
0.00.042.972 I print_info: freq_base_train  = 10000.0
0.00.042.972 I print_info: freq_scale_train = 1
0.00.042.972 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.973 I print_info: rope_finetuned   = unknown
0.00.042.973 I print_info: ssm_d_conv       = 0
0.00.042.973 I print_info: ssm_d_inner      = 0
0.00.042.973 I print_info: ssm_d_state      = 0
0.00.042.973 I print_info: ssm_dt_rank      = 0
0.00.042.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.980 I print_info: model type       = 1.4B
0.00.042.981 I print_info: model params     = 1.41 B
0.00.042.981 I print_info: general.name     = 1.4B
0.00.042.982 I print_info: vocab type       = BPE
0.00.042.982 I print_info: n_vocab          = 50304
0.00.042.982 I print_info: n_merges         = 50009
0.00.042.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.983 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.983 I print_info: LF token         = 128 'Ä'
0.00.042.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.984 I print_info: max token length = 1024
0.00.593.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.507 I load_tensors: offloading output layer to GPU
0.00.593.508 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.542 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.548 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.594.826 I llama_init_from_model: n_seq_max     = 1
0.00.594.832 I llama_init_from_model: n_ctx         = 2048
0.00.594.832 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.833 I llama_init_from_model: n_batch       = 2048
0.00.594.833 I llama_init_from_model: n_ubatch      = 512
0.00.594.833 I llama_init_from_model: flash_attn    = 0
0.00.594.835 I llama_init_from_model: freq_base     = 10000.0
0.00.594.836 I llama_init_from_model: freq_scale    = 1
0.00.594.839 I ggml_metal_init: allocating
0.00.594.912 I ggml_metal_init: found device: Apple M4
0.00.594.925 I ggml_metal_init: picking default device: Apple M4
0.00.596.686 I ggml_metal_init: using embedded metal library
0.00.602.178 I ggml_metal_init: GPU name:   Apple M4
0.00.602.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.243 I ggml_metal_init: simdgroup reduction   = true
0.00.602.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.257 I ggml_metal_init: has residency sets    = true
0.00.602.258 I ggml_metal_init: has bfloat            = true
0.00.602.258 I ggml_metal_init: use bfloat            = true
0.00.602.262 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.846 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.405 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.414 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.459 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.024 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.026 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.027 I llama_init_from_model: graph nodes  = 967
0.00.685.027 I llama_init_from_model: graph splits = 2
0.00.685.031 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.976 I main: llama threadpool init, n_threads = 4
0.00.743.023 I 
0.00.743.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.048 I 
0.00.743.232 I sampler seed: 1234
0.00.743.236 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.248 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.248 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.422.698 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.422.699 I llama_perf_context_print:        load time =     730.12 ms
0.01.422.703 I llama_perf_context_print: prompt eval time =      44.96 ms /     7 tokens (    6.42 ms per token,   155.71 tokens per second)
0.01.422.704 I llama_perf_context_print:        eval time =     631.55 ms /    63 runs   (   10.02 ms per token,    99.75 tokens per second)
0.01.422.704 I llama_perf_context_print:       total time =     680.70 ms /    70 tokens
0.01.422.939 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.111s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.021.322 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.031.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.040.545 I llama_model_loader: - type  f32:  194 tensors
0.00.040.546 I llama_model_loader: - type q4_1:   97 tensors
0.00.040.546 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.547 I print_info: file format = GGUF V3 (latest)
0.00.040.547 I print_info: file type   = Q4_1
0.00.040.552 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.592 I load: special tokens cache size = 25
0.00.055.644 I load: token to piece cache size = 0.2984 MB
0.00.055.647 I print_info: arch             = gptneox
0.00.055.648 I print_info: vocab_only       = 0
0.00.055.648 I print_info: n_ctx_train      = 2048
0.00.055.648 I print_info: n_embd           = 2048
0.00.055.648 I print_info: n_layer          = 24
0.00.055.651 I print_info: n_head           = 16
0.00.055.652 I print_info: n_head_kv        = 16
0.00.055.652 I print_info: n_rot            = 32
0.00.055.652 I print_info: n_swa            = 0
0.00.055.653 I print_info: n_embd_head_k    = 128
0.00.055.653 I print_info: n_embd_head_v    = 128
0.00.055.654 I print_info: n_gqa            = 1
0.00.055.655 I print_info: n_embd_k_gqa     = 2048
0.00.055.655 I print_info: n_embd_v_gqa     = 2048
0.00.055.656 I print_info: f_norm_eps       = 1.0e-05
0.00.055.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.657 I print_info: f_logit_scale    = 0.0e+00
0.00.055.657 I print_info: n_ff             = 8192
0.00.055.658 I print_info: n_expert         = 0
0.00.055.658 I print_info: n_expert_used    = 0
0.00.055.658 I print_info: causal attn      = 1
0.00.055.658 I print_info: pooling type     = 0
0.00.055.658 I print_info: rope type        = 2
0.00.055.658 I print_info: rope scaling     = linear
0.00.055.659 I print_info: freq_base_train  = 10000.0
0.00.055.659 I print_info: freq_scale_train = 1
0.00.055.659 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.660 I print_info: rope_finetuned   = unknown
0.00.055.660 I print_info: ssm_d_conv       = 0
0.00.055.660 I print_info: ssm_d_inner      = 0
0.00.055.660 I print_info: ssm_d_state      = 0
0.00.055.660 I print_info: ssm_dt_rank      = 0
0.00.055.660 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.661 I print_info: model type       = 1.4B
0.00.055.661 I print_info: model params     = 1.41 B
0.00.055.661 I print_info: general.name     = 1.4B
0.00.055.662 I print_info: vocab type       = BPE
0.00.055.662 I print_info: n_vocab          = 50304
0.00.055.662 I print_info: n_merges         = 50009
0.00.055.662 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.663 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.663 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.663 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.663 I print_info: LF token         = 128 'Ä'
0.00.055.664 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.664 I print_info: max token length = 1024
0.00.654.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.379 I load_tensors: offloading output layer to GPU
0.00.654.380 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.414 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.654.416 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.655.774 I llama_init_from_model: n_seq_max     = 1
0.00.655.779 I llama_init_from_model: n_ctx         = 2048
0.00.655.779 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.780 I llama_init_from_model: n_batch       = 2048
0.00.655.780 I llama_init_from_model: n_ubatch      = 512
0.00.655.780 I llama_init_from_model: flash_attn    = 0
0.00.655.782 I llama_init_from_model: freq_base     = 10000.0
0.00.655.783 I llama_init_from_model: freq_scale    = 1
0.00.655.790 I ggml_metal_init: allocating
0.00.655.901 I ggml_metal_init: found device: Apple M4
0.00.655.915 I ggml_metal_init: picking default device: Apple M4
0.00.657.803 I ggml_metal_init: using embedded metal library
0.00.664.551 I ggml_metal_init: GPU name:   Apple M4
0.00.664.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.557 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.559 I ggml_metal_init: simdgroup reduction   = true
0.00.664.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.559 I ggml_metal_init: has residency sets    = true
0.00.664.560 I ggml_metal_init: has bfloat            = true
0.00.664.560 I ggml_metal_init: use bfloat            = true
0.00.664.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.562 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.311 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.751 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.761 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.792 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.384 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.386 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.387 I llama_init_from_model: graph nodes  = 967
0.00.747.387 I llama_init_from_model: graph splits = 2
0.00.747.392 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.046 I main: llama threadpool init, n_threads = 4
0.00.804.090 I 
0.00.804.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.115 I 
0.00.804.295 I sampler seed: 1234
0.00.804.299 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.324 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.324 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.523.837 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.523.837 I llama_perf_context_print:        load time =     781.82 ms
0.01.523.838 I llama_perf_context_print: prompt eval time =      44.65 ms /     7 tokens (    6.38 ms per token,   156.79 tokens per second)
0.01.523.839 I llama_perf_context_print:        eval time =     672.03 ms /    63 runs   (   10.67 ms per token,    93.75 tokens per second)
0.01.523.840 I llama_perf_context_print:       total time =     720.69 ms /    70 tokens
0.01.524.118 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.115s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.227 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.972 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.978 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.979 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.568 I llama_model_loader: - type  f32:  194 tensors
0.00.026.569 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.570 I print_info: file format = GGUF V3 (latest)
0.00.026.570 I print_info: file type   = Q5_0
0.00.026.574 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.391 I load: special tokens cache size = 25
0.00.040.385 I load: token to piece cache size = 0.2984 MB
0.00.040.388 I print_info: arch             = gptneox
0.00.040.388 I print_info: vocab_only       = 0
0.00.040.388 I print_info: n_ctx_train      = 2048
0.00.040.389 I print_info: n_embd           = 2048
0.00.040.389 I print_info: n_layer          = 24
0.00.040.392 I print_info: n_head           = 16
0.00.040.392 I print_info: n_head_kv        = 16
0.00.040.393 I print_info: n_rot            = 32
0.00.040.393 I print_info: n_swa            = 0
0.00.040.393 I print_info: n_embd_head_k    = 128
0.00.040.393 I print_info: n_embd_head_v    = 128
0.00.040.394 I print_info: n_gqa            = 1
0.00.040.395 I print_info: n_embd_k_gqa     = 2048
0.00.040.395 I print_info: n_embd_v_gqa     = 2048
0.00.040.396 I print_info: f_norm_eps       = 1.0e-05
0.00.040.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.399 I print_info: f_logit_scale    = 0.0e+00
0.00.040.399 I print_info: n_ff             = 8192
0.00.040.399 I print_info: n_expert         = 0
0.00.040.400 I print_info: n_expert_used    = 0
0.00.040.400 I print_info: causal attn      = 1
0.00.040.400 I print_info: pooling type     = 0
0.00.040.400 I print_info: rope type        = 2
0.00.040.400 I print_info: rope scaling     = linear
0.00.040.401 I print_info: freq_base_train  = 10000.0
0.00.040.401 I print_info: freq_scale_train = 1
0.00.040.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.401 I print_info: rope_finetuned   = unknown
0.00.040.402 I print_info: ssm_d_conv       = 0
0.00.040.403 I print_info: ssm_d_inner      = 0
0.00.040.403 I print_info: ssm_d_state      = 0
0.00.040.403 I print_info: ssm_dt_rank      = 0
0.00.040.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.404 I print_info: model type       = 1.4B
0.00.040.404 I print_info: model params     = 1.41 B
0.00.040.404 I print_info: general.name     = 1.4B
0.00.040.405 I print_info: vocab type       = BPE
0.00.040.405 I print_info: n_vocab          = 50304
0.00.040.405 I print_info: n_merges         = 50009
0.00.040.405 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.405 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.406 I print_info: LF token         = 128 'Ä'
0.00.040.406 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.406 I print_info: max token length = 1024
0.00.655.272 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.284 I load_tensors: offloading output layer to GPU
0.00.655.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.319 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.655.320 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.656.603 I llama_init_from_model: n_seq_max     = 1
0.00.656.608 I llama_init_from_model: n_ctx         = 2048
0.00.656.609 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.656.609 I llama_init_from_model: n_batch       = 2048
0.00.656.610 I llama_init_from_model: n_ubatch      = 512
0.00.656.610 I llama_init_from_model: flash_attn    = 0
0.00.656.612 I llama_init_from_model: freq_base     = 10000.0
0.00.656.613 I llama_init_from_model: freq_scale    = 1
0.00.656.615 I ggml_metal_init: allocating
0.00.656.687 I ggml_metal_init: found device: Apple M4
0.00.656.701 I ggml_metal_init: picking default device: Apple M4
0.00.658.553 I ggml_metal_init: using embedded metal library
0.00.665.004 I ggml_metal_init: GPU name:   Apple M4
0.00.665.008 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.010 I ggml_metal_init: simdgroup reduction   = true
0.00.665.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.010 I ggml_metal_init: has residency sets    = true
0.00.665.011 I ggml_metal_init: has bfloat            = true
0.00.665.011 I ggml_metal_init: use bfloat            = true
0.00.665.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.092 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.825 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.833 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.735 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.737 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.738 I llama_init_from_model: graph nodes  = 967
0.00.740.738 I llama_init_from_model: graph splits = 2
0.00.740.745 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.341 I main: llama threadpool init, n_threads = 4
0.00.799.397 I 
0.00.799.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.423 I 
0.00.799.573 I sampler seed: 1234
0.00.799.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.589 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.589 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.589 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.589.119 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.589.120 I llama_perf_context_print:        load time =     788.22 ms
0.01.589.121 I llama_perf_context_print: prompt eval time =      52.65 ms /     7 tokens (    7.52 ms per token,   132.96 tokens per second)
0.01.589.121 I llama_perf_context_print:        eval time =     733.97 ms /    63 runs   (   11.65 ms per token,    85.83 tokens per second)
0.01.589.122 I llama_perf_context_print:       total time =     790.67 ms /    70 tokens
0.01.589.352 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.109s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.850 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.851 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.853 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.855 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.857 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.644 I llama_model_loader: - type  f32:  194 tensors
0.00.026.645 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.645 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.646 I print_info: file format = GGUF V3 (latest)
0.00.026.646 I print_info: file type   = Q5_1
0.00.026.651 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.391 I load: special tokens cache size = 25
0.00.040.418 I load: token to piece cache size = 0.2984 MB
0.00.040.421 I print_info: arch             = gptneox
0.00.040.421 I print_info: vocab_only       = 0
0.00.040.421 I print_info: n_ctx_train      = 2048
0.00.040.422 I print_info: n_embd           = 2048
0.00.040.422 I print_info: n_layer          = 24
0.00.040.425 I print_info: n_head           = 16
0.00.040.425 I print_info: n_head_kv        = 16
0.00.040.426 I print_info: n_rot            = 32
0.00.040.426 I print_info: n_swa            = 0
0.00.040.426 I print_info: n_embd_head_k    = 128
0.00.040.427 I print_info: n_embd_head_v    = 128
0.00.040.428 I print_info: n_gqa            = 1
0.00.040.429 I print_info: n_embd_k_gqa     = 2048
0.00.040.429 I print_info: n_embd_v_gqa     = 2048
0.00.040.430 I print_info: f_norm_eps       = 1.0e-05
0.00.040.430 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.430 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.431 I print_info: f_logit_scale    = 0.0e+00
0.00.040.431 I print_info: n_ff             = 8192
0.00.040.432 I print_info: n_expert         = 0
0.00.040.432 I print_info: n_expert_used    = 0
0.00.040.432 I print_info: causal attn      = 1
0.00.040.432 I print_info: pooling type     = 0
0.00.040.432 I print_info: rope type        = 2
0.00.040.432 I print_info: rope scaling     = linear
0.00.040.433 I print_info: freq_base_train  = 10000.0
0.00.040.435 I print_info: freq_scale_train = 1
0.00.040.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.435 I print_info: rope_finetuned   = unknown
0.00.040.436 I print_info: ssm_d_conv       = 0
0.00.040.436 I print_info: ssm_d_inner      = 0
0.00.040.436 I print_info: ssm_d_state      = 0
0.00.040.436 I print_info: ssm_dt_rank      = 0
0.00.040.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.436 I print_info: model type       = 1.4B
0.00.040.437 I print_info: model params     = 1.41 B
0.00.040.437 I print_info: general.name     = 1.4B
0.00.040.437 I print_info: vocab type       = BPE
0.00.040.438 I print_info: n_vocab          = 50304
0.00.040.440 I print_info: n_merges         = 50009
0.00.040.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.441 I print_info: LF token         = 128 'Ä'
0.00.040.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.441 I print_info: max token length = 1024
0.00.720.120 I load_tensors: offloading 24 repeating layers to GPU
0.00.720.134 I load_tensors: offloading output layer to GPU
0.00.720.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.720.168 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.720.170 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.721.501 I llama_init_from_model: n_seq_max     = 1
0.00.721.505 I llama_init_from_model: n_ctx         = 2048
0.00.721.505 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.721.506 I llama_init_from_model: n_batch       = 2048
0.00.721.506 I llama_init_from_model: n_ubatch      = 512
0.00.721.506 I llama_init_from_model: flash_attn    = 0
0.00.721.507 I llama_init_from_model: freq_base     = 10000.0
0.00.721.508 I llama_init_from_model: freq_scale    = 1
0.00.721.509 I ggml_metal_init: allocating
0.00.721.527 I ggml_metal_init: found device: Apple M4
0.00.721.537 I ggml_metal_init: picking default device: Apple M4
0.00.722.978 I ggml_metal_init: using embedded metal library
0.00.729.250 I ggml_metal_init: GPU name:   Apple M4
0.00.729.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.254 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.255 I ggml_metal_init: simdgroup reduction   = true
0.00.729.255 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.256 I ggml_metal_init: has residency sets    = true
0.00.729.256 I ggml_metal_init: has bfloat            = true
0.00.729.256 I ggml_metal_init: use bfloat            = true
0.00.729.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.746.068 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.798.075 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.798.081 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.798.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.802.412 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.802.414 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.802.414 I llama_init_from_model: graph nodes  = 967
0.00.802.414 I llama_init_from_model: graph splits = 2
0.00.802.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.802.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.802.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.756 I main: llama threadpool init, n_threads = 4
0.00.858.799 I 
0.00.858.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.826 I 
0.00.858.973 I sampler seed: 1234
0.00.858.978 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.990 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.687.500 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.687.500 I llama_perf_context_print:        load time =     847.96 ms
0.01.687.501 I llama_perf_context_print: prompt eval time =      41.90 ms /     7 tokens (    5.99 ms per token,   167.06 tokens per second)
0.01.687.502 I llama_perf_context_print:        eval time =     783.68 ms /    63 runs   (   12.44 ms per token,    80.39 tokens per second)
0.01.687.502 I llama_perf_context_print:       total time =     829.61 ms /    70 tokens
0.01.687.753 I ggml_metal_free: deallocating

real	0m1.706s
user	0m0.108s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.268 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.923 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.631 I llama_model_loader: - type  f32:  194 tensors
0.00.024.631 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.631 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.632 I print_info: file format = GGUF V3 (latest)
0.00.024.633 I print_info: file type   = Q2_K - Medium
0.00.024.638 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.883 I load: special tokens cache size = 25
0.00.038.857 I load: token to piece cache size = 0.2984 MB
0.00.038.860 I print_info: arch             = gptneox
0.00.038.860 I print_info: vocab_only       = 0
0.00.038.860 I print_info: n_ctx_train      = 2048
0.00.038.860 I print_info: n_embd           = 2048
0.00.038.860 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.864 I print_info: n_head_kv        = 16
0.00.038.864 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.864 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.866 I print_info: n_embd_k_gqa     = 2048
0.00.038.867 I print_info: n_embd_v_gqa     = 2048
0.00.038.867 I print_info: f_norm_eps       = 1.0e-05
0.00.038.868 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.868 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.868 I print_info: f_logit_scale    = 0.0e+00
0.00.038.869 I print_info: n_ff             = 8192
0.00.038.869 I print_info: n_expert         = 0
0.00.038.869 I print_info: n_expert_used    = 0
0.00.038.869 I print_info: causal attn      = 1
0.00.038.869 I print_info: pooling type     = 0
0.00.038.870 I print_info: rope type        = 2
0.00.038.870 I print_info: rope scaling     = linear
0.00.038.870 I print_info: freq_base_train  = 10000.0
0.00.038.871 I print_info: freq_scale_train = 1
0.00.038.871 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.871 I print_info: rope_finetuned   = unknown
0.00.038.871 I print_info: ssm_d_conv       = 0
0.00.038.871 I print_info: ssm_d_inner      = 0
0.00.038.871 I print_info: ssm_d_state      = 0
0.00.038.872 I print_info: ssm_dt_rank      = 0
0.00.038.872 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.872 I print_info: model type       = 1.4B
0.00.038.872 I print_info: model params     = 1.41 B
0.00.038.873 I print_info: general.name     = 1.4B
0.00.038.873 I print_info: vocab type       = BPE
0.00.038.873 I print_info: n_vocab          = 50304
0.00.038.873 I print_info: n_merges         = 50009
0.00.038.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: LF token         = 128 'Ä'
0.00.038.875 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: max token length = 1024
0.00.376.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.376.259 I load_tensors: offloading output layer to GPU
0.00.376.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.376.288 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.376.289 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.377.588 I llama_init_from_model: n_seq_max     = 1
0.00.377.594 I llama_init_from_model: n_ctx         = 2048
0.00.377.595 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.377.595 I llama_init_from_model: n_batch       = 2048
0.00.377.595 I llama_init_from_model: n_ubatch      = 512
0.00.377.596 I llama_init_from_model: flash_attn    = 0
0.00.377.598 I llama_init_from_model: freq_base     = 10000.0
0.00.377.598 I llama_init_from_model: freq_scale    = 1
0.00.377.601 I ggml_metal_init: allocating
0.00.377.667 I ggml_metal_init: found device: Apple M4
0.00.377.680 I ggml_metal_init: picking default device: Apple M4
0.00.379.383 I ggml_metal_init: using embedded metal library
0.00.385.021 I ggml_metal_init: GPU name:   Apple M4
0.00.385.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.385.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.385.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.385.034 I ggml_metal_init: simdgroup reduction   = true
0.00.385.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.385.035 I ggml_metal_init: has residency sets    = true
0.00.385.035 I ggml_metal_init: has bfloat            = true
0.00.385.035 I ggml_metal_init: use bfloat            = true
0.00.385.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.385.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.407.502 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.468.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.468.406 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.468.429 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.946 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.472.949 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.472.950 I llama_init_from_model: graph nodes  = 967
0.00.472.950 I llama_init_from_model: graph splits = 2
0.00.472.957 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.473.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.473.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.125 I main: llama threadpool init, n_threads = 4
0.00.533.173 I 
0.00.533.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.198 I 
0.00.533.375 I sampler seed: 1234
0.00.533.382 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.533.402 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.533.403 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.533.403 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.214.975 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.214.976 I llama_perf_context_print:        load time =     522.97 ms
0.01.214.976 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.46 tokens per second)
0.01.214.977 I llama_perf_context_print:        eval time =     634.24 ms /    63 runs   (   10.07 ms per token,    99.33 tokens per second)
0.01.214.977 I llama_perf_context_print:       total time =     682.74 ms /    70 tokens
0.01.215.216 I ggml_metal_free: deallocating

real	0m1.233s
user	0m0.113s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.511 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.262 I llama_model_loader: - type  f32:  194 tensors
0.00.025.262 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.263 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.263 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.264 I print_info: file format = GGUF V3 (latest)
0.00.025.264 I print_info: file type   = Q3_K - Medium
0.00.025.265 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.453 I load: special tokens cache size = 25
0.00.039.441 I load: token to piece cache size = 0.2984 MB
0.00.039.444 I print_info: arch             = gptneox
0.00.039.444 I print_info: vocab_only       = 0
0.00.039.444 I print_info: n_ctx_train      = 2048
0.00.039.444 I print_info: n_embd           = 2048
0.00.039.445 I print_info: n_layer          = 24
0.00.039.447 I print_info: n_head           = 16
0.00.039.448 I print_info: n_head_kv        = 16
0.00.039.448 I print_info: n_rot            = 32
0.00.039.449 I print_info: n_swa            = 0
0.00.039.449 I print_info: n_embd_head_k    = 128
0.00.039.449 I print_info: n_embd_head_v    = 128
0.00.039.450 I print_info: n_gqa            = 1
0.00.039.450 I print_info: n_embd_k_gqa     = 2048
0.00.039.451 I print_info: n_embd_v_gqa     = 2048
0.00.039.452 I print_info: f_norm_eps       = 1.0e-05
0.00.039.452 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.452 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.453 I print_info: f_logit_scale    = 0.0e+00
0.00.039.453 I print_info: n_ff             = 8192
0.00.039.453 I print_info: n_expert         = 0
0.00.039.454 I print_info: n_expert_used    = 0
0.00.039.454 I print_info: causal attn      = 1
0.00.039.456 I print_info: pooling type     = 0
0.00.039.456 I print_info: rope type        = 2
0.00.039.457 I print_info: rope scaling     = linear
0.00.039.457 I print_info: freq_base_train  = 10000.0
0.00.039.457 I print_info: freq_scale_train = 1
0.00.039.457 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.458 I print_info: rope_finetuned   = unknown
0.00.039.458 I print_info: ssm_d_conv       = 0
0.00.039.458 I print_info: ssm_d_inner      = 0
0.00.039.458 I print_info: ssm_d_state      = 0
0.00.039.458 I print_info: ssm_dt_rank      = 0
0.00.039.458 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.459 I print_info: model type       = 1.4B
0.00.039.459 I print_info: model params     = 1.41 B
0.00.039.459 I print_info: general.name     = 1.4B
0.00.039.460 I print_info: vocab type       = BPE
0.00.039.460 I print_info: n_vocab          = 50304
0.00.039.460 I print_info: n_merges         = 50009
0.00.039.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: LF token         = 128 'Ä'
0.00.039.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.463 I print_info: max token length = 1024
0.00.442.331 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.346 I load_tensors: offloading output layer to GPU
0.00.442.347 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.381 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.383 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.443.849 I llama_init_from_model: n_seq_max     = 1
0.00.443.857 I llama_init_from_model: n_ctx         = 2048
0.00.443.858 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.443.858 I llama_init_from_model: n_batch       = 2048
0.00.443.858 I llama_init_from_model: n_ubatch      = 512
0.00.443.859 I llama_init_from_model: flash_attn    = 0
0.00.443.860 I llama_init_from_model: freq_base     = 10000.0
0.00.443.861 I llama_init_from_model: freq_scale    = 1
0.00.443.864 I ggml_metal_init: allocating
0.00.443.950 I ggml_metal_init: found device: Apple M4
0.00.443.986 I ggml_metal_init: picking default device: Apple M4
0.00.446.212 I ggml_metal_init: using embedded metal library
0.00.452.741 I ggml_metal_init: GPU name:   Apple M4
0.00.452.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.758 I ggml_metal_init: simdgroup reduction   = true
0.00.452.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.758 I ggml_metal_init: has residency sets    = true
0.00.452.759 I ggml_metal_init: has bfloat            = true
0.00.452.759 I ggml_metal_init: use bfloat            = true
0.00.452.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.230 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.533.646 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.533.653 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.533.675 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.538.089 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.538.091 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.538.091 I llama_init_from_model: graph nodes  = 967
0.00.538.091 I llama_init_from_model: graph splits = 2
0.00.538.099 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.538.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.538.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.717 I main: llama threadpool init, n_threads = 4
0.00.596.756 I 
0.00.596.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.781 I 
0.00.596.936 I sampler seed: 1234
0.00.596.940 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.958 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.958 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.636 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.01.341.641 I llama_perf_context_print:        load time =     586.77 ms
0.01.341.642 I llama_perf_context_print: prompt eval time =      49.83 ms /     7 tokens (    7.12 ms per token,   140.48 tokens per second)
0.01.341.642 I llama_perf_context_print:        eval time =     692.37 ms /    63 runs   (   10.99 ms per token,    90.99 tokens per second)
0.01.341.643 I llama_perf_context_print:       total time =     745.80 ms /    70 tokens
0.01.341.891 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.111s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.477 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.020.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.863 I llama_model_loader: - type  f32:  194 tensors
0.00.028.863 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.864 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.864 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.864 I print_info: file format = GGUF V3 (latest)
0.00.028.865 I print_info: file type   = Q4_K - Medium
0.00.028.866 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.855 I load: special tokens cache size = 25
0.00.042.935 I load: token to piece cache size = 0.2984 MB
0.00.042.938 I print_info: arch             = gptneox
0.00.042.938 I print_info: vocab_only       = 0
0.00.042.938 I print_info: n_ctx_train      = 2048
0.00.042.939 I print_info: n_embd           = 2048
0.00.042.939 I print_info: n_layer          = 24
0.00.042.942 I print_info: n_head           = 16
0.00.042.943 I print_info: n_head_kv        = 16
0.00.042.943 I print_info: n_rot            = 32
0.00.042.943 I print_info: n_swa            = 0
0.00.042.943 I print_info: n_embd_head_k    = 128
0.00.042.943 I print_info: n_embd_head_v    = 128
0.00.042.944 I print_info: n_gqa            = 1
0.00.042.945 I print_info: n_embd_k_gqa     = 2048
0.00.042.946 I print_info: n_embd_v_gqa     = 2048
0.00.042.946 I print_info: f_norm_eps       = 1.0e-05
0.00.042.947 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.947 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.947 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.947 I print_info: f_logit_scale    = 0.0e+00
0.00.042.948 I print_info: n_ff             = 8192
0.00.042.948 I print_info: n_expert         = 0
0.00.042.948 I print_info: n_expert_used    = 0
0.00.042.948 I print_info: causal attn      = 1
0.00.042.948 I print_info: pooling type     = 0
0.00.042.951 I print_info: rope type        = 2
0.00.042.951 I print_info: rope scaling     = linear
0.00.042.951 I print_info: freq_base_train  = 10000.0
0.00.042.952 I print_info: freq_scale_train = 1
0.00.042.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.952 I print_info: rope_finetuned   = unknown
0.00.042.952 I print_info: ssm_d_conv       = 0
0.00.042.952 I print_info: ssm_d_inner      = 0
0.00.042.952 I print_info: ssm_d_state      = 0
0.00.042.952 I print_info: ssm_dt_rank      = 0
0.00.042.953 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.953 I print_info: model type       = 1.4B
0.00.042.953 I print_info: model params     = 1.41 B
0.00.042.953 I print_info: general.name     = 1.4B
0.00.042.955 I print_info: vocab type       = BPE
0.00.042.955 I print_info: n_vocab          = 50304
0.00.042.955 I print_info: n_merges         = 50009
0.00.042.955 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.956 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.956 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.956 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.956 I print_info: LF token         = 128 'Ä'
0.00.042.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.957 I print_info: max token length = 1024
0.00.533.962 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.976 I load_tensors: offloading output layer to GPU
0.00.533.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.005 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.006 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.534.981 I llama_init_from_model: n_seq_max     = 1
0.00.534.990 I llama_init_from_model: n_ctx         = 2048
0.00.534.990 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.534.991 I llama_init_from_model: n_batch       = 2048
0.00.534.991 I llama_init_from_model: n_ubatch      = 512
0.00.534.991 I llama_init_from_model: flash_attn    = 0
0.00.534.993 I llama_init_from_model: freq_base     = 10000.0
0.00.534.998 I llama_init_from_model: freq_scale    = 1
0.00.535.000 I ggml_metal_init: allocating
0.00.535.079 I ggml_metal_init: found device: Apple M4
0.00.535.093 I ggml_metal_init: picking default device: Apple M4
0.00.536.535 I ggml_metal_init: using embedded metal library
0.00.541.213 I ggml_metal_init: GPU name:   Apple M4
0.00.541.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.541.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.541.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.541.225 I ggml_metal_init: simdgroup reduction   = true
0.00.541.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.541.226 I ggml_metal_init: has residency sets    = true
0.00.541.226 I ggml_metal_init: has bfloat            = true
0.00.541.226 I ggml_metal_init: use bfloat            = true
0.00.541.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.541.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.555.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.586.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.586.379 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.586.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.848 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.590.850 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.590.850 I llama_init_from_model: graph nodes  = 967
0.00.590.850 I llama_init_from_model: graph splits = 2
0.00.590.856 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.590.987 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.590.988 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.825 I main: llama threadpool init, n_threads = 4
0.00.636.857 I 
0.00.636.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.878 I 
0.00.636.999 I sampler seed: 1234
0.00.637.004 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.014 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.014 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.435.466 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.435.466 I llama_perf_context_print:        load time =     623.45 ms
0.01.435.467 I llama_perf_context_print: prompt eval time =      58.00 ms /     7 tokens (    8.29 ms per token,   120.69 tokens per second)
0.01.435.468 I llama_perf_context_print:        eval time =     737.56 ms /    63 runs   (   11.71 ms per token,    85.42 tokens per second)
0.01.435.469 I llama_perf_context_print:       total time =     799.54 ms /    70 tokens
0.01.435.754 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.105s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.304 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.013 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.020 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.022 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.023 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.023 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.024 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.024 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.025 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.898 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.709 I llama_model_loader: - type  f32:  194 tensors
0.00.026.710 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.710 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.710 I print_info: file format = GGUF V3 (latest)
0.00.026.711 I print_info: file type   = Q5_K - Medium
0.00.026.713 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.618 I load: special tokens cache size = 25
0.00.040.398 I load: token to piece cache size = 0.2984 MB
0.00.040.401 I print_info: arch             = gptneox
0.00.040.401 I print_info: vocab_only       = 0
0.00.040.401 I print_info: n_ctx_train      = 2048
0.00.040.401 I print_info: n_embd           = 2048
0.00.040.402 I print_info: n_layer          = 24
0.00.040.404 I print_info: n_head           = 16
0.00.040.405 I print_info: n_head_kv        = 16
0.00.040.407 I print_info: n_rot            = 32
0.00.040.407 I print_info: n_swa            = 0
0.00.040.407 I print_info: n_embd_head_k    = 128
0.00.040.407 I print_info: n_embd_head_v    = 128
0.00.040.408 I print_info: n_gqa            = 1
0.00.040.409 I print_info: n_embd_k_gqa     = 2048
0.00.040.409 I print_info: n_embd_v_gqa     = 2048
0.00.040.410 I print_info: f_norm_eps       = 1.0e-05
0.00.040.410 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.411 I print_info: f_logit_scale    = 0.0e+00
0.00.040.421 I print_info: n_ff             = 8192
0.00.040.421 I print_info: n_expert         = 0
0.00.040.421 I print_info: n_expert_used    = 0
0.00.040.422 I print_info: causal attn      = 1
0.00.040.423 I print_info: pooling type     = 0
0.00.040.424 I print_info: rope type        = 2
0.00.040.424 I print_info: rope scaling     = linear
0.00.040.424 I print_info: freq_base_train  = 10000.0
0.00.040.425 I print_info: freq_scale_train = 1
0.00.040.425 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.425 I print_info: rope_finetuned   = unknown
0.00.040.425 I print_info: ssm_d_conv       = 0
0.00.040.425 I print_info: ssm_d_inner      = 0
0.00.040.425 I print_info: ssm_d_state      = 0
0.00.040.426 I print_info: ssm_dt_rank      = 0
0.00.040.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.427 I print_info: model type       = 1.4B
0.00.040.427 I print_info: model params     = 1.41 B
0.00.040.427 I print_info: general.name     = 1.4B
0.00.040.428 I print_info: vocab type       = BPE
0.00.040.428 I print_info: n_vocab          = 50304
0.00.040.428 I print_info: n_merges         = 50009
0.00.040.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.430 I print_info: LF token         = 128 'Ä'
0.00.040.432 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.432 I print_info: max token length = 1024
0.00.605.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.140 I load_tensors: offloading output layer to GPU
0.00.605.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.175 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.605.181 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.606.365 I llama_init_from_model: n_seq_max     = 1
0.00.606.370 I llama_init_from_model: n_ctx         = 2048
0.00.606.371 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.371 I llama_init_from_model: n_batch       = 2048
0.00.606.372 I llama_init_from_model: n_ubatch      = 512
0.00.606.372 I llama_init_from_model: flash_attn    = 0
0.00.606.374 I llama_init_from_model: freq_base     = 10000.0
0.00.606.374 I llama_init_from_model: freq_scale    = 1
0.00.606.377 I ggml_metal_init: allocating
0.00.606.457 I ggml_metal_init: found device: Apple M4
0.00.606.471 I ggml_metal_init: picking default device: Apple M4
0.00.608.322 I ggml_metal_init: using embedded metal library
0.00.615.572 I ggml_metal_init: GPU name:   Apple M4
0.00.615.578 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.580 I ggml_metal_init: simdgroup reduction   = true
0.00.615.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.581 I ggml_metal_init: has residency sets    = true
0.00.615.581 I ggml_metal_init: has bfloat            = true
0.00.615.582 I ggml_metal_init: use bfloat            = true
0.00.615.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.615 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.895 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.933 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.668 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.670 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.670 I llama_init_from_model: graph nodes  = 967
0.00.698.671 I llama_init_from_model: graph splits = 2
0.00.698.676 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.792 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.888 I main: llama threadpool init, n_threads = 4
0.00.757.920 I 
0.00.757.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.942 I 
0.00.758.074 I sampler seed: 1234
0.00.758.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.120 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.123 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.123 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.638.000 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.638.003 I llama_perf_context_print:        load time =     746.63 ms
0.01.638.004 I llama_perf_context_print: prompt eval time =      51.82 ms /     7 tokens (    7.40 ms per token,   135.08 tokens per second)
0.01.638.004 I llama_perf_context_print:        eval time =     825.18 ms /    63 runs   (   13.10 ms per token,    76.35 tokens per second)
0.01.638.005 I llama_perf_context_print:       total time =     881.06 ms /    70 tokens
0.01.638.279 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.110s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.460 I llama_model_loader: - type  f32:  194 tensors
0.00.027.460 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.461 I print_info: file format = GGUF V3 (latest)
0.00.027.461 I print_info: file type   = Q6_K
0.00.027.462 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.311 I load: special tokens cache size = 25
0.00.041.256 I load: token to piece cache size = 0.2984 MB
0.00.041.259 I print_info: arch             = gptneox
0.00.041.259 I print_info: vocab_only       = 0
0.00.041.259 I print_info: n_ctx_train      = 2048
0.00.041.260 I print_info: n_embd           = 2048
0.00.041.260 I print_info: n_layer          = 24
0.00.041.263 I print_info: n_head           = 16
0.00.041.263 I print_info: n_head_kv        = 16
0.00.041.264 I print_info: n_rot            = 32
0.00.041.264 I print_info: n_swa            = 0
0.00.041.264 I print_info: n_embd_head_k    = 128
0.00.041.264 I print_info: n_embd_head_v    = 128
0.00.041.265 I print_info: n_gqa            = 1
0.00.041.266 I print_info: n_embd_k_gqa     = 2048
0.00.041.266 I print_info: n_embd_v_gqa     = 2048
0.00.041.267 I print_info: f_norm_eps       = 1.0e-05
0.00.041.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.268 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.268 I print_info: f_logit_scale    = 0.0e+00
0.00.041.268 I print_info: n_ff             = 8192
0.00.041.269 I print_info: n_expert         = 0
0.00.041.269 I print_info: n_expert_used    = 0
0.00.041.269 I print_info: causal attn      = 1
0.00.041.269 I print_info: pooling type     = 0
0.00.041.269 I print_info: rope type        = 2
0.00.041.272 I print_info: rope scaling     = linear
0.00.041.272 I print_info: freq_base_train  = 10000.0
0.00.041.273 I print_info: freq_scale_train = 1
0.00.041.273 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.273 I print_info: rope_finetuned   = unknown
0.00.041.273 I print_info: ssm_d_conv       = 0
0.00.041.273 I print_info: ssm_d_inner      = 0
0.00.041.273 I print_info: ssm_d_state      = 0
0.00.041.273 I print_info: ssm_dt_rank      = 0
0.00.041.274 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.274 I print_info: model type       = 1.4B
0.00.041.274 I print_info: model params     = 1.41 B
0.00.041.274 I print_info: general.name     = 1.4B
0.00.041.275 I print_info: vocab type       = BPE
0.00.041.275 I print_info: n_vocab          = 50304
0.00.041.275 I print_info: n_merges         = 50009
0.00.041.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: LF token         = 128 'Ä'
0.00.041.277 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.280 I print_info: max token length = 1024
0.00.652.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.179 I load_tensors: offloading output layer to GPU
0.00.652.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.216 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.652.218 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.653.402 I llama_init_from_model: n_seq_max     = 1
0.00.653.409 I llama_init_from_model: n_ctx         = 2048
0.00.653.409 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.410 I llama_init_from_model: n_batch       = 2048
0.00.653.410 I llama_init_from_model: n_ubatch      = 512
0.00.653.410 I llama_init_from_model: flash_attn    = 0
0.00.653.412 I llama_init_from_model: freq_base     = 10000.0
0.00.653.413 I llama_init_from_model: freq_scale    = 1
0.00.653.415 I ggml_metal_init: allocating
0.00.653.494 I ggml_metal_init: found device: Apple M4
0.00.653.510 I ggml_metal_init: picking default device: Apple M4
0.00.655.178 I ggml_metal_init: using embedded metal library
0.00.661.634 I ggml_metal_init: GPU name:   Apple M4
0.00.661.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.640 I ggml_metal_init: simdgroup reduction   = true
0.00.661.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.640 I ggml_metal_init: has residency sets    = true
0.00.661.641 I ggml_metal_init: has bfloat            = true
0.00.661.641 I ggml_metal_init: use bfloat            = true
0.00.661.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.353 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.359 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.944 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.739.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.739.947 I llama_init_from_model: graph nodes  = 967
0.00.739.947 I llama_init_from_model: graph splits = 2
0.00.739.953 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.375 I main: llama threadpool init, n_threads = 4
0.00.795.417 I 
0.00.795.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.441 I 
0.00.795.544 I sampler seed: 1234
0.00.795.549 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.558 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.558 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.558 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.717.339 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.717.340 I llama_perf_context_print:        load time =     784.03 ms
0.01.717.341 I llama_perf_context_print: prompt eval time =      54.25 ms /     7 tokens (    7.75 ms per token,   129.04 tokens per second)
0.01.717.341 I llama_perf_context_print:        eval time =     864.58 ms /    63 runs   (   13.72 ms per token,    72.87 tokens per second)
0.01.717.342 I llama_perf_context_print:       total time =     922.84 ms /    70 tokens
0.01.717.604 I ggml_metal_free: deallocating

real	0m1.735s
user	0m0.110s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.666 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.133 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.939 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.517 I llama_model_loader: - type  f32:  194 tensors
0.00.057.518 I llama_model_loader: - type  f16:   98 tensors
0.00.057.518 I print_info: file format = GGUF V3 (latest)
0.00.057.519 I print_info: file type   = all F32 (guessed)
0.00.057.521 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.286 I load: special tokens cache size = 25
0.00.078.020 I load: token to piece cache size = 0.2984 MB
0.00.078.023 I print_info: arch             = gptneox
0.00.078.023 I print_info: vocab_only       = 0
0.00.078.023 I print_info: n_ctx_train      = 2048
0.00.078.023 I print_info: n_embd           = 2048
0.00.078.024 I print_info: n_layer          = 24
0.00.078.027 I print_info: n_head           = 16
0.00.078.028 I print_info: n_head_kv        = 16
0.00.078.028 I print_info: n_rot            = 32
0.00.078.029 I print_info: n_swa            = 0
0.00.078.029 I print_info: n_embd_head_k    = 128
0.00.078.029 I print_info: n_embd_head_v    = 128
0.00.078.030 I print_info: n_gqa            = 1
0.00.078.030 I print_info: n_embd_k_gqa     = 2048
0.00.078.031 I print_info: n_embd_v_gqa     = 2048
0.00.078.032 I print_info: f_norm_eps       = 1.0e-05
0.00.078.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.035 I print_info: f_logit_scale    = 0.0e+00
0.00.078.035 I print_info: n_ff             = 8192
0.00.078.036 I print_info: n_expert         = 0
0.00.078.036 I print_info: n_expert_used    = 0
0.00.078.036 I print_info: causal attn      = 1
0.00.078.036 I print_info: pooling type     = 0
0.00.078.036 I print_info: rope type        = 2
0.00.078.037 I print_info: rope scaling     = linear
0.00.078.037 I print_info: freq_base_train  = 10000.0
0.00.078.037 I print_info: freq_scale_train = 1
0.00.078.037 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.038 I print_info: rope_finetuned   = unknown
0.00.078.038 I print_info: ssm_d_conv       = 0
0.00.078.038 I print_info: ssm_d_inner      = 0
0.00.078.038 I print_info: ssm_d_state      = 0
0.00.078.038 I print_info: ssm_dt_rank      = 0
0.00.078.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.039 I print_info: model type       = 1.4B
0.00.078.039 I print_info: model params     = 1.41 B
0.00.078.039 I print_info: general.name     = 1.4B
0.00.078.040 I print_info: vocab type       = BPE
0.00.078.040 I print_info: n_vocab          = 50304
0.00.078.040 I print_info: n_merges         = 50009
0.00.078.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.041 I print_info: LF token         = 128 'Ä'
0.00.078.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.042 I print_info: max token length = 1024
0.00.918.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.918.209 I load_tensors: offloading output layer to GPU
0.00.918.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.918.240 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.918.242 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.919.124 I llama_init_from_model: n_seq_max     = 1
0.00.919.126 I llama_init_from_model: n_ctx         = 128
0.00.919.126 I llama_init_from_model: n_ctx_per_seq = 128
0.00.919.126 I llama_init_from_model: n_batch       = 128
0.00.919.126 I llama_init_from_model: n_ubatch      = 128
0.00.919.128 I llama_init_from_model: flash_attn    = 0
0.00.919.129 I llama_init_from_model: freq_base     = 10000.0
0.00.919.129 I llama_init_from_model: freq_scale    = 1
0.00.919.129 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.919.132 I ggml_metal_init: allocating
0.00.919.211 I ggml_metal_init: found device: Apple M4
0.00.919.221 I ggml_metal_init: picking default device: Apple M4
0.00.920.418 I ggml_metal_init: using embedded metal library
0.00.924.600 I ggml_metal_init: GPU name:   Apple M4
0.00.924.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.924.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.924.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.924.605 I ggml_metal_init: simdgroup reduction   = true
0.00.924.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.924.606 I ggml_metal_init: has residency sets    = true
0.00.924.606 I ggml_metal_init: has bfloat            = true
0.00.924.606 I ggml_metal_init: use bfloat            = true
0.00.924.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.924.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.935.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.937.766 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.937.768 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.937.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.939.398 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.939.399 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.939.399 I llama_init_from_model: graph nodes  = 967
0.00.939.400 I llama_init_from_model: graph splits = 2
0.00.939.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.939.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.975.279 I 
0.00.975.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.975.348 I perplexity: tokenizing the input ..
0.00.980.718 I perplexity: tokenization took 5.368 ms
0.00.980.739 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.099.302 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.100.637 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.100.659 I llama_perf_context_print:        load time =     949.13 ms
0.01.100.660 I llama_perf_context_print: prompt eval time =     118.21 ms /   128 tokens (    0.92 ms per token,  1082.84 tokens per second)
0.01.100.661 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.100.661 I llama_perf_context_print:       total time =     125.38 ms /   129 tokens
0.01.101.085 I ggml_metal_free: deallocating

real	0m1.288s
user	0m0.100s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.589 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.921 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.712 I llama_model_loader: - type  f32:  194 tensors
0.00.025.712 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.713 I print_info: file format = GGUF V3 (latest)
0.00.025.713 I print_info: file type   = Q8_0
0.00.025.714 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.846 I load: special tokens cache size = 25
0.00.039.634 I load: token to piece cache size = 0.2984 MB
0.00.039.638 I print_info: arch             = gptneox
0.00.039.639 I print_info: vocab_only       = 0
0.00.039.639 I print_info: n_ctx_train      = 2048
0.00.039.639 I print_info: n_embd           = 2048
0.00.039.639 I print_info: n_layer          = 24
0.00.039.644 I print_info: n_head           = 16
0.00.039.644 I print_info: n_head_kv        = 16
0.00.039.645 I print_info: n_rot            = 32
0.00.039.648 I print_info: n_swa            = 0
0.00.039.649 I print_info: n_embd_head_k    = 128
0.00.039.649 I print_info: n_embd_head_v    = 128
0.00.039.649 I print_info: n_gqa            = 1
0.00.039.650 I print_info: n_embd_k_gqa     = 2048
0.00.039.650 I print_info: n_embd_v_gqa     = 2048
0.00.039.651 I print_info: f_norm_eps       = 1.0e-05
0.00.039.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.651 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.652 I print_info: f_logit_scale    = 0.0e+00
0.00.039.652 I print_info: n_ff             = 8192
0.00.039.653 I print_info: n_expert         = 0
0.00.039.653 I print_info: n_expert_used    = 0
0.00.039.654 I print_info: causal attn      = 1
0.00.039.654 I print_info: pooling type     = 0
0.00.039.654 I print_info: rope type        = 2
0.00.039.654 I print_info: rope scaling     = linear
0.00.039.655 I print_info: freq_base_train  = 10000.0
0.00.039.655 I print_info: freq_scale_train = 1
0.00.039.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.655 I print_info: rope_finetuned   = unknown
0.00.039.655 I print_info: ssm_d_conv       = 0
0.00.039.655 I print_info: ssm_d_inner      = 0
0.00.039.656 I print_info: ssm_d_state      = 0
0.00.039.656 I print_info: ssm_dt_rank      = 0
0.00.039.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.656 I print_info: model type       = 1.4B
0.00.039.657 I print_info: model params     = 1.41 B
0.00.039.657 I print_info: general.name     = 1.4B
0.00.039.657 I print_info: vocab type       = BPE
0.00.039.657 I print_info: n_vocab          = 50304
0.00.039.657 I print_info: n_merges         = 50009
0.00.039.659 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: LF token         = 128 'Ä'
0.00.039.660 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.660 I print_info: max token length = 1024
0.00.874.352 I load_tensors: offloading 24 repeating layers to GPU
0.00.874.358 I load_tensors: offloading output layer to GPU
0.00.874.359 I load_tensors: offloaded 25/25 layers to GPU
0.00.874.388 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.874.391 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.875.699 I llama_init_from_model: n_seq_max     = 1
0.00.875.700 I llama_init_from_model: n_ctx         = 128
0.00.875.701 I llama_init_from_model: n_ctx_per_seq = 128
0.00.875.701 I llama_init_from_model: n_batch       = 128
0.00.875.702 I llama_init_from_model: n_ubatch      = 128
0.00.875.702 I llama_init_from_model: flash_attn    = 0
0.00.875.703 I llama_init_from_model: freq_base     = 10000.0
0.00.875.704 I llama_init_from_model: freq_scale    = 1
0.00.875.704 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.875.706 I ggml_metal_init: allocating
0.00.875.746 I ggml_metal_init: found device: Apple M4
0.00.875.757 I ggml_metal_init: picking default device: Apple M4
0.00.877.000 I ggml_metal_init: using embedded metal library
0.00.882.336 I ggml_metal_init: GPU name:   Apple M4
0.00.882.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.882.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.882.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.882.340 I ggml_metal_init: simdgroup reduction   = true
0.00.882.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.882.341 I ggml_metal_init: has residency sets    = true
0.00.882.341 I ggml_metal_init: has bfloat            = true
0.00.882.341 I ggml_metal_init: use bfloat            = true
0.00.882.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.882.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.898.199 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.901.525 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.901.528 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.901.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.904.651 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.904.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.904.653 I llama_init_from_model: graph nodes  = 967
0.00.904.653 I llama_init_from_model: graph splits = 2
0.00.904.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.904.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.220 I 
0.00.933.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.350 I perplexity: tokenizing the input ..
0.00.940.811 I perplexity: tokenization took 7.458 ms
0.00.940.838 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.079.583 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.080.921 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.080.936 I llama_perf_context_print:        load time =     923.62 ms
0.01.080.938 I llama_perf_context_print: prompt eval time =     137.84 ms /   128 tokens (    1.08 ms per token,   928.58 tokens per second)
0.01.080.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.080.939 I llama_perf_context_print:       total time =     147.72 ms /   129 tokens
0.01.081.296 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.077s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.283 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.338 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.339 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.339 I llama_model_loader: - type  f32:  194 tensors
0.00.026.340 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.340 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.340 I print_info: file format = GGUF V3 (latest)
0.00.026.341 I print_info: file type   = Q4_0
0.00.026.342 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.597 I load: special tokens cache size = 25
0.00.040.577 I load: token to piece cache size = 0.2984 MB
0.00.040.579 I print_info: arch             = gptneox
0.00.040.580 I print_info: vocab_only       = 0
0.00.040.580 I print_info: n_ctx_train      = 2048
0.00.040.580 I print_info: n_embd           = 2048
0.00.040.580 I print_info: n_layer          = 24
0.00.040.583 I print_info: n_head           = 16
0.00.040.584 I print_info: n_head_kv        = 16
0.00.040.584 I print_info: n_rot            = 32
0.00.040.584 I print_info: n_swa            = 0
0.00.040.585 I print_info: n_embd_head_k    = 128
0.00.040.585 I print_info: n_embd_head_v    = 128
0.00.040.585 I print_info: n_gqa            = 1
0.00.040.586 I print_info: n_embd_k_gqa     = 2048
0.00.040.587 I print_info: n_embd_v_gqa     = 2048
0.00.040.588 I print_info: f_norm_eps       = 1.0e-05
0.00.040.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.589 I print_info: f_logit_scale    = 0.0e+00
0.00.040.589 I print_info: n_ff             = 8192
0.00.040.590 I print_info: n_expert         = 0
0.00.040.590 I print_info: n_expert_used    = 0
0.00.040.590 I print_info: causal attn      = 1
0.00.040.590 I print_info: pooling type     = 0
0.00.040.590 I print_info: rope type        = 2
0.00.040.590 I print_info: rope scaling     = linear
0.00.040.591 I print_info: freq_base_train  = 10000.0
0.00.040.591 I print_info: freq_scale_train = 1
0.00.040.591 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.591 I print_info: rope_finetuned   = unknown
0.00.040.594 I print_info: ssm_d_conv       = 0
0.00.040.594 I print_info: ssm_d_inner      = 0
0.00.040.594 I print_info: ssm_d_state      = 0
0.00.040.594 I print_info: ssm_dt_rank      = 0
0.00.040.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.594 I print_info: model type       = 1.4B
0.00.040.595 I print_info: model params     = 1.41 B
0.00.040.595 I print_info: general.name     = 1.4B
0.00.040.595 I print_info: vocab type       = BPE
0.00.040.596 I print_info: n_vocab          = 50304
0.00.040.596 I print_info: n_merges         = 50009
0.00.040.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.601 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.601 I print_info: LF token         = 128 'Ä'
0.00.040.601 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.602 I print_info: max token length = 1024
0.00.586.921 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.937 I load_tensors: offloading output layer to GPU
0.00.586.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.969 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.586.970 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.588.400 I llama_init_from_model: n_seq_max     = 1
0.00.588.405 I llama_init_from_model: n_ctx         = 128
0.00.588.405 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.406 I llama_init_from_model: n_batch       = 128
0.00.588.407 I llama_init_from_model: n_ubatch      = 128
0.00.588.407 I llama_init_from_model: flash_attn    = 0
0.00.588.409 I llama_init_from_model: freq_base     = 10000.0
0.00.588.410 I llama_init_from_model: freq_scale    = 1
0.00.588.410 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.413 I ggml_metal_init: allocating
0.00.588.507 I ggml_metal_init: found device: Apple M4
0.00.588.524 I ggml_metal_init: picking default device: Apple M4
0.00.590.276 I ggml_metal_init: using embedded metal library
0.00.595.826 I ggml_metal_init: GPU name:   Apple M4
0.00.595.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.834 I ggml_metal_init: simdgroup reduction   = true
0.00.595.834 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.835 I ggml_metal_init: has residency sets    = true
0.00.595.835 I ggml_metal_init: has bfloat            = true
0.00.595.835 I ggml_metal_init: use bfloat            = true
0.00.595.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.370 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.996 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.000 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.027 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.354 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.356 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.357 I llama_init_from_model: graph nodes  = 967
0.00.622.357 I llama_init_from_model: graph splits = 2
0.00.622.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.027 I 
0.00.650.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.122 I perplexity: tokenizing the input ..
0.00.657.434 I perplexity: tokenization took 7.307 ms
0.00.657.456 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.116 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.794.442 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.794.454 I llama_perf_context_print:        load time =     639.74 ms
0.00.794.455 I llama_perf_context_print: prompt eval time =     134.71 ms /   128 tokens (    1.05 ms per token,   950.22 tokens per second)
0.00.794.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.456 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.794.847 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.281 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.868 I llama_model_loader: - type  f32:  194 tensors
0.00.024.869 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.870 I print_info: file format = GGUF V3 (latest)
0.00.024.870 I print_info: file type   = Q4_1
0.00.024.871 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.069 I load: special tokens cache size = 25
0.00.038.971 I load: token to piece cache size = 0.2984 MB
0.00.038.975 I print_info: arch             = gptneox
0.00.038.975 I print_info: vocab_only       = 0
0.00.038.975 I print_info: n_ctx_train      = 2048
0.00.038.975 I print_info: n_embd           = 2048
0.00.038.976 I print_info: n_layer          = 24
0.00.038.979 I print_info: n_head           = 16
0.00.038.981 I print_info: n_head_kv        = 16
0.00.038.982 I print_info: n_rot            = 32
0.00.038.982 I print_info: n_swa            = 0
0.00.038.982 I print_info: n_embd_head_k    = 128
0.00.038.982 I print_info: n_embd_head_v    = 128
0.00.038.983 I print_info: n_gqa            = 1
0.00.038.984 I print_info: n_embd_k_gqa     = 2048
0.00.038.984 I print_info: n_embd_v_gqa     = 2048
0.00.038.985 I print_info: f_norm_eps       = 1.0e-05
0.00.038.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.986 I print_info: f_logit_scale    = 0.0e+00
0.00.038.987 I print_info: n_ff             = 8192
0.00.038.987 I print_info: n_expert         = 0
0.00.038.987 I print_info: n_expert_used    = 0
0.00.038.987 I print_info: causal attn      = 1
0.00.038.987 I print_info: pooling type     = 0
0.00.038.988 I print_info: rope type        = 2
0.00.038.988 I print_info: rope scaling     = linear
0.00.038.988 I print_info: freq_base_train  = 10000.0
0.00.038.989 I print_info: freq_scale_train = 1
0.00.038.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.991 I print_info: rope_finetuned   = unknown
0.00.038.991 I print_info: ssm_d_conv       = 0
0.00.038.991 I print_info: ssm_d_inner      = 0
0.00.038.991 I print_info: ssm_d_state      = 0
0.00.038.991 I print_info: ssm_dt_rank      = 0
0.00.038.991 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.991 I print_info: model type       = 1.4B
0.00.038.992 I print_info: model params     = 1.41 B
0.00.038.992 I print_info: general.name     = 1.4B
0.00.038.993 I print_info: vocab type       = BPE
0.00.038.993 I print_info: n_vocab          = 50304
0.00.038.993 I print_info: n_merges         = 50009
0.00.038.993 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.993 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.994 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.994 I print_info: LF token         = 128 'Ä'
0.00.038.996 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: max token length = 1024
0.00.631.349 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.360 I load_tensors: offloading output layer to GPU
0.00.631.361 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.394 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.631.395 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.632.815 I llama_init_from_model: n_seq_max     = 1
0.00.632.821 I llama_init_from_model: n_ctx         = 128
0.00.632.821 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.822 I llama_init_from_model: n_batch       = 128
0.00.632.823 I llama_init_from_model: n_ubatch      = 128
0.00.632.823 I llama_init_from_model: flash_attn    = 0
0.00.632.826 I llama_init_from_model: freq_base     = 10000.0
0.00.632.826 I llama_init_from_model: freq_scale    = 1
0.00.632.827 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.830 I ggml_metal_init: allocating
0.00.632.873 I ggml_metal_init: found device: Apple M4
0.00.632.886 I ggml_metal_init: picking default device: Apple M4
0.00.634.505 I ggml_metal_init: using embedded metal library
0.00.640.128 I ggml_metal_init: GPU name:   Apple M4
0.00.640.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.135 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.136 I ggml_metal_init: simdgroup reduction   = true
0.00.640.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.137 I ggml_metal_init: has residency sets    = true
0.00.640.137 I ggml_metal_init: has bfloat            = true
0.00.640.137 I ggml_metal_init: use bfloat            = true
0.00.640.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.653 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.130 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.134 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.425 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.427 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.427 I llama_init_from_model: graph nodes  = 967
0.00.666.428 I llama_init_from_model: graph splits = 2
0.00.666.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.681 I 
0.00.693.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.794 I perplexity: tokenizing the input ..
0.00.700.740 I perplexity: tokenization took 6.945 ms
0.00.700.751 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.134 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.837.578 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.837.591 I llama_perf_context_print:        load time =     684.80 ms
0.00.837.592 I llama_perf_context_print: prompt eval time =     135.13 ms /   128 tokens (    1.06 ms per token,   947.26 tokens per second)
0.00.837.592 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.593 I llama_perf_context_print:       total time =     143.91 ms /   129 tokens
0.00.837.966 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.078s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.087 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.381 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.245 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.247 I llama_model_loader: - type  f32:  194 tensors
0.00.026.247 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.247 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.248 I print_info: file format = GGUF V3 (latest)
0.00.026.249 I print_info: file type   = Q5_0
0.00.026.255 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.476 I load: special tokens cache size = 25
0.00.040.507 I load: token to piece cache size = 0.2984 MB
0.00.040.510 I print_info: arch             = gptneox
0.00.040.510 I print_info: vocab_only       = 0
0.00.040.510 I print_info: n_ctx_train      = 2048
0.00.040.510 I print_info: n_embd           = 2048
0.00.040.510 I print_info: n_layer          = 24
0.00.040.513 I print_info: n_head           = 16
0.00.040.514 I print_info: n_head_kv        = 16
0.00.040.516 I print_info: n_rot            = 32
0.00.040.516 I print_info: n_swa            = 0
0.00.040.516 I print_info: n_embd_head_k    = 128
0.00.040.517 I print_info: n_embd_head_v    = 128
0.00.040.517 I print_info: n_gqa            = 1
0.00.040.518 I print_info: n_embd_k_gqa     = 2048
0.00.040.519 I print_info: n_embd_v_gqa     = 2048
0.00.040.519 I print_info: f_norm_eps       = 1.0e-05
0.00.040.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.520 I print_info: f_logit_scale    = 0.0e+00
0.00.040.521 I print_info: n_ff             = 8192
0.00.040.521 I print_info: n_expert         = 0
0.00.040.522 I print_info: n_expert_used    = 0
0.00.040.522 I print_info: causal attn      = 1
0.00.040.523 I print_info: pooling type     = 0
0.00.040.524 I print_info: rope type        = 2
0.00.040.524 I print_info: rope scaling     = linear
0.00.040.524 I print_info: freq_base_train  = 10000.0
0.00.040.525 I print_info: freq_scale_train = 1
0.00.040.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.525 I print_info: rope_finetuned   = unknown
0.00.040.525 I print_info: ssm_d_conv       = 0
0.00.040.525 I print_info: ssm_d_inner      = 0
0.00.040.525 I print_info: ssm_d_state      = 0
0.00.040.526 I print_info: ssm_dt_rank      = 0
0.00.040.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.526 I print_info: model type       = 1.4B
0.00.040.526 I print_info: model params     = 1.41 B
0.00.040.527 I print_info: general.name     = 1.4B
0.00.040.528 I print_info: vocab type       = BPE
0.00.040.528 I print_info: n_vocab          = 50304
0.00.040.528 I print_info: n_merges         = 50009
0.00.040.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.529 I print_info: LF token         = 128 'Ä'
0.00.040.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.530 I print_info: max token length = 1024
0.00.633.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.601 I load_tensors: offloading output layer to GPU
0.00.633.601 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.630 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.633.632 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.635.235 I llama_init_from_model: n_seq_max     = 1
0.00.635.239 I llama_init_from_model: n_ctx         = 128
0.00.635.240 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.244 I llama_init_from_model: n_batch       = 128
0.00.635.244 I llama_init_from_model: n_ubatch      = 128
0.00.635.245 I llama_init_from_model: flash_attn    = 0
0.00.635.247 I llama_init_from_model: freq_base     = 10000.0
0.00.635.248 I llama_init_from_model: freq_scale    = 1
0.00.635.249 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.250 I ggml_metal_init: allocating
0.00.635.346 I ggml_metal_init: found device: Apple M4
0.00.635.359 I ggml_metal_init: picking default device: Apple M4
0.00.637.145 I ggml_metal_init: using embedded metal library
0.00.643.814 I ggml_metal_init: GPU name:   Apple M4
0.00.643.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.824 I ggml_metal_init: simdgroup reduction   = true
0.00.643.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.825 I ggml_metal_init: has residency sets    = true
0.00.643.825 I ggml_metal_init: has bfloat            = true
0.00.643.832 I ggml_metal_init: use bfloat            = true
0.00.643.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.835 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.543 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.967 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.974 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.018 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.150 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.152 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.152 I llama_init_from_model: graph nodes  = 967
0.00.668.153 I llama_init_from_model: graph splits = 2
0.00.668.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.940 I 
0.00.695.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.045 I perplexity: tokenizing the input ..
0.00.701.426 I perplexity: tokenization took 6.379 ms
0.00.701.440 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.135 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.837.473 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.837.488 I llama_perf_context_print:        load time =     684.84 ms
0.00.837.489 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.35 tokens per second)
0.00.837.490 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.490 I llama_perf_context_print:       total time =     142.56 ms /   129 tokens
0.00.837.882 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.159 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.906 I llama_model_loader: - type  f32:  194 tensors
0.00.024.906 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.906 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.907 I print_info: file format = GGUF V3 (latest)
0.00.024.907 I print_info: file type   = Q5_1
0.00.024.908 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.060 I load: special tokens cache size = 25
0.00.039.031 I load: token to piece cache size = 0.2984 MB
0.00.039.034 I print_info: arch             = gptneox
0.00.039.034 I print_info: vocab_only       = 0
0.00.039.034 I print_info: n_ctx_train      = 2048
0.00.039.035 I print_info: n_embd           = 2048
0.00.039.035 I print_info: n_layer          = 24
0.00.039.037 I print_info: n_head           = 16
0.00.039.038 I print_info: n_head_kv        = 16
0.00.039.038 I print_info: n_rot            = 32
0.00.039.039 I print_info: n_swa            = 0
0.00.039.039 I print_info: n_embd_head_k    = 128
0.00.039.039 I print_info: n_embd_head_v    = 128
0.00.039.040 I print_info: n_gqa            = 1
0.00.039.040 I print_info: n_embd_k_gqa     = 2048
0.00.039.041 I print_info: n_embd_v_gqa     = 2048
0.00.039.042 I print_info: f_norm_eps       = 1.0e-05
0.00.039.042 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.042 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.042 I print_info: f_logit_scale    = 0.0e+00
0.00.039.043 I print_info: n_ff             = 8192
0.00.039.043 I print_info: n_expert         = 0
0.00.039.044 I print_info: n_expert_used    = 0
0.00.039.044 I print_info: causal attn      = 1
0.00.039.044 I print_info: pooling type     = 0
0.00.039.044 I print_info: rope type        = 2
0.00.039.044 I print_info: rope scaling     = linear
0.00.039.045 I print_info: freq_base_train  = 10000.0
0.00.039.045 I print_info: freq_scale_train = 1
0.00.039.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.045 I print_info: rope_finetuned   = unknown
0.00.039.047 I print_info: ssm_d_conv       = 0
0.00.039.047 I print_info: ssm_d_inner      = 0
0.00.039.047 I print_info: ssm_d_state      = 0
0.00.039.047 I print_info: ssm_dt_rank      = 0
0.00.039.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.053 I print_info: model type       = 1.4B
0.00.039.056 I print_info: model params     = 1.41 B
0.00.039.056 I print_info: general.name     = 1.4B
0.00.039.057 I print_info: vocab type       = BPE
0.00.039.057 I print_info: n_vocab          = 50304
0.00.039.058 I print_info: n_merges         = 50009
0.00.039.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: LF token         = 128 'Ä'
0.00.039.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: max token length = 1024
0.00.690.987 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.999 I load_tensors: offloading output layer to GPU
0.00.690.999 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.035 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.691.036 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.692.596 I llama_init_from_model: n_seq_max     = 1
0.00.692.601 I llama_init_from_model: n_ctx         = 128
0.00.692.601 I llama_init_from_model: n_ctx_per_seq = 128
0.00.692.606 I llama_init_from_model: n_batch       = 128
0.00.692.607 I llama_init_from_model: n_ubatch      = 128
0.00.692.607 I llama_init_from_model: flash_attn    = 0
0.00.692.609 I llama_init_from_model: freq_base     = 10000.0
0.00.692.614 I llama_init_from_model: freq_scale    = 1
0.00.692.615 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.692.620 I ggml_metal_init: allocating
0.00.692.730 I ggml_metal_init: found device: Apple M4
0.00.692.744 I ggml_metal_init: picking default device: Apple M4
0.00.694.578 I ggml_metal_init: using embedded metal library
0.00.701.093 I ggml_metal_init: GPU name:   Apple M4
0.00.701.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.099 I ggml_metal_init: simdgroup reduction   = true
0.00.701.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.100 I ggml_metal_init: has residency sets    = true
0.00.701.100 I ggml_metal_init: has bfloat            = true
0.00.701.100 I ggml_metal_init: use bfloat            = true
0.00.701.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.885 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.721.889 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.350 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.725.351 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.725.352 I llama_init_from_model: graph nodes  = 967
0.00.725.352 I llama_init_from_model: graph splits = 2
0.00.725.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.725.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.246 I 
0.00.755.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.349 I perplexity: tokenizing the input ..
0.00.763.021 I perplexity: tokenization took 7.668 ms
0.00.763.045 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.718 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.913.114 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.913.127 I llama_perf_context_print:        load time =     746.32 ms
0.00.913.129 I llama_perf_context_print: prompt eval time =     147.74 ms /   128 tokens (    1.15 ms per token,   866.38 tokens per second)
0.00.913.129 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.130 I llama_perf_context_print:       total time =     157.89 ms /   129 tokens
0.00.913.504 I ggml_metal_free: deallocating

real	0m0.927s
user	0m0.080s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.770 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.770 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.771 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.776 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.349 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.349 I llama_model_loader: - type  f32:  194 tensors
0.00.025.349 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.350 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.350 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.351 I print_info: file format = GGUF V3 (latest)
0.00.025.351 I print_info: file type   = Q2_K - Medium
0.00.025.352 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.133 I load: special tokens cache size = 25
0.00.038.886 I load: token to piece cache size = 0.2984 MB
0.00.038.889 I print_info: arch             = gptneox
0.00.038.889 I print_info: vocab_only       = 0
0.00.038.889 I print_info: n_ctx_train      = 2048
0.00.038.889 I print_info: n_embd           = 2048
0.00.038.890 I print_info: n_layer          = 24
0.00.038.893 I print_info: n_head           = 16
0.00.038.900 I print_info: n_head_kv        = 16
0.00.038.904 I print_info: n_rot            = 32
0.00.038.904 I print_info: n_swa            = 0
0.00.038.904 I print_info: n_embd_head_k    = 128
0.00.038.905 I print_info: n_embd_head_v    = 128
0.00.038.911 I print_info: n_gqa            = 1
0.00.038.912 I print_info: n_embd_k_gqa     = 2048
0.00.038.912 I print_info: n_embd_v_gqa     = 2048
0.00.038.913 I print_info: f_norm_eps       = 1.0e-05
0.00.038.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.915 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.915 I print_info: f_logit_scale    = 0.0e+00
0.00.038.915 I print_info: n_ff             = 8192
0.00.038.915 I print_info: n_expert         = 0
0.00.038.915 I print_info: n_expert_used    = 0
0.00.038.916 I print_info: causal attn      = 1
0.00.038.917 I print_info: pooling type     = 0
0.00.038.917 I print_info: rope type        = 2
0.00.038.918 I print_info: rope scaling     = linear
0.00.038.918 I print_info: freq_base_train  = 10000.0
0.00.038.918 I print_info: freq_scale_train = 1
0.00.038.919 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.919 I print_info: rope_finetuned   = unknown
0.00.038.919 I print_info: ssm_d_conv       = 0
0.00.038.920 I print_info: ssm_d_inner      = 0
0.00.038.920 I print_info: ssm_d_state      = 0
0.00.038.920 I print_info: ssm_dt_rank      = 0
0.00.038.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.921 I print_info: model type       = 1.4B
0.00.038.921 I print_info: model params     = 1.41 B
0.00.038.921 I print_info: general.name     = 1.4B
0.00.038.922 I print_info: vocab type       = BPE
0.00.038.922 I print_info: n_vocab          = 50304
0.00.038.922 I print_info: n_merges         = 50009
0.00.038.922 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.922 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.922 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.923 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.923 I print_info: LF token         = 128 'Ä'
0.00.038.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.923 I print_info: max token length = 1024
0.00.373.844 I load_tensors: offloading 24 repeating layers to GPU
0.00.373.853 I load_tensors: offloading output layer to GPU
0.00.373.853 I load_tensors: offloaded 25/25 layers to GPU
0.00.373.887 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.373.891 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.375.152 I llama_init_from_model: n_seq_max     = 1
0.00.375.162 I llama_init_from_model: n_ctx         = 128
0.00.375.162 I llama_init_from_model: n_ctx_per_seq = 128
0.00.375.166 I llama_init_from_model: n_batch       = 128
0.00.375.166 I llama_init_from_model: n_ubatch      = 128
0.00.375.167 I llama_init_from_model: flash_attn    = 0
0.00.375.169 I llama_init_from_model: freq_base     = 10000.0
0.00.375.169 I llama_init_from_model: freq_scale    = 1
0.00.375.170 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.375.172 I ggml_metal_init: allocating
0.00.375.258 I ggml_metal_init: found device: Apple M4
0.00.375.273 I ggml_metal_init: picking default device: Apple M4
0.00.377.336 I ggml_metal_init: using embedded metal library
0.00.383.609 I ggml_metal_init: GPU name:   Apple M4
0.00.383.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.624 I ggml_metal_init: simdgroup reduction   = true
0.00.383.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.625 I ggml_metal_init: has residency sets    = true
0.00.383.625 I ggml_metal_init: has bfloat            = true
0.00.383.625 I ggml_metal_init: use bfloat            = true
0.00.383.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.878 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.409.699 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.409.704 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.409.729 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.413.033 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.413.035 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.413.035 I llama_init_from_model: graph nodes  = 967
0.00.413.036 I llama_init_from_model: graph splits = 2
0.00.413.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.413.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.557 I 
0.00.441.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.657 I perplexity: tokenizing the input ..
0.00.448.201 I perplexity: tokenization took 6.543 ms
0.00.448.216 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.580.660 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.581.981 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.581.996 I llama_perf_context_print:        load time =     431.54 ms
0.00.581.997 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.35 tokens per second)
0.00.581.997 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.581.998 I llama_perf_context_print:       total time =     140.44 ms /   129 tokens
0.00.582.375 I ggml_metal_free: deallocating

real	0m0.602s
user	0m0.079s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.836 I llama_model_loader: - type  f32:  194 tensors
0.00.024.836 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.837 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.837 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.838 I print_info: file format = GGUF V3 (latest)
0.00.024.838 I print_info: file type   = Q3_K - Medium
0.00.024.839 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.940 I load: special tokens cache size = 25
0.00.038.941 I load: token to piece cache size = 0.2984 MB
0.00.038.944 I print_info: arch             = gptneox
0.00.038.945 I print_info: vocab_only       = 0
0.00.038.945 I print_info: n_ctx_train      = 2048
0.00.038.945 I print_info: n_embd           = 2048
0.00.038.945 I print_info: n_layer          = 24
0.00.038.948 I print_info: n_head           = 16
0.00.038.949 I print_info: n_head_kv        = 16
0.00.038.950 I print_info: n_rot            = 32
0.00.038.950 I print_info: n_swa            = 0
0.00.038.950 I print_info: n_embd_head_k    = 128
0.00.038.952 I print_info: n_embd_head_v    = 128
0.00.038.953 I print_info: n_gqa            = 1
0.00.038.954 I print_info: n_embd_k_gqa     = 2048
0.00.038.955 I print_info: n_embd_v_gqa     = 2048
0.00.038.955 I print_info: f_norm_eps       = 1.0e-05
0.00.038.957 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.957 I print_info: f_logit_scale    = 0.0e+00
0.00.038.958 I print_info: n_ff             = 8192
0.00.038.958 I print_info: n_expert         = 0
0.00.038.958 I print_info: n_expert_used    = 0
0.00.038.959 I print_info: causal attn      = 1
0.00.038.959 I print_info: pooling type     = 0
0.00.038.959 I print_info: rope type        = 2
0.00.038.959 I print_info: rope scaling     = linear
0.00.038.960 I print_info: freq_base_train  = 10000.0
0.00.038.960 I print_info: freq_scale_train = 1
0.00.038.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.960 I print_info: rope_finetuned   = unknown
0.00.038.961 I print_info: ssm_d_conv       = 0
0.00.038.961 I print_info: ssm_d_inner      = 0
0.00.038.961 I print_info: ssm_d_state      = 0
0.00.038.961 I print_info: ssm_dt_rank      = 0
0.00.038.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.961 I print_info: model type       = 1.4B
0.00.038.965 I print_info: model params     = 1.41 B
0.00.038.965 I print_info: general.name     = 1.4B
0.00.038.966 I print_info: vocab type       = BPE
0.00.038.966 I print_info: n_vocab          = 50304
0.00.038.966 I print_info: n_merges         = 50009
0.00.038.966 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: LF token         = 128 'Ä'
0.00.038.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.969 I print_info: max token length = 1024
0.00.433.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.505 I load_tensors: offloading output layer to GPU
0.00.433.505 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.545 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.547 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.435.073 I llama_init_from_model: n_seq_max     = 1
0.00.435.079 I llama_init_from_model: n_ctx         = 128
0.00.435.079 I llama_init_from_model: n_ctx_per_seq = 128
0.00.435.080 I llama_init_from_model: n_batch       = 128
0.00.435.080 I llama_init_from_model: n_ubatch      = 128
0.00.435.080 I llama_init_from_model: flash_attn    = 0
0.00.435.082 I llama_init_from_model: freq_base     = 10000.0
0.00.435.083 I llama_init_from_model: freq_scale    = 1
0.00.435.084 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.435.085 I ggml_metal_init: allocating
0.00.435.154 I ggml_metal_init: found device: Apple M4
0.00.435.168 I ggml_metal_init: picking default device: Apple M4
0.00.436.840 I ggml_metal_init: using embedded metal library
0.00.442.532 I ggml_metal_init: GPU name:   Apple M4
0.00.442.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.540 I ggml_metal_init: simdgroup reduction   = true
0.00.442.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.541 I ggml_metal_init: has residency sets    = true
0.00.442.541 I ggml_metal_init: has bfloat            = true
0.00.442.541 I ggml_metal_init: use bfloat            = true
0.00.442.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.107 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.465.685 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.465.689 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.465.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.469.186 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.469.188 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.469.189 I llama_init_from_model: graph nodes  = 967
0.00.469.189 I llama_init_from_model: graph splits = 2
0.00.469.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.469.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.464 I 
0.00.494.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.603 I perplexity: tokenizing the input ..
0.00.501.708 I perplexity: tokenization took 7.101 ms
0.00.501.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.628 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.635.933 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.635.951 I llama_perf_context_print:        load time =     485.57 ms
0.00.635.952 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.54 tokens per second)
0.00.635.953 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.953 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.636.296 I ggml_metal_free: deallocating

real	0m0.650s
user	0m0.080s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.027 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.028 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.028 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.028 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.031 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.032 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.032 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.896 I llama_model_loader: - type  f32:  194 tensors
0.00.025.896 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.897 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.897 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.898 I print_info: file format = GGUF V3 (latest)
0.00.025.898 I print_info: file type   = Q4_K - Medium
0.00.025.899 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.065 I load: special tokens cache size = 25
0.00.040.072 I load: token to piece cache size = 0.2984 MB
0.00.040.075 I print_info: arch             = gptneox
0.00.040.075 I print_info: vocab_only       = 0
0.00.040.075 I print_info: n_ctx_train      = 2048
0.00.040.075 I print_info: n_embd           = 2048
0.00.040.076 I print_info: n_layer          = 24
0.00.040.079 I print_info: n_head           = 16
0.00.040.080 I print_info: n_head_kv        = 16
0.00.040.080 I print_info: n_rot            = 32
0.00.040.080 I print_info: n_swa            = 0
0.00.040.080 I print_info: n_embd_head_k    = 128
0.00.040.081 I print_info: n_embd_head_v    = 128
0.00.040.081 I print_info: n_gqa            = 1
0.00.040.082 I print_info: n_embd_k_gqa     = 2048
0.00.040.083 I print_info: n_embd_v_gqa     = 2048
0.00.040.084 I print_info: f_norm_eps       = 1.0e-05
0.00.040.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.084 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.084 I print_info: f_logit_scale    = 0.0e+00
0.00.040.087 I print_info: n_ff             = 8192
0.00.040.087 I print_info: n_expert         = 0
0.00.040.087 I print_info: n_expert_used    = 0
0.00.040.088 I print_info: causal attn      = 1
0.00.040.088 I print_info: pooling type     = 0
0.00.040.088 I print_info: rope type        = 2
0.00.040.088 I print_info: rope scaling     = linear
0.00.040.089 I print_info: freq_base_train  = 10000.0
0.00.040.089 I print_info: freq_scale_train = 1
0.00.040.089 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.089 I print_info: rope_finetuned   = unknown
0.00.040.090 I print_info: ssm_d_conv       = 0
0.00.040.090 I print_info: ssm_d_inner      = 0
0.00.040.090 I print_info: ssm_d_state      = 0
0.00.040.090 I print_info: ssm_dt_rank      = 0
0.00.040.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.090 I print_info: model type       = 1.4B
0.00.040.091 I print_info: model params     = 1.41 B
0.00.040.091 I print_info: general.name     = 1.4B
0.00.040.091 I print_info: vocab type       = BPE
0.00.040.092 I print_info: n_vocab          = 50304
0.00.040.092 I print_info: n_merges         = 50009
0.00.040.092 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.093 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: LF token         = 128 'Ä'
0.00.040.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: max token length = 1024
0.00.520.614 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.629 I load_tensors: offloading output layer to GPU
0.00.520.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.666 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.668 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.522.264 I llama_init_from_model: n_seq_max     = 1
0.00.522.269 I llama_init_from_model: n_ctx         = 128
0.00.522.269 I llama_init_from_model: n_ctx_per_seq = 128
0.00.522.270 I llama_init_from_model: n_batch       = 128
0.00.522.270 I llama_init_from_model: n_ubatch      = 128
0.00.522.271 I llama_init_from_model: flash_attn    = 0
0.00.522.273 I llama_init_from_model: freq_base     = 10000.0
0.00.522.273 I llama_init_from_model: freq_scale    = 1
0.00.522.274 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.522.286 I ggml_metal_init: allocating
0.00.522.366 I ggml_metal_init: found device: Apple M4
0.00.522.379 I ggml_metal_init: picking default device: Apple M4
0.00.524.202 I ggml_metal_init: using embedded metal library
0.00.530.911 I ggml_metal_init: GPU name:   Apple M4
0.00.530.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.918 I ggml_metal_init: simdgroup reduction   = true
0.00.530.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.918 I ggml_metal_init: has residency sets    = true
0.00.530.918 I ggml_metal_init: has bfloat            = true
0.00.530.919 I ggml_metal_init: use bfloat            = true
0.00.530.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.159 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.551.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.551.562 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.551.597 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.554.803 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.554.805 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.554.805 I llama_init_from_model: graph nodes  = 967
0.00.554.806 I llama_init_from_model: graph splits = 2
0.00.554.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.554.809 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.613 I 
0.00.584.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.712 I perplexity: tokenizing the input ..
0.00.591.732 I perplexity: tokenization took 7.017 ms
0.00.591.753 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.737.333 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.738.703 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.738.721 I llama_perf_context_print:        load time =     574.76 ms
0.00.738.722 I llama_perf_context_print: prompt eval time =     144.68 ms /   128 tokens (    1.13 ms per token,   884.69 tokens per second)
0.00.738.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.723 I llama_perf_context_print:       total time =     154.11 ms /   129 tokens
0.00.739.154 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.179 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.214 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.927 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.927 I llama_model_loader: - type  f32:  194 tensors
0.00.024.927 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.928 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.928 I print_info: file format = GGUF V3 (latest)
0.00.024.928 I print_info: file type   = Q5_K - Medium
0.00.024.929 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.759 I load: special tokens cache size = 25
0.00.038.798 I load: token to piece cache size = 0.2984 MB
0.00.038.801 I print_info: arch             = gptneox
0.00.038.801 I print_info: vocab_only       = 0
0.00.038.802 I print_info: n_ctx_train      = 2048
0.00.038.802 I print_info: n_embd           = 2048
0.00.038.802 I print_info: n_layer          = 24
0.00.038.805 I print_info: n_head           = 16
0.00.038.807 I print_info: n_head_kv        = 16
0.00.038.807 I print_info: n_rot            = 32
0.00.038.808 I print_info: n_swa            = 0
0.00.038.808 I print_info: n_embd_head_k    = 128
0.00.038.808 I print_info: n_embd_head_v    = 128
0.00.038.809 I print_info: n_gqa            = 1
0.00.038.809 I print_info: n_embd_k_gqa     = 2048
0.00.038.810 I print_info: n_embd_v_gqa     = 2048
0.00.038.811 I print_info: f_norm_eps       = 1.0e-05
0.00.038.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.811 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.811 I print_info: f_logit_scale    = 0.0e+00
0.00.038.812 I print_info: n_ff             = 8192
0.00.038.812 I print_info: n_expert         = 0
0.00.038.813 I print_info: n_expert_used    = 0
0.00.038.813 I print_info: causal attn      = 1
0.00.038.813 I print_info: pooling type     = 0
0.00.038.813 I print_info: rope type        = 2
0.00.038.813 I print_info: rope scaling     = linear
0.00.038.816 I print_info: freq_base_train  = 10000.0
0.00.038.816 I print_info: freq_scale_train = 1
0.00.038.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.816 I print_info: rope_finetuned   = unknown
0.00.038.817 I print_info: ssm_d_conv       = 0
0.00.038.817 I print_info: ssm_d_inner      = 0
0.00.038.817 I print_info: ssm_d_state      = 0
0.00.038.817 I print_info: ssm_dt_rank      = 0
0.00.038.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.817 I print_info: model type       = 1.4B
0.00.038.818 I print_info: model params     = 1.41 B
0.00.038.818 I print_info: general.name     = 1.4B
0.00.038.818 I print_info: vocab type       = BPE
0.00.038.819 I print_info: n_vocab          = 50304
0.00.038.819 I print_info: n_merges         = 50009
0.00.038.819 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.819 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: LF token         = 128 'Ä'
0.00.038.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.824 I print_info: max token length = 1024
0.00.595.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.497 I load_tensors: offloading output layer to GPU
0.00.595.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.524 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.527 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.596.680 I llama_init_from_model: n_seq_max     = 1
0.00.596.683 I llama_init_from_model: n_ctx         = 128
0.00.596.683 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.687 I llama_init_from_model: n_batch       = 128
0.00.596.687 I llama_init_from_model: n_ubatch      = 128
0.00.596.688 I llama_init_from_model: flash_attn    = 0
0.00.596.689 I llama_init_from_model: freq_base     = 10000.0
0.00.596.689 I llama_init_from_model: freq_scale    = 1
0.00.596.694 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.695 I ggml_metal_init: allocating
0.00.596.772 I ggml_metal_init: found device: Apple M4
0.00.596.783 I ggml_metal_init: picking default device: Apple M4
0.00.598.235 I ggml_metal_init: using embedded metal library
0.00.604.364 I ggml_metal_init: GPU name:   Apple M4
0.00.604.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.370 I ggml_metal_init: simdgroup reduction   = true
0.00.604.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.371 I ggml_metal_init: has residency sets    = true
0.00.604.371 I ggml_metal_init: has bfloat            = true
0.00.604.372 I ggml_metal_init: use bfloat            = true
0.00.604.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.846 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.341 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.344 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.372 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.603 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.604 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.605 I llama_init_from_model: graph nodes  = 967
0.00.627.605 I llama_init_from_model: graph splits = 2
0.00.627.608 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.275 I 
0.00.658.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.382 I perplexity: tokenizing the input ..
0.00.665.804 I perplexity: tokenization took 7.419 ms
0.00.665.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.764 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.808.109 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.808.122 I llama_perf_context_print:        load time =     649.09 ms
0.00.808.123 I llama_perf_context_print: prompt eval time =     140.39 ms /   128 tokens (    1.10 ms per token,   911.73 tokens per second)
0.00.808.124 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.124 I llama_perf_context_print:       total time =     149.85 ms /   129 tokens
0.00.808.491 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.078s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.164 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.027 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.028 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.028 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.689 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.690 I llama_model_loader: - type  f32:  194 tensors
0.00.025.690 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.691 I print_info: file format = GGUF V3 (latest)
0.00.025.691 I print_info: file type   = Q6_K
0.00.025.692 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.528 I load: special tokens cache size = 25
0.00.039.544 I load: token to piece cache size = 0.2984 MB
0.00.039.546 I print_info: arch             = gptneox
0.00.039.547 I print_info: vocab_only       = 0
0.00.039.547 I print_info: n_ctx_train      = 2048
0.00.039.547 I print_info: n_embd           = 2048
0.00.039.547 I print_info: n_layer          = 24
0.00.039.550 I print_info: n_head           = 16
0.00.039.551 I print_info: n_head_kv        = 16
0.00.039.551 I print_info: n_rot            = 32
0.00.039.551 I print_info: n_swa            = 0
0.00.039.551 I print_info: n_embd_head_k    = 128
0.00.039.551 I print_info: n_embd_head_v    = 128
0.00.039.552 I print_info: n_gqa            = 1
0.00.039.555 I print_info: n_embd_k_gqa     = 2048
0.00.039.556 I print_info: n_embd_v_gqa     = 2048
0.00.039.556 I print_info: f_norm_eps       = 1.0e-05
0.00.039.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.557 I print_info: f_logit_scale    = 0.0e+00
0.00.039.558 I print_info: n_ff             = 8192
0.00.039.558 I print_info: n_expert         = 0
0.00.039.558 I print_info: n_expert_used    = 0
0.00.039.559 I print_info: causal attn      = 1
0.00.039.559 I print_info: pooling type     = 0
0.00.039.559 I print_info: rope type        = 2
0.00.039.559 I print_info: rope scaling     = linear
0.00.039.561 I print_info: freq_base_train  = 10000.0
0.00.039.561 I print_info: freq_scale_train = 1
0.00.039.561 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.561 I print_info: rope_finetuned   = unknown
0.00.039.562 I print_info: ssm_d_conv       = 0
0.00.039.562 I print_info: ssm_d_inner      = 0
0.00.039.562 I print_info: ssm_d_state      = 0
0.00.039.562 I print_info: ssm_dt_rank      = 0
0.00.039.562 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.562 I print_info: model type       = 1.4B
0.00.039.563 I print_info: model params     = 1.41 B
0.00.039.563 I print_info: general.name     = 1.4B
0.00.039.563 I print_info: vocab type       = BPE
0.00.039.564 I print_info: n_vocab          = 50304
0.00.039.564 I print_info: n_merges         = 50009
0.00.039.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.564 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.565 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.565 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.569 I print_info: LF token         = 128 'Ä'
0.00.039.569 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.569 I print_info: max token length = 1024
0.00.136.403 I load_tensors: offloading 24 repeating layers to GPU
0.00.136.406 I load_tensors: offloading output layer to GPU
0.00.136.407 I load_tensors: offloaded 25/25 layers to GPU
0.00.136.424 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.136.426 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.137.265 I llama_init_from_model: n_seq_max     = 1
0.00.137.267 I llama_init_from_model: n_ctx         = 128
0.00.137.267 I llama_init_from_model: n_ctx_per_seq = 128
0.00.137.267 I llama_init_from_model: n_batch       = 128
0.00.137.268 I llama_init_from_model: n_ubatch      = 128
0.00.137.268 I llama_init_from_model: flash_attn    = 0
0.00.137.269 I llama_init_from_model: freq_base     = 10000.0
0.00.137.269 I llama_init_from_model: freq_scale    = 1
0.00.137.270 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.137.271 I ggml_metal_init: allocating
0.00.137.295 I ggml_metal_init: found device: Apple M4
0.00.137.302 I ggml_metal_init: picking default device: Apple M4
0.00.138.194 I ggml_metal_init: using embedded metal library
0.00.142.982 I ggml_metal_init: GPU name:   Apple M4
0.00.142.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.142.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.142.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.142.987 I ggml_metal_init: simdgroup reduction   = true
0.00.142.987 I ggml_metal_init: simdgroup matrix mul. = true
0.00.142.987 I ggml_metal_init: has residency sets    = true
0.00.142.988 I ggml_metal_init: has bfloat            = true
0.00.142.988 I ggml_metal_init: use bfloat            = true
0.00.142.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.142.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.157.324 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.159.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.159.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.159.750 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.162.095 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.162.096 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.162.097 I llama_init_from_model: graph nodes  = 967
0.00.162.097 I llama_init_from_model: graph splits = 2
0.00.162.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.162.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.195.277 I 
0.00.195.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.195.339 I perplexity: tokenizing the input ..
0.00.199.629 I perplexity: tokenization took 4.288 ms
0.00.199.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.338.831 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.340.167 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.340.181 I llama_perf_context_print:        load time =     185.11 ms
0.00.340.182 I llama_perf_context_print: prompt eval time =     138.96 ms /   128 tokens (    1.09 ms per token,   921.12 tokens per second)
0.00.340.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.340.183 I llama_perf_context_print:       total time =     144.91 ms /   129 tokens
0.00.340.567 I ggml_metal_free: deallocating

real	0m0.356s
user	0m0.071s
sys	0m0.054s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.252 I build: 4574 (7fee2889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.117 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.521 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.538 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.899 I llama_model_loader: - type  f32:  194 tensors
0.00.053.899 I llama_model_loader: - type  f16:   98 tensors
0.00.053.900 I print_info: file format = GGUF V3 (latest)
0.00.053.901 I print_info: file type   = all F32 (guessed)
0.00.053.902 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.295 I load: special tokens cache size = 25
0.00.072.514 I load: token to piece cache size = 0.2984 MB
0.00.072.517 I print_info: arch             = gptneox
0.00.072.517 I print_info: vocab_only       = 0
0.00.072.517 I print_info: n_ctx_train      = 2048
0.00.072.518 I print_info: n_embd           = 2048
0.00.072.518 I print_info: n_layer          = 24
0.00.072.520 I print_info: n_head           = 16
0.00.072.521 I print_info: n_head_kv        = 16
0.00.072.521 I print_info: n_rot            = 32
0.00.072.523 I print_info: n_swa            = 0
0.00.072.523 I print_info: n_embd_head_k    = 128
0.00.072.523 I print_info: n_embd_head_v    = 128
0.00.072.524 I print_info: n_gqa            = 1
0.00.072.525 I print_info: n_embd_k_gqa     = 2048
0.00.072.525 I print_info: n_embd_v_gqa     = 2048
0.00.072.526 I print_info: f_norm_eps       = 1.0e-05
0.00.072.526 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.527 I print_info: f_logit_scale    = 0.0e+00
0.00.072.528 I print_info: n_ff             = 8192
0.00.072.528 I print_info: n_expert         = 0
0.00.072.528 I print_info: n_expert_used    = 0
0.00.072.529 I print_info: causal attn      = 1
0.00.072.529 I print_info: pooling type     = 0
0.00.072.529 I print_info: rope type        = 2
0.00.072.529 I print_info: rope scaling     = linear
0.00.072.529 I print_info: freq_base_train  = 10000.0
0.00.072.530 I print_info: freq_scale_train = 1
0.00.072.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.530 I print_info: rope_finetuned   = unknown
0.00.072.530 I print_info: ssm_d_conv       = 0
0.00.072.532 I print_info: ssm_d_inner      = 0
0.00.072.532 I print_info: ssm_d_state      = 0
0.00.072.532 I print_info: ssm_dt_rank      = 0
0.00.072.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.532 I print_info: model type       = 1.4B
0.00.072.533 I print_info: model params     = 1.41 B
0.00.072.533 I print_info: general.name     = 1.4B
0.00.072.533 I print_info: vocab type       = BPE
0.00.072.534 I print_info: n_vocab          = 50304
0.00.072.534 I print_info: n_merges         = 50009
0.00.072.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.534 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.534 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.538 I print_info: LF token         = 128 'Ä'
0.00.072.538 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.539 I print_info: max token length = 1024
0.01.138.807 I load_tensors: offloading 24 repeating layers to GPU
0.01.138.811 I load_tensors: offloading output layer to GPU
0.01.138.811 I load_tensors: offloaded 25/25 layers to GPU
0.01.138.835 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.138.837 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.139.605 I llama_init_from_model: n_seq_max     = 1
0.01.139.606 I llama_init_from_model: n_ctx         = 128
0.01.139.607 I llama_init_from_model: n_ctx_per_seq = 128
0.01.139.607 I llama_init_from_model: n_batch       = 128
0.01.139.607 I llama_init_from_model: n_ubatch      = 128
0.01.139.607 I llama_init_from_model: flash_attn    = 0
0.01.139.608 I llama_init_from_model: freq_base     = 10000.0
0.01.139.608 I llama_init_from_model: freq_scale    = 1
0.01.139.608 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.139.610 I ggml_metal_init: allocating
0.01.139.641 I ggml_metal_init: found device: Apple M4
0.01.139.647 I ggml_metal_init: picking default device: Apple M4
0.01.140.595 I ggml_metal_init: using embedded metal library
0.01.144.368 I ggml_metal_init: GPU name:   Apple M4
0.01.144.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.144.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.144.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.144.372 I ggml_metal_init: simdgroup reduction   = true
0.01.144.372 I ggml_metal_init: simdgroup matrix mul. = true
0.01.144.372 I ggml_metal_init: has residency sets    = true
0.01.144.372 I ggml_metal_init: has bfloat            = true
0.01.144.372 I ggml_metal_init: use bfloat            = true
0.01.144.373 I ggml_metal_init: hasUnifiedMemory      = true
0.01.144.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.154.821 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.156.509 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.156.513 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.156.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.158.124 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.158.126 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.158.126 I llama_init_from_model: graph nodes  = 967
0.01.158.126 I llama_init_from_model: graph splits = 2
0.01.158.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.158.128 I 
0.01.158.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.158.165 I compute_imatrix: tokenizing the input ..
0.01.162.230 I compute_imatrix: tokenization took 4.064 ms
0.01.162.232 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.427.405 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.429.963 I llama_perf_context_print:        load time =    1404.29 ms
0.01.429.964 I llama_perf_context_print: prompt eval time =     263.43 ms /   128 tokens (    2.06 ms per token,   485.90 tokens per second)
0.01.429.965 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.429.965 I llama_perf_context_print:       total time =    1406.84 ms /   129 tokens
0.01.430.507 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.125s
sys	0m0.247s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4574 (7fee2889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119604bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1196052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119605890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119605e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1196063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1196069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119606f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119607500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119607ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119607fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1196084b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1196089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1196094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119609c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11960a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11960abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11960b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11960b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11960c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11960c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11960d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11960d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11960de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11960e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11960ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11960f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11960f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119610340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119610b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1196112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119611b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119612330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1196127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119612c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119613110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1196135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119613a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119613ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119614390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119614cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119614f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1196155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119615bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1196164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119616ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1196170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119617700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119617d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1196195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119619d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11961a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11961ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11961ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11961b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11961b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11961bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11961c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11961c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11961c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11961ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11961d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11961d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11961dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11961e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11961e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11961eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11961f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11961f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11961faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11961fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119621530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119621a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119621fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119622a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119623fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119624a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119624fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1196254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119625a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119625f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1196264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1196161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119626950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119627100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119627650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119627ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1196280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119628640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119628b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1196290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119629b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11962a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11962a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11962ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11962b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11962b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11962bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11962bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11962c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11962c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11962cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11962d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11962d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11962db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11962dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11962e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11962e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11962ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11962f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11962f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11962fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119630010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1196304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119630df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119632070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119632510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1196329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1196332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1196340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119634570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119634a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119635350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1196357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119635c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1196365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119636a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1196373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119637850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119637cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119638ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119638f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1196398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11963a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11963a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11963ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11963afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11963b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11963b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11963bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11963c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11963c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11963cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11963d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11963d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11963d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11963de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11963e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11963e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11963ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11963f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11963f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11963f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11963fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119640310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1196407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119640c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1196410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119641590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119641a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119641ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119642d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1196432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119643800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119643d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119644010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119644620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119645a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119646190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1196467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119646db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1196475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119649080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1196495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119649b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11964a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11964a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11964ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11964b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11964b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11964bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11964c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11964c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11964caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11964d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11964d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11964dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11964e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11964e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11964ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11964f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11964f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11964fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119650560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119650ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119651000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119651550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119651aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119651ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119652540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119652a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119652fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119653530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119653a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119653fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119654520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119654a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119654fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119655510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119655a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119655fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119656500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119656a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119656fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1196574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119657a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119657f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1196584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119658a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119658f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1196594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119659a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119659f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11965a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11965aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11965af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11965b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11965b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11965bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11965c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11965c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11965cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11965d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11965d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11965d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11965de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11965e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11965e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11965ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11965f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11965f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11965fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11965ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119660680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1196614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119661be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119661ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119662690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119662950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119662f60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.724.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119662c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119646450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1196442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119644ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119617fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1196179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119619fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119646a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11960f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119615e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119616790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119615250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1196173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11960e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11961a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119626c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119662160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119611560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119645500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11960f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11960fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11960ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1196633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119663680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119663940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119663c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119663ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119664180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119664440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119664700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1196649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119664c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1196654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119665780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119665a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119665d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119665fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119666280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119666540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119666800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119666ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119666d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119667040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119667300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1196675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119667880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119667b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119667e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1196680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119668380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119668640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119668900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119668bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119668e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119669140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119669400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1196696c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119669980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119669c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119669f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11966a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11966a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11966a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115204280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1152046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115204b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115204fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115205440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1152058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115205d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115206190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115206600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115206a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115206ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115207350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1152077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115207c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1152080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115208510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115208980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115208df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115209260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1152096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115209b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115209fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11520a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11520a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11520ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11520b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11520b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11520ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11520bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11520c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11520c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11520cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11520d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11520d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11520d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11520ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11520e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11520e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11520eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11520ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11520f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11520f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11520fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115210150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1152105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115210a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115210ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115211310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115211780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115211bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115212060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1152124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115212940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115212db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115213220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115213690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115213b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115213f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1152143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115214850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115214cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115215130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1152155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115215a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115215e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1152162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115216760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115216bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115217040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1152174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115217920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115217d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115218200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115218670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115218ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115218f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1152193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115219830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115219ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11521a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11521a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11521a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11521ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11521b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11521b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11521bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11521c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11521c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11521c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11521cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11521d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11521d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11521dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11521df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11521e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11521e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11521ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11521f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11521fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11521ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115220280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1152206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115220b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115220fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115221440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1152218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115221d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115222190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115222600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115222a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115222ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115223350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1152237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115223c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1152240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115224510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115224980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115224df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115225260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1152256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115225b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115225fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115226420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115226890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115226d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115227170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1152275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115227a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115227ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115228330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1152287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115228c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115229080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1152294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115229a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115229f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11522a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11522a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11522acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11522b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11522b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11522bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11522c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11522c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11522cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11522d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11522dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11522e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11522e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11522ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11522f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11522f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11522fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115230300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1152308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115230e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115231440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115231a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115231fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115232580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115232b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115233100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1152336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115233c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115234240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115234800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115234dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115235380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115235940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115235f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1152364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115236a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115237040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115237600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115237bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115238180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115238740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115238d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1152392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115239880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115239e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11523a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11523a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11523af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11523b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11523bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11523c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11523c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11523cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11523d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11523d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11523dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11523e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11523e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11523eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11523f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11523fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115240000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1152405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115240b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115241080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115241580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115241a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115241f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115242480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115242980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115242e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115243380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115243880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115243d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115244280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115244780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115244c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115245180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115245680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115246090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1152467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115246ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1152475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1152478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1152480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115248360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115248970 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11966aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11966acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11966b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11966b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11966b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11966b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11966bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11966be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11966c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11966c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11966c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11966c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11966cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11966d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11966daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11966ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11966e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11966e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11966e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11966e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11966eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11966ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11966f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11966f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11966f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11966f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11966fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11966feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119670170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119670430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1196706f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1196709b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119670c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119670f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1196711f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1196714b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119671770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119671a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119671cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119671fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119672270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119672530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1196727f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119672ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119672d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119673030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1196732f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1196735b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119673870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119673b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119673df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1196740b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119674370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119674630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1196748f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119674bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119674e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119675130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1196753f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1196756b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119675970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119675c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119675ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1196761b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119676470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119676730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1196769f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119676cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119676f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119677230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1196774f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1196777b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119677a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119677d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119677ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1196782b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119678570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119678830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119678af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119678db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119679070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119679330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1196795f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1196798b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119679b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119679e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11967a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11967a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11967a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11967a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11967abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11967aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11967b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11967b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11967b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11967b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11967bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11967bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11967c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11967c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11967c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11967ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11967ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11967cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11967d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11967d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11967d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11967dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11967dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11967e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11967e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11967e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11967e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11967eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11967edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11967f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11967f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11967f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11967f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11967fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11967fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119680130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1196803f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1196806b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119680970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119680c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119680ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1196811b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119681470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119681730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1196819f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119681cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119681f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119682230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1196824f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1196827b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119682a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119682d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119682ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1196832b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119683570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119683830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119683af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119683db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119684070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119684330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1196845f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1196848b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119684b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119684e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1196850f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1196853b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119685670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119685930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119685bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119685eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119686170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119686430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1196866f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1196869b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119686c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119686f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1196871f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1196874b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119687770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119687a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119687cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119687fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119688270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119688530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1196887f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119688ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119688d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119689030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1196892f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1196895b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119689870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119689b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119689df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11968a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11968a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11968a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11968a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11968abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11968ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11968b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11968b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11968b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11968bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11968bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11968c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11968c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11968cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11968cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11968d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11968d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11968dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11968e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11968e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11968e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11968ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11968f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11968f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11968fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119690100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119690570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1196909e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119690e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119691370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119691880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1196923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1196926b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119692c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119693230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1196937f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119693db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119694370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119694930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119694ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1196954b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119695a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119696030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1196965f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119696bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119697170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119697730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119697cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1196982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119698870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119698e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1196993f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1196999b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119699f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11969a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11969aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11969b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11969b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11969bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11969c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11969c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11969cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11969d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11969d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11969deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11969e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11969ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11969eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11969f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11969fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1196a0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1196a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1196a0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1196a1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1196a1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1196a1df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1196a23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1196a2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1196a2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1196a34f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1196a3ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1196a4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1196a4630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1196a4bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1196a51b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1196a5770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1196a5d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1196a62f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1196a68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1196a6db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1196a72b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1196a77b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1196a7cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1196a81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1196a86b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1196a8bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1196a90b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1196a95b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1196a9ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1196a9fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1196aa4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1196aa9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1196aaeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1196ab3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1196abdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1196ac4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1196acc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1196ad320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1196ad5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1196addd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1196ae090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1196ae6a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.796s
user	0m0.285s
sys	0m0.338s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4574 (7fee2889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12460cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12460d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12460db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12460e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12460e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12460ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12460f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12460f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12460fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124610280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124610780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1246117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124611f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1246135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124613cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1246143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124614bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1246152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1246159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124616110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1246169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1246170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124617390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1246179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124618610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124618b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124618e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1246192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124619570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124619e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12461a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12461a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12461aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12461af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12461b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12461b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12461bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12461c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12461c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12461cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12461cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12461d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12461d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12461de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12461e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12461edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12461f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12461f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12461ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1246205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124620c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1246213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124621890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124621ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124622600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124622df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1246230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124623550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1246239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124623e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124624330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1246247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124625110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1246255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124625a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124626390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124626830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124626d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1246272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124627820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124627d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1246282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124628810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124628d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1246292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124629800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124629d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12462a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12462a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12462ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12462b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12462b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12462bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12462c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12462c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12462cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12462d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12462d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12462dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12462e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12462e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12461e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12462ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12462f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12462f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12462fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1246303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124630910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124630e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1246313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124631900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1246323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1246328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124632e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124633390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1246338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124633d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124634220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1246346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124635000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1246354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124635940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124635de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124636280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124636720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124636bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124637060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124637500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1246379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124637e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1246382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124638780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124638c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1246390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124639560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124639ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12463a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12463a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12463ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12463b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12463b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12463ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12463bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12463c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12463c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12463cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12463d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12463d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12463dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12463df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12463e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12463e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12463ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12463f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12463f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12463fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12463ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124640900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124640da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124641240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1246416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124641b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1246424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124642960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1246432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124643740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124643be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124644520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1246449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124644e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1246457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124645c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1246460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124646580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124646a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124646ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124647360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124647800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124647ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124648140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1246485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1246493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124649860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124649d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12464a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12464a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12464aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12464b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12464b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12464bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12464c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12464c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12464c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12464cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12464d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12464dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12464e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12464e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12464ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12464f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12464f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12464fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1246501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124650650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124650e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124651350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1246518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124651df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124652340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124652890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124652de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124653330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124653880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124653dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124654320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124654870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124654dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124655310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124655860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124655db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124656300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124656850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124656da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1246572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124657840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124657d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1246582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124658830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124658d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1246592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124659820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124659d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12465a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12465a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12465ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12465b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12465b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12465bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12465c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12465c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12465cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12465d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12465d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12465dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12465e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12465e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12465ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12465f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12465f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12465fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124660260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1246607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124660d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124661250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1246617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124661cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124662240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124662790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124662ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124663230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124663780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124663c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1246640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124664560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124664a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124664ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124665340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1246657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124665c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124666120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1246665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124666a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124666f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1246673a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124667840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124667ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124668230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124668950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124669070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124669790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124669eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12466a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12466a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12466ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12466b230 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.117.608 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12466aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12464cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12464c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12464d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1246202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12461fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1246222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12464ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12461e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12461ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12461f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12461d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12461f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124616650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12460c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124620ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1246228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12462eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12466a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124619830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12464f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12464d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124617c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124617f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1246181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12466b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12466b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12466bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12466bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12466c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12466c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12466c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12466c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12466cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12466cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12466d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12466d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12466d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12466da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12466dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12466dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12466e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12466e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12466e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12466ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12466ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12466f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12466f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12466f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12466f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12466fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12466fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1246700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124670390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124670650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124670910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124670bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124670e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124671150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124671410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1246716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124671990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124671c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124671f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1246721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124672490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124672750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124672a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124672cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124672f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124673250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124673510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1246737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124673a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124673d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124674010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1246742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124674590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124674850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124674b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124674dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124675090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124675350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124675610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1246758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124675b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124675e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124676110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1246763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124676690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124676950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124676c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124676ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124677190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124677450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124677710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1246779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124677c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124677f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124678210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1246784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124678790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124678a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124678d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124678fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124679290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124679550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124679810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124679ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124679d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12467a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12467a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12467a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12467a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12467ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12467ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12467b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12467b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12467b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12467b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12467bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12467be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12467c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12467c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12467c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12467c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12467cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12467cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12467d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12467d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12467d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12467da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12467dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12467df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12467e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12467e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12467e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12467ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12467ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12467f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12467f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12467f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12467f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12467fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12467fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124680090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124680350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124680610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1246808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124680b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124680e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124681110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1246813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124681690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124681950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124681c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124681ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124682190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124682450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124682710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1246829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124682f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124683210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1246834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124683790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124683a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124683d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124683fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124684290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124684550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124684810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124684ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124684d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124685050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1246855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124685890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124685b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124685e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1246860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124686390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124686650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124686910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124686bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124686e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124687150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124687410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1246876d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124687990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124687c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124687f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1246881d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124688490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124688750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124688a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124688cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124688f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124689250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124689510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1246897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124689a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124689d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12468a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12468a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12468a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12468a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12468ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12468b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12468b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12468b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12468b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12468bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12468bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12468c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12468c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12468c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12468c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12468cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12468cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12468d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12468d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12468d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12468da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12468dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12468dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12468e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12468e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12468e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12468eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12468ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12468f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12468f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12468f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12468f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12468fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12468fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1246900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124690360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124690620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1246908e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124690e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124691380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1246918d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124691e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124692370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1246928c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124692e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124693360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1246938b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124693e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124694350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1246948a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124694df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124695340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124695890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124695de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124696330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124696880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124696dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124697320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124697870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124697dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124698310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124698860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124698b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124698de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1246992e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1246997e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124699ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12469a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12469a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12469abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12469b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12469b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12469bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12469bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12469c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12469c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12469cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12469d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12469ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12469e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12469ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12469f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12469f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12469fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1246a00c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1246a06d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12600a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12600a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12600ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12600b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12600b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12600b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12600bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12600c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12600c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12600c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12600cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12600d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12600dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12600e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12600eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12600f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12600fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126010400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126010b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1260112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126011a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126012850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126012f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126013690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126013950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126013c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126014080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1260144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126014960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126014dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126015300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126015770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126015a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126015ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126016310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126016780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126016bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126017060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1260174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126017940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126017db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126018220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126018690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126018b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126018f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1260193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126019850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126019cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12601a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12601a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12601aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12601ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12601b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12601b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12601bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12601c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12601c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12601cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12601cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12601d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12601d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12601dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12601e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12601e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12601e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12601ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12601f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12601f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12601fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12601fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126020460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1260208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126020d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1260211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126021620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126021a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126021f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126022370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1260227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126022c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1260230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126023530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1260239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126023e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126024280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1260246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126024b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126024fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126025440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1260258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126025d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126026190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126026600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126026a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126026ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126027350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1260277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126027c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1260280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126028510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126028980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126028df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126029680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126029940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126029db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12602a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12602a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12602ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12602af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12602b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12602b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12602bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12602c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12602c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12602ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12602ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12602d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12602d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12602dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12602e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12602e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12602e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12602ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12602f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12602f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12602fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12602ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1260303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126030830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126030ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126031110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126031580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1260319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126031e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1260322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126032740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126032bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126033020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126033490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126033900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126033d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1260341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126034650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126034ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126034f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1260353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126035810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126035c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1260360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1260369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1260372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126037720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126037b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126038000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126038470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1260388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126038d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1260391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126039aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126039f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12603a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12603a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12603ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12603b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12603b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12603b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12603be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12603c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12603c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12603cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12603cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12603d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12603d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12603dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12603e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12603e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12603ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12603eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12603f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12603f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12603fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1260400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126040990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126040e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126041270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1260416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126041b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126042430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1260428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126042d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1260435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126043a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126043ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1260447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126044c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126045500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126045970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126045de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1260466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126046b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1260476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126047970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1260480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126048510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126048980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126049260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1260496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126049b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126049fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12604a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12604a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12604ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12604b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12604b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12604ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12604bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12604c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12604c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12604cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12604d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12604d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12604d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12604ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12604e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12604e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12604eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12604ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12604f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12604f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12604fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126050150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1260505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126050a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126050ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126051310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126051780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126051bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126052060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1260524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126052940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126052db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126053220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126053690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126053b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126053f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1260543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126054850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126054cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126055130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1260555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126055a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126055e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1260562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126056760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126056bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126057040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1260574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126057920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126057d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126058200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126058670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126058ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126058f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1260593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126059830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126059ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12605a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12605a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12605a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12605ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12605b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12605bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12605c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12605cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12605d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12605d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12605d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12605dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12605e5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.975s
user	0m0.241s
sys	0m0.179s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
