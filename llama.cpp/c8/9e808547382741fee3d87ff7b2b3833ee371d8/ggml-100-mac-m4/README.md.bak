### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.30 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.25 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.26 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.79 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.35 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.64 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.50 sec*proc (28 tests)

Total Test time (real) = 220.51 sec

real	3m40.645s
user	7m40.679s
sys	0m6.339s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.11 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.17 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.16 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.51 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.39 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.06 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.20 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.38 sec*proc (28 tests)

Total Test time (real) =  51.39 sec

real	0m51.404s
user	1m11.653s
sys	0m5.894s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.069 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.516 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.904 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.914 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.915 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.916 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.917 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.918 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.919 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.920 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.920 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.921 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.923 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.924 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.925 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.925 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.926 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.926 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.927 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.432 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.434 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.434 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.435 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.435 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.436 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.436 I llama_model_loader: - type  f32:  124 tensors
0.00.026.437 I llama_model_loader: - type  f16:   73 tensors
0.00.026.438 I print_info: file format = GGUF V3 (latest)
0.00.026.438 I print_info: file type   = F16
0.00.026.439 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.545 I load: special tokens cache size = 5
0.00.032.751 I load: token to piece cache size = 0.2032 MB
0.00.032.778 I print_info: arch             = bert
0.00.032.780 I print_info: n_vocab (hp)     = 30522
0.00.032.780 I print_info: vocab_only       = 0
0.00.032.781 I print_info: n_ctx_train      = 512
0.00.032.781 I print_info: n_embd           = 384
0.00.032.781 I print_info: n_layer          = 12
0.00.032.784 I print_info: n_head           = 12
0.00.032.785 I print_info: n_head_kv        = 12
0.00.032.787 I print_info: n_rot            = 32
0.00.032.788 I print_info: n_swa            = 0
0.00.032.788 I print_info: n_embd_head_k    = 32
0.00.032.788 I print_info: n_embd_head_v    = 32
0.00.032.789 I print_info: n_gqa            = 1
0.00.032.790 I print_info: n_embd_k_gqa     = 384
0.00.032.791 I print_info: n_embd_v_gqa     = 384
0.00.032.792 I print_info: f_norm_eps       = 1.0e-12
0.00.032.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.792 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.793 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.793 I print_info: f_logit_scale    = 0.0e+00
0.00.032.794 I print_info: n_ff             = 1536
0.00.032.794 I print_info: n_expert         = 0
0.00.032.794 I print_info: n_expert_used    = 0
0.00.032.795 I print_info: causal attn      = 0
0.00.032.796 I print_info: pooling type     = 2
0.00.032.796 I print_info: rope type        = 2
0.00.032.798 I print_info: rope scaling     = linear
0.00.032.799 I print_info: freq_base_train  = 10000.0
0.00.032.799 I print_info: freq_scale_train = 1
0.00.032.799 I print_info: n_ctx_orig_yarn  = 512
0.00.032.800 I print_info: rope_finetuned   = unknown
0.00.032.801 I print_info: ssm_d_conv       = 0
0.00.032.803 I print_info: ssm_d_inner      = 0
0.00.032.803 I print_info: ssm_d_state      = 0
0.00.032.803 I print_info: ssm_dt_rank      = 0
0.00.032.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.804 I print_info: model type       = 33M
0.00.032.804 I print_info: model params     = 33.21 M
0.00.032.805 I print_info: general.name     = Bge Small
0.00.032.805 I print_info: vocab type       = WPM
0.00.032.806 I print_info: n_vocab          = 30522
0.00.032.806 I print_info: n_merges         = 0
0.00.032.806 I print_info: UNK token        = 100 '[UNK]'
0.00.032.806 I print_info: SEP token        = 102 '[SEP]'
0.00.032.807 I print_info: PAD token        = 0 '[PAD]'
0.00.032.807 I print_info: CLS token        = 101 '[CLS]'
0.00.032.807 I print_info: MASK token       = 103 '[MASK]'
0.00.032.813 I print_info: LF token         = 0 '[PAD]'
0.00.032.813 I print_info: max token length = 21
0.00.034.698 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.698 I load_tensors: offloading output layer to GPU
0.00.034.699 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.726 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.727 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.947 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.949 I llama_new_context_with_model: n_ctx         = 512
0.00.034.949 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.949 I llama_new_context_with_model: n_batch       = 2048
0.00.034.950 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.950 I llama_new_context_with_model: flash_attn    = 0
0.00.034.950 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.951 I llama_new_context_with_model: freq_scale    = 1
0.00.034.951 I ggml_metal_init: allocating
0.00.034.955 I ggml_metal_init: found device: Apple M4
0.00.034.958 I ggml_metal_init: picking default device: Apple M4
0.00.035.749 I ggml_metal_init: using embedded metal library
0.00.039.762 I ggml_metal_init: GPU name:   Apple M4
0.00.039.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.766 I ggml_metal_init: simdgroup reduction   = true
0.00.039.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.766 I ggml_metal_init: has bfloat            = true
0.00.039.766 I ggml_metal_init: use bfloat            = true
0.00.039.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.414 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.952 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.955 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.975 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.052.708 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.052.709 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.052.710 I llama_new_context_with_model: graph nodes  = 429
0.00.052.710 I llama_new_context_with_model: graph splits = 2
0.00.052.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.915 I 
0.00.058.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.575 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.379 I llama_perf_context_print:        load time =      43.39 ms
0.00.064.380 I llama_perf_context_print: prompt eval time =       4.66 ms /     9 tokens (    0.52 ms per token,  1932.16 tokens per second)
0.00.064.381 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.381 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.064.506 I ggml_metal_free: deallocating

real	0m0.240s
user	0m0.047s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.332 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.101 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.106 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.109 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.110 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.110 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.111 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.111 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.111 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.112 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.112 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.114 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.114 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.115 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.117 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.117 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.117 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.578 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.265 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.266 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.266 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.267 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.267 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.267 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.267 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.268 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.268 I llama_model_loader: - type  f32:  124 tensors
0.00.015.269 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.269 I print_info: file format = GGUF V3 (latest)
0.00.015.269 I print_info: file type   = Q8_0
0.00.015.270 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.774 I load: special tokens cache size = 5
0.00.019.028 I load: token to piece cache size = 0.2032 MB
0.00.019.037 I print_info: arch             = bert
0.00.019.038 I print_info: n_vocab (hp)     = 30522
0.00.019.038 I print_info: vocab_only       = 0
0.00.019.039 I print_info: n_ctx_train      = 512
0.00.019.039 I print_info: n_embd           = 384
0.00.019.039 I print_info: n_layer          = 12
0.00.019.041 I print_info: n_head           = 12
0.00.019.042 I print_info: n_head_kv        = 12
0.00.019.042 I print_info: n_rot            = 32
0.00.019.044 I print_info: n_swa            = 0
0.00.019.044 I print_info: n_embd_head_k    = 32
0.00.019.044 I print_info: n_embd_head_v    = 32
0.00.019.045 I print_info: n_gqa            = 1
0.00.019.045 I print_info: n_embd_k_gqa     = 384
0.00.019.046 I print_info: n_embd_v_gqa     = 384
0.00.019.046 I print_info: f_norm_eps       = 1.0e-12
0.00.019.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.047 I print_info: f_logit_scale    = 0.0e+00
0.00.019.047 I print_info: n_ff             = 1536
0.00.019.048 I print_info: n_expert         = 0
0.00.019.048 I print_info: n_expert_used    = 0
0.00.019.048 I print_info: causal attn      = 0
0.00.019.048 I print_info: pooling type     = 2
0.00.019.048 I print_info: rope type        = 2
0.00.019.048 I print_info: rope scaling     = linear
0.00.019.049 I print_info: freq_base_train  = 10000.0
0.00.019.049 I print_info: freq_scale_train = 1
0.00.019.049 I print_info: n_ctx_orig_yarn  = 512
0.00.019.049 I print_info: rope_finetuned   = unknown
0.00.019.049 I print_info: ssm_d_conv       = 0
0.00.019.050 I print_info: ssm_d_inner      = 0
0.00.019.050 I print_info: ssm_d_state      = 0
0.00.019.050 I print_info: ssm_dt_rank      = 0
0.00.019.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.050 I print_info: model type       = 33M
0.00.019.050 I print_info: model params     = 33.21 M
0.00.019.050 I print_info: general.name     = Bge Small
0.00.019.051 I print_info: vocab type       = WPM
0.00.019.051 I print_info: n_vocab          = 30522
0.00.019.051 I print_info: n_merges         = 0
0.00.019.051 I print_info: UNK token        = 100 '[UNK]'
0.00.019.051 I print_info: SEP token        = 102 '[SEP]'
0.00.019.052 I print_info: PAD token        = 0 '[PAD]'
0.00.019.052 I print_info: CLS token        = 101 '[CLS]'
0.00.019.052 I print_info: MASK token       = 103 '[MASK]'
0.00.019.052 I print_info: LF token         = 0 '[PAD]'
0.00.019.052 I print_info: max token length = 21
0.00.020.318 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.322 I load_tensors: offloading output layer to GPU
0.00.020.322 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.330 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.331 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.474 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.474 I llama_new_context_with_model: n_ctx         = 512
0.00.020.474 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.475 I llama_new_context_with_model: n_batch       = 2048
0.00.020.475 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.475 I llama_new_context_with_model: flash_attn    = 0
0.00.020.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.476 I llama_new_context_with_model: freq_scale    = 1
0.00.020.476 I ggml_metal_init: allocating
0.00.020.479 I ggml_metal_init: found device: Apple M4
0.00.020.482 I ggml_metal_init: picking default device: Apple M4
0.00.021.073 I ggml_metal_init: using embedded metal library
0.00.023.607 I ggml_metal_init: GPU name:   Apple M4
0.00.023.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.610 I ggml_metal_init: simdgroup reduction   = true
0.00.023.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.610 I ggml_metal_init: has bfloat            = true
0.00.023.610 I ggml_metal_init: use bfloat            = true
0.00.023.611 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.611 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.014 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.496 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.498 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.506 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.127 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.128 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.129 I llama_new_context_with_model: graph nodes  = 429
0.00.035.129 I llama_new_context_with_model: graph splits = 2
0.00.035.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.367 I 
0.00.039.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.915 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.937 I llama_perf_context_print:        load time =      30.03 ms
0.00.044.938 I llama_perf_context_print: prompt eval time =       4.91 ms /     9 tokens (    0.55 ms per token,  1833.74 tokens per second)
0.00.044.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.939 I llama_perf_context_print:       total time =       5.57 ms /    10 tokens
0.00.045.112 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.032s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.222 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.396 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.222 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.229 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.231 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.232 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.232 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.234 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.235 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.235 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.236 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.237 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.240 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.240 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.241 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.606 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.606 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.606 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.607 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.607 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.607 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.608 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.608 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.609 I llama_model_loader: - type  f32:   40 tensors
0.00.050.609 I llama_model_loader: - type  f16:   30 tensors
0.00.050.610 I print_info: file format = GGUF V3 (latest)
0.00.050.610 I print_info: file type   = F16
0.00.050.612 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.066.900 W load: empty token at index 5
0.00.071.299 W load: model vocab missing newline token, using special_pad_id instead
0.00.072.638 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.664 I load: special tokens cache size = 5
0.00.332.041 I load: token to piece cache size = 1.5060 MB
0.00.332.072 I print_info: arch             = jina-bert-v2
0.00.332.073 I print_info: n_vocab (hp)     = 61056
0.00.332.073 I print_info: vocab_only       = 0
0.00.332.073 I print_info: n_ctx_train      = 8192
0.00.332.074 I print_info: n_embd           = 384
0.00.332.074 I print_info: n_layer          = 4
0.00.332.079 I print_info: n_head           = 12
0.00.332.080 I print_info: n_head_kv        = 12
0.00.332.080 I print_info: n_rot            = 32
0.00.332.080 I print_info: n_swa            = 0
0.00.332.080 I print_info: n_embd_head_k    = 32
0.00.332.081 I print_info: n_embd_head_v    = 32
0.00.332.081 I print_info: n_gqa            = 1
0.00.332.082 I print_info: n_embd_k_gqa     = 384
0.00.332.085 I print_info: n_embd_v_gqa     = 384
0.00.332.086 I print_info: f_norm_eps       = 1.0e-12
0.00.332.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.332.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.332.088 I print_info: f_max_alibi_bias = 8.0e+00
0.00.332.088 I print_info: f_logit_scale    = 0.0e+00
0.00.332.089 I print_info: n_ff             = 1536
0.00.332.089 I print_info: n_expert         = 0
0.00.332.091 I print_info: n_expert_used    = 0
0.00.332.091 I print_info: causal attn      = 0
0.00.332.091 I print_info: pooling type     = -1
0.00.332.091 I print_info: rope type        = -1
0.00.332.091 I print_info: rope scaling     = linear
0.00.332.092 I print_info: freq_base_train  = 10000.0
0.00.332.092 I print_info: freq_scale_train = 1
0.00.332.092 I print_info: n_ctx_orig_yarn  = 8192
0.00.332.092 I print_info: rope_finetuned   = unknown
0.00.332.093 I print_info: ssm_d_conv       = 0
0.00.332.093 I print_info: ssm_d_inner      = 0
0.00.332.093 I print_info: ssm_d_state      = 0
0.00.332.093 I print_info: ssm_dt_rank      = 0
0.00.332.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.332.093 I print_info: model type       = 33M
0.00.332.093 I print_info: model params     = 32.90 M
0.00.332.094 I print_info: general.name     = Jina Bert Implementation
0.00.332.095 I print_info: vocab type       = BPE
0.00.332.095 I print_info: n_vocab          = 61056
0.00.332.095 I print_info: n_merges         = 39382
0.00.332.096 I print_info: BOS token        = 0 '<s>'
0.00.332.096 I print_info: EOS token        = 2 '</s>'
0.00.332.096 I print_info: UNK token        = 3 '<unk>'
0.00.332.097 I print_info: SEP token        = 2 '</s>'
0.00.332.097 I print_info: PAD token        = 1 '<pad>'
0.00.332.101 I print_info: CLS token        = 0 '<s>'
0.00.332.102 I print_info: MASK token       = 4 '<mask>'
0.00.332.102 I print_info: EOG token        = 2 '</s>'
0.00.332.102 I print_info: max token length = 45
0.00.333.388 I load_tensors: offloading 4 repeating layers to GPU
0.00.333.388 I load_tensors: offloading output layer to GPU
0.00.333.388 I load_tensors: offloaded 5/5 layers to GPU
0.00.333.412 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.413 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.333.758 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.759 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.759 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.759 I llama_new_context_with_model: n_batch       = 2048
0.00.333.760 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.760 I llama_new_context_with_model: flash_attn    = 0
0.00.333.760 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.761 I llama_new_context_with_model: freq_scale    = 1
0.00.333.761 I ggml_metal_init: allocating
0.00.333.765 I ggml_metal_init: found device: Apple M4
0.00.333.767 I ggml_metal_init: picking default device: Apple M4
0.00.334.791 I ggml_metal_init: using embedded metal library
0.00.349.334 I ggml_metal_init: GPU name:   Apple M4
0.00.349.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.336 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.337 I ggml_metal_init: simdgroup reduction   = true
0.00.349.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.337 I ggml_metal_init: has bfloat            = true
0.00.349.337 I ggml_metal_init: use bfloat            = true
0.00.349.337 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.338 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.357.800 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.360.256 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.360.259 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.360.282 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.360.956 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.360.957 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.360.957 I llama_new_context_with_model: graph nodes  = 154
0.00.360.957 I llama_new_context_with_model: graph splits = 2
0.00.360.958 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.360.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.038 I 
0.00.373.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.226 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.373.227 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.373.239 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.373.240 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.373.244 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.373.244 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.373.729 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.377.218 I llama_perf_context_print:        load time =     349.63 ms
0.00.377.219 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17826.34 tokens per second)
0.00.377.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.377.222 I llama_perf_context_print:       total time =       4.18 ms /    63 tokens
0.00.377.484 I ggml_metal_free: deallocating

real	0m1.093s
user	0m0.338s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.167 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.316 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.027.911 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.629 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.664 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.452 I llama_model_loader: - type  f32:  194 tensors
0.00.060.453 I llama_model_loader: - type  f16:   98 tensors
0.00.060.454 I print_info: file format = GGUF V3 (latest)
0.00.060.455 I print_info: file type   = all F32 (guessed)
0.00.060.456 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.088.344 I load: special tokens cache size = 25
0.00.094.923 I load: token to piece cache size = 0.2984 MB
0.00.094.945 I print_info: arch             = gptneox
0.00.094.946 I print_info: n_vocab (hp)     = 50304
0.00.094.946 I print_info: vocab_only       = 0
0.00.094.946 I print_info: n_ctx_train      = 2048
0.00.094.946 I print_info: n_embd           = 2048
0.00.094.947 I print_info: n_layer          = 24
0.00.094.949 I print_info: n_head           = 16
0.00.094.950 I print_info: n_head_kv        = 16
0.00.094.950 I print_info: n_rot            = 32
0.00.094.950 I print_info: n_swa            = 0
0.00.094.950 I print_info: n_embd_head_k    = 128
0.00.094.950 I print_info: n_embd_head_v    = 128
0.00.094.951 I print_info: n_gqa            = 1
0.00.094.952 I print_info: n_embd_k_gqa     = 2048
0.00.094.953 I print_info: n_embd_v_gqa     = 2048
0.00.094.953 I print_info: f_norm_eps       = 1.0e-05
0.00.094.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.094.954 I print_info: f_clamp_kqv      = 0.0e+00
0.00.094.955 I print_info: f_max_alibi_bias = 0.0e+00
0.00.094.956 I print_info: f_logit_scale    = 0.0e+00
0.00.094.956 I print_info: n_ff             = 8192
0.00.094.956 I print_info: n_expert         = 0
0.00.094.957 I print_info: n_expert_used    = 0
0.00.094.957 I print_info: causal attn      = 1
0.00.094.957 I print_info: pooling type     = 0
0.00.094.958 I print_info: rope type        = 2
0.00.094.958 I print_info: rope scaling     = linear
0.00.094.959 I print_info: freq_base_train  = 10000.0
0.00.094.959 I print_info: freq_scale_train = 1
0.00.094.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.094.959 I print_info: rope_finetuned   = unknown
0.00.094.959 I print_info: ssm_d_conv       = 0
0.00.094.959 I print_info: ssm_d_inner      = 0
0.00.094.959 I print_info: ssm_d_state      = 0
0.00.094.960 I print_info: ssm_dt_rank      = 0
0.00.094.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.961 I print_info: model type       = 1.4B
0.00.094.961 I print_info: model params     = 1.41 B
0.00.094.962 I print_info: general.name     = 1.4B
0.00.094.962 I print_info: vocab type       = BPE
0.00.094.962 I print_info: n_vocab          = 50304
0.00.094.962 I print_info: n_merges         = 50009
0.00.094.962 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.962 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.963 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.963 I print_info: LF token         = 128 ''
0.00.094.967 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.967 I print_info: max token length = 1024
0.00.097.010 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.010 I load_tensors: offloading output layer to GPU
0.00.097.010 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.028 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.029 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.097.295 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.296 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.296 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.296 I llama_new_context_with_model: n_batch       = 2048
0.00.097.296 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.297 I llama_new_context_with_model: flash_attn    = 0
0.00.097.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.297 I llama_new_context_with_model: freq_scale    = 1
0.00.097.298 I ggml_metal_init: allocating
0.00.097.301 I ggml_metal_init: found device: Apple M4
0.00.097.303 I ggml_metal_init: picking default device: Apple M4
0.00.097.998 I ggml_metal_init: using embedded metal library
0.00.107.826 I ggml_metal_init: GPU name:   Apple M4
0.00.107.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.829 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.829 I ggml_metal_init: simdgroup reduction   = true
0.00.107.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.829 I ggml_metal_init: has bfloat            = true
0.00.107.829 I ggml_metal_init: use bfloat            = true
0.00.107.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.443 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.160.698 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.705 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.748 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.731 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.732 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.733 I llama_new_context_with_model: graph nodes  = 967
0.00.161.733 I llama_new_context_with_model: graph splits = 2
0.00.161.736 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.238.532 I main: llama threadpool init, n_threads = 4
0.00.238.574 I 
0.00.238.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.238.610 I 
0.00.238.687 I sampler seed: 1234
0.00.238.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.238.715 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.238.716 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.238.716 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.037.494 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.02.037.496 I llama_perf_context_print:        load time =     210.61 ms
0.02.037.497 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.58 tokens per second)
0.02.037.497 I llama_perf_context_print:        eval time =    1752.23 ms /    63 runs   (   27.81 ms per token,    35.95 tokens per second)
0.02.037.498 I llama_perf_context_print:       total time =    1798.97 ms /    70 tokens
0.02.037.714 I ggml_metal_free: deallocating

real	0m2.355s
user	0m0.148s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.525 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.367 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.289 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.292 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.792 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.797 I llama_model_loader: - type  f32:  194 tensors
0.00.056.797 I llama_model_loader: - type  f16:   98 tensors
0.00.056.798 I print_info: file format = GGUF V3 (latest)
0.00.056.799 I print_info: file type   = all F32 (guessed)
0.00.056.800 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.550 I load: special tokens cache size = 25
0.00.089.930 I load: token to piece cache size = 0.2984 MB
0.00.089.943 I print_info: arch             = gptneox
0.00.089.944 I print_info: n_vocab (hp)     = 50304
0.00.089.944 I print_info: vocab_only       = 0
0.00.089.945 I print_info: n_ctx_train      = 2048
0.00.089.945 I print_info: n_embd           = 2048
0.00.089.945 I print_info: n_layer          = 24
0.00.089.947 I print_info: n_head           = 16
0.00.089.948 I print_info: n_head_kv        = 16
0.00.089.948 I print_info: n_rot            = 32
0.00.089.948 I print_info: n_swa            = 0
0.00.089.948 I print_info: n_embd_head_k    = 128
0.00.089.948 I print_info: n_embd_head_v    = 128
0.00.089.949 I print_info: n_gqa            = 1
0.00.089.950 I print_info: n_embd_k_gqa     = 2048
0.00.089.950 I print_info: n_embd_v_gqa     = 2048
0.00.089.951 I print_info: f_norm_eps       = 1.0e-05
0.00.089.953 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.953 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.953 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.953 I print_info: f_logit_scale    = 0.0e+00
0.00.089.954 I print_info: n_ff             = 8192
0.00.089.954 I print_info: n_expert         = 0
0.00.089.954 I print_info: n_expert_used    = 0
0.00.089.954 I print_info: causal attn      = 1
0.00.089.954 I print_info: pooling type     = 0
0.00.089.955 I print_info: rope type        = 2
0.00.089.955 I print_info: rope scaling     = linear
0.00.089.955 I print_info: freq_base_train  = 10000.0
0.00.089.955 I print_info: freq_scale_train = 1
0.00.089.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.956 I print_info: rope_finetuned   = unknown
0.00.089.956 I print_info: ssm_d_conv       = 0
0.00.089.956 I print_info: ssm_d_inner      = 0
0.00.089.956 I print_info: ssm_d_state      = 0
0.00.089.956 I print_info: ssm_dt_rank      = 0
0.00.089.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.957 I print_info: model type       = 1.4B
0.00.089.957 I print_info: model params     = 1.41 B
0.00.089.957 I print_info: general.name     = 1.4B
0.00.089.957 I print_info: vocab type       = BPE
0.00.089.959 I print_info: n_vocab          = 50304
0.00.089.960 I print_info: n_merges         = 50009
0.00.089.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.960 I print_info: LF token         = 128 ''
0.00.089.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.961 I print_info: max token length = 1024
0.00.091.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.888 I load_tensors: offloading output layer to GPU
0.00.091.888 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.898 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.900 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.174 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.174 I llama_new_context_with_model: n_ctx         = 128
0.00.092.175 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.175 I llama_new_context_with_model: n_batch       = 128
0.00.092.175 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.175 I llama_new_context_with_model: flash_attn    = 0
0.00.092.176 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.176 I llama_new_context_with_model: freq_scale    = 1
0.00.092.176 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.177 I ggml_metal_init: allocating
0.00.092.179 I ggml_metal_init: found device: Apple M4
0.00.092.181 I ggml_metal_init: picking default device: Apple M4
0.00.092.758 I ggml_metal_init: using embedded metal library
0.00.095.335 I ggml_metal_init: GPU name:   Apple M4
0.00.095.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.337 I ggml_metal_init: simdgroup reduction   = true
0.00.095.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.338 I ggml_metal_init: has bfloat            = true
0.00.095.338 I ggml_metal_init: use bfloat            = true
0.00.095.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.955 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.271 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.297 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.236 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.237 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.237 I llama_new_context_with_model: graph nodes  = 967
0.00.107.237 I llama_new_context_with_model: graph splits = 2
0.00.107.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.103.615 I 
0.01.103.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.103.658 I perplexity: tokenizing the input ..
0.01.113.325 I perplexity: tokenization took 9.665 ms
0.01.113.329 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.233.068 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.235.831 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.235.918 I llama_perf_context_print:        load time =    1079.24 ms
0.01.235.919 I llama_perf_context_print: prompt eval time =     119.37 ms /   128 tokens (    0.93 ms per token,  1072.31 tokens per second)
0.01.235.923 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.235.924 I llama_perf_context_print:       total time =     132.30 ms /   129 tokens
0.01.237.307 I ggml_metal_free: deallocating

real	0m1.435s
user	0m0.126s
sys	0m0.264s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.991 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.209 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.210 I llama_model_loader: - type  f32:  194 tensors
0.00.038.211 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.211 I print_info: file format = GGUF V3 (latest)
0.00.038.212 I print_info: file type   = Q8_0
0.00.038.214 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.999 I load: special tokens cache size = 25
0.00.069.247 I load: token to piece cache size = 0.2984 MB
0.00.069.264 I print_info: arch             = gptneox
0.00.069.265 I print_info: n_vocab (hp)     = 50304
0.00.069.265 I print_info: vocab_only       = 0
0.00.069.266 I print_info: n_ctx_train      = 2048
0.00.069.266 I print_info: n_embd           = 2048
0.00.069.266 I print_info: n_layer          = 24
0.00.069.272 I print_info: n_head           = 16
0.00.069.273 I print_info: n_head_kv        = 16
0.00.069.273 I print_info: n_rot            = 32
0.00.069.273 I print_info: n_swa            = 0
0.00.069.273 I print_info: n_embd_head_k    = 128
0.00.069.273 I print_info: n_embd_head_v    = 128
0.00.069.274 I print_info: n_gqa            = 1
0.00.069.275 I print_info: n_embd_k_gqa     = 2048
0.00.069.275 I print_info: n_embd_v_gqa     = 2048
0.00.069.276 I print_info: f_norm_eps       = 1.0e-05
0.00.069.276 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.276 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.277 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.277 I print_info: f_logit_scale    = 0.0e+00
0.00.069.278 I print_info: n_ff             = 8192
0.00.069.278 I print_info: n_expert         = 0
0.00.069.278 I print_info: n_expert_used    = 0
0.00.069.278 I print_info: causal attn      = 1
0.00.069.278 I print_info: pooling type     = 0
0.00.069.281 I print_info: rope type        = 2
0.00.069.281 I print_info: rope scaling     = linear
0.00.069.282 I print_info: freq_base_train  = 10000.0
0.00.069.282 I print_info: freq_scale_train = 1
0.00.069.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.283 I print_info: rope_finetuned   = unknown
0.00.069.283 I print_info: ssm_d_conv       = 0
0.00.069.283 I print_info: ssm_d_inner      = 0
0.00.069.283 I print_info: ssm_d_state      = 0
0.00.069.283 I print_info: ssm_dt_rank      = 0
0.00.069.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.284 I print_info: model type       = 1.4B
0.00.069.284 I print_info: model params     = 1.41 B
0.00.069.285 I print_info: general.name     = 1.4B
0.00.069.286 I print_info: vocab type       = BPE
0.00.069.286 I print_info: n_vocab          = 50304
0.00.069.286 I print_info: n_merges         = 50009
0.00.069.286 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.291 I print_info: LF token         = 128 ''
0.00.069.292 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.292 I print_info: max token length = 1024
0.00.071.605 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.606 I load_tensors: offloading output layer to GPU
0.00.071.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.617 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.618 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.071.968 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.969 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.969 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.969 I llama_new_context_with_model: n_batch       = 2048
0.00.071.970 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.970 I llama_new_context_with_model: flash_attn    = 0
0.00.071.970 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.970 I llama_new_context_with_model: freq_scale    = 1
0.00.071.971 I ggml_metal_init: allocating
0.00.071.976 I ggml_metal_init: found device: Apple M4
0.00.071.978 I ggml_metal_init: picking default device: Apple M4
0.00.072.802 I ggml_metal_init: using embedded metal library
0.00.075.842 I ggml_metal_init: GPU name:   Apple M4
0.00.075.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.845 I ggml_metal_init: simdgroup reduction   = true
0.00.075.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.845 I ggml_metal_init: has bfloat            = true
0.00.075.845 I ggml_metal_init: use bfloat            = true
0.00.075.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.182 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.362 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.370 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.595 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.597 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.598 I llama_new_context_with_model: graph nodes  = 967
0.00.113.598 I llama_new_context_with_model: graph splits = 2
0.00.113.602 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.362.922 I main: llama threadpool init, n_threads = 4
0.01.362.962 I 
0.01.362.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.362.983 I 
0.01.363.161 I sampler seed: 1234
0.01.363.166 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.363.182 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.363.183 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.363.183 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.449.299 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.02.449.300 I llama_perf_context_print:        load time =    1352.93 ms
0.02.449.301 I llama_perf_context_print: prompt eval time =      40.18 ms /     7 tokens (    5.74 ms per token,   174.21 tokens per second)
0.02.449.301 I llama_perf_context_print:        eval time =    1042.98 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.449.302 I llama_perf_context_print:       total time =    1086.38 ms /    70 tokens
0.02.449.528 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.117s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.150 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.525 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.464 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.467 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.321 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.322 I llama_model_loader: - type  f32:  194 tensors
0.00.038.323 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.323 I print_info: file format = GGUF V3 (latest)
0.00.038.324 I print_info: file type   = Q8_0
0.00.038.326 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.428 I load: special tokens cache size = 25
0.00.066.257 I load: token to piece cache size = 0.2984 MB
0.00.066.272 I print_info: arch             = gptneox
0.00.066.274 I print_info: n_vocab (hp)     = 50304
0.00.066.274 I print_info: vocab_only       = 0
0.00.066.274 I print_info: n_ctx_train      = 2048
0.00.066.274 I print_info: n_embd           = 2048
0.00.066.274 I print_info: n_layer          = 24
0.00.066.280 I print_info: n_head           = 16
0.00.066.280 I print_info: n_head_kv        = 16
0.00.066.281 I print_info: n_rot            = 32
0.00.066.281 I print_info: n_swa            = 0
0.00.066.281 I print_info: n_embd_head_k    = 128
0.00.066.281 I print_info: n_embd_head_v    = 128
0.00.066.282 I print_info: n_gqa            = 1
0.00.066.283 I print_info: n_embd_k_gqa     = 2048
0.00.066.283 I print_info: n_embd_v_gqa     = 2048
0.00.066.284 I print_info: f_norm_eps       = 1.0e-05
0.00.066.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.285 I print_info: f_logit_scale    = 0.0e+00
0.00.066.285 I print_info: n_ff             = 8192
0.00.066.286 I print_info: n_expert         = 0
0.00.066.286 I print_info: n_expert_used    = 0
0.00.066.286 I print_info: causal attn      = 1
0.00.066.286 I print_info: pooling type     = 0
0.00.066.286 I print_info: rope type        = 2
0.00.066.286 I print_info: rope scaling     = linear
0.00.066.287 I print_info: freq_base_train  = 10000.0
0.00.066.287 I print_info: freq_scale_train = 1
0.00.066.287 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.287 I print_info: rope_finetuned   = unknown
0.00.066.287 I print_info: ssm_d_conv       = 0
0.00.066.287 I print_info: ssm_d_inner      = 0
0.00.066.287 I print_info: ssm_d_state      = 0
0.00.066.288 I print_info: ssm_dt_rank      = 0
0.00.066.288 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.288 I print_info: model type       = 1.4B
0.00.066.288 I print_info: model params     = 1.41 B
0.00.066.288 I print_info: general.name     = 1.4B
0.00.066.291 I print_info: vocab type       = BPE
0.00.066.291 I print_info: n_vocab          = 50304
0.00.066.291 I print_info: n_merges         = 50009
0.00.066.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.292 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.292 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.292 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.292 I print_info: LF token         = 128 ''
0.00.066.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.293 I print_info: max token length = 1024
0.00.068.501 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.501 I load_tensors: offloading output layer to GPU
0.00.068.501 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.512 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.513 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.827 I llama_new_context_with_model: n_ctx         = 128
0.00.068.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.827 I llama_new_context_with_model: n_batch       = 128
0.00.068.828 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.828 I llama_new_context_with_model: flash_attn    = 0
0.00.068.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.828 I llama_new_context_with_model: freq_scale    = 1
0.00.068.829 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.829 I ggml_metal_init: allocating
0.00.068.831 I ggml_metal_init: found device: Apple M4
0.00.068.833 I ggml_metal_init: picking default device: Apple M4
0.00.069.545 I ggml_metal_init: using embedded metal library
0.00.072.001 I ggml_metal_init: GPU name:   Apple M4
0.00.072.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.003 I ggml_metal_init: simdgroup reduction   = true
0.00.072.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.004 I ggml_metal_init: has bfloat            = true
0.00.072.004 I ggml_metal_init: use bfloat            = true
0.00.072.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.645 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.675 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.707 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.711 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.711 I llama_new_context_with_model: graph nodes  = 967
0.00.083.711 I llama_new_context_with_model: graph splits = 2
0.00.083.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.047.036 I 
0.01.047.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.047.070 I perplexity: tokenizing the input ..
0.01.054.360 I perplexity: tokenization took 7.289 ms
0.01.054.364 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.178.698 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.179.792 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.179.818 I llama_perf_context_print:        load time =    1032.50 ms
0.01.179.821 I llama_perf_context_print: prompt eval time =     124.10 ms /   128 tokens (    0.97 ms per token,  1031.39 tokens per second)
0.01.179.822 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.179.822 I llama_perf_context_print:       total time =     132.78 ms /   129 tokens
0.01.180.298 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.092s
sys	0m0.205s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.012.240 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.540 I llama_model_loader: - type  f32:  194 tensors
0.00.036.540 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.541 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.541 I print_info: file format = GGUF V3 (latest)
0.00.036.542 I print_info: file type   = Q4_0
0.00.036.543 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.059.180 I load: special tokens cache size = 25
0.00.065.816 I load: token to piece cache size = 0.2984 MB
0.00.065.832 I print_info: arch             = gptneox
0.00.065.833 I print_info: n_vocab (hp)     = 50304
0.00.065.833 I print_info: vocab_only       = 0
0.00.065.833 I print_info: n_ctx_train      = 2048
0.00.065.833 I print_info: n_embd           = 2048
0.00.065.834 I print_info: n_layer          = 24
0.00.065.838 I print_info: n_head           = 16
0.00.065.838 I print_info: n_head_kv        = 16
0.00.065.840 I print_info: n_rot            = 32
0.00.065.840 I print_info: n_swa            = 0
0.00.065.840 I print_info: n_embd_head_k    = 128
0.00.065.840 I print_info: n_embd_head_v    = 128
0.00.065.841 I print_info: n_gqa            = 1
0.00.065.842 I print_info: n_embd_k_gqa     = 2048
0.00.065.842 I print_info: n_embd_v_gqa     = 2048
0.00.065.843 I print_info: f_norm_eps       = 1.0e-05
0.00.065.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.844 I print_info: f_logit_scale    = 0.0e+00
0.00.065.844 I print_info: n_ff             = 8192
0.00.065.844 I print_info: n_expert         = 0
0.00.065.845 I print_info: n_expert_used    = 0
0.00.065.845 I print_info: causal attn      = 1
0.00.065.845 I print_info: pooling type     = 0
0.00.065.845 I print_info: rope type        = 2
0.00.065.845 I print_info: rope scaling     = linear
0.00.065.845 I print_info: freq_base_train  = 10000.0
0.00.065.846 I print_info: freq_scale_train = 1
0.00.065.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.846 I print_info: rope_finetuned   = unknown
0.00.065.846 I print_info: ssm_d_conv       = 0
0.00.065.846 I print_info: ssm_d_inner      = 0
0.00.065.846 I print_info: ssm_d_state      = 0
0.00.065.846 I print_info: ssm_dt_rank      = 0
0.00.065.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.847 I print_info: model type       = 1.4B
0.00.065.848 I print_info: model params     = 1.41 B
0.00.065.850 I print_info: general.name     = 1.4B
0.00.065.850 I print_info: vocab type       = BPE
0.00.065.850 I print_info: n_vocab          = 50304
0.00.065.851 I print_info: n_merges         = 50009
0.00.065.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.851 I print_info: LF token         = 128 ''
0.00.065.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.852 I print_info: max token length = 1024
0.00.068.048 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.049 I load_tensors: offloading output layer to GPU
0.00.068.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.060 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.068.061 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.068.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.383 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.383 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.383 I llama_new_context_with_model: n_batch       = 2048
0.00.068.383 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.383 I llama_new_context_with_model: flash_attn    = 0
0.00.068.384 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.384 I llama_new_context_with_model: freq_scale    = 1
0.00.068.385 I ggml_metal_init: allocating
0.00.068.387 I ggml_metal_init: found device: Apple M4
0.00.068.389 I ggml_metal_init: picking default device: Apple M4
0.00.069.185 I ggml_metal_init: using embedded metal library
0.00.072.079 I ggml_metal_init: GPU name:   Apple M4
0.00.072.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.082 I ggml_metal_init: simdgroup reduction   = true
0.00.072.082 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.083 I ggml_metal_init: has bfloat            = true
0.00.072.083 I ggml_metal_init: use bfloat            = true
0.00.072.083 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.411 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.181 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.188 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.225 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.385 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.389 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.389 I llama_new_context_with_model: graph nodes  = 967
0.00.109.389 I llama_new_context_with_model: graph splits = 2
0.00.109.393 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.342 I main: llama threadpool init, n_threads = 4
0.00.762.379 I 
0.00.762.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.401 I 
0.00.762.610 I sampler seed: 1234
0.00.762.616 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.625 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.626 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.626 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.438.994 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.438.994 I llama_perf_context_print:        load time =     750.10 ms
0.01.438.995 I llama_perf_context_print: prompt eval time =      40.05 ms /     7 tokens (    5.72 ms per token,   174.78 tokens per second)
0.01.438.996 I llama_perf_context_print:        eval time =     633.32 ms /    63 runs   (   10.05 ms per token,    99.48 tokens per second)
0.01.438.996 I llama_perf_context_print:       total time =     676.65 ms /    70 tokens
0.01.439.220 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.115s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.538 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.920 I llama_model_loader: - type  f32:  194 tensors
0.00.024.920 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.921 I print_info: file format = GGUF V3 (latest)
0.00.024.921 I print_info: file type   = Q4_0
0.00.024.922 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.125 I load: special tokens cache size = 25
0.00.049.851 I load: token to piece cache size = 0.2984 MB
0.00.049.865 I print_info: arch             = gptneox
0.00.049.866 I print_info: n_vocab (hp)     = 50304
0.00.049.867 I print_info: vocab_only       = 0
0.00.049.867 I print_info: n_ctx_train      = 2048
0.00.049.867 I print_info: n_embd           = 2048
0.00.049.867 I print_info: n_layer          = 24
0.00.049.870 I print_info: n_head           = 16
0.00.049.870 I print_info: n_head_kv        = 16
0.00.049.871 I print_info: n_rot            = 32
0.00.049.871 I print_info: n_swa            = 0
0.00.049.871 I print_info: n_embd_head_k    = 128
0.00.049.871 I print_info: n_embd_head_v    = 128
0.00.049.872 I print_info: n_gqa            = 1
0.00.049.873 I print_info: n_embd_k_gqa     = 2048
0.00.049.873 I print_info: n_embd_v_gqa     = 2048
0.00.049.874 I print_info: f_norm_eps       = 1.0e-05
0.00.049.874 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.876 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.876 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.877 I print_info: f_logit_scale    = 0.0e+00
0.00.049.878 I print_info: n_ff             = 8192
0.00.049.878 I print_info: n_expert         = 0
0.00.049.878 I print_info: n_expert_used    = 0
0.00.049.878 I print_info: causal attn      = 1
0.00.049.879 I print_info: pooling type     = 0
0.00.049.879 I print_info: rope type        = 2
0.00.049.879 I print_info: rope scaling     = linear
0.00.049.879 I print_info: freq_base_train  = 10000.0
0.00.049.880 I print_info: freq_scale_train = 1
0.00.049.880 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.880 I print_info: rope_finetuned   = unknown
0.00.049.880 I print_info: ssm_d_conv       = 0
0.00.049.880 I print_info: ssm_d_inner      = 0
0.00.049.880 I print_info: ssm_d_state      = 0
0.00.049.881 I print_info: ssm_dt_rank      = 0
0.00.049.881 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.881 I print_info: model type       = 1.4B
0.00.049.881 I print_info: model params     = 1.41 B
0.00.049.881 I print_info: general.name     = 1.4B
0.00.049.882 I print_info: vocab type       = BPE
0.00.049.882 I print_info: n_vocab          = 50304
0.00.049.882 I print_info: n_merges         = 50009
0.00.049.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.883 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.889 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.891 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.891 I print_info: LF token         = 128 ''
0.00.049.892 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.892 I print_info: max token length = 1024
0.00.051.660 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.660 I load_tensors: offloading output layer to GPU
0.00.051.660 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.670 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.671 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.922 I llama_new_context_with_model: n_ctx         = 128
0.00.051.922 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.922 I llama_new_context_with_model: n_batch       = 128
0.00.051.922 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.923 I llama_new_context_with_model: flash_attn    = 0
0.00.051.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.923 I llama_new_context_with_model: freq_scale    = 1
0.00.051.924 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.924 I ggml_metal_init: allocating
0.00.051.927 I ggml_metal_init: found device: Apple M4
0.00.051.929 I ggml_metal_init: picking default device: Apple M4
0.00.052.502 I ggml_metal_init: using embedded metal library
0.00.054.823 I ggml_metal_init: GPU name:   Apple M4
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.825 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.825 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.826 I ggml_metal_init: simdgroup reduction   = true
0.00.054.826 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.826 I ggml_metal_init: has bfloat            = true
0.00.054.826 I ggml_metal_init: use bfloat            = true
0.00.054.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.827 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.399 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.648 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.681 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.543 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.544 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.544 I llama_new_context_with_model: graph nodes  = 967
0.00.066.545 I llama_new_context_with_model: graph splits = 2
0.00.066.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.863 I 
0.00.665.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.896 I perplexity: tokenizing the input ..
0.00.673.857 I perplexity: tokenization took 7.96 ms
0.00.673.864 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.072 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.162 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.798.180 I llama_perf_context_print:        load time =     656.32 ms
0.00.798.181 I llama_perf_context_print: prompt eval time =     122.99 ms /   128 tokens (    0.96 ms per token,  1040.77 tokens per second)
0.00.798.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.182 I llama_perf_context_print:       total time =     132.32 ms /   129 tokens
0.00.798.536 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.078s
sys	0m0.125s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.222 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.189 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.205 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.858 I llama_model_loader: - type  f32:  194 tensors
0.00.025.858 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.858 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.859 I print_info: file format = GGUF V3 (latest)
0.00.025.859 I print_info: file type   = Q4_1
0.00.025.860 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.312 I load: special tokens cache size = 25
0.00.050.088 I load: token to piece cache size = 0.2984 MB
0.00.050.102 I print_info: arch             = gptneox
0.00.050.103 I print_info: n_vocab (hp)     = 50304
0.00.050.103 I print_info: vocab_only       = 0
0.00.050.104 I print_info: n_ctx_train      = 2048
0.00.050.104 I print_info: n_embd           = 2048
0.00.050.104 I print_info: n_layer          = 24
0.00.050.107 I print_info: n_head           = 16
0.00.050.108 I print_info: n_head_kv        = 16
0.00.050.108 I print_info: n_rot            = 32
0.00.050.108 I print_info: n_swa            = 0
0.00.050.108 I print_info: n_embd_head_k    = 128
0.00.050.108 I print_info: n_embd_head_v    = 128
0.00.050.109 I print_info: n_gqa            = 1
0.00.050.111 I print_info: n_embd_k_gqa     = 2048
0.00.050.112 I print_info: n_embd_v_gqa     = 2048
0.00.050.113 I print_info: f_norm_eps       = 1.0e-05
0.00.050.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.114 I print_info: f_logit_scale    = 0.0e+00
0.00.050.114 I print_info: n_ff             = 8192
0.00.050.114 I print_info: n_expert         = 0
0.00.050.114 I print_info: n_expert_used    = 0
0.00.050.115 I print_info: causal attn      = 1
0.00.050.115 I print_info: pooling type     = 0
0.00.050.115 I print_info: rope type        = 2
0.00.050.115 I print_info: rope scaling     = linear
0.00.050.116 I print_info: freq_base_train  = 10000.0
0.00.050.117 I print_info: freq_scale_train = 1
0.00.050.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.117 I print_info: rope_finetuned   = unknown
0.00.050.117 I print_info: ssm_d_conv       = 0
0.00.050.117 I print_info: ssm_d_inner      = 0
0.00.050.117 I print_info: ssm_d_state      = 0
0.00.050.117 I print_info: ssm_dt_rank      = 0
0.00.050.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.119 I print_info: model type       = 1.4B
0.00.050.119 I print_info: model params     = 1.41 B
0.00.050.119 I print_info: general.name     = 1.4B
0.00.050.119 I print_info: vocab type       = BPE
0.00.050.120 I print_info: n_vocab          = 50304
0.00.050.120 I print_info: n_merges         = 50009
0.00.050.120 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.121 I print_info: LF token         = 128 ''
0.00.050.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.121 I print_info: max token length = 1024
0.00.051.818 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.818 I load_tensors: offloading output layer to GPU
0.00.051.818 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.828 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.829 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.088 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.089 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.089 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.089 I llama_new_context_with_model: n_batch       = 2048
0.00.052.089 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.089 I llama_new_context_with_model: flash_attn    = 0
0.00.052.090 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.090 I llama_new_context_with_model: freq_scale    = 1
0.00.052.090 I ggml_metal_init: allocating
0.00.052.093 I ggml_metal_init: found device: Apple M4
0.00.052.095 I ggml_metal_init: picking default device: Apple M4
0.00.052.668 I ggml_metal_init: using embedded metal library
0.00.054.996 I ggml_metal_init: GPU name:   Apple M4
0.00.054.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.998 I ggml_metal_init: simdgroup reduction   = true
0.00.054.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.999 I ggml_metal_init: has bfloat            = true
0.00.054.999 I ggml_metal_init: use bfloat            = true
0.00.054.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.445 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.451 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.482 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.412 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.412 I llama_new_context_with_model: graph nodes  = 967
0.00.083.413 I llama_new_context_with_model: graph splits = 2
0.00.083.416 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.891 I main: llama threadpool init, n_threads = 4
0.00.761.930 I 
0.00.761.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.952 I 
0.00.762.104 I sampler seed: 1234
0.00.762.109 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.120 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.122 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.487.387 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65498.15 tokens per second)
0.01.487.388 I llama_perf_context_print:        load time =     752.66 ms
0.01.487.388 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.57 tokens per second)
0.01.487.390 I llama_perf_context_print:        eval time =     678.56 ms /    63 runs   (   10.77 ms per token,    92.84 tokens per second)
0.01.487.390 I llama_perf_context_print:       total time =     725.50 ms /    70 tokens
0.01.487.580 I ggml_metal_free: deallocating

real	0m1.505s
user	0m0.107s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.243 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.444 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.023 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.026 I llama_model_loader: - type  f32:  194 tensors
0.00.025.026 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.027 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.027 I print_info: file format = GGUF V3 (latest)
0.00.025.027 I print_info: file type   = Q4_1
0.00.025.028 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.123 I load: special tokens cache size = 25
0.00.049.930 I load: token to piece cache size = 0.2984 MB
0.00.049.946 I print_info: arch             = gptneox
0.00.049.947 I print_info: n_vocab (hp)     = 50304
0.00.049.948 I print_info: vocab_only       = 0
0.00.049.948 I print_info: n_ctx_train      = 2048
0.00.049.948 I print_info: n_embd           = 2048
0.00.049.948 I print_info: n_layer          = 24
0.00.049.951 I print_info: n_head           = 16
0.00.049.951 I print_info: n_head_kv        = 16
0.00.049.952 I print_info: n_rot            = 32
0.00.049.952 I print_info: n_swa            = 0
0.00.049.952 I print_info: n_embd_head_k    = 128
0.00.049.952 I print_info: n_embd_head_v    = 128
0.00.049.953 I print_info: n_gqa            = 1
0.00.049.954 I print_info: n_embd_k_gqa     = 2048
0.00.049.954 I print_info: n_embd_v_gqa     = 2048
0.00.049.955 I print_info: f_norm_eps       = 1.0e-05
0.00.049.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.955 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.956 I print_info: f_logit_scale    = 0.0e+00
0.00.049.956 I print_info: n_ff             = 8192
0.00.049.956 I print_info: n_expert         = 0
0.00.049.957 I print_info: n_expert_used    = 0
0.00.049.957 I print_info: causal attn      = 1
0.00.049.957 I print_info: pooling type     = 0
0.00.049.957 I print_info: rope type        = 2
0.00.049.957 I print_info: rope scaling     = linear
0.00.049.958 I print_info: freq_base_train  = 10000.0
0.00.049.958 I print_info: freq_scale_train = 1
0.00.049.958 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.958 I print_info: rope_finetuned   = unknown
0.00.049.958 I print_info: ssm_d_conv       = 0
0.00.049.958 I print_info: ssm_d_inner      = 0
0.00.049.959 I print_info: ssm_d_state      = 0
0.00.049.959 I print_info: ssm_dt_rank      = 0
0.00.049.959 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.959 I print_info: model type       = 1.4B
0.00.049.960 I print_info: model params     = 1.41 B
0.00.049.960 I print_info: general.name     = 1.4B
0.00.049.960 I print_info: vocab type       = BPE
0.00.049.960 I print_info: n_vocab          = 50304
0.00.049.961 I print_info: n_merges         = 50009
0.00.049.961 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.961 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.962 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.962 I print_info: LF token         = 128 ''
0.00.049.962 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.962 I print_info: max token length = 1024
0.00.051.727 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.727 I load_tensors: offloading output layer to GPU
0.00.051.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.737 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.738 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.001 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.001 I llama_new_context_with_model: n_ctx         = 128
0.00.052.002 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.002 I llama_new_context_with_model: n_batch       = 128
0.00.052.002 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.002 I llama_new_context_with_model: flash_attn    = 0
0.00.052.002 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.003 I llama_new_context_with_model: freq_scale    = 1
0.00.052.003 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.003 I ggml_metal_init: allocating
0.00.052.006 I ggml_metal_init: found device: Apple M4
0.00.052.008 I ggml_metal_init: picking default device: Apple M4
0.00.052.580 I ggml_metal_init: using embedded metal library
0.00.054.881 I ggml_metal_init: GPU name:   Apple M4
0.00.054.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.884 I ggml_metal_init: simdgroup reduction   = true
0.00.054.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.884 I ggml_metal_init: has bfloat            = true
0.00.054.884 I ggml_metal_init: use bfloat            = true
0.00.054.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.699 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.701 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.584 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.585 I llama_new_context_with_model: graph nodes  = 967
0.00.066.585 I llama_new_context_with_model: graph splits = 2
0.00.066.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.362 I 
0.00.718.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.406 I perplexity: tokenizing the input ..
0.00.725.901 I perplexity: tokenization took 7.493 ms
0.00.725.905 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.938 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.032 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.059 I llama_perf_context_print:        load time =     709.11 ms
0.00.850.060 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.23 tokens per second)
0.00.850.061 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.061 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.850.474 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.078s
sys	0m0.141s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.601 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.112 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.115 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.115 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.594 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.595 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.595 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.595 I llama_model_loader: - type  f32:  194 tensors
0.00.024.596 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.596 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.597 I print_info: file format = GGUF V3 (latest)
0.00.024.597 I print_info: file type   = Q5_0
0.00.024.598 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.053 I load: special tokens cache size = 25
0.00.048.754 I load: token to piece cache size = 0.2984 MB
0.00.048.768 I print_info: arch             = gptneox
0.00.048.769 I print_info: n_vocab (hp)     = 50304
0.00.048.769 I print_info: vocab_only       = 0
0.00.048.770 I print_info: n_ctx_train      = 2048
0.00.048.770 I print_info: n_embd           = 2048
0.00.048.770 I print_info: n_layer          = 24
0.00.048.773 I print_info: n_head           = 16
0.00.048.773 I print_info: n_head_kv        = 16
0.00.048.773 I print_info: n_rot            = 32
0.00.048.774 I print_info: n_swa            = 0
0.00.048.774 I print_info: n_embd_head_k    = 128
0.00.048.774 I print_info: n_embd_head_v    = 128
0.00.048.775 I print_info: n_gqa            = 1
0.00.048.776 I print_info: n_embd_k_gqa     = 2048
0.00.048.776 I print_info: n_embd_v_gqa     = 2048
0.00.048.777 I print_info: f_norm_eps       = 1.0e-05
0.00.048.777 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.778 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.778 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.778 I print_info: f_logit_scale    = 0.0e+00
0.00.048.779 I print_info: n_ff             = 8192
0.00.048.779 I print_info: n_expert         = 0
0.00.048.779 I print_info: n_expert_used    = 0
0.00.048.779 I print_info: causal attn      = 1
0.00.048.779 I print_info: pooling type     = 0
0.00.048.779 I print_info: rope type        = 2
0.00.048.780 I print_info: rope scaling     = linear
0.00.048.780 I print_info: freq_base_train  = 10000.0
0.00.048.780 I print_info: freq_scale_train = 1
0.00.048.780 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.781 I print_info: rope_finetuned   = unknown
0.00.048.781 I print_info: ssm_d_conv       = 0
0.00.048.781 I print_info: ssm_d_inner      = 0
0.00.048.782 I print_info: ssm_d_state      = 0
0.00.048.782 I print_info: ssm_dt_rank      = 0
0.00.048.782 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.782 I print_info: model type       = 1.4B
0.00.048.783 I print_info: model params     = 1.41 B
0.00.048.783 I print_info: general.name     = 1.4B
0.00.048.783 I print_info: vocab type       = BPE
0.00.048.783 I print_info: n_vocab          = 50304
0.00.048.784 I print_info: n_merges         = 50009
0.00.048.784 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.784 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.785 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.785 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.785 I print_info: LF token         = 128 ''
0.00.048.785 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.785 I print_info: max token length = 1024
0.00.050.471 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.472 I load_tensors: offloading output layer to GPU
0.00.050.472 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.483 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.050.484 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.772 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.773 I llama_new_context_with_model: n_ctx         = 2048
0.00.050.774 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.050.774 I llama_new_context_with_model: n_batch       = 2048
0.00.050.774 I llama_new_context_with_model: n_ubatch      = 512
0.00.050.774 I llama_new_context_with_model: flash_attn    = 0
0.00.050.775 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.775 I llama_new_context_with_model: freq_scale    = 1
0.00.050.775 I ggml_metal_init: allocating
0.00.050.779 I ggml_metal_init: found device: Apple M4
0.00.050.780 I ggml_metal_init: picking default device: Apple M4
0.00.051.354 I ggml_metal_init: using embedded metal library
0.00.053.683 I ggml_metal_init: GPU name:   Apple M4
0.00.053.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.686 I ggml_metal_init: simdgroup reduction   = true
0.00.053.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.686 I ggml_metal_init: has bfloat            = true
0.00.053.686 I ggml_metal_init: use bfloat            = true
0.00.053.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.265 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.525 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.577 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.592 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.594 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.594 I llama_new_context_with_model: graph nodes  = 967
0.00.083.594 I llama_new_context_with_model: graph splits = 2
0.00.083.597 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.488 I main: llama threadpool init, n_threads = 4
0.00.779.526 I 
0.00.779.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.559 I 
0.00.779.710 I sampler seed: 1234
0.00.779.715 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.745 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.747 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.747 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.565.452 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.565.453 I llama_perf_context_print:        load time =     770.88 ms
0.01.565.453 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.86 tokens per second)
0.01.565.455 I llama_perf_context_print:        eval time =     739.12 ms /    63 runs   (   11.73 ms per token,    85.24 tokens per second)
0.01.565.455 I llama_perf_context_print:       total time =     785.97 ms /    70 tokens
0.01.565.651 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.053 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.705 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.214 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.215 I llama_model_loader: - type  f32:  194 tensors
0.00.026.216 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.217 I print_info: file format = GGUF V3 (latest)
0.00.026.217 I print_info: file type   = Q5_0
0.00.026.218 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.593 I load: special tokens cache size = 25
0.00.050.368 I load: token to piece cache size = 0.2984 MB
0.00.050.382 I print_info: arch             = gptneox
0.00.050.383 I print_info: n_vocab (hp)     = 50304
0.00.050.383 I print_info: vocab_only       = 0
0.00.050.384 I print_info: n_ctx_train      = 2048
0.00.050.384 I print_info: n_embd           = 2048
0.00.050.384 I print_info: n_layer          = 24
0.00.050.387 I print_info: n_head           = 16
0.00.050.387 I print_info: n_head_kv        = 16
0.00.050.388 I print_info: n_rot            = 32
0.00.050.388 I print_info: n_swa            = 0
0.00.050.388 I print_info: n_embd_head_k    = 128
0.00.050.388 I print_info: n_embd_head_v    = 128
0.00.050.389 I print_info: n_gqa            = 1
0.00.050.390 I print_info: n_embd_k_gqa     = 2048
0.00.050.390 I print_info: n_embd_v_gqa     = 2048
0.00.050.391 I print_info: f_norm_eps       = 1.0e-05
0.00.050.391 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.391 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.392 I print_info: f_logit_scale    = 0.0e+00
0.00.050.392 I print_info: n_ff             = 8192
0.00.050.393 I print_info: n_expert         = 0
0.00.050.393 I print_info: n_expert_used    = 0
0.00.050.393 I print_info: causal attn      = 1
0.00.050.393 I print_info: pooling type     = 0
0.00.050.393 I print_info: rope type        = 2
0.00.050.393 I print_info: rope scaling     = linear
0.00.050.394 I print_info: freq_base_train  = 10000.0
0.00.050.394 I print_info: freq_scale_train = 1
0.00.050.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.394 I print_info: rope_finetuned   = unknown
0.00.050.394 I print_info: ssm_d_conv       = 0
0.00.050.394 I print_info: ssm_d_inner      = 0
0.00.050.394 I print_info: ssm_d_state      = 0
0.00.050.395 I print_info: ssm_dt_rank      = 0
0.00.050.395 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.395 I print_info: model type       = 1.4B
0.00.050.395 I print_info: model params     = 1.41 B
0.00.050.395 I print_info: general.name     = 1.4B
0.00.050.396 I print_info: vocab type       = BPE
0.00.050.396 I print_info: n_vocab          = 50304
0.00.050.396 I print_info: n_merges         = 50009
0.00.050.396 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.397 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.397 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.397 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.401 I print_info: LF token         = 128 ''
0.00.050.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.403 I print_info: max token length = 1024
0.00.052.135 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.136 I load_tensors: offloading output layer to GPU
0.00.052.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.146 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.147 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.428 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.429 I llama_new_context_with_model: n_ctx         = 128
0.00.052.429 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.429 I llama_new_context_with_model: n_batch       = 128
0.00.052.429 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.429 I llama_new_context_with_model: flash_attn    = 0
0.00.052.430 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.430 I llama_new_context_with_model: freq_scale    = 1
0.00.052.430 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.431 I ggml_metal_init: allocating
0.00.052.434 I ggml_metal_init: found device: Apple M4
0.00.052.436 I ggml_metal_init: picking default device: Apple M4
0.00.052.994 I ggml_metal_init: using embedded metal library
0.00.055.327 I ggml_metal_init: GPU name:   Apple M4
0.00.055.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.330 I ggml_metal_init: simdgroup reduction   = true
0.00.055.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.330 I ggml_metal_init: has bfloat            = true
0.00.055.330 I ggml_metal_init: use bfloat            = true
0.00.055.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.828 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.101 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.130 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.996 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.997 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.997 I llama_new_context_with_model: graph nodes  = 967
0.00.065.997 I llama_new_context_with_model: graph splits = 2
0.00.065.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.573 I 
0.00.730.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.609 I perplexity: tokenizing the input ..
0.00.737.962 I perplexity: tokenization took 7.352 ms
0.00.737.967 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.473 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.874.587 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.874.610 I llama_perf_context_print:        load time =     719.52 ms
0.00.874.611 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.14 tokens per second)
0.00.874.612 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.613 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.875.038 I ggml_metal_free: deallocating

real	0m0.890s
user	0m0.076s
sys	0m0.148s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.545 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.419 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.144 I llama_model_loader: - type  f32:  194 tensors
0.00.026.144 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.145 I print_info: file format = GGUF V3 (latest)
0.00.026.146 I print_info: file type   = Q5_1
0.00.026.146 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.625 I load: special tokens cache size = 25
0.00.051.622 I load: token to piece cache size = 0.2984 MB
0.00.051.636 I print_info: arch             = gptneox
0.00.051.636 I print_info: n_vocab (hp)     = 50304
0.00.051.637 I print_info: vocab_only       = 0
0.00.051.637 I print_info: n_ctx_train      = 2048
0.00.051.637 I print_info: n_embd           = 2048
0.00.051.637 I print_info: n_layer          = 24
0.00.051.640 I print_info: n_head           = 16
0.00.051.641 I print_info: n_head_kv        = 16
0.00.051.641 I print_info: n_rot            = 32
0.00.051.642 I print_info: n_swa            = 0
0.00.051.642 I print_info: n_embd_head_k    = 128
0.00.051.642 I print_info: n_embd_head_v    = 128
0.00.051.643 I print_info: n_gqa            = 1
0.00.051.644 I print_info: n_embd_k_gqa     = 2048
0.00.051.644 I print_info: n_embd_v_gqa     = 2048
0.00.051.645 I print_info: f_norm_eps       = 1.0e-05
0.00.051.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.646 I print_info: f_logit_scale    = 0.0e+00
0.00.051.646 I print_info: n_ff             = 8192
0.00.051.648 I print_info: n_expert         = 0
0.00.051.648 I print_info: n_expert_used    = 0
0.00.051.648 I print_info: causal attn      = 1
0.00.051.648 I print_info: pooling type     = 0
0.00.051.648 I print_info: rope type        = 2
0.00.051.649 I print_info: rope scaling     = linear
0.00.051.649 I print_info: freq_base_train  = 10000.0
0.00.051.650 I print_info: freq_scale_train = 1
0.00.051.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.650 I print_info: rope_finetuned   = unknown
0.00.051.650 I print_info: ssm_d_conv       = 0
0.00.051.651 I print_info: ssm_d_inner      = 0
0.00.051.651 I print_info: ssm_d_state      = 0
0.00.051.652 I print_info: ssm_dt_rank      = 0
0.00.051.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.652 I print_info: model type       = 1.4B
0.00.051.652 I print_info: model params     = 1.41 B
0.00.051.652 I print_info: general.name     = 1.4B
0.00.051.653 I print_info: vocab type       = BPE
0.00.051.653 I print_info: n_vocab          = 50304
0.00.051.653 I print_info: n_merges         = 50009
0.00.051.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: LF token         = 128 ''
0.00.051.654 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: max token length = 1024
0.00.053.426 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.426 I load_tensors: offloading output layer to GPU
0.00.053.426 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.436 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.438 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.710 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.710 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.711 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.711 I llama_new_context_with_model: n_batch       = 2048
0.00.053.711 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.711 I llama_new_context_with_model: flash_attn    = 0
0.00.053.712 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.712 I llama_new_context_with_model: freq_scale    = 1
0.00.053.712 I ggml_metal_init: allocating
0.00.053.716 I ggml_metal_init: found device: Apple M4
0.00.053.718 I ggml_metal_init: picking default device: Apple M4
0.00.054.315 I ggml_metal_init: using embedded metal library
0.00.056.648 I ggml_metal_init: GPU name:   Apple M4
0.00.056.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.651 I ggml_metal_init: simdgroup reduction   = true
0.00.056.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.651 I ggml_metal_init: has bfloat            = true
0.00.056.651 I ggml_metal_init: use bfloat            = true
0.00.056.652 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.433 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.880 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.885 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.016 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.018 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.018 I llama_new_context_with_model: graph nodes  = 967
0.00.087.018 I llama_new_context_with_model: graph splits = 2
0.00.087.022 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.175 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.008 I main: llama threadpool init, n_threads = 4
0.00.762.043 I 
0.00.762.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.066 I 
0.00.762.223 I sampler seed: 1234
0.00.762.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.274 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.593.523 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.593.524 I llama_perf_context_print:        load time =     752.46 ms
0.01.593.524 I llama_perf_context_print: prompt eval time =      42.17 ms /     7 tokens (    6.02 ms per token,   166.00 tokens per second)
0.01.593.525 I llama_perf_context_print:        eval time =     786.15 ms /    63 runs   (   12.48 ms per token,    80.14 tokens per second)
0.01.593.526 I llama_perf_context_print:       total time =     831.52 ms /    70 tokens
0.01.593.751 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.377 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.241 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.748 I llama_model_loader: - type  f32:  194 tensors
0.00.025.748 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.748 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.749 I print_info: file format = GGUF V3 (latest)
0.00.025.749 I print_info: file type   = Q5_1
0.00.025.750 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.804 I load: special tokens cache size = 25
0.00.050.804 I load: token to piece cache size = 0.2984 MB
0.00.050.819 I print_info: arch             = gptneox
0.00.050.820 I print_info: n_vocab (hp)     = 50304
0.00.050.820 I print_info: vocab_only       = 0
0.00.050.821 I print_info: n_ctx_train      = 2048
0.00.050.821 I print_info: n_embd           = 2048
0.00.050.821 I print_info: n_layer          = 24
0.00.050.823 I print_info: n_head           = 16
0.00.050.824 I print_info: n_head_kv        = 16
0.00.050.824 I print_info: n_rot            = 32
0.00.050.824 I print_info: n_swa            = 0
0.00.050.825 I print_info: n_embd_head_k    = 128
0.00.050.825 I print_info: n_embd_head_v    = 128
0.00.050.825 I print_info: n_gqa            = 1
0.00.050.826 I print_info: n_embd_k_gqa     = 2048
0.00.050.827 I print_info: n_embd_v_gqa     = 2048
0.00.050.828 I print_info: f_norm_eps       = 1.0e-05
0.00.050.828 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.828 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.829 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.829 I print_info: f_logit_scale    = 0.0e+00
0.00.050.830 I print_info: n_ff             = 8192
0.00.050.830 I print_info: n_expert         = 0
0.00.050.830 I print_info: n_expert_used    = 0
0.00.050.830 I print_info: causal attn      = 1
0.00.050.830 I print_info: pooling type     = 0
0.00.050.830 I print_info: rope type        = 2
0.00.050.831 I print_info: rope scaling     = linear
0.00.050.831 I print_info: freq_base_train  = 10000.0
0.00.050.832 I print_info: freq_scale_train = 1
0.00.050.836 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.836 I print_info: rope_finetuned   = unknown
0.00.050.836 I print_info: ssm_d_conv       = 0
0.00.050.837 I print_info: ssm_d_inner      = 0
0.00.050.837 I print_info: ssm_d_state      = 0
0.00.050.837 I print_info: ssm_dt_rank      = 0
0.00.050.837 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.837 I print_info: model type       = 1.4B
0.00.050.837 I print_info: model params     = 1.41 B
0.00.050.838 I print_info: general.name     = 1.4B
0.00.050.838 I print_info: vocab type       = BPE
0.00.050.839 I print_info: n_vocab          = 50304
0.00.050.840 I print_info: n_merges         = 50009
0.00.050.840 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.840 I print_info: LF token         = 128 ''
0.00.050.841 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.841 I print_info: max token length = 1024
0.00.052.608 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.608 I load_tensors: offloading output layer to GPU
0.00.052.609 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.619 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.620 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.886 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.887 I llama_new_context_with_model: n_ctx         = 128
0.00.052.887 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.888 I llama_new_context_with_model: n_batch       = 128
0.00.052.888 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.888 I llama_new_context_with_model: flash_attn    = 0
0.00.052.888 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.888 I llama_new_context_with_model: freq_scale    = 1
0.00.052.889 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.889 I ggml_metal_init: allocating
0.00.052.892 I ggml_metal_init: found device: Apple M4
0.00.052.893 I ggml_metal_init: picking default device: Apple M4
0.00.053.435 I ggml_metal_init: using embedded metal library
0.00.055.785 I ggml_metal_init: GPU name:   Apple M4
0.00.055.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.788 I ggml_metal_init: simdgroup reduction   = true
0.00.055.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.788 I ggml_metal_init: has bfloat            = true
0.00.055.788 I ggml_metal_init: use bfloat            = true
0.00.055.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.512 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.646 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.647 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.647 I llama_new_context_with_model: graph nodes  = 967
0.00.066.648 I llama_new_context_with_model: graph splits = 2
0.00.066.649 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.648 I 
0.00.693.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.684 I perplexity: tokenizing the input ..
0.00.701.196 I perplexity: tokenization took 7.509 ms
0.00.701.200 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.604 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.837.717 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.837.750 I llama_perf_context_print:        load time =     683.26 ms
0.00.837.750 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.86 tokens per second)
0.00.837.751 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.752 I llama_perf_context_print:       total time =     144.10 ms /   129 tokens
0.00.838.243 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.077s
sys	0m0.151s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.370 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.255 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.258 I llama_model_loader: - type  f32:  194 tensors
0.00.026.258 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.258 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.259 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.259 I print_info: file format = GGUF V3 (latest)
0.00.026.259 I print_info: file type   = Q2_K - Medium
0.00.026.260 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.619 I load: special tokens cache size = 25
0.00.050.512 I load: token to piece cache size = 0.2984 MB
0.00.050.526 I print_info: arch             = gptneox
0.00.050.527 I print_info: n_vocab (hp)     = 50304
0.00.050.527 I print_info: vocab_only       = 0
0.00.050.527 I print_info: n_ctx_train      = 2048
0.00.050.527 I print_info: n_embd           = 2048
0.00.050.527 I print_info: n_layer          = 24
0.00.050.530 I print_info: n_head           = 16
0.00.050.531 I print_info: n_head_kv        = 16
0.00.050.531 I print_info: n_rot            = 32
0.00.050.531 I print_info: n_swa            = 0
0.00.050.532 I print_info: n_embd_head_k    = 128
0.00.050.532 I print_info: n_embd_head_v    = 128
0.00.050.532 I print_info: n_gqa            = 1
0.00.050.533 I print_info: n_embd_k_gqa     = 2048
0.00.050.534 I print_info: n_embd_v_gqa     = 2048
0.00.050.535 I print_info: f_norm_eps       = 1.0e-05
0.00.050.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.538 I print_info: f_logit_scale    = 0.0e+00
0.00.050.538 I print_info: n_ff             = 8192
0.00.050.539 I print_info: n_expert         = 0
0.00.050.539 I print_info: n_expert_used    = 0
0.00.050.539 I print_info: causal attn      = 1
0.00.050.539 I print_info: pooling type     = 0
0.00.050.539 I print_info: rope type        = 2
0.00.050.539 I print_info: rope scaling     = linear
0.00.050.539 I print_info: freq_base_train  = 10000.0
0.00.050.540 I print_info: freq_scale_train = 1
0.00.050.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.540 I print_info: rope_finetuned   = unknown
0.00.050.540 I print_info: ssm_d_conv       = 0
0.00.050.541 I print_info: ssm_d_inner      = 0
0.00.050.541 I print_info: ssm_d_state      = 0
0.00.050.541 I print_info: ssm_dt_rank      = 0
0.00.050.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.545 I print_info: model type       = 1.4B
0.00.050.545 I print_info: model params     = 1.41 B
0.00.050.545 I print_info: general.name     = 1.4B
0.00.050.546 I print_info: vocab type       = BPE
0.00.050.546 I print_info: n_vocab          = 50304
0.00.050.547 I print_info: n_merges         = 50009
0.00.050.548 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.548 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.548 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.548 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.548 I print_info: LF token         = 128 ''
0.00.050.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.550 I print_info: max token length = 1024
0.00.052.148 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.148 I load_tensors: offloading output layer to GPU
0.00.052.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.158 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.159 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.418 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.419 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.419 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.419 I llama_new_context_with_model: n_batch       = 2048
0.00.052.419 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.419 I llama_new_context_with_model: flash_attn    = 0
0.00.052.420 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.420 I llama_new_context_with_model: freq_scale    = 1
0.00.052.421 I ggml_metal_init: allocating
0.00.052.424 I ggml_metal_init: found device: Apple M4
0.00.052.426 I ggml_metal_init: picking default device: Apple M4
0.00.053.016 I ggml_metal_init: using embedded metal library
0.00.055.369 I ggml_metal_init: GPU name:   Apple M4
0.00.055.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.371 I ggml_metal_init: simdgroup reduction   = true
0.00.055.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.371 I ggml_metal_init: has bfloat            = true
0.00.055.371 I ggml_metal_init: use bfloat            = true
0.00.055.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.372 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.334 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.339 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.368 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.285 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.286 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.287 I llama_new_context_with_model: graph nodes  = 967
0.00.084.287 I llama_new_context_with_model: graph splits = 2
0.00.084.290 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.099 I main: llama threadpool init, n_threads = 4
0.00.446.145 I 
0.00.446.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.169 I 
0.00.446.342 I sampler seed: 1234
0.00.446.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.446.373 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.446.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.446.374 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.136.397 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63734.29 tokens per second)
0.01.136.398 I llama_perf_context_print:        load time =     434.72 ms
0.01.136.399 I llama_perf_context_print: prompt eval time =      36.16 ms /     7 tokens (    5.17 ms per token,   193.59 tokens per second)
0.01.136.399 I llama_perf_context_print:        eval time =     650.93 ms /    63 runs   (   10.33 ms per token,    96.78 tokens per second)
0.01.136.400 I llama_perf_context_print:       total time =     690.30 ms /    70 tokens
0.01.136.607 I ggml_metal_free: deallocating

real	0m1.152s
user	0m0.108s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.917 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.512 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.525 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.527 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.952 I llama_model_loader: - type  f32:  194 tensors
0.00.024.953 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.953 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.954 I print_info: file format = GGUF V3 (latest)
0.00.024.954 I print_info: file type   = Q2_K - Medium
0.00.024.955 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.286 I load: special tokens cache size = 25
0.00.049.174 I load: token to piece cache size = 0.2984 MB
0.00.049.189 I print_info: arch             = gptneox
0.00.049.190 I print_info: n_vocab (hp)     = 50304
0.00.049.190 I print_info: vocab_only       = 0
0.00.049.190 I print_info: n_ctx_train      = 2048
0.00.049.190 I print_info: n_embd           = 2048
0.00.049.191 I print_info: n_layer          = 24
0.00.049.193 I print_info: n_head           = 16
0.00.049.194 I print_info: n_head_kv        = 16
0.00.049.194 I print_info: n_rot            = 32
0.00.049.194 I print_info: n_swa            = 0
0.00.049.194 I print_info: n_embd_head_k    = 128
0.00.049.194 I print_info: n_embd_head_v    = 128
0.00.049.195 I print_info: n_gqa            = 1
0.00.049.196 I print_info: n_embd_k_gqa     = 2048
0.00.049.196 I print_info: n_embd_v_gqa     = 2048
0.00.049.197 I print_info: f_norm_eps       = 1.0e-05
0.00.049.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.198 I print_info: f_logit_scale    = 0.0e+00
0.00.049.198 I print_info: n_ff             = 8192
0.00.049.199 I print_info: n_expert         = 0
0.00.049.199 I print_info: n_expert_used    = 0
0.00.049.199 I print_info: causal attn      = 1
0.00.049.199 I print_info: pooling type     = 0
0.00.049.199 I print_info: rope type        = 2
0.00.049.199 I print_info: rope scaling     = linear
0.00.049.200 I print_info: freq_base_train  = 10000.0
0.00.049.201 I print_info: freq_scale_train = 1
0.00.049.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.201 I print_info: rope_finetuned   = unknown
0.00.049.201 I print_info: ssm_d_conv       = 0
0.00.049.201 I print_info: ssm_d_inner      = 0
0.00.049.201 I print_info: ssm_d_state      = 0
0.00.049.201 I print_info: ssm_dt_rank      = 0
0.00.049.201 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.202 I print_info: model type       = 1.4B
0.00.049.202 I print_info: model params     = 1.41 B
0.00.049.202 I print_info: general.name     = 1.4B
0.00.049.202 I print_info: vocab type       = BPE
0.00.049.203 I print_info: n_vocab          = 50304
0.00.049.203 I print_info: n_merges         = 50009
0.00.049.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.203 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.203 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.203 I print_info: LF token         = 128 ''
0.00.049.204 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.204 I print_info: max token length = 1024
0.00.050.900 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.901 I load_tensors: offloading output layer to GPU
0.00.050.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.911 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.912 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.174 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.175 I llama_new_context_with_model: n_ctx         = 128
0.00.051.175 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.175 I llama_new_context_with_model: n_batch       = 128
0.00.051.175 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.176 I llama_new_context_with_model: flash_attn    = 0
0.00.051.176 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.176 I llama_new_context_with_model: freq_scale    = 1
0.00.051.176 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.177 I ggml_metal_init: allocating
0.00.051.179 I ggml_metal_init: found device: Apple M4
0.00.051.181 I ggml_metal_init: picking default device: Apple M4
0.00.051.730 I ggml_metal_init: using embedded metal library
0.00.054.026 I ggml_metal_init: GPU name:   Apple M4
0.00.054.027 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.028 I ggml_metal_init: simdgroup reduction   = true
0.00.054.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.029 I ggml_metal_init: has bfloat            = true
0.00.054.029 I ggml_metal_init: use bfloat            = true
0.00.054.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.753 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.703 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.704 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.704 I llama_new_context_with_model: graph nodes  = 967
0.00.064.705 I llama_new_context_with_model: graph splits = 2
0.00.064.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.994 I 
0.00.405.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.405.038 I perplexity: tokenizing the input ..
0.00.412.370 I perplexity: tokenization took 7.33 ms
0.00.412.373 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.957 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.546.056 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.546.079 I llama_perf_context_print:        load time =     395.07 ms
0.00.546.080 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.05 tokens per second)
0.00.546.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.081 I llama_perf_context_print:       total time =     141.09 ms /   129 tokens
0.00.546.540 I ggml_metal_free: deallocating

real	0m0.561s
user	0m0.076s
sys	0m0.087s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.673 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.731 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.732 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.732 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.733 I llama_model_loader: - type  f32:  194 tensors
0.00.026.733 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.733 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.734 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.734 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.734 I print_info: file format = GGUF V3 (latest)
0.00.026.735 I print_info: file type   = Q3_K - Medium
0.00.026.740 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.296 I load: special tokens cache size = 25
0.00.051.054 I load: token to piece cache size = 0.2984 MB
0.00.051.071 I print_info: arch             = gptneox
0.00.051.072 I print_info: n_vocab (hp)     = 50304
0.00.051.072 I print_info: vocab_only       = 0
0.00.051.072 I print_info: n_ctx_train      = 2048
0.00.051.072 I print_info: n_embd           = 2048
0.00.051.073 I print_info: n_layer          = 24
0.00.051.075 I print_info: n_head           = 16
0.00.051.076 I print_info: n_head_kv        = 16
0.00.051.076 I print_info: n_rot            = 32
0.00.051.076 I print_info: n_swa            = 0
0.00.051.076 I print_info: n_embd_head_k    = 128
0.00.051.077 I print_info: n_embd_head_v    = 128
0.00.051.077 I print_info: n_gqa            = 1
0.00.051.078 I print_info: n_embd_k_gqa     = 2048
0.00.051.079 I print_info: n_embd_v_gqa     = 2048
0.00.051.079 I print_info: f_norm_eps       = 1.0e-05
0.00.051.080 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.081 I print_info: f_logit_scale    = 0.0e+00
0.00.051.082 I print_info: n_ff             = 8192
0.00.051.082 I print_info: n_expert         = 0
0.00.051.082 I print_info: n_expert_used    = 0
0.00.051.082 I print_info: causal attn      = 1
0.00.051.083 I print_info: pooling type     = 0
0.00.051.083 I print_info: rope type        = 2
0.00.051.083 I print_info: rope scaling     = linear
0.00.051.083 I print_info: freq_base_train  = 10000.0
0.00.051.083 I print_info: freq_scale_train = 1
0.00.051.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.084 I print_info: rope_finetuned   = unknown
0.00.051.084 I print_info: ssm_d_conv       = 0
0.00.051.084 I print_info: ssm_d_inner      = 0
0.00.051.084 I print_info: ssm_d_state      = 0
0.00.051.084 I print_info: ssm_dt_rank      = 0
0.00.051.084 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.085 I print_info: model type       = 1.4B
0.00.051.085 I print_info: model params     = 1.41 B
0.00.051.085 I print_info: general.name     = 1.4B
0.00.051.085 I print_info: vocab type       = BPE
0.00.051.086 I print_info: n_vocab          = 50304
0.00.051.086 I print_info: n_merges         = 50009
0.00.051.086 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.087 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.087 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.087 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: LF token         = 128 ''
0.00.051.088 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: max token length = 1024
0.00.052.814 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.814 I load_tensors: offloading output layer to GPU
0.00.052.814 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.824 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.825 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.097 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.097 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.097 I llama_new_context_with_model: n_batch       = 2048
0.00.053.097 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.097 I llama_new_context_with_model: flash_attn    = 0
0.00.053.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.098 I llama_new_context_with_model: freq_scale    = 1
0.00.053.098 I ggml_metal_init: allocating
0.00.053.101 I ggml_metal_init: found device: Apple M4
0.00.053.103 I ggml_metal_init: picking default device: Apple M4
0.00.053.679 I ggml_metal_init: using embedded metal library
0.00.055.959 I ggml_metal_init: GPU name:   Apple M4
0.00.055.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.961 I ggml_metal_init: simdgroup reduction   = true
0.00.055.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.962 I ggml_metal_init: has bfloat            = true
0.00.055.962 I ggml_metal_init: use bfloat            = true
0.00.055.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.399 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.079 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.086 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.136 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.089 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.090 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.090 I llama_new_context_with_model: graph nodes  = 967
0.00.085.091 I llama_new_context_with_model: graph splits = 2
0.00.085.094 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.017 I main: llama threadpool init, n_threads = 4
0.00.537.067 I 
0.00.537.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.099 I 
0.00.537.259 I sampler seed: 1234
0.00.537.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.273 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.274 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.277 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.945 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.289.946 I llama_perf_context_print:        load time =     527.34 ms
0.01.289.946 I llama_perf_context_print: prompt eval time =      40.85 ms /     7 tokens (    5.84 ms per token,   171.35 tokens per second)
0.01.289.947 I llama_perf_context_print:        eval time =     708.84 ms /    63 runs   (   11.25 ms per token,    88.88 tokens per second)
0.01.289.947 I llama_perf_context_print:       total time =     752.93 ms /    70 tokens
0.01.290.122 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.107s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.391 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.444 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.176 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.945 I llama_model_loader: - type  f32:  194 tensors
0.00.025.946 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.946 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.946 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.946 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.947 I print_info: file format = GGUF V3 (latest)
0.00.025.947 I print_info: file type   = Q3_K - Medium
0.00.025.948 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.096 I load: special tokens cache size = 25
0.00.051.075 I load: token to piece cache size = 0.2984 MB
0.00.051.090 I print_info: arch             = gptneox
0.00.051.091 I print_info: n_vocab (hp)     = 50304
0.00.051.091 I print_info: vocab_only       = 0
0.00.051.091 I print_info: n_ctx_train      = 2048
0.00.051.092 I print_info: n_embd           = 2048
0.00.051.092 I print_info: n_layer          = 24
0.00.051.094 I print_info: n_head           = 16
0.00.051.095 I print_info: n_head_kv        = 16
0.00.051.095 I print_info: n_rot            = 32
0.00.051.096 I print_info: n_swa            = 0
0.00.051.096 I print_info: n_embd_head_k    = 128
0.00.051.096 I print_info: n_embd_head_v    = 128
0.00.051.097 I print_info: n_gqa            = 1
0.00.051.097 I print_info: n_embd_k_gqa     = 2048
0.00.051.099 I print_info: n_embd_v_gqa     = 2048
0.00.051.100 I print_info: f_norm_eps       = 1.0e-05
0.00.051.101 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.101 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.101 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.101 I print_info: f_logit_scale    = 0.0e+00
0.00.051.102 I print_info: n_ff             = 8192
0.00.051.103 I print_info: n_expert         = 0
0.00.051.103 I print_info: n_expert_used    = 0
0.00.051.103 I print_info: causal attn      = 1
0.00.051.103 I print_info: pooling type     = 0
0.00.051.104 I print_info: rope type        = 2
0.00.051.104 I print_info: rope scaling     = linear
0.00.051.104 I print_info: freq_base_train  = 10000.0
0.00.051.104 I print_info: freq_scale_train = 1
0.00.051.104 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.105 I print_info: rope_finetuned   = unknown
0.00.051.105 I print_info: ssm_d_conv       = 0
0.00.051.105 I print_info: ssm_d_inner      = 0
0.00.051.108 I print_info: ssm_d_state      = 0
0.00.051.108 I print_info: ssm_dt_rank      = 0
0.00.051.109 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.109 I print_info: model type       = 1.4B
0.00.051.109 I print_info: model params     = 1.41 B
0.00.051.109 I print_info: general.name     = 1.4B
0.00.051.110 I print_info: vocab type       = BPE
0.00.051.110 I print_info: n_vocab          = 50304
0.00.051.110 I print_info: n_merges         = 50009
0.00.051.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: LF token         = 128 ''
0.00.051.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: max token length = 1024
0.00.052.855 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.855 I load_tensors: offloading output layer to GPU
0.00.052.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.866 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.867 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.142 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.143 I llama_new_context_with_model: n_ctx         = 128
0.00.053.143 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.143 I llama_new_context_with_model: n_batch       = 128
0.00.053.143 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.143 I llama_new_context_with_model: flash_attn    = 0
0.00.053.144 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.144 I llama_new_context_with_model: freq_scale    = 1
0.00.053.144 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.145 I ggml_metal_init: allocating
0.00.053.148 I ggml_metal_init: found device: Apple M4
0.00.053.150 I ggml_metal_init: picking default device: Apple M4
0.00.053.708 I ggml_metal_init: using embedded metal library
0.00.056.011 I ggml_metal_init: GPU name:   Apple M4
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.013 I ggml_metal_init: simdgroup reduction   = true
0.00.056.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.014 I ggml_metal_init: has bfloat            = true
0.00.056.014 I ggml_metal_init: use bfloat            = true
0.00.056.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.675 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.000 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.004 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.031 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.892 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.893 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.893 I llama_new_context_with_model: graph nodes  = 967
0.00.067.894 I llama_new_context_with_model: graph splits = 2
0.00.067.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.041 I 
0.00.518.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.080 I perplexity: tokenizing the input ..
0.00.525.921 I perplexity: tokenization took 7.839 ms
0.00.525.925 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.658.567 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.685 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.708 I llama_perf_context_print:        load time =     507.65 ms
0.00.659.709 I llama_perf_context_print: prompt eval time =     132.42 ms /   128 tokens (    1.03 ms per token,   966.61 tokens per second)
0.00.659.709 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.712 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.660.194 I ggml_metal_free: deallocating

real	0m0.675s
user	0m0.078s
sys	0m0.111s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.203 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.404 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.406 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.407 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.409 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.144 I llama_model_loader: - type  f32:  194 tensors
0.00.027.144 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.144 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.144 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.145 I print_info: file format = GGUF V3 (latest)
0.00.027.145 I print_info: file type   = Q4_K - Medium
0.00.027.146 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.472 I load: special tokens cache size = 25
0.00.052.465 I load: token to piece cache size = 0.2984 MB
0.00.052.481 I print_info: arch             = gptneox
0.00.052.482 I print_info: n_vocab (hp)     = 50304
0.00.052.483 I print_info: vocab_only       = 0
0.00.052.483 I print_info: n_ctx_train      = 2048
0.00.052.483 I print_info: n_embd           = 2048
0.00.052.483 I print_info: n_layer          = 24
0.00.052.486 I print_info: n_head           = 16
0.00.052.486 I print_info: n_head_kv        = 16
0.00.052.486 I print_info: n_rot            = 32
0.00.052.486 I print_info: n_swa            = 0
0.00.052.487 I print_info: n_embd_head_k    = 128
0.00.052.487 I print_info: n_embd_head_v    = 128
0.00.052.487 I print_info: n_gqa            = 1
0.00.052.488 I print_info: n_embd_k_gqa     = 2048
0.00.052.489 I print_info: n_embd_v_gqa     = 2048
0.00.052.489 I print_info: f_norm_eps       = 1.0e-05
0.00.052.490 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.490 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.490 I print_info: f_logit_scale    = 0.0e+00
0.00.052.491 I print_info: n_ff             = 8192
0.00.052.491 I print_info: n_expert         = 0
0.00.052.491 I print_info: n_expert_used    = 0
0.00.052.492 I print_info: causal attn      = 1
0.00.052.492 I print_info: pooling type     = 0
0.00.052.492 I print_info: rope type        = 2
0.00.052.492 I print_info: rope scaling     = linear
0.00.052.493 I print_info: freq_base_train  = 10000.0
0.00.052.495 I print_info: freq_scale_train = 1
0.00.052.495 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.495 I print_info: rope_finetuned   = unknown
0.00.052.495 I print_info: ssm_d_conv       = 0
0.00.052.496 I print_info: ssm_d_inner      = 0
0.00.052.496 I print_info: ssm_d_state      = 0
0.00.052.496 I print_info: ssm_dt_rank      = 0
0.00.052.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.496 I print_info: model type       = 1.4B
0.00.052.496 I print_info: model params     = 1.41 B
0.00.052.496 I print_info: general.name     = 1.4B
0.00.052.497 I print_info: vocab type       = BPE
0.00.052.497 I print_info: n_vocab          = 50304
0.00.052.497 I print_info: n_merges         = 50009
0.00.052.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.499 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.499 I print_info: LF token         = 128 ''
0.00.052.500 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.500 I print_info: max token length = 1024
0.00.054.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.296 I load_tensors: offloading output layer to GPU
0.00.054.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.306 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.307 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.574 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.574 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.575 I llama_new_context_with_model: n_batch       = 2048
0.00.054.575 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.575 I llama_new_context_with_model: flash_attn    = 0
0.00.054.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.576 I llama_new_context_with_model: freq_scale    = 1
0.00.054.576 I ggml_metal_init: allocating
0.00.054.580 I ggml_metal_init: found device: Apple M4
0.00.054.582 I ggml_metal_init: picking default device: Apple M4
0.00.055.163 I ggml_metal_init: using embedded metal library
0.00.057.484 I ggml_metal_init: GPU name:   Apple M4
0.00.057.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.486 I ggml_metal_init: simdgroup reduction   = true
0.00.057.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.486 I ggml_metal_init: has bfloat            = true
0.00.057.487 I ggml_metal_init: use bfloat            = true
0.00.057.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.189 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.493 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.533 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.672 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.673 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.673 I llama_new_context_with_model: graph nodes  = 967
0.00.087.673 I llama_new_context_with_model: graph splits = 2
0.00.087.677 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.412 I main: llama threadpool init, n_threads = 4
0.00.652.446 I 
0.00.652.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.469 I 
0.00.652.628 I sampler seed: 1234
0.00.652.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.652.651 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.652.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.652.651 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.406.602 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.406.605 I llama_perf_context_print:        load time =     642.21 ms
0.01.406.606 I llama_perf_context_print: prompt eval time =      47.53 ms /     7 tokens (    6.79 ms per token,   147.27 tokens per second)
0.01.406.607 I llama_perf_context_print:        eval time =     703.38 ms /    63 runs   (   11.16 ms per token,    89.57 tokens per second)
0.01.406.607 I llama_perf_context_print:       total time =     754.19 ms /    70 tokens
0.01.406.849 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.109s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.261 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.814 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.815 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.816 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.263 I llama_model_loader: - type  f32:  194 tensors
0.00.026.263 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.263 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.263 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.264 I print_info: file format = GGUF V3 (latest)
0.00.026.264 I print_info: file type   = Q4_K - Medium
0.00.026.265 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.552 I load: special tokens cache size = 25
0.00.050.211 I load: token to piece cache size = 0.2984 MB
0.00.050.227 I print_info: arch             = gptneox
0.00.050.228 I print_info: n_vocab (hp)     = 50304
0.00.050.228 I print_info: vocab_only       = 0
0.00.050.228 I print_info: n_ctx_train      = 2048
0.00.050.228 I print_info: n_embd           = 2048
0.00.050.229 I print_info: n_layer          = 24
0.00.050.231 I print_info: n_head           = 16
0.00.050.232 I print_info: n_head_kv        = 16
0.00.050.232 I print_info: n_rot            = 32
0.00.050.232 I print_info: n_swa            = 0
0.00.050.232 I print_info: n_embd_head_k    = 128
0.00.050.232 I print_info: n_embd_head_v    = 128
0.00.050.236 I print_info: n_gqa            = 1
0.00.050.240 I print_info: n_embd_k_gqa     = 2048
0.00.050.241 I print_info: n_embd_v_gqa     = 2048
0.00.050.242 I print_info: f_norm_eps       = 1.0e-05
0.00.050.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.249 I print_info: f_logit_scale    = 0.0e+00
0.00.050.256 I print_info: n_ff             = 8192
0.00.050.256 I print_info: n_expert         = 0
0.00.050.256 I print_info: n_expert_used    = 0
0.00.050.256 I print_info: causal attn      = 1
0.00.050.256 I print_info: pooling type     = 0
0.00.050.257 I print_info: rope type        = 2
0.00.050.257 I print_info: rope scaling     = linear
0.00.050.258 I print_info: freq_base_train  = 10000.0
0.00.050.258 I print_info: freq_scale_train = 1
0.00.050.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.258 I print_info: rope_finetuned   = unknown
0.00.050.259 I print_info: ssm_d_conv       = 0
0.00.050.259 I print_info: ssm_d_inner      = 0
0.00.050.259 I print_info: ssm_d_state      = 0
0.00.050.259 I print_info: ssm_dt_rank      = 0
0.00.050.259 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.260 I print_info: model type       = 1.4B
0.00.050.260 I print_info: model params     = 1.41 B
0.00.050.261 I print_info: general.name     = 1.4B
0.00.050.262 I print_info: vocab type       = BPE
0.00.050.262 I print_info: n_vocab          = 50304
0.00.050.262 I print_info: n_merges         = 50009
0.00.050.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.263 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.263 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.263 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.264 I print_info: LF token         = 128 ''
0.00.050.264 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.264 I print_info: max token length = 1024
0.00.052.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.039 I load_tensors: offloading output layer to GPU
0.00.052.039 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.049 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.050 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.311 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.312 I llama_new_context_with_model: n_ctx         = 128
0.00.052.312 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.312 I llama_new_context_with_model: n_batch       = 128
0.00.052.312 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.312 I llama_new_context_with_model: flash_attn    = 0
0.00.052.313 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.313 I llama_new_context_with_model: freq_scale    = 1
0.00.052.313 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.314 I ggml_metal_init: allocating
0.00.052.316 I ggml_metal_init: found device: Apple M4
0.00.052.318 I ggml_metal_init: picking default device: Apple M4
0.00.052.874 I ggml_metal_init: using embedded metal library
0.00.055.211 I ggml_metal_init: GPU name:   Apple M4
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.213 I ggml_metal_init: simdgroup reduction   = true
0.00.055.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.214 I ggml_metal_init: has bfloat            = true
0.00.055.214 I ggml_metal_init: use bfloat            = true
0.00.055.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.530 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.774 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.778 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.804 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.682 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.683 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.683 I llama_new_context_with_model: graph nodes  = 967
0.00.065.683 I llama_new_context_with_model: graph splits = 2
0.00.065.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.564 I 
0.00.600.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.602 I perplexity: tokenizing the input ..
0.00.608.107 I perplexity: tokenization took 7.503 ms
0.00.608.110 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.539 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.743.671 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.743.697 I llama_perf_context_print:        load time =     589.30 ms
0.00.743.699 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.74 tokens per second)
0.00.743.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.702 I llama_perf_context_print:       total time =     143.13 ms /   129 tokens
0.00.744.217 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.075s
sys	0m0.132s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.732 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.473 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.476 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.286 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.315 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.049 I llama_model_loader: - type  f32:  194 tensors
0.00.025.049 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.049 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.050 I print_info: file format = GGUF V3 (latest)
0.00.025.050 I print_info: file type   = Q5_K - Medium
0.00.025.051 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.532 I load: special tokens cache size = 25
0.00.049.433 I load: token to piece cache size = 0.2984 MB
0.00.049.447 I print_info: arch             = gptneox
0.00.049.448 I print_info: n_vocab (hp)     = 50304
0.00.049.448 I print_info: vocab_only       = 0
0.00.049.449 I print_info: n_ctx_train      = 2048
0.00.049.449 I print_info: n_embd           = 2048
0.00.049.449 I print_info: n_layer          = 24
0.00.049.451 I print_info: n_head           = 16
0.00.049.452 I print_info: n_head_kv        = 16
0.00.049.452 I print_info: n_rot            = 32
0.00.049.452 I print_info: n_swa            = 0
0.00.049.452 I print_info: n_embd_head_k    = 128
0.00.049.453 I print_info: n_embd_head_v    = 128
0.00.049.453 I print_info: n_gqa            = 1
0.00.049.454 I print_info: n_embd_k_gqa     = 2048
0.00.049.455 I print_info: n_embd_v_gqa     = 2048
0.00.049.455 I print_info: f_norm_eps       = 1.0e-05
0.00.049.456 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.456 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.456 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.456 I print_info: f_logit_scale    = 0.0e+00
0.00.049.457 I print_info: n_ff             = 8192
0.00.049.457 I print_info: n_expert         = 0
0.00.049.457 I print_info: n_expert_used    = 0
0.00.049.458 I print_info: causal attn      = 1
0.00.049.458 I print_info: pooling type     = 0
0.00.049.458 I print_info: rope type        = 2
0.00.049.458 I print_info: rope scaling     = linear
0.00.049.458 I print_info: freq_base_train  = 10000.0
0.00.049.459 I print_info: freq_scale_train = 1
0.00.049.459 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.459 I print_info: rope_finetuned   = unknown
0.00.049.459 I print_info: ssm_d_conv       = 0
0.00.049.459 I print_info: ssm_d_inner      = 0
0.00.049.460 I print_info: ssm_d_state      = 0
0.00.049.460 I print_info: ssm_dt_rank      = 0
0.00.049.460 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.460 I print_info: model type       = 1.4B
0.00.049.460 I print_info: model params     = 1.41 B
0.00.049.460 I print_info: general.name     = 1.4B
0.00.049.461 I print_info: vocab type       = BPE
0.00.049.461 I print_info: n_vocab          = 50304
0.00.049.461 I print_info: n_merges         = 50009
0.00.049.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: LF token         = 128 ''
0.00.049.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.463 I print_info: max token length = 1024
0.00.051.211 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.211 I load_tensors: offloading output layer to GPU
0.00.051.211 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.222 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.223 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.492 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.492 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.492 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.493 I llama_new_context_with_model: n_batch       = 2048
0.00.051.493 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.493 I llama_new_context_with_model: flash_attn    = 0
0.00.051.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.494 I llama_new_context_with_model: freq_scale    = 1
0.00.051.494 I ggml_metal_init: allocating
0.00.051.496 I ggml_metal_init: found device: Apple M4
0.00.051.498 I ggml_metal_init: picking default device: Apple M4
0.00.052.094 I ggml_metal_init: using embedded metal library
0.00.054.391 I ggml_metal_init: GPU name:   Apple M4
0.00.054.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.393 I ggml_metal_init: simdgroup reduction   = true
0.00.054.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.393 I ggml_metal_init: has bfloat            = true
0.00.054.394 I ggml_metal_init: use bfloat            = true
0.00.054.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.857 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.805 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.810 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.849 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.870 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.872 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.873 I llama_new_context_with_model: graph nodes  = 967
0.00.083.873 I llama_new_context_with_model: graph splits = 2
0.00.083.876 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.896 I main: llama threadpool init, n_threads = 4
0.00.735.934 I 
0.00.735.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.977 I 
0.00.736.125 I sampler seed: 1234
0.00.736.131 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.143 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.143 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.579.901 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.579.902 I llama_perf_context_print:        load time =     727.16 ms
0.01.579.903 I llama_perf_context_print: prompt eval time =      51.92 ms /     7 tokens (    7.42 ms per token,   134.83 tokens per second)
0.01.579.905 I llama_perf_context_print:        eval time =     788.89 ms /    63 runs   (   12.52 ms per token,    79.86 tokens per second)
0.01.579.905 I llama_perf_context_print:       total time =     844.01 ms /    70 tokens
0.01.580.138 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.107s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.635 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.276 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.672 I llama_model_loader: - type  f32:  194 tensors
0.00.023.672 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.673 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.673 I print_info: file format = GGUF V3 (latest)
0.00.023.673 I print_info: file type   = Q5_K - Medium
0.00.023.674 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.041.879 I load: special tokens cache size = 25
0.00.047.707 I load: token to piece cache size = 0.2984 MB
0.00.047.724 I print_info: arch             = gptneox
0.00.047.725 I print_info: n_vocab (hp)     = 50304
0.00.047.725 I print_info: vocab_only       = 0
0.00.047.725 I print_info: n_ctx_train      = 2048
0.00.047.725 I print_info: n_embd           = 2048
0.00.047.725 I print_info: n_layer          = 24
0.00.047.728 I print_info: n_head           = 16
0.00.047.729 I print_info: n_head_kv        = 16
0.00.047.729 I print_info: n_rot            = 32
0.00.047.729 I print_info: n_swa            = 0
0.00.047.730 I print_info: n_embd_head_k    = 128
0.00.047.730 I print_info: n_embd_head_v    = 128
0.00.047.730 I print_info: n_gqa            = 1
0.00.047.733 I print_info: n_embd_k_gqa     = 2048
0.00.047.733 I print_info: n_embd_v_gqa     = 2048
0.00.047.734 I print_info: f_norm_eps       = 1.0e-05
0.00.047.734 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.735 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.738 I print_info: f_logit_scale    = 0.0e+00
0.00.047.739 I print_info: n_ff             = 8192
0.00.047.739 I print_info: n_expert         = 0
0.00.047.739 I print_info: n_expert_used    = 0
0.00.047.739 I print_info: causal attn      = 1
0.00.047.739 I print_info: pooling type     = 0
0.00.047.739 I print_info: rope type        = 2
0.00.047.740 I print_info: rope scaling     = linear
0.00.047.740 I print_info: freq_base_train  = 10000.0
0.00.047.740 I print_info: freq_scale_train = 1
0.00.047.740 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.741 I print_info: rope_finetuned   = unknown
0.00.047.741 I print_info: ssm_d_conv       = 0
0.00.047.741 I print_info: ssm_d_inner      = 0
0.00.047.741 I print_info: ssm_d_state      = 0
0.00.047.741 I print_info: ssm_dt_rank      = 0
0.00.047.741 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.742 I print_info: model type       = 1.4B
0.00.047.742 I print_info: model params     = 1.41 B
0.00.047.742 I print_info: general.name     = 1.4B
0.00.047.743 I print_info: vocab type       = BPE
0.00.047.743 I print_info: n_vocab          = 50304
0.00.047.743 I print_info: n_merges         = 50009
0.00.047.744 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.744 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.744 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.744 I print_info: LF token         = 128 ''
0.00.047.744 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.745 I print_info: max token length = 1024
0.00.049.464 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.464 I load_tensors: offloading output layer to GPU
0.00.049.465 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.475 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.049.476 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.049.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.049.738 I llama_new_context_with_model: n_ctx         = 128
0.00.049.738 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.049.738 I llama_new_context_with_model: n_batch       = 128
0.00.049.738 I llama_new_context_with_model: n_ubatch      = 128
0.00.049.739 I llama_new_context_with_model: flash_attn    = 0
0.00.049.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.049.739 I llama_new_context_with_model: freq_scale    = 1
0.00.049.739 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.049.740 I ggml_metal_init: allocating
0.00.049.742 I ggml_metal_init: found device: Apple M4
0.00.049.744 I ggml_metal_init: picking default device: Apple M4
0.00.050.314 I ggml_metal_init: using embedded metal library
0.00.052.629 I ggml_metal_init: GPU name:   Apple M4
0.00.052.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.631 I ggml_metal_init: simdgroup reduction   = true
0.00.052.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.632 I ggml_metal_init: has bfloat            = true
0.00.052.632 I ggml_metal_init: use bfloat            = true
0.00.052.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.111 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.371 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.373 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.260 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.261 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.262 I llama_new_context_with_model: graph nodes  = 967
0.00.063.262 I llama_new_context_with_model: graph splits = 2
0.00.063.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.252 I 
0.00.843.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.292 I perplexity: tokenizing the input ..
0.00.850.885 I perplexity: tokenization took 7.592 ms
0.00.850.891 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.991.799 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.992.904 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.992.932 I llama_perf_context_print:        load time =     834.61 ms
0.00.992.933 I llama_perf_context_print: prompt eval time =     140.69 ms /   128 tokens (    1.10 ms per token,   909.81 tokens per second)
0.00.992.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.992.934 I llama_perf_context_print:       total time =     149.68 ms /   129 tokens
0.00.993.339 I ggml_metal_free: deallocating

real	0m1.007s
user	0m0.076s
sys	0m0.147s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.982 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.372 I llama_model_loader: - type  f32:  194 tensors
0.00.026.372 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.373 I print_info: file format = GGUF V3 (latest)
0.00.026.373 I print_info: file type   = Q6_K
0.00.026.374 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.830 I load: special tokens cache size = 25
0.00.050.751 I load: token to piece cache size = 0.2984 MB
0.00.050.766 I print_info: arch             = gptneox
0.00.050.767 I print_info: n_vocab (hp)     = 50304
0.00.050.767 I print_info: vocab_only       = 0
0.00.050.767 I print_info: n_ctx_train      = 2048
0.00.050.768 I print_info: n_embd           = 2048
0.00.050.768 I print_info: n_layer          = 24
0.00.050.770 I print_info: n_head           = 16
0.00.050.771 I print_info: n_head_kv        = 16
0.00.050.771 I print_info: n_rot            = 32
0.00.050.771 I print_info: n_swa            = 0
0.00.050.771 I print_info: n_embd_head_k    = 128
0.00.050.772 I print_info: n_embd_head_v    = 128
0.00.050.772 I print_info: n_gqa            = 1
0.00.050.773 I print_info: n_embd_k_gqa     = 2048
0.00.050.774 I print_info: n_embd_v_gqa     = 2048
0.00.050.774 I print_info: f_norm_eps       = 1.0e-05
0.00.050.775 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.775 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.775 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.775 I print_info: f_logit_scale    = 0.0e+00
0.00.050.776 I print_info: n_ff             = 8192
0.00.050.776 I print_info: n_expert         = 0
0.00.050.776 I print_info: n_expert_used    = 0
0.00.050.776 I print_info: causal attn      = 1
0.00.050.776 I print_info: pooling type     = 0
0.00.050.777 I print_info: rope type        = 2
0.00.050.779 I print_info: rope scaling     = linear
0.00.050.779 I print_info: freq_base_train  = 10000.0
0.00.050.779 I print_info: freq_scale_train = 1
0.00.050.779 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.779 I print_info: rope_finetuned   = unknown
0.00.050.780 I print_info: ssm_d_conv       = 0
0.00.050.780 I print_info: ssm_d_inner      = 0
0.00.050.780 I print_info: ssm_d_state      = 0
0.00.050.780 I print_info: ssm_dt_rank      = 0
0.00.050.780 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.780 I print_info: model type       = 1.4B
0.00.050.780 I print_info: model params     = 1.41 B
0.00.050.781 I print_info: general.name     = 1.4B
0.00.050.781 I print_info: vocab type       = BPE
0.00.050.781 I print_info: n_vocab          = 50304
0.00.050.781 I print_info: n_merges         = 50009
0.00.050.781 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.782 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.782 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.785 I print_info: LF token         = 128 ''
0.00.050.786 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.786 I print_info: max token length = 1024
0.00.052.591 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.592 I load_tensors: offloading output layer to GPU
0.00.052.592 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.602 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.603 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.863 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.864 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.864 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.864 I llama_new_context_with_model: n_batch       = 2048
0.00.052.864 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.865 I llama_new_context_with_model: flash_attn    = 0
0.00.052.865 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.865 I llama_new_context_with_model: freq_scale    = 1
0.00.052.866 I ggml_metal_init: allocating
0.00.052.869 I ggml_metal_init: found device: Apple M4
0.00.052.871 I ggml_metal_init: picking default device: Apple M4
0.00.053.467 I ggml_metal_init: using embedded metal library
0.00.055.789 I ggml_metal_init: GPU name:   Apple M4
0.00.055.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.791 I ggml_metal_init: simdgroup reduction   = true
0.00.055.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.791 I ggml_metal_init: has bfloat            = true
0.00.055.791 I ggml_metal_init: use bfloat            = true
0.00.055.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.392 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.358 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.366 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.408 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.467 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.469 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.469 I llama_new_context_with_model: graph nodes  = 967
0.00.085.469 I llama_new_context_with_model: graph splits = 2
0.00.085.472 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.075 I main: llama threadpool init, n_threads = 4
0.00.805.112 I 
0.00.805.136 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.136 I 
0.00.805.285 I sampler seed: 1234
0.00.805.289 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.356 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.356 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.672.356 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.672.356 I llama_perf_context_print:        load time =     795.09 ms
0.01.672.357 I llama_perf_context_print: prompt eval time =      54.77 ms /     7 tokens (    7.82 ms per token,   127.80 tokens per second)
0.01.672.358 I llama_perf_context_print:        eval time =     809.12 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.672.358 I llama_perf_context_print:       total time =     867.28 ms /    70 tokens
0.01.672.553 I ggml_metal_free: deallocating

real	0m1.689s
user	0m0.107s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4473 (c89e8085) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.312 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.025.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.921 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.429 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.036.080 I llama_model_loader: - type  f32:  194 tensors
0.00.036.081 I llama_model_loader: - type q6_K:   98 tensors
0.00.036.081 I print_info: file format = GGUF V3 (latest)
0.00.036.082 I print_info: file type   = Q6_K
0.00.036.082 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.062.434 I load: special tokens cache size = 25
0.00.071.007 I load: token to piece cache size = 0.2984 MB
0.00.071.021 I print_info: arch             = gptneox
0.00.071.023 I print_info: n_vocab (hp)     = 50304
0.00.071.023 I print_info: vocab_only       = 0
0.00.071.023 I print_info: n_ctx_train      = 2048
0.00.071.024 I print_info: n_embd           = 2048
0.00.071.024 I print_info: n_layer          = 24
0.00.071.031 I print_info: n_head           = 16
0.00.071.032 I print_info: n_head_kv        = 16
0.00.071.032 I print_info: n_rot            = 32
0.00.071.033 I print_info: n_swa            = 0
0.00.071.033 I print_info: n_embd_head_k    = 128
0.00.071.033 I print_info: n_embd_head_v    = 128
0.00.071.034 I print_info: n_gqa            = 1
0.00.071.035 I print_info: n_embd_k_gqa     = 2048
0.00.071.036 I print_info: n_embd_v_gqa     = 2048
0.00.071.036 I print_info: f_norm_eps       = 1.0e-05
0.00.071.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.037 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.037 I print_info: f_logit_scale    = 0.0e+00
0.00.071.037 I print_info: n_ff             = 8192
0.00.071.038 I print_info: n_expert         = 0
0.00.071.038 I print_info: n_expert_used    = 0
0.00.071.038 I print_info: causal attn      = 1
0.00.071.038 I print_info: pooling type     = 0
0.00.071.038 I print_info: rope type        = 2
0.00.071.046 I print_info: rope scaling     = linear
0.00.071.048 I print_info: freq_base_train  = 10000.0
0.00.071.048 I print_info: freq_scale_train = 1
0.00.071.048 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.049 I print_info: rope_finetuned   = unknown
0.00.071.049 I print_info: ssm_d_conv       = 0
0.00.071.049 I print_info: ssm_d_inner      = 0
0.00.071.049 I print_info: ssm_d_state      = 0
0.00.071.049 I print_info: ssm_dt_rank      = 0
0.00.071.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.050 I print_info: model type       = 1.4B
0.00.071.050 I print_info: model params     = 1.41 B
0.00.071.051 I print_info: general.name     = 1.4B
0.00.071.051 I print_info: vocab type       = BPE
0.00.071.051 I print_info: n_vocab          = 50304
0.00.071.051 I print_info: n_merges         = 50009
0.00.071.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.052 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.052 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.053 I print_info: LF token         = 128 ''
0.00.071.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.053 I print_info: max token length = 1024
0.00.073.272 I load_tensors: offloading 24 repeating layers to GPU
0.00.073.273 I load_tensors: offloading output layer to GPU
0.00.073.273 I load_tensors: offloaded 25/25 layers to GPU
0.00.073.283 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.073.284 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.073.619 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.620 I llama_new_context_with_model: n_ctx         = 128
0.00.073.620 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.073.621 I llama_new_context_with_model: n_batch       = 128
0.00.073.621 I llama_new_context_with_model: n_ubatch      = 128
0.00.073.621 I llama_new_context_with_model: flash_attn    = 0
0.00.073.621 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.622 I llama_new_context_with_model: freq_scale    = 1
0.00.073.622 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.073.623 I ggml_metal_init: allocating
0.00.073.626 I ggml_metal_init: found device: Apple M4
0.00.073.628 I ggml_metal_init: picking default device: Apple M4
0.00.074.278 I ggml_metal_init: using embedded metal library
0.00.077.532 I ggml_metal_init: GPU name:   Apple M4
0.00.077.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.536 I ggml_metal_init: simdgroup reduction   = true
0.00.077.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.536 I ggml_metal_init: has bfloat            = true
0.00.077.536 I ggml_metal_init: use bfloat            = true
0.00.077.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.910 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.558 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.795 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.797 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.797 I llama_new_context_with_model: graph nodes  = 967
0.00.093.797 I llama_new_context_with_model: graph splits = 2
0.00.093.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.093.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.230.482 I 
0.00.230.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.230.524 I perplexity: tokenizing the input ..
0.00.238.248 I perplexity: tokenization took 7.722 ms
0.00.238.254 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.378.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.379.315 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.379.341 I llama_perf_context_print:        load time =     216.16 ms
0.00.379.342 I llama_perf_context_print: prompt eval time =     139.63 ms /   128 tokens (    1.09 ms per token,   916.72 tokens per second)
0.00.379.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.379.343 I llama_perf_context_print:       total time =     148.86 ms /   129 tokens
0.00.379.796 I ggml_metal_free: deallocating

real	0m0.402s
user	0m0.095s
sys	0m0.050s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4473 (c89e8085)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158307ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158308600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158308bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158309160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158309710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158309cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15830a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15830a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15830add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15830b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15830b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15830bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15830c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15830cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15830d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15830ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15830e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15830ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15830f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15830fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158310320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158310a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158311160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158311a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158312120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1583123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1583129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158313660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158313ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158313e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158314300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1583145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158314e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158315390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158315650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158315af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158315f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158316430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1583168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158316d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158317210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1583176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158317b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158317ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1583182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1583188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158318ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1583197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158319e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15831a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15831aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15831b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15831b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15831bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15831c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15831c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15831cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15831d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15831d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15831de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15831e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15831e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15831ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15831eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15831f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15831f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15831fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158320160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158320600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158320aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158320f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1583213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158321880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158321dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158322320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158322870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158322dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158323310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158323860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158323db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158324300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158324850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158324da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1583252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158325840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158325d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1583262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158326830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158326d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1583272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158327820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158327d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1583282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158328810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158328d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1583292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158329800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1583194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158329c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15832a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15832a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15832aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15832b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15832b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15832beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15832c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15832c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15832cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15832d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15832d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15832de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15832e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15832e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15832edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15832f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15832f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15832fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158330050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1583304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158330990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158330e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1583312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158331770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158331c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1583320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158332550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1583329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158332e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158333330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1583337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158333c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158334110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1583345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158334a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158334ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158335390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158335830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158335cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158336170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158336610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158336ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158336f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1583373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158337890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158337d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1583381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158338670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158338b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158338fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158339450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1583398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158339d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15833a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15833a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15833ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15833b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15833b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15833b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15833bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15833c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15833c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15833cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15833d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15833d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15833d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15833de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15833e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15833e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15833ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15833f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15833f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15833fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15833feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158340350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1583407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158340c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158341130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1583415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158341a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158341f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1583423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158342850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158342cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158343190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158343630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158343ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158343f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158344410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1583448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158344d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1583451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158345690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158345b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158346080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1583465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158346b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158347070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158347330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158347940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158347f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158348560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158348d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1583491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1583494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158349ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15834a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15834a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15834ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15834b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15834b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15834be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15834c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15834c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15834ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15834d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15834d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15834de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15834e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15834e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15834ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15834f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15834f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15834fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158350360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1583508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158350e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158351350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1583518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158351df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158352340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158352890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158352de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158353330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158353880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158353dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158354320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158354870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158354dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158355310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158355860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158355db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158356300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158356850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158356da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1583572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158357840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158357d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1583582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158358830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158358d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1583592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158359820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158359d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15835a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15835a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15835ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15835b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15835b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15835bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15835c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15835c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15835cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15835d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15835d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15835dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15835e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15835e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15835ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15835f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15835f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15835fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15835fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158360390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158360830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158360cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158361170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158361610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158361ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158361f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1583623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158362890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158362d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158363280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1583639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1583640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1583647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158364f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1583651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1583659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158365c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158366280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.191.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.191.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158208c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158206630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158208f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1582093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158209810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158209c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15820a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15820a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15820a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15820ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15820b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15820b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15820c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15820cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15820d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15820dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15820e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15820ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15820f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15820f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15820ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1582106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158210dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1582114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158211c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158211ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158212180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1582125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158212a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158212ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158213340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158213870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158213ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158213fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158214410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158214880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158214cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158215160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1582155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158215a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158215eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158216320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158216790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158216c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158217070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1582174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158217950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158217dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158218230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1582186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158218b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158218f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1582193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158219860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158219cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15821a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15821a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15821abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15821b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15821b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15821b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15821bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15821c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15821c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15821cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15821cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15821d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15821d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15821dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15821e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15821e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15821e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15821ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15821f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15821f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15821fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158220000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158220470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1582208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158220d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1582211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158221630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158221aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158221f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158222380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1582227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158222c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1582230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158223540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1582239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158223e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158224290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158224700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158224b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158224fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158225450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1582258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158225d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1582261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158226610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158226a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158226ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158227360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1582277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158227c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1582280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158228520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158228990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158228e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158229270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1582296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158229b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158229fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15822a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15822a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15822ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15822b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15822b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15822ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15822bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15822c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15822c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15822cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15822d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15822d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15822d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15822dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15822e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15822e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15822eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15822efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15822f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15822f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15822fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158230160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1582305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158230a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158230eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158231320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158231790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158231c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158232070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1582324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158232950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158232dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158233230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1582336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158233b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158233f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1582343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158234860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158234cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158235140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1582355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158235a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158235e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158236300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158236770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158236be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158237050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1582374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158237930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158237da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158238210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158238680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158238af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1582392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158239560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1582399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158239e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15823a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15823a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15823ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15823b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15823b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15823b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15823bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15823c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15823c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15823caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15823cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15823d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15823d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15823dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15823e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15823e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15823e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106b04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106b046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106b04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106b04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106b053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106b05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106b05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106b06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106b065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106b06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106b06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106b07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106b07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106b07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106b08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106b084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106b08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106b08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106b09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106b09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106b09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106b09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106b0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106b0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106b0b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106b0b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106b0b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106b0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106b0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106b0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106b0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106b0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106b0d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106b0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106b0dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106b0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106b0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106b0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106b0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106b0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106b0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106b0fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106b10080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106b104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106b10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106b10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106b11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106b116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106b11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106b11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106b12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106b12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106b12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106b13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106b135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106b13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106b13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106b14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106b14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106b14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106b15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106b154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106b15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106b15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106b16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106b16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106b16b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106b16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106b173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106b17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106b17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106b18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106b185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106b18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106b18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106b192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106b19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106b19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106b1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106b1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106b1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106b1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106b1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106b1b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106b1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106b1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106b1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106b1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106b1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106b1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106b1d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106b1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106b1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106b1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106b1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106b1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106b1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106b1fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106b20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106b20b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106b20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106b212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106b218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106b21ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15820bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158238db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1582061d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15820f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15823ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15823ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15823f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15823f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15823fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15823fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158240130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1582403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1582409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158240f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1582415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158241880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158241dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158242300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158242840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158243010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158243550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158243a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158243fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158244510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158244a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158244f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158245250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158245510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1582457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158245a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158245d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158246010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1582462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158246590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158246850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158246b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158246dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158247090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158247350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158247610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1582478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158247b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158247e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158248110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1582483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158248690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158248950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158248c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158248ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158249190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158249710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1582499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158249c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158249f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15824a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15824a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15824a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15824aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15824ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15824afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15824b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15824b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15824b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15824bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15824bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15824c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15824c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15824c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15824c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15824cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15824ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15824d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15824d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15824d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15824d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15824dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15824de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15824e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15824e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15824e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15824e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15824ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15824ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15824f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15824f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15824f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15824fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15824fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15824ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158250250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158250510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1582507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158250a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158250d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158251010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1582512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158251590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158251850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158251b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158251dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158252090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158252350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158252610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1582528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158252b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158252e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158253110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1582533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158253690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158253950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158253c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158253ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158254190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158254450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1582549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158254c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158254f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158255210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1582554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158255790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158255a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158255d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158255fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158256290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158256550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158256810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158256ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158256d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158257050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158257310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1582575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158257890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158257b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158257e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1582580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158258390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158258650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158258910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158258bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158258e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158259150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158259410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1582596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158259990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158259c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158259f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15825a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15825a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15825a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15825aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15825acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15825af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15825b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15825b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15825b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15825ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15825bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15825c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15825c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15825c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15825c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15825cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15825cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15825d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15825d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15825d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15825d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15825db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15825de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15825e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15825e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15825e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15825e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15825ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15825eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15825f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15825f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15825f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15825fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158260010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158260510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158260a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158260f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158261410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158261910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158261e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158262310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158262810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158262d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158263210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158263710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158263c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1582641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158264770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158264d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1582652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1582658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158265ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158266500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158266cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158267190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158267450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158267a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158268070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158268860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158268d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1582691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158269640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158269df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15826a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15826a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15826ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15826b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15826b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15826bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15826c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15826c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15826cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15826d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15826d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15826ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15826e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15826e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15826eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15826f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15826f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15826fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1582702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158270830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158270d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1582712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158271820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158271d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1582722c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158272810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158272d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1582732b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158273800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158273d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1582742a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1582747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158274d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158275290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1582757e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158275d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158276280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1582767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158276d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158277270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1582777c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158277d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158278260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1582787b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158278d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158279250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1582797a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158279cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15827a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15827a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15827ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15827b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15827b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15827bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15827c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15827c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15827cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15827d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15827d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15827d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15827de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15827e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15827e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15827ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15827f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15827f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15827fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15827fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158280390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158280830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158280cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158281220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158281940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158282060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158282780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158282ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158283160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158283950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158283c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158284220 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.929s
user	0m0.320s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4473 (c89e8085)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1367102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1367109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136712090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1367131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1367136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136713ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1367140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136715b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1367162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1367169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1367170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1367186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13671a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13671a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13671adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13671ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13671bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13671c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13671c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13671c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13671d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13671d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13671da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13671dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13671e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13671e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13671eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13671f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13671f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13671fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13671ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1367203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136720c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1367212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1367221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1367227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136723a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136726210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1367264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1367272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136727750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136727bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136728090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136728530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1367289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136728e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1367297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13672a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13672a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13672ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13672b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13672b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13672bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13672c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13672c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13672cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13672d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13672d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13672dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13672e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13672e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13672ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13672f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13672f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13672fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136730140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136730be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136731130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136731bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1367218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136732040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1367327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136732d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136733290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1367337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136734280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1367347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1367357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136735d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136736260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1367367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1367371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1367388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1367396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13673a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13673a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13673adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13673b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13673b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13673bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13673c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13673c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13673c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13673ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13673d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13673d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13673dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13673e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13673e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13673e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13673ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13673f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13673f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13673fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1367405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136742aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1367433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1367441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136744fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1367458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136745d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1367466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136746b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136747000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1367474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136747de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136748280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136748bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136749060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136749500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1367499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136749e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13674a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13674a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13674ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13674b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13674b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13674ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13674bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13674c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13674c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13674cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13674d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13674d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13674da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13674df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13674e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13674e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13674eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13674f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13674f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13674fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136751120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1367515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1367524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1367535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136754220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136754cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136755760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136755cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136756200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136756750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136756ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1367571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136757740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136757c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1367581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136758c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1367591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136759720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136759c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13675a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13675a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13675ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13675b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13675b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13675bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13675c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13675c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13675cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13675d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13675d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13675dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13675e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13675e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13675ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13675f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13675f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13675fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136760160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1367606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136760c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136761150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1367616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136761bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136762140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136762690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136762be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136763680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136763bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136764120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136764bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136765bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136766100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136766650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136766ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136767040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1367674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136767980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136767e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1367682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136768760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136768c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1367690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1367699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136769e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13676a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13676a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13676ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13676b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13676b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13676bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13676c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13676cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13676d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13676d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13676dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13676e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13676e650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137804d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1378051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137805660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137805ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137805f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1378063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137806820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137806c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137807100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137807570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1378079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1378080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137808bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1378093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137809bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13780a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13780a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13780b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13780b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13780bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13780c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13780cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13780d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13780dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13780e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13780e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13780e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13780ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13780f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13780f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13780fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13780ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1378103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1378106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137810b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137810f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1378113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137811860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137811cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1378125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137812a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137812e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137813300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137813770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137813be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137814050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1378144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137814930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137815680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137815af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137815f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1378163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137816840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137816db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1378172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137817b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137818470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1378188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137818d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1378191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137819630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137819aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137819f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13781a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13781a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13781ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13781b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13781b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13781b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13781be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13781c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13781c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13781cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13781cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13781d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13781d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13781dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13781e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13781e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13781ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13781eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13781f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13781f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13781fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1378200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137820520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137820990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137820e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137821270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1378216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137821b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137822430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1378228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137822d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1378235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137823a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137823ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137824340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1378247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137824c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137825090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137825500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137825970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137825de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137826250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1378266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137827410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137827cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137828160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1378285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137828a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137828eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137829320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137829790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137829c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13782a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13782a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13782a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13782adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13782b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13782b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13782bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13782bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13782c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13782c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13782ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13782d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13782d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13782da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13782de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13782e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13782e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13782ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13782f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13782f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13782f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13782fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137830210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137830680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137830f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1378313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137831cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137832120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137832590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137832a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137832e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1378332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137833750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137833bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137834030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1378344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137834910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137834d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1378351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1378360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1378363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1378370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1378379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1378382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1378398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13783a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13783a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13783aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13783af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13783b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13783b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13783bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13783c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13783c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13783c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13783ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13783d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13783d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13783db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13783dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13783e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13783e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13783ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13783f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13783f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13783fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137840080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1378404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137840960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137841760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1378427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137842aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137843060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137843be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1378441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137844760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1378452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1378458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1378469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137847560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1378480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1378486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137848c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137849220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1378497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137849da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13784a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13784a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13784aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13784b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13784ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13784c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13784c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13784cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13784d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13784d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13784dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13784e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13784e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13784ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13784f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13784f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13784ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137850520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137850ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1378510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137851c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1378521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1378527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137852d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137853320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1378538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137854460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1378555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1378566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137856ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1378571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1378576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137857ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1378580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1378585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137858aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137858fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1378594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1378599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137859ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13785a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13785a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13785ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13785b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13785b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13785c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13785c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13785cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13785d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13785d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13785e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13785e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13785ea90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136608430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1366088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136608b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136608fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136609290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136609b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136609fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13660a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13660a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13660ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13660b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13660beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13660c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13660ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13660d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13660dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13660e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13660eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13660f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13660f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136610100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136610820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136610f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136611920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136611be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136612050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1366124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136612930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136612da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1366132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136613740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136613e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1366142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136614750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136614bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1366154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136615910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136615d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1366161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136616660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136616ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1366173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136617820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136617c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136618100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1366189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1366192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13661a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13661a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13661aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13661aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13661b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13661b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13661bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13661c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13661c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13661c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13661ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13661d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13661d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13661db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13661dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13661e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13661e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13661ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13661f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13661f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13661fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13661fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1366207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136620c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136621090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136621500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136621970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136621de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136622250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1366226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136622b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136622fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136623410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136623880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136623cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136624160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1366245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136624a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136625320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136625790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136626070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1366264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136626950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136626dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136627650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136627910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136627d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1366281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136628ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136628f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1366293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136629820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136629c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13662a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13662a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13662a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13662ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13662b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13662b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13662bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13662c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13662c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13662c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13662cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13662d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13662d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13662dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13662df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13662e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13662e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13662ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13662f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13662f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13662fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1366302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136630b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1366318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136631d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1366321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136632620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136632a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136632f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1366337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1366340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136634530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1366349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1366356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136635fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136636440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1366368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136636d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136637600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136637a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136637ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136638350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1366387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136638c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1366390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136639510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136639980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136639df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13663a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13663a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13663ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13663afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13663b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13663b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13663bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13663c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13663c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13663ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13663cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13663d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13663d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13663dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13663e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13663e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13663e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13663edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13663f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13663f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13663fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13663ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136640400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136640870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136640ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136641150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1366415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136641a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136641ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136642310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136642780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136642bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136643060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1366434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136643940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136644220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136644b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136645940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136645c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1366464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136646950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136646dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136647230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1366476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136647b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136647f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1366483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136648860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136648cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136649140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1366495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136649a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13664a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13664a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13664abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13664b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13664b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13664b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13664bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13664c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13664c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13664caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13664cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13664d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13664d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13664dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13664e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13664e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13664ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13664ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13664f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13664f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13664fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136650030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1366504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136650910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136650d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1366511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136651660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136651ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136651f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1366523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136653100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136653570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1366539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1366542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136654730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136654ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136655010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136655480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1366558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136655d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1366561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136656ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136656f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136657800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136657c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1366580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136658550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1366589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136658e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1366592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136659d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13665a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13665ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13665b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13665b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13665b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13665bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13665c5b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.894s
user	0m0.242s
sys	0m0.121s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.53 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.09 sec*proc (2 tests)

Total Test time (real) =   1.10 sec
        1.12 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
