+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/c8/74db853dcb3b2f0891e9a0773b5dbf646adda0/ggml-0-x86-cpu-low-perf/ctest_debug.log
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/c8/74db853dcb3b2f0891e9a0773b5dbf646adda0/ggml-0-x86-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.711s
user	0m0.540s
sys	0m0.173s
+ tee -a /home/ggml/results/llama.cpp/c8/74db853dcb3b2f0891e9a0773b5dbf646adda0/ggml-0-x86-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  3%] Built target sha256
[  3%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target sha1
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target build_info
[  7%] Built target ggml-base
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 13%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 13%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Linking CXX executable ../../bin/llama-gguf
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Built target llama-gguf-hash
[ 18%] Built target llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘void llama_kv_cache::state_read(const llama_kv_cache::io&, const llama_hparams&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:751:20: error: ‘runtime_error’ is not a member of ‘std’
  751 |         throw std::runtime_error("failed to restore kv cache");
      |                    ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:202: src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_get_data_internal(llama_context*, llama_data_write&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1150:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1150 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1153:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1153 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1156:5: error: missing initializer for member ‘llama_kv_cache::io::read’ [-Werror=missing-field-initializers]
 1156 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1156:5: error: missing initializer for member ‘llama_kv_cache::io::read_to’ [-Werror=missing-field-initializers]
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_set_data_internal(llama_context*, llama_data_read&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1196:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1196 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1199:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1199 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1202:5: error: missing initializer for member ‘llama_kv_cache::io::write’ [-Werror=missing-field-initializers]
 1202 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1202:5: error: missing initializer for member ‘llama_kv_cache::io::write_tensor_data’ [-Werror=missing-field-initializers]
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_get_data_internal(llama_context*, llama_data_write&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1301:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1301 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1304:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1304 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1307:5: error: missing initializer for member ‘llama_kv_cache::io::read’ [-Werror=missing-field-initializers]
 1307 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1307:5: error: missing initializer for member ‘llama_kv_cache::io::read_to’ [-Werror=missing-field-initializers]
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_set_data_internal(llama_context*, llama_data_read&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1333:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1333 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1336:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1336 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1339:5: error: missing initializer for member ‘llama_kv_cache::io::write’ [-Werror=missing-field-initializers]
 1339 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1339:5: error: missing initializer for member ‘llama_kv_cache::io::write_tensor_data’ [-Werror=missing-field-initializers]
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m2.510s
user	0m4.906s
sys	0m0.576s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/c8/74db853dcb3b2f0891e9a0773b5dbf646adda0/ggml-0-x86-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
