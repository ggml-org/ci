Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.699s
user	0m0.713s
sys	0m1.010s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  7%] Built target build_info
[  7%] Built target xxhash
[  7%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target sha256
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llama-simple
[ 33%] Built target common
[ 33%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-sampling
[ 47%] Built target test-json-schema-to-grammar
[ 47%] Built target test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Built target test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-barrier
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Built target test-chat-template
[ 60%] Built target test-model-load-cancel
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-quantize-perf
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-rope
[ 68%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Built target llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gbnf-validator
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-infill
[ 74%] Built target llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Generating loading.html.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-merge
[ 83%] Generating index.html.hpp
[ 83%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Built target llama-cli
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-quantize
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-save-load-state
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-speculative
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 99%] Built target llama-q8dot
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.440s
user	0m5.556s
sys	0m9.431s

main: quantize time =  2522.74 ms
main:    total time =  2522.74 ms

main: quantize time =  1313.64 ms
main:    total time =  1313.64 ms

main: quantize time =  1304.32 ms
main:    total time =  1304.32 ms

main: quantize time =  1397.32 ms
main:    total time =  1397.32 ms

main: quantize time =  2103.33 ms
main:    total time =  2103.33 ms

main: quantize time =  6523.16 ms
main:    total time =  6523.16 ms

main: quantize time =  5724.62 ms
main:    total time =  5724.62 ms

main: quantize time =  7044.22 ms
main:    total time =  7044.22 ms

main: quantize time =  5969.41 ms
main:    total time =  5969.41 ms

main: quantize time =  4580.82 ms
main:    total time =  4580.82 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.225 I main: llama backend init
0.00.000.232 I main: load the model and apply lora adapter, if any
0.00.034.021 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.308 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.321 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.328 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.330 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.707 I llama_model_loader: - type  f32:  194 tensors
0.00.064.708 I llama_model_loader: - type  f16:   98 tensors
0.00.098.365 I llm_load_vocab: special tokens cache size = 25
0.00.105.658 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.660 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.661 I llm_load_print_meta: arch             = gptneox
0.00.105.661 I llm_load_print_meta: vocab type       = BPE
0.00.105.661 I llm_load_print_meta: n_vocab          = 50304
0.00.105.661 I llm_load_print_meta: n_merges         = 50009
0.00.105.662 I llm_load_print_meta: vocab_only       = 0
0.00.105.662 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.662 I llm_load_print_meta: n_embd           = 2048
0.00.105.662 I llm_load_print_meta: n_layer          = 24
0.00.105.665 I llm_load_print_meta: n_head           = 16
0.00.105.668 I llm_load_print_meta: n_head_kv        = 16
0.00.105.668 I llm_load_print_meta: n_rot            = 32
0.00.105.669 I llm_load_print_meta: n_swa            = 0
0.00.105.669 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.669 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.670 I llm_load_print_meta: n_gqa            = 1
0.00.105.670 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.675 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.676 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.677 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.678 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.678 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.679 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.679 I llm_load_print_meta: n_ff             = 8192
0.00.105.680 I llm_load_print_meta: n_expert         = 0
0.00.105.680 I llm_load_print_meta: n_expert_used    = 0
0.00.105.680 I llm_load_print_meta: causal attn      = 1
0.00.105.680 I llm_load_print_meta: pooling type     = 0
0.00.105.680 I llm_load_print_meta: rope type        = 2
0.00.105.681 I llm_load_print_meta: rope scaling     = linear
0.00.105.681 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.681 I llm_load_print_meta: freq_scale_train = 1
0.00.105.683 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.683 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.683 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.683 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.683 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.683 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.684 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.684 I llm_load_print_meta: model type       = 1.4B
0.00.105.685 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.105.685 I llm_load_print_meta: model params     = 1.41 B
0.00.105.685 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.105.691 I llm_load_print_meta: general.name     = 1.4B
0.00.105.693 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.693 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.693 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.693 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.694 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.105.694 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.694 I llm_load_print_meta: max token length = 1024
0.00.108.423 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.108.423 I llm_load_tensors: offloading output layer to GPU
0.00.108.423 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.108.441 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.108.442 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.109.398 I llama_new_context_with_model: n_seq_max     = 1
0.00.109.399 I llama_new_context_with_model: n_ctx         = 2048
0.00.109.399 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.109.399 I llama_new_context_with_model: n_batch       = 2048
0.00.109.399 I llama_new_context_with_model: n_ubatch      = 512
0.00.109.400 I llama_new_context_with_model: flash_attn    = 0
0.00.109.400 I llama_new_context_with_model: freq_base     = 10000.0
0.00.109.400 I llama_new_context_with_model: freq_scale    = 1
0.00.109.401 I ggml_metal_init: allocating
0.00.109.409 I ggml_metal_init: found device: Apple M4
0.00.109.411 I ggml_metal_init: picking default device: Apple M4
0.00.110.123 I ggml_metal_init: using embedded metal library
0.00.119.596 I ggml_metal_init: GPU name:   Apple M4
0.00.119.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.599 I ggml_metal_init: simdgroup reduction   = true
0.00.119.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.600 I ggml_metal_init: has bfloat            = true
0.00.119.600 I ggml_metal_init: use bfloat            = true
0.00.119.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.164.532 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.164.539 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.164.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.165.481 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.165.483 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.165.484 I llama_new_context_with_model: graph nodes  = 967
0.00.165.484 I llama_new_context_with_model: graph splits = 2
0.00.165.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.247.219 I main: llama threadpool init, n_threads = 4
0.00.247.258 I 
0.00.247.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.247.295 I 
0.00.247.377 I sampler seed: 1234
0.00.247.382 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.247.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.247.408 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.247.409 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.093.180 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.02.093.180 I llama_perf_context_print:        load time =     213.19 ms
0.02.093.181 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.56 tokens per second)
0.02.093.182 I llama_perf_context_print:        eval time =    1798.95 ms /    63 runs   (   28.55 ms per token,    35.02 tokens per second)
0.02.093.182 I llama_perf_context_print:       total time =    1845.96 ms /    70 tokens
0.02.093.371 I ggml_metal_free: deallocating

real	0m2.396s
user	0m0.147s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.036.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.045.850 I llama_model_loader: - type  f32:  194 tensors
0.00.045.850 I llama_model_loader: - type q8_0:   98 tensors
0.00.072.468 I llm_load_vocab: special tokens cache size = 25
0.00.081.966 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.970 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.970 I llm_load_print_meta: arch             = gptneox
0.00.081.970 I llm_load_print_meta: vocab type       = BPE
0.00.081.971 I llm_load_print_meta: n_vocab          = 50304
0.00.081.971 I llm_load_print_meta: n_merges         = 50009
0.00.081.971 I llm_load_print_meta: vocab_only       = 0
0.00.081.971 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.972 I llm_load_print_meta: n_embd           = 2048
0.00.081.972 I llm_load_print_meta: n_layer          = 24
0.00.081.977 I llm_load_print_meta: n_head           = 16
0.00.081.981 I llm_load_print_meta: n_head_kv        = 16
0.00.081.981 I llm_load_print_meta: n_rot            = 32
0.00.081.981 I llm_load_print_meta: n_swa            = 0
0.00.081.982 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.982 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.983 I llm_load_print_meta: n_gqa            = 1
0.00.081.984 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.984 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.985 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.986 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.986 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.986 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.987 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.988 I llm_load_print_meta: n_ff             = 8192
0.00.081.988 I llm_load_print_meta: n_expert         = 0
0.00.081.988 I llm_load_print_meta: n_expert_used    = 0
0.00.081.988 I llm_load_print_meta: causal attn      = 1
0.00.081.989 I llm_load_print_meta: pooling type     = 0
0.00.081.991 I llm_load_print_meta: rope type        = 2
0.00.081.991 I llm_load_print_meta: rope scaling     = linear
0.00.081.992 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.992 I llm_load_print_meta: freq_scale_train = 1
0.00.081.993 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.993 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.993 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.993 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.994 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.994 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.994 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.994 I llm_load_print_meta: model type       = 1.4B
0.00.081.995 I llm_load_print_meta: model ftype      = Q8_0
0.00.081.996 I llm_load_print_meta: model params     = 1.41 B
0.00.081.996 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.081.996 I llm_load_print_meta: general.name     = 1.4B
0.00.081.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.997 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.997 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.998 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.998 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.081.998 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.999 I llm_load_print_meta: max token length = 1024
0.00.084.973 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.974 I llm_load_tensors: offloading output layer to GPU
0.00.084.974 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.981 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.084.984 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.086.621 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.623 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.623 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.624 I llama_new_context_with_model: n_batch       = 2048
0.00.086.624 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.624 I llama_new_context_with_model: flash_attn    = 0
0.00.086.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.625 I llama_new_context_with_model: freq_scale    = 1
0.00.086.626 I ggml_metal_init: allocating
0.00.086.634 I ggml_metal_init: found device: Apple M4
0.00.086.638 I ggml_metal_init: picking default device: Apple M4
0.00.087.750 I ggml_metal_init: using embedded metal library
0.00.091.872 I ggml_metal_init: GPU name:   Apple M4
0.00.091.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.876 I ggml_metal_init: simdgroup reduction   = true
0.00.091.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.877 I ggml_metal_init: has bfloat            = true
0.00.091.877 I ggml_metal_init: use bfloat            = true
0.00.091.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.129.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.740 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.917 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.130.919 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.130.919 I llama_new_context_with_model: graph nodes  = 967
0.00.130.919 I llama_new_context_with_model: graph splits = 2
0.00.130.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.352.114 I main: llama threadpool init, n_threads = 4
0.01.352.155 I 
0.01.352.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.352.186 I 
0.01.352.445 I sampler seed: 1234
0.01.352.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.352.489 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.352.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.352.496 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.446.628 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.02.446.628 I llama_perf_context_print:        load time =    1342.31 ms
0.02.446.629 I llama_perf_context_print: prompt eval time =      47.73 ms /     7 tokens (    6.82 ms per token,   146.65 tokens per second)
0.02.446.630 I llama_perf_context_print:        eval time =    1043.64 ms /    63 runs   (   16.57 ms per token,    60.37 tokens per second)
0.02.446.630 I llama_perf_context_print:       total time =    1094.52 ms /    70 tokens
0.02.446.828 I ggml_metal_free: deallocating

real	0m2.465s
user	0m0.128s
sys	0m0.235s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.848 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.515 I llama_model_loader: - type  f32:  194 tensors
0.00.027.516 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.516 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.675 I llm_load_vocab: special tokens cache size = 25
0.00.054.814 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.817 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.818 I llm_load_print_meta: arch             = gptneox
0.00.054.818 I llm_load_print_meta: vocab type       = BPE
0.00.054.819 I llm_load_print_meta: n_vocab          = 50304
0.00.054.819 I llm_load_print_meta: n_merges         = 50009
0.00.054.819 I llm_load_print_meta: vocab_only       = 0
0.00.054.819 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.819 I llm_load_print_meta: n_embd           = 2048
0.00.054.820 I llm_load_print_meta: n_layer          = 24
0.00.054.824 I llm_load_print_meta: n_head           = 16
0.00.054.825 I llm_load_print_meta: n_head_kv        = 16
0.00.054.825 I llm_load_print_meta: n_rot            = 32
0.00.054.825 I llm_load_print_meta: n_swa            = 0
0.00.054.829 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.829 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.830 I llm_load_print_meta: n_gqa            = 1
0.00.054.831 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.832 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.832 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.833 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.833 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.833 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.835 I llm_load_print_meta: n_ff             = 8192
0.00.054.835 I llm_load_print_meta: n_expert         = 0
0.00.054.836 I llm_load_print_meta: n_expert_used    = 0
0.00.054.836 I llm_load_print_meta: causal attn      = 1
0.00.054.836 I llm_load_print_meta: pooling type     = 0
0.00.054.836 I llm_load_print_meta: rope type        = 2
0.00.054.836 I llm_load_print_meta: rope scaling     = linear
0.00.054.837 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.837 I llm_load_print_meta: freq_scale_train = 1
0.00.054.837 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.839 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.839 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.841 I llm_load_print_meta: model type       = 1.4B
0.00.054.841 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.841 I llm_load_print_meta: model params     = 1.41 B
0.00.054.842 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.842 I llm_load_print_meta: general.name     = 1.4B
0.00.054.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.844 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.844 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.844 I llm_load_print_meta: max token length = 1024
0.00.057.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.156 I llm_load_tensors: offloading output layer to GPU
0.00.057.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.168 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.170 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.174 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.174 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.175 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.175 I llama_new_context_with_model: n_batch       = 2048
0.00.058.175 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.175 I llama_new_context_with_model: flash_attn    = 0
0.00.058.175 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.176 I llama_new_context_with_model: freq_scale    = 1
0.00.058.176 I ggml_metal_init: allocating
0.00.058.182 I ggml_metal_init: found device: Apple M4
0.00.058.187 I ggml_metal_init: picking default device: Apple M4
0.00.058.918 I ggml_metal_init: using embedded metal library
0.00.061.539 I ggml_metal_init: GPU name:   Apple M4
0.00.061.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.542 I ggml_metal_init: simdgroup reduction   = true
0.00.061.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.542 I ggml_metal_init: has bfloat            = true
0.00.061.542 I ggml_metal_init: use bfloat            = true
0.00.061.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.739 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.873 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.875 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.876 I llama_new_context_with_model: graph nodes  = 967
0.00.095.876 I llama_new_context_with_model: graph splits = 2
0.00.095.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.230 I main: llama threadpool init, n_threads = 4
0.00.691.276 I 
0.00.691.302 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.302 I 
0.00.691.539 I sampler seed: 1234
0.00.691.544 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.555 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.555 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.366.897 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.366.898 I llama_perf_context_print:        load time =     679.38 ms
0.01.366.899 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.90 tokens per second)
0.01.366.900 I llama_perf_context_print:        eval time =     632.63 ms /    63 runs   (   10.04 ms per token,    99.58 tokens per second)
0.01.366.900 I llama_perf_context_print:       total time =     675.67 ms /    70 tokens
0.01.367.085 I ggml_metal_free: deallocating

real	0m1.384s
user	0m0.111s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.994 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.914 I llama_model_loader: - type  f32:  194 tensors
0.00.025.914 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.915 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.786 I llm_load_vocab: special tokens cache size = 25
0.00.051.716 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.719 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.719 I llm_load_print_meta: arch             = gptneox
0.00.051.719 I llm_load_print_meta: vocab type       = BPE
0.00.051.719 I llm_load_print_meta: n_vocab          = 50304
0.00.051.720 I llm_load_print_meta: n_merges         = 50009
0.00.051.720 I llm_load_print_meta: vocab_only       = 0
0.00.051.720 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.720 I llm_load_print_meta: n_embd           = 2048
0.00.051.720 I llm_load_print_meta: n_layer          = 24
0.00.051.723 I llm_load_print_meta: n_head           = 16
0.00.051.724 I llm_load_print_meta: n_head_kv        = 16
0.00.051.724 I llm_load_print_meta: n_rot            = 32
0.00.051.724 I llm_load_print_meta: n_swa            = 0
0.00.051.725 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.726 I llm_load_print_meta: n_gqa            = 1
0.00.051.727 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.728 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.729 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.729 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.729 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.729 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.730 I llm_load_print_meta: n_ff             = 8192
0.00.051.730 I llm_load_print_meta: n_expert         = 0
0.00.051.730 I llm_load_print_meta: n_expert_used    = 0
0.00.051.732 I llm_load_print_meta: causal attn      = 1
0.00.051.733 I llm_load_print_meta: pooling type     = 0
0.00.051.733 I llm_load_print_meta: rope type        = 2
0.00.051.734 I llm_load_print_meta: rope scaling     = linear
0.00.051.734 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.734 I llm_load_print_meta: freq_scale_train = 1
0.00.051.735 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.735 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.735 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.736 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.736 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.736 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.736 I llm_load_print_meta: model type       = 1.4B
0.00.051.736 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.737 I llm_load_print_meta: model params     = 1.41 B
0.00.051.737 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.739 I llm_load_print_meta: general.name     = 1.4B
0.00.051.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.740 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.740 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.740 I llm_load_print_meta: max token length = 1024
0.00.053.524 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.524 I llm_load_tensors: offloading output layer to GPU
0.00.053.524 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.530 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.530 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.406 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.406 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.407 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.407 I llama_new_context_with_model: n_batch       = 2048
0.00.054.407 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.407 I llama_new_context_with_model: flash_attn    = 0
0.00.054.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.408 I llama_new_context_with_model: freq_scale    = 1
0.00.054.409 I ggml_metal_init: allocating
0.00.054.415 I ggml_metal_init: found device: Apple M4
0.00.054.417 I ggml_metal_init: picking default device: Apple M4
0.00.055.025 I ggml_metal_init: using embedded metal library
0.00.057.324 I ggml_metal_init: GPU name:   Apple M4
0.00.057.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.326 I ggml_metal_init: simdgroup reduction   = true
0.00.057.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.327 I ggml_metal_init: has bfloat            = true
0.00.057.328 I ggml_metal_init: use bfloat            = true
0.00.057.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.442 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.447 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.465 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.509 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.511 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.511 I llama_new_context_with_model: graph nodes  = 967
0.00.087.511 I llama_new_context_with_model: graph splits = 2
0.00.087.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.303 I main: llama threadpool init, n_threads = 4
0.00.735.344 I 
0.00.735.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.374 I 
0.00.735.622 I sampler seed: 1234
0.00.735.625 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.636 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.637 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.637 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.460.824 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.01.460.825 I llama_perf_context_print:        load time =     724.31 ms
0.01.460.826 I llama_perf_context_print: prompt eval time =      39.64 ms /     7 tokens (    5.66 ms per token,   176.60 tokens per second)
0.01.460.827 I llama_perf_context_print:        eval time =     682.54 ms /    63 runs   (   10.83 ms per token,    92.30 tokens per second)
0.01.460.827 I llama_perf_context_print:       total time =     725.52 ms /    70 tokens
0.01.461.022 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.317 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.319 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.995 I llama_model_loader: - type  f32:  194 tensors
0.00.025.995 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.001 I llm_load_vocab: special tokens cache size = 25
0.00.051.861 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.863 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.864 I llm_load_print_meta: arch             = gptneox
0.00.051.864 I llm_load_print_meta: vocab type       = BPE
0.00.051.864 I llm_load_print_meta: n_vocab          = 50304
0.00.051.864 I llm_load_print_meta: n_merges         = 50009
0.00.051.865 I llm_load_print_meta: vocab_only       = 0
0.00.051.865 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.865 I llm_load_print_meta: n_embd           = 2048
0.00.051.865 I llm_load_print_meta: n_layer          = 24
0.00.051.868 I llm_load_print_meta: n_head           = 16
0.00.051.869 I llm_load_print_meta: n_head_kv        = 16
0.00.051.869 I llm_load_print_meta: n_rot            = 32
0.00.051.869 I llm_load_print_meta: n_swa            = 0
0.00.051.869 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.869 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.870 I llm_load_print_meta: n_gqa            = 1
0.00.051.871 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.872 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.873 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.873 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.874 I llm_load_print_meta: n_ff             = 8192
0.00.051.874 I llm_load_print_meta: n_expert         = 0
0.00.051.874 I llm_load_print_meta: n_expert_used    = 0
0.00.051.874 I llm_load_print_meta: causal attn      = 1
0.00.051.874 I llm_load_print_meta: pooling type     = 0
0.00.051.875 I llm_load_print_meta: rope type        = 2
0.00.051.875 I llm_load_print_meta: rope scaling     = linear
0.00.051.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.876 I llm_load_print_meta: freq_scale_train = 1
0.00.051.877 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.877 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.877 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.877 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.878 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.878 I llm_load_print_meta: model type       = 1.4B
0.00.051.880 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.880 I llm_load_print_meta: model params     = 1.41 B
0.00.051.881 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.881 I llm_load_print_meta: general.name     = 1.4B
0.00.051.881 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.881 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.881 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.882 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: max token length = 1024
0.00.053.830 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.830 I llm_load_tensors: offloading output layer to GPU
0.00.053.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.841 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.842 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.750 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.751 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.751 I llama_new_context_with_model: n_batch       = 2048
0.00.054.751 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.751 I llama_new_context_with_model: flash_attn    = 0
0.00.054.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.752 I llama_new_context_with_model: freq_scale    = 1
0.00.054.752 I ggml_metal_init: allocating
0.00.054.756 I ggml_metal_init: found device: Apple M4
0.00.054.758 I ggml_metal_init: picking default device: Apple M4
0.00.055.348 I ggml_metal_init: using embedded metal library
0.00.057.663 I ggml_metal_init: GPU name:   Apple M4
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.666 I ggml_metal_init: simdgroup reduction   = true
0.00.057.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.666 I ggml_metal_init: has bfloat            = true
0.00.057.666 I ggml_metal_init: use bfloat            = true
0.00.057.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.391 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.397 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.415 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.528 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.529 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.529 I llama_new_context_with_model: graph nodes  = 967
0.00.087.530 I llama_new_context_with_model: graph splits = 2
0.00.087.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.393 I main: llama threadpool init, n_threads = 4
0.00.790.426 I 
0.00.790.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.456 I 
0.00.790.728 I sampler seed: 1234
0.00.790.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.754 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.755 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.578.400 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.578.401 I llama_perf_context_print:        load time =     779.65 ms
0.01.578.401 I llama_perf_context_print: prompt eval time =      46.08 ms /     7 tokens (    6.58 ms per token,   151.92 tokens per second)
0.01.578.402 I llama_perf_context_print:        eval time =     738.63 ms /    63 runs   (   11.72 ms per token,    85.29 tokens per second)
0.01.578.403 I llama_perf_context_print:       total time =     788.01 ms /    70 tokens
0.01.578.593 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.108s
sys	0m0.166s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.620 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.864 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.862 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.694 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.695 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.695 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.696 I llama_model_loader: - type  f32:  194 tensors
0.00.024.696 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.696 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.770 I llm_load_vocab: special tokens cache size = 25
0.00.050.676 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.678 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.679 I llm_load_print_meta: arch             = gptneox
0.00.050.679 I llm_load_print_meta: vocab type       = BPE
0.00.050.679 I llm_load_print_meta: n_vocab          = 50304
0.00.050.680 I llm_load_print_meta: n_merges         = 50009
0.00.050.680 I llm_load_print_meta: vocab_only       = 0
0.00.050.680 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.680 I llm_load_print_meta: n_embd           = 2048
0.00.050.680 I llm_load_print_meta: n_layer          = 24
0.00.050.683 I llm_load_print_meta: n_head           = 16
0.00.050.684 I llm_load_print_meta: n_head_kv        = 16
0.00.050.684 I llm_load_print_meta: n_rot            = 32
0.00.050.684 I llm_load_print_meta: n_swa            = 0
0.00.050.684 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.684 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.685 I llm_load_print_meta: n_gqa            = 1
0.00.050.686 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.687 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.688 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.689 I llm_load_print_meta: n_ff             = 8192
0.00.050.689 I llm_load_print_meta: n_expert         = 0
0.00.050.689 I llm_load_print_meta: n_expert_used    = 0
0.00.050.691 I llm_load_print_meta: causal attn      = 1
0.00.050.693 I llm_load_print_meta: pooling type     = 0
0.00.050.693 I llm_load_print_meta: rope type        = 2
0.00.050.693 I llm_load_print_meta: rope scaling     = linear
0.00.050.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.694 I llm_load_print_meta: freq_scale_train = 1
0.00.050.694 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.694 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.695 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.695 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.695 I llm_load_print_meta: model type       = 1.4B
0.00.050.695 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.696 I llm_load_print_meta: model params     = 1.41 B
0.00.050.696 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.698 I llm_load_print_meta: general.name     = 1.4B
0.00.050.698 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.698 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.699 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.699 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.699 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.699 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.701 I llm_load_print_meta: max token length = 1024
0.00.052.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.689 I llm_load_tensors: offloading output layer to GPU
0.00.052.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.700 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.701 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.640 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.640 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.640 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.641 I llama_new_context_with_model: n_batch       = 2048
0.00.053.641 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.641 I llama_new_context_with_model: flash_attn    = 0
0.00.053.641 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.642 I llama_new_context_with_model: freq_scale    = 1
0.00.053.642 I ggml_metal_init: allocating
0.00.053.645 I ggml_metal_init: found device: Apple M4
0.00.053.647 I ggml_metal_init: picking default device: Apple M4
0.00.054.260 I ggml_metal_init: using embedded metal library
0.00.056.606 I ggml_metal_init: GPU name:   Apple M4
0.00.056.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.609 I ggml_metal_init: simdgroup reduction   = true
0.00.056.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.611 I ggml_metal_init: has bfloat            = true
0.00.056.611 I ggml_metal_init: use bfloat            = true
0.00.056.611 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.831 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.837 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.859 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.967 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.969 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.969 I llama_new_context_with_model: graph nodes  = 967
0.00.086.969 I llama_new_context_with_model: graph splits = 2
0.00.086.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.592 I main: llama threadpool init, n_threads = 4
0.00.714.628 I 
0.00.714.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.666 I 
0.00.714.897 I sampler seed: 1234
0.00.714.901 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.925 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.926 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.559.327 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.559.328 I llama_perf_context_print:        load time =     704.97 ms
0.01.559.329 I llama_perf_context_print: prompt eval time =      42.35 ms /     7 tokens (    6.05 ms per token,   165.30 tokens per second)
0.01.559.329 I llama_perf_context_print:        eval time =     799.50 ms /    63 runs   (   12.69 ms per token,    78.80 tokens per second)
0.01.559.330 I llama_perf_context_print:       total time =     844.74 ms /    70 tokens
0.01.559.544 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.504 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.983 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.983 I llama_model_loader: - type  f32:  194 tensors
0.00.023.984 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.984 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.984 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.960 I llm_load_vocab: special tokens cache size = 25
0.00.049.943 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.947 I llm_load_print_meta: arch             = gptneox
0.00.049.948 I llm_load_print_meta: vocab type       = BPE
0.00.049.948 I llm_load_print_meta: n_vocab          = 50304
0.00.049.948 I llm_load_print_meta: n_merges         = 50009
0.00.049.949 I llm_load_print_meta: vocab_only       = 0
0.00.049.949 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.949 I llm_load_print_meta: n_embd           = 2048
0.00.049.949 I llm_load_print_meta: n_layer          = 24
0.00.049.952 I llm_load_print_meta: n_head           = 16
0.00.049.953 I llm_load_print_meta: n_head_kv        = 16
0.00.049.953 I llm_load_print_meta: n_rot            = 32
0.00.049.953 I llm_load_print_meta: n_swa            = 0
0.00.049.954 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.954 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.954 I llm_load_print_meta: n_gqa            = 1
0.00.049.955 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.956 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.958 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.959 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.959 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.960 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.960 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.961 I llm_load_print_meta: n_ff             = 8192
0.00.049.961 I llm_load_print_meta: n_expert         = 0
0.00.049.961 I llm_load_print_meta: n_expert_used    = 0
0.00.049.961 I llm_load_print_meta: causal attn      = 1
0.00.049.961 I llm_load_print_meta: pooling type     = 0
0.00.049.961 I llm_load_print_meta: rope type        = 2
0.00.049.962 I llm_load_print_meta: rope scaling     = linear
0.00.049.962 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.962 I llm_load_print_meta: freq_scale_train = 1
0.00.049.962 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.963 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.963 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.963 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.963 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.963 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.963 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.964 I llm_load_print_meta: model type       = 1.4B
0.00.049.968 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.969 I llm_load_print_meta: model params     = 1.41 B
0.00.049.969 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.969 I llm_load_print_meta: general.name     = 1.4B
0.00.049.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.970 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.970 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.972 I llm_load_print_meta: max token length = 1024
0.00.051.742 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.742 I llm_load_tensors: offloading output layer to GPU
0.00.051.742 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.748 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.748 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.756 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.757 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.757 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.757 I llama_new_context_with_model: n_batch       = 2048
0.00.052.758 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.758 I llama_new_context_with_model: flash_attn    = 0
0.00.052.758 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.758 I llama_new_context_with_model: freq_scale    = 1
0.00.052.759 I ggml_metal_init: allocating
0.00.052.762 I ggml_metal_init: found device: Apple M4
0.00.052.764 I ggml_metal_init: picking default device: Apple M4
0.00.053.364 I ggml_metal_init: using embedded metal library
0.00.055.725 I ggml_metal_init: GPU name:   Apple M4
0.00.055.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.728 I ggml_metal_init: simdgroup reduction   = true
0.00.055.728 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.728 I ggml_metal_init: has bfloat            = true
0.00.055.729 I ggml_metal_init: use bfloat            = true
0.00.055.729 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.731 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.892 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.898 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.874 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.875 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.876 I llama_new_context_with_model: graph nodes  = 967
0.00.087.876 I llama_new_context_with_model: graph splits = 2
0.00.087.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.148 I main: llama threadpool init, n_threads = 4
0.00.437.185 I 
0.00.437.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.214 I 
0.00.437.442 I sampler seed: 1234
0.00.437.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.495 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.495 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.117.114 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.117.115 I llama_perf_context_print:        load time =     427.64 ms
0.01.117.116 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.65 tokens per second)
0.01.117.116 I llama_perf_context_print:        eval time =     640.96 ms /    63 runs   (   10.17 ms per token,    98.29 tokens per second)
0.01.117.120 I llama_perf_context_print:       total time =     679.97 ms /    70 tokens
0.01.117.315 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.109s
sys	0m0.109s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.276 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.812 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.855 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.994 I llama_model_loader: - type  f32:  194 tensors
0.00.025.994 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.994 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.994 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.831 I llm_load_vocab: special tokens cache size = 25
0.00.052.826 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.829 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.829 I llm_load_print_meta: arch             = gptneox
0.00.052.829 I llm_load_print_meta: vocab type       = BPE
0.00.052.830 I llm_load_print_meta: n_vocab          = 50304
0.00.052.830 I llm_load_print_meta: n_merges         = 50009
0.00.052.830 I llm_load_print_meta: vocab_only       = 0
0.00.052.830 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.830 I llm_load_print_meta: n_embd           = 2048
0.00.052.830 I llm_load_print_meta: n_layer          = 24
0.00.052.834 I llm_load_print_meta: n_head           = 16
0.00.052.834 I llm_load_print_meta: n_head_kv        = 16
0.00.052.834 I llm_load_print_meta: n_rot            = 32
0.00.052.837 I llm_load_print_meta: n_swa            = 0
0.00.052.837 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.837 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.838 I llm_load_print_meta: n_gqa            = 1
0.00.052.839 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.839 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.840 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.840 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.840 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.841 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.841 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.841 I llm_load_print_meta: n_ff             = 8192
0.00.052.842 I llm_load_print_meta: n_expert         = 0
0.00.052.842 I llm_load_print_meta: n_expert_used    = 0
0.00.052.842 I llm_load_print_meta: causal attn      = 1
0.00.052.842 I llm_load_print_meta: pooling type     = 0
0.00.052.842 I llm_load_print_meta: rope type        = 2
0.00.052.842 I llm_load_print_meta: rope scaling     = linear
0.00.052.843 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.843 I llm_load_print_meta: freq_scale_train = 1
0.00.052.843 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.844 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.844 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.844 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.845 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.845 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.845 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.845 I llm_load_print_meta: model type       = 1.4B
0.00.052.846 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.846 I llm_load_print_meta: model params     = 1.41 B
0.00.052.847 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.847 I llm_load_print_meta: general.name     = 1.4B
0.00.052.847 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.849 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.849 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.849 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.849 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.850 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.850 I llm_load_print_meta: max token length = 1024
0.00.054.819 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.819 I llm_load_tensors: offloading output layer to GPU
0.00.054.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.829 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.830 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.772 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.773 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.773 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.773 I llama_new_context_with_model: n_batch       = 2048
0.00.055.773 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.774 I llama_new_context_with_model: flash_attn    = 0
0.00.055.774 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.774 I llama_new_context_with_model: freq_scale    = 1
0.00.055.775 I ggml_metal_init: allocating
0.00.055.778 I ggml_metal_init: found device: Apple M4
0.00.055.780 I ggml_metal_init: picking default device: Apple M4
0.00.056.385 I ggml_metal_init: using embedded metal library
0.00.058.718 I ggml_metal_init: GPU name:   Apple M4
0.00.058.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.720 I ggml_metal_init: simdgroup reduction   = true
0.00.058.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.721 I ggml_metal_init: has bfloat            = true
0.00.058.721 I ggml_metal_init: use bfloat            = true
0.00.058.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.571 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.649 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.650 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.650 I llama_new_context_with_model: graph nodes  = 967
0.00.089.651 I llama_new_context_with_model: graph splits = 2
0.00.089.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.231 I main: llama threadpool init, n_threads = 4
0.00.533.276 I 
0.00.533.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.308 I 
0.00.533.473 I sampler seed: 1234
0.00.533.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.533.487 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.533.488 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.533.488 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.043 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.279.044 I llama_perf_context_print:        load time =     522.95 ms
0.01.279.045 I llama_perf_context_print: prompt eval time =      40.44 ms /     7 tokens (    5.78 ms per token,   173.10 tokens per second)
0.01.279.045 I llama_perf_context_print:        eval time =     702.09 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.279.046 I llama_perf_context_print:       total time =     745.82 ms /    70 tokens
0.01.279.228 I ggml_metal_free: deallocating

real	0m1.294s
user	0m0.111s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.763 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.373 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.375 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.376 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.376 I llama_model_loader: - type  f32:  194 tensors
0.00.025.376 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.376 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.377 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.275 I llm_load_vocab: special tokens cache size = 25
0.00.051.333 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.336 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.336 I llm_load_print_meta: arch             = gptneox
0.00.051.336 I llm_load_print_meta: vocab type       = BPE
0.00.051.337 I llm_load_print_meta: n_vocab          = 50304
0.00.051.337 I llm_load_print_meta: n_merges         = 50009
0.00.051.337 I llm_load_print_meta: vocab_only       = 0
0.00.051.337 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.337 I llm_load_print_meta: n_embd           = 2048
0.00.051.338 I llm_load_print_meta: n_layer          = 24
0.00.051.340 I llm_load_print_meta: n_head           = 16
0.00.051.341 I llm_load_print_meta: n_head_kv        = 16
0.00.051.341 I llm_load_print_meta: n_rot            = 32
0.00.051.341 I llm_load_print_meta: n_swa            = 0
0.00.051.341 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.342 I llm_load_print_meta: n_gqa            = 1
0.00.051.343 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.344 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.344 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.344 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.348 I llm_load_print_meta: n_ff             = 8192
0.00.051.348 I llm_load_print_meta: n_expert         = 0
0.00.051.350 I llm_load_print_meta: n_expert_used    = 0
0.00.051.351 I llm_load_print_meta: causal attn      = 1
0.00.051.351 I llm_load_print_meta: pooling type     = 0
0.00.051.351 I llm_load_print_meta: rope type        = 2
0.00.051.352 I llm_load_print_meta: rope scaling     = linear
0.00.051.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.352 I llm_load_print_meta: freq_scale_train = 1
0.00.051.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.353 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.353 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.353 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.354 I llm_load_print_meta: model type       = 1.4B
0.00.051.358 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.358 I llm_load_print_meta: model params     = 1.41 B
0.00.051.359 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.359 I llm_load_print_meta: general.name     = 1.4B
0.00.051.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: max token length = 1024
0.00.053.126 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.126 I llm_load_tensors: offloading output layer to GPU
0.00.053.127 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.132 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.132 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.986 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.987 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.987 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.987 I llama_new_context_with_model: n_batch       = 2048
0.00.053.987 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.987 I llama_new_context_with_model: flash_attn    = 0
0.00.053.988 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.988 I llama_new_context_with_model: freq_scale    = 1
0.00.053.988 I ggml_metal_init: allocating
0.00.053.992 I ggml_metal_init: found device: Apple M4
0.00.053.994 I ggml_metal_init: picking default device: Apple M4
0.00.054.619 I ggml_metal_init: using embedded metal library
0.00.056.981 I ggml_metal_init: GPU name:   Apple M4
0.00.056.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.985 I ggml_metal_init: simdgroup reduction   = true
0.00.056.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.985 I ggml_metal_init: has bfloat            = true
0.00.056.985 I ggml_metal_init: use bfloat            = true
0.00.056.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.620 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.630 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.651 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.718 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.720 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.721 I llama_new_context_with_model: graph nodes  = 967
0.00.087.721 I llama_new_context_with_model: graph splits = 2
0.00.087.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.775 I main: llama threadpool init, n_threads = 4
0.00.629.817 I 
0.00.629.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.862 I 
0.00.630.104 I sampler seed: 1234
0.00.630.109 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.153 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.155 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.155 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.934 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.392.934 I llama_perf_context_print:        load time =     620.01 ms
0.01.392.936 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.59 tokens per second)
0.01.392.937 I llama_perf_context_print:        eval time =     712.56 ms /    63 runs   (   11.31 ms per token,    88.41 tokens per second)
0.01.392.938 I llama_perf_context_print:       total time =     763.16 ms /    70 tokens
0.01.393.136 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.546 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.913 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.914 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.916 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.918 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.918 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.925 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.806 I llama_model_loader: - type  f32:  194 tensors
0.00.025.806 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.806 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.808 I llm_load_vocab: special tokens cache size = 25
0.00.052.731 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.734 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.734 I llm_load_print_meta: arch             = gptneox
0.00.052.735 I llm_load_print_meta: vocab type       = BPE
0.00.052.735 I llm_load_print_meta: n_vocab          = 50304
0.00.052.735 I llm_load_print_meta: n_merges         = 50009
0.00.052.735 I llm_load_print_meta: vocab_only       = 0
0.00.052.735 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.736 I llm_load_print_meta: n_embd           = 2048
0.00.052.736 I llm_load_print_meta: n_layer          = 24
0.00.052.738 I llm_load_print_meta: n_head           = 16
0.00.052.739 I llm_load_print_meta: n_head_kv        = 16
0.00.052.739 I llm_load_print_meta: n_rot            = 32
0.00.052.739 I llm_load_print_meta: n_swa            = 0
0.00.052.740 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.740 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.741 I llm_load_print_meta: n_gqa            = 1
0.00.052.741 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.742 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.743 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.744 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.744 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.744 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.745 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.745 I llm_load_print_meta: n_ff             = 8192
0.00.052.745 I llm_load_print_meta: n_expert         = 0
0.00.052.746 I llm_load_print_meta: n_expert_used    = 0
0.00.052.747 I llm_load_print_meta: causal attn      = 1
0.00.052.749 I llm_load_print_meta: pooling type     = 0
0.00.052.749 I llm_load_print_meta: rope type        = 2
0.00.052.749 I llm_load_print_meta: rope scaling     = linear
0.00.052.750 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.750 I llm_load_print_meta: freq_scale_train = 1
0.00.052.750 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.750 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.751 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.751 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.751 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.751 I llm_load_print_meta: model type       = 1.4B
0.00.052.752 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.752 I llm_load_print_meta: model params     = 1.41 B
0.00.052.753 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.753 I llm_load_print_meta: general.name     = 1.4B
0.00.052.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.753 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.754 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.756 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.756 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.756 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.756 I llm_load_print_meta: max token length = 1024
0.00.054.808 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.808 I llm_load_tensors: offloading output layer to GPU
0.00.054.808 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.819 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.820 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.694 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.695 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.695 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.695 I llama_new_context_with_model: n_batch       = 2048
0.00.055.696 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.696 I llama_new_context_with_model: flash_attn    = 0
0.00.055.696 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.696 I llama_new_context_with_model: freq_scale    = 1
0.00.055.697 I ggml_metal_init: allocating
0.00.055.703 I ggml_metal_init: found device: Apple M4
0.00.055.705 I ggml_metal_init: picking default device: Apple M4
0.00.056.296 I ggml_metal_init: using embedded metal library
0.00.058.614 I ggml_metal_init: GPU name:   Apple M4
0.00.058.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.617 I ggml_metal_init: simdgroup reduction   = true
0.00.058.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.618 I ggml_metal_init: has bfloat            = true
0.00.058.618 I ggml_metal_init: use bfloat            = true
0.00.058.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.333 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.346 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.368 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.286 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.288 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.288 I llama_new_context_with_model: graph nodes  = 967
0.00.088.288 I llama_new_context_with_model: graph splits = 2
0.00.088.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.988 I main: llama threadpool init, n_threads = 4
0.00.703.028 I 
0.00.703.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.060 I 
0.00.703.311 I sampler seed: 1234
0.00.703.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.337 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.338 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.338 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.905 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.557.905 I llama_perf_context_print:        load time =     692.44 ms
0.01.557.910 I llama_perf_context_print: prompt eval time =      58.45 ms /     7 tokens (    8.35 ms per token,   119.77 tokens per second)
0.01.557.911 I llama_perf_context_print:        eval time =     793.47 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.557.911 I llama_perf_context_print:       total time =     854.92 ms /    70 tokens
0.01.558.136 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.826 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.604 I llama_model_loader: - type  f32:  194 tensors
0.00.025.604 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.361 I llm_load_vocab: special tokens cache size = 25
0.00.052.304 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.307 I llm_load_print_meta: arch             = gptneox
0.00.052.307 I llm_load_print_meta: vocab type       = BPE
0.00.052.308 I llm_load_print_meta: n_vocab          = 50304
0.00.052.308 I llm_load_print_meta: n_merges         = 50009
0.00.052.308 I llm_load_print_meta: vocab_only       = 0
0.00.052.308 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.308 I llm_load_print_meta: n_embd           = 2048
0.00.052.309 I llm_load_print_meta: n_layer          = 24
0.00.052.311 I llm_load_print_meta: n_head           = 16
0.00.052.312 I llm_load_print_meta: n_head_kv        = 16
0.00.052.312 I llm_load_print_meta: n_rot            = 32
0.00.052.312 I llm_load_print_meta: n_swa            = 0
0.00.052.313 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.313 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.313 I llm_load_print_meta: n_gqa            = 1
0.00.052.314 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.315 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.315 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.316 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.316 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.316 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.316 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.317 I llm_load_print_meta: n_ff             = 8192
0.00.052.318 I llm_load_print_meta: n_expert         = 0
0.00.052.318 I llm_load_print_meta: n_expert_used    = 0
0.00.052.318 I llm_load_print_meta: causal attn      = 1
0.00.052.318 I llm_load_print_meta: pooling type     = 0
0.00.052.319 I llm_load_print_meta: rope type        = 2
0.00.052.319 I llm_load_print_meta: rope scaling     = linear
0.00.052.319 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.320 I llm_load_print_meta: freq_scale_train = 1
0.00.052.320 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.320 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.320 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.321 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.321 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.321 I llm_load_print_meta: model type       = 1.4B
0.00.052.322 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.322 I llm_load_print_meta: model params     = 1.41 B
0.00.052.323 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.323 I llm_load_print_meta: general.name     = 1.4B
0.00.052.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.324 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: max token length = 1024
0.00.054.362 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.362 I llm_load_tensors: offloading output layer to GPU
0.00.054.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.373 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.374 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.298 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.298 I llama_new_context_with_model: n_batch       = 2048
0.00.055.299 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.299 I llama_new_context_with_model: flash_attn    = 0
0.00.055.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.299 I llama_new_context_with_model: freq_scale    = 1
0.00.055.300 I ggml_metal_init: allocating
0.00.055.306 I ggml_metal_init: found device: Apple M4
0.00.055.308 I ggml_metal_init: picking default device: Apple M4
0.00.055.895 I ggml_metal_init: using embedded metal library
0.00.058.198 I ggml_metal_init: GPU name:   Apple M4
0.00.058.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.201 I ggml_metal_init: simdgroup reduction   = true
0.00.058.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.201 I ggml_metal_init: has bfloat            = true
0.00.058.201 I ggml_metal_init: use bfloat            = true
0.00.058.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.055 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.060 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.078 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.073 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.074 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.074 I llama_new_context_with_model: graph nodes  = 967
0.00.089.075 I llama_new_context_with_model: graph splits = 2
0.00.089.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.166 I main: llama threadpool init, n_threads = 4
0.00.754.200 I 
0.00.754.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.249 I 
0.00.754.480 I sampler seed: 1234
0.00.754.484 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.496 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.637.577 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.637.578 I llama_perf_context_print:        load time =     744.34 ms
0.01.637.578 I llama_perf_context_print: prompt eval time =      54.66 ms /     7 tokens (    7.81 ms per token,   128.06 tokens per second)
0.01.637.580 I llama_perf_context_print:        eval time =     825.47 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.637.580 I llama_perf_context_print:       total time =     883.41 ms /    70 tokens
0.01.637.765 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.109s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.532 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.895 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.909 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.648 I llama_model_loader: - type  f32:  194 tensors
0.00.053.648 I llama_model_loader: - type  f16:   98 tensors
0.00.083.362 I llm_load_vocab: special tokens cache size = 25
0.00.090.186 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.188 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.188 I llm_load_print_meta: arch             = gptneox
0.00.090.188 I llm_load_print_meta: vocab type       = BPE
0.00.090.189 I llm_load_print_meta: n_vocab          = 50304
0.00.090.189 I llm_load_print_meta: n_merges         = 50009
0.00.090.189 I llm_load_print_meta: vocab_only       = 0
0.00.090.189 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.189 I llm_load_print_meta: n_embd           = 2048
0.00.090.189 I llm_load_print_meta: n_layer          = 24
0.00.090.193 I llm_load_print_meta: n_head           = 16
0.00.090.193 I llm_load_print_meta: n_head_kv        = 16
0.00.090.193 I llm_load_print_meta: n_rot            = 32
0.00.090.194 I llm_load_print_meta: n_swa            = 0
0.00.090.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.196 I llm_load_print_meta: n_gqa            = 1
0.00.090.197 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.198 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.198 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.199 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.199 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.199 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.199 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.201 I llm_load_print_meta: n_ff             = 8192
0.00.090.201 I llm_load_print_meta: n_expert         = 0
0.00.090.201 I llm_load_print_meta: n_expert_used    = 0
0.00.090.201 I llm_load_print_meta: causal attn      = 1
0.00.090.202 I llm_load_print_meta: pooling type     = 0
0.00.090.202 I llm_load_print_meta: rope type        = 2
0.00.090.202 I llm_load_print_meta: rope scaling     = linear
0.00.090.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.204 I llm_load_print_meta: freq_scale_train = 1
0.00.090.204 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.204 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.204 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.204 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.205 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.205 I llm_load_print_meta: model type       = 1.4B
0.00.090.206 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.209 I llm_load_print_meta: model params     = 1.41 B
0.00.090.210 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.210 I llm_load_print_meta: general.name     = 1.4B
0.00.090.210 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.212 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.212 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.212 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.212 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.213 I llm_load_print_meta: max token length = 1024
0.00.092.806 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.806 I llm_load_tensors: offloading output layer to GPU
0.00.092.807 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.817 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.818 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.784 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.785 I llama_new_context_with_model: n_ctx         = 128
0.00.093.785 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.785 I llama_new_context_with_model: n_batch       = 128
0.00.093.785 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.786 I llama_new_context_with_model: flash_attn    = 0
0.00.093.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.786 I llama_new_context_with_model: freq_scale    = 1
0.00.093.787 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.787 I ggml_metal_init: allocating
0.00.093.790 I ggml_metal_init: found device: Apple M4
0.00.093.792 I ggml_metal_init: picking default device: Apple M4
0.00.094.389 I ggml_metal_init: using embedded metal library
0.00.096.956 I ggml_metal_init: GPU name:   Apple M4
0.00.096.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.959 I ggml_metal_init: simdgroup reduction   = true
0.00.096.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.959 I ggml_metal_init: has bfloat            = true
0.00.096.959 I ggml_metal_init: use bfloat            = true
0.00.096.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.822 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.826 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.759 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.760 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.760 I llama_new_context_with_model: graph nodes  = 967
0.00.108.760 I llama_new_context_with_model: graph splits = 2
0.00.108.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.845 I 
0.00.843.938 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.969 I perplexity: tokenizing the input ..
0.00.855.741 I perplexity: tokenization took 11.769 ms
0.00.855.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.976.510 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.978.282 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.978.293 I llama_perf_context_print:        load time =     819.97 ms
0.00.978.295 I llama_perf_context_print: prompt eval time =     120.36 ms /   128 tokens (    0.94 ms per token,  1063.47 tokens per second)
0.00.978.297 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.978.297 I llama_perf_context_print:       total time =     134.46 ms /   129 tokens
0.00.978.819 I ggml_metal_free: deallocating

real	0m1.168s
user	0m0.121s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.580 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.581 I llama_model_loader: - type  f32:  194 tensors
0.00.031.581 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.678 I llm_load_vocab: special tokens cache size = 25
0.00.061.846 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.849 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.850 I llm_load_print_meta: arch             = gptneox
0.00.061.850 I llm_load_print_meta: vocab type       = BPE
0.00.061.850 I llm_load_print_meta: n_vocab          = 50304
0.00.061.851 I llm_load_print_meta: n_merges         = 50009
0.00.061.851 I llm_load_print_meta: vocab_only       = 0
0.00.061.851 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.851 I llm_load_print_meta: n_embd           = 2048
0.00.061.851 I llm_load_print_meta: n_layer          = 24
0.00.061.856 I llm_load_print_meta: n_head           = 16
0.00.061.856 I llm_load_print_meta: n_head_kv        = 16
0.00.061.857 I llm_load_print_meta: n_rot            = 32
0.00.061.857 I llm_load_print_meta: n_swa            = 0
0.00.061.857 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.857 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.858 I llm_load_print_meta: n_gqa            = 1
0.00.061.859 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.860 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.860 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.861 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.861 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.864 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.864 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.865 I llm_load_print_meta: n_ff             = 8192
0.00.061.865 I llm_load_print_meta: n_expert         = 0
0.00.061.865 I llm_load_print_meta: n_expert_used    = 0
0.00.061.865 I llm_load_print_meta: causal attn      = 1
0.00.061.865 I llm_load_print_meta: pooling type     = 0
0.00.061.866 I llm_load_print_meta: rope type        = 2
0.00.061.866 I llm_load_print_meta: rope scaling     = linear
0.00.061.866 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.867 I llm_load_print_meta: freq_scale_train = 1
0.00.061.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.867 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.867 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.867 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.869 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.870 I llm_load_print_meta: model type       = 1.4B
0.00.061.870 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.871 I llm_load_print_meta: model params     = 1.41 B
0.00.061.871 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.871 I llm_load_print_meta: general.name     = 1.4B
0.00.061.872 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.872 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.872 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.872 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.873 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.873 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.873 I llm_load_print_meta: max token length = 1024
0.00.064.195 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.196 I llm_load_tensors: offloading output layer to GPU
0.00.064.196 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.207 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.208 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.154 I llama_new_context_with_model: n_ctx         = 128
0.00.065.154 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.154 I llama_new_context_with_model: n_batch       = 128
0.00.065.154 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.155 I llama_new_context_with_model: flash_attn    = 0
0.00.065.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.155 I llama_new_context_with_model: freq_scale    = 1
0.00.065.156 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.156 I ggml_metal_init: allocating
0.00.065.159 I ggml_metal_init: found device: Apple M4
0.00.065.161 I ggml_metal_init: picking default device: Apple M4
0.00.065.787 I ggml_metal_init: using embedded metal library
0.00.068.358 I ggml_metal_init: GPU name:   Apple M4
0.00.068.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.360 I ggml_metal_init: simdgroup reduction   = true
0.00.068.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.361 I ggml_metal_init: has bfloat            = true
0.00.068.361 I ggml_metal_init: use bfloat            = true
0.00.068.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.183 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.185 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.201 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.099 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.100 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.101 I llama_new_context_with_model: graph nodes  = 967
0.00.081.101 I llama_new_context_with_model: graph splits = 2
0.00.081.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.906.377 I 
0.00.906.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.906.433 I perplexity: tokenizing the input ..
0.00.914.231 I perplexity: tokenization took 7.796 ms
0.00.914.242 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.038.780 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.040.020 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.040.037 I llama_perf_context_print:        load time =     894.98 ms
0.01.040.038 I llama_perf_context_print: prompt eval time =     124.31 ms /   128 tokens (    0.97 ms per token,  1029.66 tokens per second)
0.01.040.039 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.040.040 I llama_perf_context_print:       total time =     133.66 ms /   129 tokens
0.01.040.483 I ggml_metal_free: deallocating

real	0m1.057s
user	0m0.090s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.084 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.011 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.011 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.805 I llama_model_loader: - type  f32:  194 tensors
0.00.025.805 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.806 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.730 I llm_load_vocab: special tokens cache size = 25
0.00.051.739 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.742 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.742 I llm_load_print_meta: arch             = gptneox
0.00.051.742 I llm_load_print_meta: vocab type       = BPE
0.00.051.743 I llm_load_print_meta: n_vocab          = 50304
0.00.051.743 I llm_load_print_meta: n_merges         = 50009
0.00.051.743 I llm_load_print_meta: vocab_only       = 0
0.00.051.743 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.743 I llm_load_print_meta: n_embd           = 2048
0.00.051.743 I llm_load_print_meta: n_layer          = 24
0.00.051.746 I llm_load_print_meta: n_head           = 16
0.00.051.747 I llm_load_print_meta: n_head_kv        = 16
0.00.051.747 I llm_load_print_meta: n_rot            = 32
0.00.051.747 I llm_load_print_meta: n_swa            = 0
0.00.051.747 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.748 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.748 I llm_load_print_meta: n_gqa            = 1
0.00.051.749 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.750 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.751 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.751 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.751 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.751 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.754 I llm_load_print_meta: n_ff             = 8192
0.00.051.754 I llm_load_print_meta: n_expert         = 0
0.00.051.754 I llm_load_print_meta: n_expert_used    = 0
0.00.051.754 I llm_load_print_meta: causal attn      = 1
0.00.051.754 I llm_load_print_meta: pooling type     = 0
0.00.051.763 I llm_load_print_meta: rope type        = 2
0.00.051.764 I llm_load_print_meta: rope scaling     = linear
0.00.051.766 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.766 I llm_load_print_meta: freq_scale_train = 1
0.00.051.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.767 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.768 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.768 I llm_load_print_meta: model type       = 1.4B
0.00.051.768 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.770 I llm_load_print_meta: model params     = 1.41 B
0.00.051.771 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.771 I llm_load_print_meta: general.name     = 1.4B
0.00.051.771 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.771 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.774 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.774 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.774 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.777 I llm_load_print_meta: max token length = 1024
0.00.053.706 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.707 I llm_load_tensors: offloading output layer to GPU
0.00.053.707 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.717 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.719 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.691 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.692 I llama_new_context_with_model: n_ctx         = 128
0.00.054.693 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.693 I llama_new_context_with_model: n_batch       = 128
0.00.054.693 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.693 I llama_new_context_with_model: flash_attn    = 0
0.00.054.693 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.694 I llama_new_context_with_model: freq_scale    = 1
0.00.054.694 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.694 I ggml_metal_init: allocating
0.00.054.697 I ggml_metal_init: found device: Apple M4
0.00.054.699 I ggml_metal_init: picking default device: Apple M4
0.00.055.258 I ggml_metal_init: using embedded metal library
0.00.057.666 I ggml_metal_init: GPU name:   Apple M4
0.00.057.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.668 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.669 I ggml_metal_init: simdgroup reduction   = true
0.00.057.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.669 I ggml_metal_init: has bfloat            = true
0.00.057.669 I ggml_metal_init: use bfloat            = true
0.00.057.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.670 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.728 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.750 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.655 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.656 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.657 I llama_new_context_with_model: graph nodes  = 967
0.00.069.657 I llama_new_context_with_model: graph splits = 2
0.00.069.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.182 I 
0.00.626.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.233 I perplexity: tokenizing the input ..
0.00.633.980 I perplexity: tokenization took 7.745 ms
0.00.633.991 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.474 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.757.638 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.757.657 I llama_perf_context_print:        load time =     615.09 ms
0.00.757.658 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.97 tokens per second)
0.00.757.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.659 I llama_perf_context_print:       total time =     131.48 ms /   129 tokens
0.00.758.131 I ggml_metal_free: deallocating

real	0m0.774s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.188 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.722 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.722 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.723 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.724 I llama_model_loader: - type  f32:  194 tensors
0.00.022.724 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.724 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.484 I llm_load_vocab: special tokens cache size = 25
0.00.048.575 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.578 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.578 I llm_load_print_meta: arch             = gptneox
0.00.048.579 I llm_load_print_meta: vocab type       = BPE
0.00.048.579 I llm_load_print_meta: n_vocab          = 50304
0.00.048.579 I llm_load_print_meta: n_merges         = 50009
0.00.048.579 I llm_load_print_meta: vocab_only       = 0
0.00.048.579 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.579 I llm_load_print_meta: n_embd           = 2048
0.00.048.580 I llm_load_print_meta: n_layer          = 24
0.00.048.582 I llm_load_print_meta: n_head           = 16
0.00.048.583 I llm_load_print_meta: n_head_kv        = 16
0.00.048.583 I llm_load_print_meta: n_rot            = 32
0.00.048.583 I llm_load_print_meta: n_swa            = 0
0.00.048.583 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.583 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.584 I llm_load_print_meta: n_gqa            = 1
0.00.048.585 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.587 I llm_load_print_meta: n_ff             = 8192
0.00.048.587 I llm_load_print_meta: n_expert         = 0
0.00.048.587 I llm_load_print_meta: n_expert_used    = 0
0.00.048.588 I llm_load_print_meta: causal attn      = 1
0.00.048.588 I llm_load_print_meta: pooling type     = 0
0.00.048.588 I llm_load_print_meta: rope type        = 2
0.00.048.588 I llm_load_print_meta: rope scaling     = linear
0.00.048.588 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.591 I llm_load_print_meta: freq_scale_train = 1
0.00.048.591 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.591 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.591 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.592 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.592 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.592 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.592 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.592 I llm_load_print_meta: model type       = 1.4B
0.00.048.593 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.593 I llm_load_print_meta: model params     = 1.41 B
0.00.048.594 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.594 I llm_load_print_meta: general.name     = 1.4B
0.00.048.594 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.598 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.598 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.599 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.599 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.599 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.600 I llm_load_print_meta: max token length = 1024
0.00.050.526 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.526 I llm_load_tensors: offloading output layer to GPU
0.00.050.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.537 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.538 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.412 I llama_new_context_with_model: n_ctx         = 128
0.00.051.412 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.412 I llama_new_context_with_model: n_batch       = 128
0.00.051.412 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.413 I llama_new_context_with_model: flash_attn    = 0
0.00.051.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.413 I llama_new_context_with_model: freq_scale    = 1
0.00.051.414 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.414 I ggml_metal_init: allocating
0.00.051.417 I ggml_metal_init: found device: Apple M4
0.00.051.419 I ggml_metal_init: picking default device: Apple M4
0.00.051.979 I ggml_metal_init: using embedded metal library
0.00.054.254 I ggml_metal_init: GPU name:   Apple M4
0.00.054.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.256 I ggml_metal_init: simdgroup reduction   = true
0.00.054.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.257 I ggml_metal_init: has bfloat            = true
0.00.054.257 I ggml_metal_init: use bfloat            = true
0.00.054.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.006 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.008 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.023 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.884 I llama_new_context_with_model: graph nodes  = 967
0.00.065.884 I llama_new_context_with_model: graph splits = 2
0.00.065.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.698 I 
0.00.694.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.753 I perplexity: tokenizing the input ..
0.00.703.011 I perplexity: tokenization took 8.256 ms
0.00.703.021 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.582 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.825.989 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.826.002 I llama_perf_context_print:        load time =     685.93 ms
0.00.826.003 I llama_perf_context_print: prompt eval time =     121.32 ms /   128 tokens (    0.95 ms per token,  1055.05 tokens per second)
0.00.826.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.004 I llama_perf_context_print:       total time =     131.31 ms /   129 tokens
0.00.826.351 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.077s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.245 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.735 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.920 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.921 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.922 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.922 I llama_model_loader: - type  f32:  194 tensors
0.00.025.922 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.923 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.877 I llm_load_vocab: special tokens cache size = 25
0.00.053.103 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.106 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.106 I llm_load_print_meta: arch             = gptneox
0.00.053.106 I llm_load_print_meta: vocab type       = BPE
0.00.053.107 I llm_load_print_meta: n_vocab          = 50304
0.00.053.107 I llm_load_print_meta: n_merges         = 50009
0.00.053.107 I llm_load_print_meta: vocab_only       = 0
0.00.053.107 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.107 I llm_load_print_meta: n_embd           = 2048
0.00.053.107 I llm_load_print_meta: n_layer          = 24
0.00.053.111 I llm_load_print_meta: n_head           = 16
0.00.053.112 I llm_load_print_meta: n_head_kv        = 16
0.00.053.112 I llm_load_print_meta: n_rot            = 32
0.00.053.113 I llm_load_print_meta: n_swa            = 0
0.00.053.113 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.113 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.114 I llm_load_print_meta: n_gqa            = 1
0.00.053.114 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.117 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.118 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.118 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.119 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.120 I llm_load_print_meta: n_ff             = 8192
0.00.053.121 I llm_load_print_meta: n_expert         = 0
0.00.053.121 I llm_load_print_meta: n_expert_used    = 0
0.00.053.121 I llm_load_print_meta: causal attn      = 1
0.00.053.121 I llm_load_print_meta: pooling type     = 0
0.00.053.121 I llm_load_print_meta: rope type        = 2
0.00.053.121 I llm_load_print_meta: rope scaling     = linear
0.00.053.122 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.122 I llm_load_print_meta: freq_scale_train = 1
0.00.053.122 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.122 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.123 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.123 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.123 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.123 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.123 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.123 I llm_load_print_meta: model type       = 1.4B
0.00.053.124 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.124 I llm_load_print_meta: model params     = 1.41 B
0.00.053.125 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.125 I llm_load_print_meta: general.name     = 1.4B
0.00.053.125 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.125 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.125 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.125 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.126 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.126 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.126 I llm_load_print_meta: max token length = 1024
0.00.055.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.102 I llm_load_tensors: offloading output layer to GPU
0.00.055.103 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.113 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.114 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.979 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.980 I llama_new_context_with_model: n_ctx         = 128
0.00.055.980 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.980 I llama_new_context_with_model: n_batch       = 128
0.00.055.980 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.981 I llama_new_context_with_model: flash_attn    = 0
0.00.055.981 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.981 I llama_new_context_with_model: freq_scale    = 1
0.00.055.982 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.982 I ggml_metal_init: allocating
0.00.055.985 I ggml_metal_init: found device: Apple M4
0.00.055.988 I ggml_metal_init: picking default device: Apple M4
0.00.056.594 I ggml_metal_init: using embedded metal library
0.00.059.037 I ggml_metal_init: GPU name:   Apple M4
0.00.059.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.040 I ggml_metal_init: simdgroup reduction   = true
0.00.059.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.040 I ggml_metal_init: has bfloat            = true
0.00.059.040 I ggml_metal_init: use bfloat            = true
0.00.059.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.388 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.286 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.287 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.288 I llama_new_context_with_model: graph nodes  = 967
0.00.071.288 I llama_new_context_with_model: graph splits = 2
0.00.071.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.828 I 
0.00.732.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.875 I perplexity: tokenizing the input ..
0.00.740.230 I perplexity: tokenization took 7.353 ms
0.00.740.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.276 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.875.433 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.875.449 I llama_perf_context_print:        load time =     722.58 ms
0.00.875.450 I llama_perf_context_print: prompt eval time =     133.77 ms /   128 tokens (    1.05 ms per token,   956.85 tokens per second)
0.00.875.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.453 I llama_perf_context_print:       total time =     142.62 ms /   129 tokens
0.00.875.850 I ggml_metal_free: deallocating

real	0m0.891s
user	0m0.080s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.127 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.463 I llama_model_loader: - type  f32:  194 tensors
0.00.023.463 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.464 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.303 I llm_load_vocab: special tokens cache size = 25
0.00.049.348 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.351 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.351 I llm_load_print_meta: arch             = gptneox
0.00.049.351 I llm_load_print_meta: vocab type       = BPE
0.00.049.352 I llm_load_print_meta: n_vocab          = 50304
0.00.049.352 I llm_load_print_meta: n_merges         = 50009
0.00.049.352 I llm_load_print_meta: vocab_only       = 0
0.00.049.352 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.352 I llm_load_print_meta: n_embd           = 2048
0.00.049.353 I llm_load_print_meta: n_layer          = 24
0.00.049.356 I llm_load_print_meta: n_head           = 16
0.00.049.356 I llm_load_print_meta: n_head_kv        = 16
0.00.049.357 I llm_load_print_meta: n_rot            = 32
0.00.049.357 I llm_load_print_meta: n_swa            = 0
0.00.049.357 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.357 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.358 I llm_load_print_meta: n_gqa            = 1
0.00.049.359 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.359 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.360 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.360 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.360 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.363 I llm_load_print_meta: n_ff             = 8192
0.00.049.364 I llm_load_print_meta: n_expert         = 0
0.00.049.364 I llm_load_print_meta: n_expert_used    = 0
0.00.049.364 I llm_load_print_meta: causal attn      = 1
0.00.049.364 I llm_load_print_meta: pooling type     = 0
0.00.049.364 I llm_load_print_meta: rope type        = 2
0.00.049.364 I llm_load_print_meta: rope scaling     = linear
0.00.049.365 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.366 I llm_load_print_meta: freq_scale_train = 1
0.00.049.366 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.367 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.367 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.367 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.367 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.367 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.367 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.368 I llm_load_print_meta: model type       = 1.4B
0.00.049.368 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.368 I llm_load_print_meta: model params     = 1.41 B
0.00.049.369 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.369 I llm_load_print_meta: general.name     = 1.4B
0.00.049.369 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.371 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.371 I llm_load_print_meta: max token length = 1024
0.00.050.974 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.974 I llm_load_tensors: offloading output layer to GPU
0.00.050.974 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.984 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.985 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.873 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.874 I llama_new_context_with_model: n_ctx         = 128
0.00.051.874 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.874 I llama_new_context_with_model: n_batch       = 128
0.00.051.874 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.875 I llama_new_context_with_model: flash_attn    = 0
0.00.051.875 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.875 I llama_new_context_with_model: freq_scale    = 1
0.00.051.876 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.876 I ggml_metal_init: allocating
0.00.051.883 I ggml_metal_init: found device: Apple M4
0.00.051.885 I ggml_metal_init: picking default device: Apple M4
0.00.052.484 I ggml_metal_init: using embedded metal library
0.00.054.872 I ggml_metal_init: GPU name:   Apple M4
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.875 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.875 I ggml_metal_init: simdgroup reduction   = true
0.00.054.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.875 I ggml_metal_init: has bfloat            = true
0.00.054.876 I ggml_metal_init: use bfloat            = true
0.00.054.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.688 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.708 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.542 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.543 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.544 I llama_new_context_with_model: graph nodes  = 967
0.00.067.544 I llama_new_context_with_model: graph splits = 2
0.00.067.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.816 I 
0.00.680.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.862 I perplexity: tokenizing the input ..
0.00.688.668 I perplexity: tokenization took 7.805 ms
0.00.688.680 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.361 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.824.554 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.824.569 I llama_perf_context_print:        load time =     671.68 ms
0.00.824.570 I llama_perf_context_print: prompt eval time =     134.42 ms /   128 tokens (    1.05 ms per token,   952.23 tokens per second)
0.00.824.570 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.571 I llama_perf_context_print:       total time =     143.76 ms /   129 tokens
0.00.825.023 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.770 I llama_model_loader: - type  f32:  194 tensors
0.00.023.770 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.770 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.500 I llm_load_vocab: special tokens cache size = 25
0.00.049.453 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.456 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.456 I llm_load_print_meta: arch             = gptneox
0.00.049.456 I llm_load_print_meta: vocab type       = BPE
0.00.049.457 I llm_load_print_meta: n_vocab          = 50304
0.00.049.457 I llm_load_print_meta: n_merges         = 50009
0.00.049.457 I llm_load_print_meta: vocab_only       = 0
0.00.049.457 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.458 I llm_load_print_meta: n_embd           = 2048
0.00.049.458 I llm_load_print_meta: n_layer          = 24
0.00.049.461 I llm_load_print_meta: n_head           = 16
0.00.049.462 I llm_load_print_meta: n_head_kv        = 16
0.00.049.462 I llm_load_print_meta: n_rot            = 32
0.00.049.462 I llm_load_print_meta: n_swa            = 0
0.00.049.462 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.462 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.463 I llm_load_print_meta: n_gqa            = 1
0.00.049.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.464 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.465 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.465 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.466 I llm_load_print_meta: n_ff             = 8192
0.00.049.467 I llm_load_print_meta: n_expert         = 0
0.00.049.467 I llm_load_print_meta: n_expert_used    = 0
0.00.049.467 I llm_load_print_meta: causal attn      = 1
0.00.049.467 I llm_load_print_meta: pooling type     = 0
0.00.049.467 I llm_load_print_meta: rope type        = 2
0.00.049.467 I llm_load_print_meta: rope scaling     = linear
0.00.049.468 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.470 I llm_load_print_meta: freq_scale_train = 1
0.00.049.470 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.470 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.471 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.471 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.471 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.471 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.471 I llm_load_print_meta: model type       = 1.4B
0.00.049.472 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.472 I llm_load_print_meta: model params     = 1.41 B
0.00.049.473 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.473 I llm_load_print_meta: general.name     = 1.4B
0.00.049.473 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.473 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.474 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.474 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.476 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.476 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.476 I llm_load_print_meta: max token length = 1024
0.00.051.048 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.048 I llm_load_tensors: offloading output layer to GPU
0.00.051.048 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.058 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.059 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.915 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.916 I llama_new_context_with_model: n_ctx         = 128
0.00.051.916 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.916 I llama_new_context_with_model: n_batch       = 128
0.00.051.916 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.917 I llama_new_context_with_model: flash_attn    = 0
0.00.051.917 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.917 I llama_new_context_with_model: freq_scale    = 1
0.00.051.918 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.918 I ggml_metal_init: allocating
0.00.051.921 I ggml_metal_init: found device: Apple M4
0.00.051.923 I ggml_metal_init: picking default device: Apple M4
0.00.052.509 I ggml_metal_init: using embedded metal library
0.00.054.829 I ggml_metal_init: GPU name:   Apple M4
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.832 I ggml_metal_init: simdgroup reduction   = true
0.00.054.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.832 I ggml_metal_init: has bfloat            = true
0.00.054.832 I ggml_metal_init: use bfloat            = true
0.00.054.832 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.833 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.895 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.909 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.759 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.760 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.760 I llama_new_context_with_model: graph nodes  = 967
0.00.066.760 I llama_new_context_with_model: graph splits = 2
0.00.066.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.253 I 
0.00.388.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.297 I perplexity: tokenizing the input ..
0.00.395.925 I perplexity: tokenization took 7.626 ms
0.00.395.937 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.144 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.355 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.371 I llama_perf_context_print:        load time =     378.49 ms
0.00.529.372 I llama_perf_context_print: prompt eval time =     131.95 ms /   128 tokens (    1.03 ms per token,   970.08 tokens per second)
0.00.529.373 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.373 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.529.816 I ggml_metal_free: deallocating

real	0m0.544s
user	0m0.078s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.857 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.402 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.190 I llama_model_loader: - type  f32:  194 tensors
0.00.025.190 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.191 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.191 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.191 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.648 I llm_load_vocab: special tokens cache size = 25
0.00.051.498 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.501 I llm_load_print_meta: arch             = gptneox
0.00.051.502 I llm_load_print_meta: vocab type       = BPE
0.00.051.502 I llm_load_print_meta: n_vocab          = 50304
0.00.051.502 I llm_load_print_meta: n_merges         = 50009
0.00.051.502 I llm_load_print_meta: vocab_only       = 0
0.00.051.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.502 I llm_load_print_meta: n_embd           = 2048
0.00.051.503 I llm_load_print_meta: n_layer          = 24
0.00.051.506 I llm_load_print_meta: n_head           = 16
0.00.051.507 I llm_load_print_meta: n_head_kv        = 16
0.00.051.507 I llm_load_print_meta: n_rot            = 32
0.00.051.509 I llm_load_print_meta: n_swa            = 0
0.00.051.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.510 I llm_load_print_meta: n_gqa            = 1
0.00.051.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.512 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.514 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.514 I llm_load_print_meta: n_ff             = 8192
0.00.051.514 I llm_load_print_meta: n_expert         = 0
0.00.051.514 I llm_load_print_meta: n_expert_used    = 0
0.00.051.515 I llm_load_print_meta: causal attn      = 1
0.00.051.515 I llm_load_print_meta: pooling type     = 0
0.00.051.516 I llm_load_print_meta: rope type        = 2
0.00.051.516 I llm_load_print_meta: rope scaling     = linear
0.00.051.517 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.517 I llm_load_print_meta: freq_scale_train = 1
0.00.051.517 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.518 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.518 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.518 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.518 I llm_load_print_meta: model type       = 1.4B
0.00.051.519 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.520 I llm_load_print_meta: model params     = 1.41 B
0.00.051.520 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.522 I llm_load_print_meta: general.name     = 1.4B
0.00.051.522 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.522 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.522 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.522 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.523 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.523 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.523 I llm_load_print_meta: max token length = 1024
0.00.053.137 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.137 I llm_load_tensors: offloading output layer to GPU
0.00.053.137 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.147 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.149 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.015 I llama_new_context_with_model: n_ctx         = 128
0.00.054.016 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.016 I llama_new_context_with_model: n_batch       = 128
0.00.054.016 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.016 I llama_new_context_with_model: flash_attn    = 0
0.00.054.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.017 I llama_new_context_with_model: freq_scale    = 1
0.00.054.017 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.018 I ggml_metal_init: allocating
0.00.054.024 I ggml_metal_init: found device: Apple M4
0.00.054.028 I ggml_metal_init: picking default device: Apple M4
0.00.054.613 I ggml_metal_init: using embedded metal library
0.00.056.971 I ggml_metal_init: GPU name:   Apple M4
0.00.056.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.974 I ggml_metal_init: simdgroup reduction   = true
0.00.056.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.974 I ggml_metal_init: has bfloat            = true
0.00.056.974 I ggml_metal_init: use bfloat            = true
0.00.056.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.688 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.692 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.706 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.583 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.585 I llama_new_context_with_model: graph nodes  = 967
0.00.068.585 I llama_new_context_with_model: graph splits = 2
0.00.068.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.594 I 
0.00.494.631 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.638 I perplexity: tokenizing the input ..
0.00.502.494 I perplexity: tokenization took 7.854 ms
0.00.502.504 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.072 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.635.208 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.635.220 I llama_perf_context_print:        load time =     483.73 ms
0.00.635.221 I llama_perf_context_print: prompt eval time =     131.31 ms /   128 tokens (    1.03 ms per token,   974.81 tokens per second)
0.00.635.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.222 I llama_perf_context_print:       total time =     140.63 ms /   129 tokens
0.00.635.594 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.143 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.709 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.712 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.362 I llama_model_loader: - type  f32:  194 tensors
0.00.024.363 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.363 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.363 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.994 I llm_load_vocab: special tokens cache size = 25
0.00.050.031 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.033 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.034 I llm_load_print_meta: arch             = gptneox
0.00.050.034 I llm_load_print_meta: vocab type       = BPE
0.00.050.034 I llm_load_print_meta: n_vocab          = 50304
0.00.050.035 I llm_load_print_meta: n_merges         = 50009
0.00.050.035 I llm_load_print_meta: vocab_only       = 0
0.00.050.035 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.035 I llm_load_print_meta: n_embd           = 2048
0.00.050.035 I llm_load_print_meta: n_layer          = 24
0.00.050.038 I llm_load_print_meta: n_head           = 16
0.00.050.039 I llm_load_print_meta: n_head_kv        = 16
0.00.050.039 I llm_load_print_meta: n_rot            = 32
0.00.050.039 I llm_load_print_meta: n_swa            = 0
0.00.050.040 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.040 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.042 I llm_load_print_meta: n_gqa            = 1
0.00.050.042 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.043 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.044 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.044 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.044 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.045 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.046 I llm_load_print_meta: n_ff             = 8192
0.00.050.046 I llm_load_print_meta: n_expert         = 0
0.00.050.046 I llm_load_print_meta: n_expert_used    = 0
0.00.050.046 I llm_load_print_meta: causal attn      = 1
0.00.050.046 I llm_load_print_meta: pooling type     = 0
0.00.050.046 I llm_load_print_meta: rope type        = 2
0.00.050.047 I llm_load_print_meta: rope scaling     = linear
0.00.050.047 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.048 I llm_load_print_meta: freq_scale_train = 1
0.00.050.048 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.048 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.048 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.048 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.048 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.049 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.051 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.051 I llm_load_print_meta: model type       = 1.4B
0.00.050.051 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.052 I llm_load_print_meta: model params     = 1.41 B
0.00.050.053 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.054 I llm_load_print_meta: general.name     = 1.4B
0.00.050.054 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.055 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.056 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.056 I llm_load_print_meta: max token length = 1024
0.00.051.643 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.643 I llm_load_tensors: offloading output layer to GPU
0.00.051.643 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.653 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.654 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.488 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.489 I llama_new_context_with_model: n_ctx         = 128
0.00.052.489 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.489 I llama_new_context_with_model: n_batch       = 128
0.00.052.490 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.490 I llama_new_context_with_model: flash_attn    = 0
0.00.052.490 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.491 I llama_new_context_with_model: freq_scale    = 1
0.00.052.491 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.491 I ggml_metal_init: allocating
0.00.052.497 I ggml_metal_init: found device: Apple M4
0.00.052.500 I ggml_metal_init: picking default device: Apple M4
0.00.053.066 I ggml_metal_init: using embedded metal library
0.00.055.484 I ggml_metal_init: GPU name:   Apple M4
0.00.055.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.487 I ggml_metal_init: simdgroup reduction   = true
0.00.055.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.487 I ggml_metal_init: has bfloat            = true
0.00.055.487 I ggml_metal_init: use bfloat            = true
0.00.055.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.096 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.098 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.113 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.993 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.994 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.995 I llama_new_context_with_model: graph nodes  = 967
0.00.066.995 I llama_new_context_with_model: graph splits = 2
0.00.067.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.579.049 I 
0.00.579.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.579.095 I perplexity: tokenizing the input ..
0.00.586.640 I perplexity: tokenization took 7.544 ms
0.00.586.655 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.886 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.722.041 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.051 I llama_perf_context_print:        load time =     568.90 ms
0.00.722.052 I llama_perf_context_print: prompt eval time =     133.97 ms /   128 tokens (    1.05 ms per token,   955.44 tokens per second)
0.00.722.053 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.053 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.722.424 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.077s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.278 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.217 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.865 I llama_model_loader: - type  f32:  194 tensors
0.00.023.865 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.866 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.339 I llm_load_vocab: special tokens cache size = 25
0.00.050.338 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.341 I llm_load_print_meta: arch             = gptneox
0.00.050.341 I llm_load_print_meta: vocab type       = BPE
0.00.050.341 I llm_load_print_meta: n_vocab          = 50304
0.00.050.342 I llm_load_print_meta: n_merges         = 50009
0.00.050.342 I llm_load_print_meta: vocab_only       = 0
0.00.050.342 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.342 I llm_load_print_meta: n_embd           = 2048
0.00.050.342 I llm_load_print_meta: n_layer          = 24
0.00.050.345 I llm_load_print_meta: n_head           = 16
0.00.050.346 I llm_load_print_meta: n_head_kv        = 16
0.00.050.346 I llm_load_print_meta: n_rot            = 32
0.00.050.346 I llm_load_print_meta: n_swa            = 0
0.00.050.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.347 I llm_load_print_meta: n_gqa            = 1
0.00.050.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.350 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.352 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.352 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.353 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.353 I llm_load_print_meta: n_ff             = 8192
0.00.050.354 I llm_load_print_meta: n_expert         = 0
0.00.050.354 I llm_load_print_meta: n_expert_used    = 0
0.00.050.354 I llm_load_print_meta: causal attn      = 1
0.00.050.354 I llm_load_print_meta: pooling type     = 0
0.00.050.354 I llm_load_print_meta: rope type        = 2
0.00.050.354 I llm_load_print_meta: rope scaling     = linear
0.00.050.364 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.366 I llm_load_print_meta: freq_scale_train = 1
0.00.050.366 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.366 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.366 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.368 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.368 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.369 I llm_load_print_meta: model type       = 1.4B
0.00.050.369 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.369 I llm_load_print_meta: model params     = 1.41 B
0.00.050.370 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.370 I llm_load_print_meta: general.name     = 1.4B
0.00.050.371 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.372 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.372 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.372 I llm_load_print_meta: max token length = 1024
0.00.051.977 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.977 I llm_load_tensors: offloading output layer to GPU
0.00.051.977 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.987 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.988 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.793 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.793 I llama_new_context_with_model: n_ctx         = 128
0.00.052.794 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.794 I llama_new_context_with_model: n_batch       = 128
0.00.052.794 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.794 I llama_new_context_with_model: flash_attn    = 0
0.00.052.795 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.795 I llama_new_context_with_model: freq_scale    = 1
0.00.052.795 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.796 I ggml_metal_init: allocating
0.00.052.799 I ggml_metal_init: found device: Apple M4
0.00.052.800 I ggml_metal_init: picking default device: Apple M4
0.00.053.379 I ggml_metal_init: using embedded metal library
0.00.055.697 I ggml_metal_init: GPU name:   Apple M4
0.00.055.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.700 I ggml_metal_init: simdgroup reduction   = true
0.00.055.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.700 I ggml_metal_init: has bfloat            = true
0.00.055.700 I ggml_metal_init: use bfloat            = true
0.00.055.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.303 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.305 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.320 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.220 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.221 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.222 I llama_new_context_with_model: graph nodes  = 967
0.00.067.222 I llama_new_context_with_model: graph splits = 2
0.00.067.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.564 I 
0.00.656.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.605 I perplexity: tokenizing the input ..
0.00.663.987 I perplexity: tokenization took 7.38 ms
0.00.663.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.634 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.798 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.807 I llama_perf_context_print:        load time =     647.28 ms
0.00.805.808 I llama_perf_context_print: prompt eval time =     140.38 ms /   128 tokens (    1.10 ms per token,   911.83 tokens per second)
0.00.805.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.814 I llama_perf_context_print:       total time =     149.24 ms /   129 tokens
0.00.806.382 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.078s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.617 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.186 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.190 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.191 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.192 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.984 I llama_model_loader: - type  f32:  194 tensors
0.00.027.985 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.896 I llm_load_vocab: special tokens cache size = 25
0.00.053.805 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.808 I llm_load_print_meta: arch             = gptneox
0.00.053.808 I llm_load_print_meta: vocab type       = BPE
0.00.053.808 I llm_load_print_meta: n_vocab          = 50304
0.00.053.808 I llm_load_print_meta: n_merges         = 50009
0.00.053.808 I llm_load_print_meta: vocab_only       = 0
0.00.053.809 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.809 I llm_load_print_meta: n_embd           = 2048
0.00.053.809 I llm_load_print_meta: n_layer          = 24
0.00.053.812 I llm_load_print_meta: n_head           = 16
0.00.053.813 I llm_load_print_meta: n_head_kv        = 16
0.00.053.813 I llm_load_print_meta: n_rot            = 32
0.00.053.813 I llm_load_print_meta: n_swa            = 0
0.00.053.813 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.814 I llm_load_print_meta: n_gqa            = 1
0.00.053.815 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.816 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.817 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.817 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.817 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.817 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.821 I llm_load_print_meta: n_ff             = 8192
0.00.053.821 I llm_load_print_meta: n_expert         = 0
0.00.053.821 I llm_load_print_meta: n_expert_used    = 0
0.00.053.821 I llm_load_print_meta: causal attn      = 1
0.00.053.821 I llm_load_print_meta: pooling type     = 0
0.00.053.821 I llm_load_print_meta: rope type        = 2
0.00.053.822 I llm_load_print_meta: rope scaling     = linear
0.00.053.822 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.822 I llm_load_print_meta: freq_scale_train = 1
0.00.053.823 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.823 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.823 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.823 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.823 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.823 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.824 I llm_load_print_meta: model type       = 1.4B
0.00.053.824 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.824 I llm_load_print_meta: model params     = 1.41 B
0.00.053.825 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.826 I llm_load_print_meta: general.name     = 1.4B
0.00.053.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.830 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.830 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.831 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.832 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.832 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.832 I llm_load_print_meta: max token length = 1024
0.00.055.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.689 I llm_load_tensors: offloading output layer to GPU
0.00.055.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.700 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.701 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.572 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.572 I llama_new_context_with_model: n_ctx         = 128
0.00.056.572 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.573 I llama_new_context_with_model: n_batch       = 128
0.00.056.573 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.573 I llama_new_context_with_model: flash_attn    = 0
0.00.056.573 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.573 I llama_new_context_with_model: freq_scale    = 1
0.00.056.574 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.574 I ggml_metal_init: allocating
0.00.056.577 I ggml_metal_init: found device: Apple M4
0.00.056.579 I ggml_metal_init: picking default device: Apple M4
0.00.057.158 I ggml_metal_init: using embedded metal library
0.00.059.457 I ggml_metal_init: GPU name:   Apple M4
0.00.059.459 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.460 I ggml_metal_init: simdgroup reduction   = true
0.00.059.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.460 I ggml_metal_init: has bfloat            = true
0.00.059.460 I ggml_metal_init: use bfloat            = true
0.00.059.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.067 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.069 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.092 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.994 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.995 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.995 I llama_new_context_with_model: graph nodes  = 967
0.00.070.996 I llama_new_context_with_model: graph splits = 2
0.00.071.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.130.093 I 
0.00.130.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.130.158 I perplexity: tokenizing the input ..
0.00.137.719 I perplexity: tokenization took 7.558 ms
0.00.137.733 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.276.526 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.277.782 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.277.800 I llama_perf_context_print:        load time =     116.47 ms
0.00.277.801 I llama_perf_context_print: prompt eval time =     138.54 ms /   128 tokens (    1.08 ms per token,   923.93 tokens per second)
0.00.277.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.277.806 I llama_perf_context_print:       total time =     147.71 ms /   129 tokens
0.00.278.375 I ggml_metal_free: deallocating

real	0m0.295s
user	0m0.077s
sys	0m0.039s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.342 I build: 4330 (7a20c287) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.281 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.291 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.297 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.216 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.220 I llama_model_loader: - type  f32:  194 tensors
0.00.050.221 I llama_model_loader: - type  f16:   98 tensors
0.00.076.697 I llm_load_vocab: special tokens cache size = 25
0.00.082.937 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.940 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.940 I llm_load_print_meta: arch             = gptneox
0.00.082.940 I llm_load_print_meta: vocab type       = BPE
0.00.082.940 I llm_load_print_meta: n_vocab          = 50304
0.00.082.941 I llm_load_print_meta: n_merges         = 50009
0.00.082.941 I llm_load_print_meta: vocab_only       = 0
0.00.082.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.941 I llm_load_print_meta: n_embd           = 2048
0.00.082.941 I llm_load_print_meta: n_layer          = 24
0.00.082.944 I llm_load_print_meta: n_head           = 16
0.00.082.945 I llm_load_print_meta: n_head_kv        = 16
0.00.082.945 I llm_load_print_meta: n_rot            = 32
0.00.082.945 I llm_load_print_meta: n_swa            = 0
0.00.082.948 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.949 I llm_load_print_meta: n_gqa            = 1
0.00.082.950 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.951 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.953 I llm_load_print_meta: n_ff             = 8192
0.00.082.953 I llm_load_print_meta: n_expert         = 0
0.00.082.954 I llm_load_print_meta: n_expert_used    = 0
0.00.082.954 I llm_load_print_meta: causal attn      = 1
0.00.082.954 I llm_load_print_meta: pooling type     = 0
0.00.082.954 I llm_load_print_meta: rope type        = 2
0.00.082.955 I llm_load_print_meta: rope scaling     = linear
0.00.082.956 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.956 I llm_load_print_meta: freq_scale_train = 1
0.00.082.956 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.958 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.958 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.958 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.958 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.958 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.958 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.959 I llm_load_print_meta: model type       = 1.4B
0.00.082.960 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.082.960 I llm_load_print_meta: model params     = 1.41 B
0.00.082.961 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.082.961 I llm_load_print_meta: general.name     = 1.4B
0.00.082.961 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.961 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.961 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.963 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.963 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.963 I llm_load_print_meta: max token length = 1024
0.00.085.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.572 I llm_load_tensors: offloading output layer to GPU
0.00.085.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.583 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.585 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.523 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.524 I llama_new_context_with_model: n_ctx         = 128
0.00.086.524 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.086.524 I llama_new_context_with_model: n_batch       = 128
0.00.086.524 I llama_new_context_with_model: n_ubatch      = 128
0.00.086.525 I llama_new_context_with_model: flash_attn    = 0
0.00.086.525 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.525 I llama_new_context_with_model: freq_scale    = 1
0.00.086.526 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.526 I ggml_metal_init: allocating
0.00.086.528 I ggml_metal_init: found device: Apple M4
0.00.086.530 I ggml_metal_init: picking default device: Apple M4
0.00.087.160 I ggml_metal_init: using embedded metal library
0.00.089.599 I ggml_metal_init: GPU name:   Apple M4
0.00.089.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.601 I ggml_metal_init: simdgroup reduction   = true
0.00.089.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.601 I ggml_metal_init: has bfloat            = true
0.00.089.602 I ggml_metal_init: use bfloat            = true
0.00.089.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.432 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.436 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.450 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.323 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.324 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.324 I llama_new_context_with_model: graph nodes  = 967
0.00.100.324 I llama_new_context_with_model: graph splits = 2
0.00.100.336 I 
0.00.100.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.100.369 I compute_imatrix: tokenizing the input ..
0.00.107.131 I compute_imatrix: tokenization took 6.761 ms
0.00.107.132 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.565.072 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.567.511 I llama_perf_context_print:        load time =    1543.18 ms
0.01.567.512 I llama_perf_context_print: prompt eval time =    1457.28 ms /   128 tokens (   11.38 ms per token,    87.84 tokens per second)
0.01.567.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.567.515 I llama_perf_context_print:       total time =    1545.62 ms /   129 tokens
0.01.568.047 I ggml_metal_free: deallocating

real	0m1.752s
user	0m0.161s
sys	0m0.240s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4330 (7a20c287)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e0d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e18cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e22550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e22e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e49d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e4a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e4c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e50220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e50770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e50cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e5ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e62780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e63560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e63a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e63ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e64340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e64c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e65120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e65d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e66bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e68060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e68670 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e47080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e49870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e4a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e52980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e53260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e59870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e5a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e5b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e5db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e5e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e5ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e60380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e61a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e61eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e62320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e47080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e49870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e4a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e52980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e53260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e59870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e5a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e5b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e5db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e5e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e5ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e5fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e60be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e61740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e62020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e62490 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.836s
user	0m0.293s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4330 (7a20c287)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12880a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12880a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12880ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12880b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12880b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12880bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12880c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12880cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12880d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12880d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12880dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12880dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12880ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12880f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12880fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1288101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1288108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128810ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128813ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128814400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1288146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128815e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1288165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1288168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1288194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12881a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12881a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12881aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12881b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12881bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12881c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12881c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12881cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12881d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12881d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12881df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12881e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12881ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12881f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12881f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12881f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1288203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1288211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128822440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1288228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128822d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1288236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128823b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1288240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128824b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1288250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1288255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128825b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128826090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1288265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128827080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1288275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1288285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128828b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1288295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128829b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12882a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12882a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12882aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12882b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12882b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12882bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12881b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12882bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12882c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12882cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12882d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12882d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12882dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12882e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12882e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12882ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12882f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12882f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12882fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1288306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128830c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1288310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128831550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1288319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1288327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1288335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128833a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128834cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128835170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128835f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1288363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128836d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1288371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128837670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1288388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128838d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128839230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1288396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12883a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12883a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12883a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12883adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12883b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12883b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12883bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12883c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12883c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12883c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12883ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12883d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12883d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12883dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12883e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12883e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12883ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12883eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12883f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12883f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12883fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1288405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128840a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128840f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1288413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1288438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1288441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128844690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128845470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128845910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1288466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128847030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1288474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1288488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128849610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12884a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12884a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12884b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12884b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12884b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12884bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12884c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12884cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12884d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12884d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12884d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12884e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12884e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12884ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12884f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12884f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12884fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1288520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1288530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128853630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128853b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1288540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128854620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1288550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128855610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1288560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128856600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1288570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1288575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1288585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128858b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1288595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128859b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12885a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12885a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12885ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12885b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12885b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12885bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12885c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12885c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12885caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12885d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12885d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12885dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12885e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12885e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12885ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12885f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12885f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12885fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128860010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128860560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128860ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128860f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1288613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128861890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128861d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1288621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128862670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128862b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128862fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1288638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128863d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128864230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1288646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128865010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128865c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1288663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128866ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1288671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1288674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128867c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128868560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12770aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12770af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12770b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12770b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12770bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12770c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12770c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12770ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12770ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12770d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12770d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12770dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12770ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12770f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12770f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1277100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127710800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127711640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127711e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127712530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127712c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1277141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127714470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127715e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127716300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1277165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127716a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127716ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127717900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127718300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127718800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127718d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127719200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127719700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12771a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12771a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12771a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12771adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12771b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12771b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12771bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12771bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12771c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12771c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12771d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12771d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12771d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12771dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12771e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12771ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12771eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12771f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12771f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12771fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127720150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1277205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127720f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1277213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127721870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127721d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1277221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127722700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127722c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1277231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1277236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127723c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1277246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127724c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127725180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1277256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127725c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127726170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1277266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127726c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127727160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1277276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127728150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1277286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127729690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12772a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12772a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12772abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12772b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12772b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12772bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12772c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12772c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12772cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12772d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12772d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12772dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12772e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12772e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12772eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12772f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12772f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12772fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12772ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127730410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1277308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127730d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1277311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127731b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127732470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127732db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127733250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1277336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127733b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127734030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1277344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127734970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127734e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1277352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127735750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127735bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127736530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1277369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127736e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127737310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1277377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1277380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127738590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127738ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127739370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127739810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12773a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12773a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12773aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12773af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12773b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12773b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12773bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12773c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12773c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12773caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12773cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12773d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12773d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12773dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12773e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12773e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12773eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12773eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12773f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12773f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12773fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127740270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127740710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127740bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127741050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1277414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127741990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1277422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127742770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127742c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1277430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127743550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1277439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127743e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127744330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1277447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127744c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1277455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127745a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127746390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127746830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127746d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1277472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127747820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127747d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127748030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127748640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127748c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127749260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12774a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12774a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12774add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12774b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12774ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12774bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12774c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12774cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12774d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12774d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12774db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12774e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12774e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12774eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12774f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12774f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12774fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127750070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1277505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127750b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127751060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1277515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127751b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127752050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1277525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127752af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127753040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127753590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127753ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127754030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127754580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127754ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127755020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127755570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127755ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127756010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127756560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127756ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127757000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127757550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127757aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127757ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127758540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127758a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127758fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127759530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127759a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127759fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12775a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12775aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12775afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12775b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12775ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12775bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12775c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12775ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12775cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12775d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12775da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12775df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12775e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12775ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12775ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12775f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12775f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12775fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1277602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127760750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127760bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127761090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127761530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1277619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127761e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127762310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1277627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127762c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1277630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127763590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127763a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127763f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1277646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127764dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1277654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127765c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127765ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1277666b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127766970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127766f80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128824c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1288250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128825550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1288259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128825e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1288262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128826710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128826b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128826ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128827460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1288278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128827eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1288287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128828f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128829700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128829df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12882a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12882abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12882b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12882bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12882c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12882ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12882d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12882d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12882def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12882e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12882e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12882ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12882f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12882f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12882f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12882fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128830270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1288309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128830e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128831280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1288316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128831b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128831fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128832440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1288328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128832d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128833190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128833600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128833a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128833ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128834350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1288347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128834c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1288350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128835510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128835980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128835df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128836260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1288366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128836b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128836fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128837420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128837890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128837d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128838170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1288385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128838a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128838ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128839330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1288397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128839c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12883a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12883a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12883a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12883add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12883b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12883b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12883bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12883bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12883c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12883c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12883cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12883d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12883d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12883da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12883dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12883e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12883e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12883ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12883f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12883f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12883f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12883fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128840220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128840690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128840b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128840f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1288413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128841cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128842130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1288425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128842a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128842e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1288432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128843760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128843bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128844040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1288444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128844920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128844d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128845200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128845670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128845ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128845f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1288463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128846830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128846ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128847110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128847580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1288479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128847e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1288482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128848740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128848bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128849020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128849490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128849900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128849d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12884a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12884a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12884aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12884af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12884b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12884b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12884bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12884c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12884c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12884c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12884ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12884d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12884d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12884db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12884e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12884e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12884e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12884ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12884f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12884f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12884faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12884ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128850380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1288507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128850c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1288510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128851540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1288519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128851e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128852290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128852700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128852fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128853450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1288538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128853d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1288541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128854610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128854a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128854ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128855360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1288557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128855c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1288560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128856520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128856990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128856e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128857270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1288576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128857b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128857fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128858430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1288588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128858d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128859180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1288595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128859a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128859ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12885a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12885a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12885ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12885b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12885b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12885b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12885bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12885c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12885c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12885cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12885cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12885d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12885d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12885dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12885e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12885e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12885ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12885eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12885f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12885f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12885fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128860070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1288604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128860950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128860dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128861230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1288619b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128861e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128862290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128862700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128862b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128862fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1288638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128863d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1288641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128864610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128864a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128864ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128865360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1288657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128865c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1288660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128866520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128866990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128866e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128867270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1288676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128867b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128867fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128868430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12880b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12880ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128809800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12880a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1288180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128818560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1288189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128818e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1288192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128819720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128819b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12881a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12881a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12881a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12881ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12881b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12881b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12881baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12881bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12881c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12881c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12881cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12881d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12881d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12881d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12881de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12881e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12881e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12881eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12881efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12881f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12881f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12881fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1288201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128820610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128820a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128820ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128821360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1288217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128821c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1288220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128822520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128822990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128822e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128823270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1288236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128823dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1288244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1288162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128816990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128816e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12880d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12880d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12880de50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.241s
sys	0m0.145s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
