Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- Performing Test GGML_COMPILER_SUPPORT_I8MM
-- Performing Test GGML_COMPILER_SUPPORT_I8MM - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.0s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.258s
user	0m0.780s
sys	0m1.119s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target build_info
[  6%] Built target sha256
[  6%] Built target sha1
[  7%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target llava
[ 34%] Built target llama-simple
[ 34%] Built target llama-simple-chat
[ 34%] Built target test-c
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 35%] Built target llama-quantize-stats
[ 35%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Built target llava_shared
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Built target test-tokenizer-0
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-json-schema-to-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Built target test-gguf
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-embedding
[ 63%] Built target test-autorelease
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Built target test-quantize-fns
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-barrier
[ 65%] Built target test-rope
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Built target llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target llama-batched-bench
[ 69%] Built target llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-infill
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Generating loading.html.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-perplexity
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-run
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.924s
user	0m6.067s
sys	0m9.472s

main: quantize time =  5294.25 ms
main:    total time =  5294.25 ms

main: quantize time =  2038.04 ms
main:    total time =  2038.04 ms

main: quantize time =  2035.10 ms
main:    total time =  2035.10 ms

main: quantize time =  2713.81 ms
main:    total time =  2713.81 ms

main: quantize time =  2570.90 ms
main:    total time =  2570.90 ms

main: quantize time =  5199.74 ms
main:    total time =  5199.74 ms

main: quantize time =  5690.47 ms
main:    total time =  5690.47 ms

main: quantize time =  7136.19 ms
main:    total time =  7136.19 ms

main: quantize time =  5776.03 ms
main:    total time =  5776.03 ms

main: quantize time =  4640.05 ms
main:    total time =  4640.05 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.106 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.226 I main: llama backend init
0.00.000.232 I main: load the model and apply lora adapter, if any
0.00.036.345 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.605 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.067.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.144 I llama_model_loader: - type  f32:  194 tensors
0.00.067.144 I llama_model_loader: - type  f16:   98 tensors
0.00.099.153 I llm_load_vocab: special tokens cache size = 25
0.00.106.267 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.106.270 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.106.271 I llm_load_print_meta: arch             = gptneox
0.00.106.271 I llm_load_print_meta: vocab type       = BPE
0.00.106.271 I llm_load_print_meta: n_vocab          = 50304
0.00.106.271 I llm_load_print_meta: n_merges         = 50009
0.00.106.271 I llm_load_print_meta: vocab_only       = 0
0.00.106.272 I llm_load_print_meta: n_ctx_train      = 2048
0.00.106.272 I llm_load_print_meta: n_embd           = 2048
0.00.106.272 I llm_load_print_meta: n_layer          = 24
0.00.106.275 I llm_load_print_meta: n_head           = 16
0.00.106.276 I llm_load_print_meta: n_head_kv        = 16
0.00.106.277 I llm_load_print_meta: n_rot            = 32
0.00.106.277 I llm_load_print_meta: n_swa            = 0
0.00.106.278 I llm_load_print_meta: n_embd_head_k    = 128
0.00.106.278 I llm_load_print_meta: n_embd_head_v    = 128
0.00.106.278 I llm_load_print_meta: n_gqa            = 1
0.00.106.279 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.106.280 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.106.281 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.106.281 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.106.281 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.106.281 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.106.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.106.283 I llm_load_print_meta: n_ff             = 8192
0.00.106.284 I llm_load_print_meta: n_expert         = 0
0.00.106.284 I llm_load_print_meta: n_expert_used    = 0
0.00.106.284 I llm_load_print_meta: causal attn      = 1
0.00.106.284 I llm_load_print_meta: pooling type     = 0
0.00.106.284 I llm_load_print_meta: rope type        = 2
0.00.106.284 I llm_load_print_meta: rope scaling     = linear
0.00.106.285 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.106.285 I llm_load_print_meta: freq_scale_train = 1
0.00.106.285 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.106.285 I llm_load_print_meta: rope_finetuned   = unknown
0.00.106.285 I llm_load_print_meta: ssm_d_conv       = 0
0.00.106.286 I llm_load_print_meta: ssm_d_inner      = 0
0.00.106.286 I llm_load_print_meta: ssm_d_state      = 0
0.00.106.286 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.106.286 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.106.286 I llm_load_print_meta: model type       = 1.4B
0.00.106.287 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.106.287 I llm_load_print_meta: model params     = 1.41 B
0.00.106.288 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.106.288 I llm_load_print_meta: general.name     = 1.4B
0.00.106.288 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.106.290 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.106.290 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.106.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.106.290 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.106.291 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.106.291 I llm_load_print_meta: max token length = 1024
0.00.108.837 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.108.837 I llm_load_tensors: offloading output layer to GPU
0.00.108.837 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.108.855 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.108.856 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.109.806 I llama_new_context_with_model: n_seq_max     = 1
0.00.109.807 I llama_new_context_with_model: n_ctx         = 2048
0.00.109.807 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.109.807 I llama_new_context_with_model: n_batch       = 2048
0.00.109.808 I llama_new_context_with_model: n_ubatch      = 512
0.00.109.808 I llama_new_context_with_model: flash_attn    = 0
0.00.109.808 I llama_new_context_with_model: freq_base     = 10000.0
0.00.109.808 I llama_new_context_with_model: freq_scale    = 1
0.00.109.809 I ggml_metal_init: allocating
0.00.109.812 I ggml_metal_init: found device: Apple M4
0.00.109.814 I ggml_metal_init: picking default device: Apple M4
0.00.110.494 I ggml_metal_init: using embedded metal library
0.00.121.068 I ggml_metal_init: GPU name:   Apple M4
0.00.121.070 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.071 I ggml_metal_init: simdgroup reduction   = true
0.00.121.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.071 I ggml_metal_init: has bfloat            = true
0.00.121.071 I ggml_metal_init: use bfloat            = true
0.00.121.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.145.633 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.166.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.166.637 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.657 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.167.738 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.167.740 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.167.740 I llama_new_context_with_model: graph nodes  = 967
0.00.167.741 I llama_new_context_with_model: graph splits = 2
0.00.167.783 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.914 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.097 I main: llama threadpool init, n_threads = 4
0.00.252.129 I 
0.00.252.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.167 I 
0.00.252.239 I sampler seed: 1234
0.00.252.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.252.269 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.252.270 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.252.270 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.093.623 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.02.093.624 I llama_perf_context_print:        load time =     215.74 ms
0.02.093.625 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.72 tokens per second)
0.02.093.627 I llama_perf_context_print:        eval time =    1794.56 ms /    63 runs   (   28.49 ms per token,    35.11 tokens per second)
0.02.093.627 I llama_perf_context_print:       total time =    1841.53 ms /    70 tokens
0.02.093.853 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.148s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.270 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.277 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.936 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.936 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.936 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.937 I llama_model_loader: - type  f32:  194 tensors
0.00.033.937 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.119 I llm_load_vocab: special tokens cache size = 25
0.00.061.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.277 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.278 I llm_load_print_meta: arch             = gptneox
0.00.061.279 I llm_load_print_meta: vocab type       = BPE
0.00.061.279 I llm_load_print_meta: n_vocab          = 50304
0.00.061.283 I llm_load_print_meta: n_merges         = 50009
0.00.061.283 I llm_load_print_meta: vocab_only       = 0
0.00.061.283 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.283 I llm_load_print_meta: n_embd           = 2048
0.00.061.283 I llm_load_print_meta: n_layer          = 24
0.00.061.289 I llm_load_print_meta: n_head           = 16
0.00.061.290 I llm_load_print_meta: n_head_kv        = 16
0.00.061.290 I llm_load_print_meta: n_rot            = 32
0.00.061.290 I llm_load_print_meta: n_swa            = 0
0.00.061.291 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.291 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.291 I llm_load_print_meta: n_gqa            = 1
0.00.061.293 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.293 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.294 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.294 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.294 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.295 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.296 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.297 I llm_load_print_meta: n_ff             = 8192
0.00.061.297 I llm_load_print_meta: n_expert         = 0
0.00.061.297 I llm_load_print_meta: n_expert_used    = 0
0.00.061.297 I llm_load_print_meta: causal attn      = 1
0.00.061.297 I llm_load_print_meta: pooling type     = 0
0.00.061.299 I llm_load_print_meta: rope type        = 2
0.00.061.299 I llm_load_print_meta: rope scaling     = linear
0.00.061.300 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.300 I llm_load_print_meta: freq_scale_train = 1
0.00.061.300 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.300 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.300 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.300 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.301 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.301 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.301 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.301 I llm_load_print_meta: model type       = 1.4B
0.00.061.302 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.302 I llm_load_print_meta: model params     = 1.41 B
0.00.061.303 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.303 I llm_load_print_meta: general.name     = 1.4B
0.00.061.303 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.303 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.310 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.310 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.314 I llm_load_print_meta: max token length = 1024
0.00.063.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.685 I llm_load_tensors: offloading output layer to GPU
0.00.063.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.697 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.698 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.706 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.707 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.707 I llama_new_context_with_model: n_batch       = 2048
0.00.064.708 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.708 I llama_new_context_with_model: flash_attn    = 0
0.00.064.708 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.708 I llama_new_context_with_model: freq_scale    = 1
0.00.064.709 I ggml_metal_init: allocating
0.00.064.713 I ggml_metal_init: found device: Apple M4
0.00.064.715 I ggml_metal_init: picking default device: Apple M4
0.00.065.427 I ggml_metal_init: using embedded metal library
0.00.067.965 I ggml_metal_init: GPU name:   Apple M4
0.00.067.966 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.967 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.967 I ggml_metal_init: simdgroup reduction   = true
0.00.067.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.968 I ggml_metal_init: has bfloat            = true
0.00.067.968 I ggml_metal_init: use bfloat            = true
0.00.067.968 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.969 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.330 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.569 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.664 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.667 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.667 I llama_new_context_with_model: graph nodes  = 967
0.00.103.668 I llama_new_context_with_model: graph splits = 2
0.00.103.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.502.930 I main: llama threadpool init, n_threads = 4
0.01.502.975 I 
0.01.503.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.503.011 I 
0.01.503.223 I sampler seed: 1234
0.01.503.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.503.277 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.503.279 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.503.279 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.600.353 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.02.600.354 I llama_perf_context_print:        load time =    1493.09 ms
0.02.600.356 I llama_perf_context_print: prompt eval time =      39.92 ms /     7 tokens (    5.70 ms per token,   175.34 tokens per second)
0.02.600.357 I llama_perf_context_print:        eval time =    1054.48 ms /    63 runs   (   16.74 ms per token,    59.74 tokens per second)
0.02.600.357 I llama_perf_context_print:       total time =    1097.43 ms /    70 tokens
0.02.600.525 I ggml_metal_free: deallocating

real	0m2.625s
user	0m0.114s
sys	0m0.214s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.016.067 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.865 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.877 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.878 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.830 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.831 I llama_model_loader: - type  f32:  194 tensors
0.00.040.831 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.080.878 I llm_load_vocab: special tokens cache size = 25
0.00.089.202 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.208 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.210 I llm_load_print_meta: arch             = gptneox
0.00.089.211 I llm_load_print_meta: vocab type       = BPE
0.00.089.211 I llm_load_print_meta: n_vocab          = 50304
0.00.089.211 I llm_load_print_meta: n_merges         = 50009
0.00.089.212 I llm_load_print_meta: vocab_only       = 0
0.00.089.214 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.214 I llm_load_print_meta: n_embd           = 2048
0.00.089.215 I llm_load_print_meta: n_layer          = 24
0.00.089.219 I llm_load_print_meta: n_head           = 16
0.00.089.220 I llm_load_print_meta: n_head_kv        = 16
0.00.089.221 I llm_load_print_meta: n_rot            = 32
0.00.089.221 I llm_load_print_meta: n_swa            = 0
0.00.089.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.222 I llm_load_print_meta: n_gqa            = 1
0.00.089.223 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.224 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.225 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.227 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.227 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.228 I llm_load_print_meta: n_ff             = 8192
0.00.089.228 I llm_load_print_meta: n_expert         = 0
0.00.089.228 I llm_load_print_meta: n_expert_used    = 0
0.00.089.228 I llm_load_print_meta: causal attn      = 1
0.00.089.228 I llm_load_print_meta: pooling type     = 0
0.00.089.229 I llm_load_print_meta: rope type        = 2
0.00.089.231 I llm_load_print_meta: rope scaling     = linear
0.00.089.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.233 I llm_load_print_meta: freq_scale_train = 1
0.00.089.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.234 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.236 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.236 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.236 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.236 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.236 I llm_load_print_meta: model type       = 1.4B
0.00.089.237 I llm_load_print_meta: model ftype      = Q4_0
0.00.089.237 I llm_load_print_meta: model params     = 1.41 B
0.00.089.238 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.089.238 I llm_load_print_meta: general.name     = 1.4B
0.00.089.238 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.239 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.239 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.240 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.241 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.241 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.241 I llm_load_print_meta: max token length = 1024
0.00.091.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.997 I llm_load_tensors: offloading output layer to GPU
0.00.091.997 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.008 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.010 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.093.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.205 I llama_new_context_with_model: n_batch       = 2048
0.00.093.205 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.205 I llama_new_context_with_model: flash_attn    = 0
0.00.093.206 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.206 I llama_new_context_with_model: freq_scale    = 1
0.00.093.207 I ggml_metal_init: allocating
0.00.093.213 I ggml_metal_init: found device: Apple M4
0.00.093.216 I ggml_metal_init: picking default device: Apple M4
0.00.094.054 I ggml_metal_init: using embedded metal library
0.00.097.310 I ggml_metal_init: GPU name:   Apple M4
0.00.097.311 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.313 I ggml_metal_init: simdgroup reduction   = true
0.00.097.313 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.313 I ggml_metal_init: has bfloat            = true
0.00.097.313 I ggml_metal_init: use bfloat            = true
0.00.097.314 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.067 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.133.269 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.275 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.300 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.134.305 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.134.307 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.134.307 I llama_new_context_with_model: graph nodes  = 967
0.00.134.308 I llama_new_context_with_model: graph splits = 2
0.00.134.335 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.134.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.972 I main: llama threadpool init, n_threads = 4
0.00.752.058 I 
0.00.752.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.140 I 
0.00.752.648 I sampler seed: 1234
0.00.752.656 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.692 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.695 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.695 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.449.030 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.449.031 I llama_perf_context_print:        load time =     735.89 ms
0.01.449.032 I llama_perf_context_print: prompt eval time =      50.79 ms /     7 tokens (    7.26 ms per token,   137.83 tokens per second)
0.01.449.032 I llama_perf_context_print:        eval time =     642.35 ms /    63 runs   (   10.20 ms per token,    98.08 tokens per second)
0.01.449.033 I llama_perf_context_print:       total time =     697.07 ms /    70 tokens
0.01.449.180 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.155s
sys	0m0.181s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.343 I llama_model_loader: - type  f32:  194 tensors
0.00.037.343 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.819 I llm_load_vocab: special tokens cache size = 25
0.00.068.267 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.270 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.271 I llm_load_print_meta: arch             = gptneox
0.00.068.271 I llm_load_print_meta: vocab type       = BPE
0.00.068.271 I llm_load_print_meta: n_vocab          = 50304
0.00.068.271 I llm_load_print_meta: n_merges         = 50009
0.00.068.271 I llm_load_print_meta: vocab_only       = 0
0.00.068.272 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.272 I llm_load_print_meta: n_embd           = 2048
0.00.068.272 I llm_load_print_meta: n_layer          = 24
0.00.068.275 I llm_load_print_meta: n_head           = 16
0.00.068.276 I llm_load_print_meta: n_head_kv        = 16
0.00.068.278 I llm_load_print_meta: n_rot            = 32
0.00.068.278 I llm_load_print_meta: n_swa            = 0
0.00.068.278 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.278 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.279 I llm_load_print_meta: n_gqa            = 1
0.00.068.280 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.280 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.281 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.281 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.281 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.281 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.282 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.282 I llm_load_print_meta: n_ff             = 8192
0.00.068.282 I llm_load_print_meta: n_expert         = 0
0.00.068.283 I llm_load_print_meta: n_expert_used    = 0
0.00.068.283 I llm_load_print_meta: causal attn      = 1
0.00.068.284 I llm_load_print_meta: pooling type     = 0
0.00.068.284 I llm_load_print_meta: rope type        = 2
0.00.068.284 I llm_load_print_meta: rope scaling     = linear
0.00.068.285 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.285 I llm_load_print_meta: freq_scale_train = 1
0.00.068.285 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.285 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.285 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.285 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.285 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.285 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.286 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.286 I llm_load_print_meta: model type       = 1.4B
0.00.068.286 I llm_load_print_meta: model ftype      = Q4_1
0.00.068.286 I llm_load_print_meta: model params     = 1.41 B
0.00.068.287 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.068.287 I llm_load_print_meta: general.name     = 1.4B
0.00.068.287 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.288 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.288 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.288 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.288 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.288 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.288 I llm_load_print_meta: max token length = 1024
0.00.070.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.403 I llm_load_tensors: offloading output layer to GPU
0.00.070.403 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.414 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.070.415 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.071.366 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.367 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.367 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.367 I llama_new_context_with_model: n_batch       = 2048
0.00.071.368 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.368 I llama_new_context_with_model: flash_attn    = 0
0.00.071.368 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.368 I llama_new_context_with_model: freq_scale    = 1
0.00.071.369 I ggml_metal_init: allocating
0.00.071.372 I ggml_metal_init: found device: Apple M4
0.00.071.374 I ggml_metal_init: picking default device: Apple M4
0.00.072.014 I ggml_metal_init: using embedded metal library
0.00.074.661 I ggml_metal_init: GPU name:   Apple M4
0.00.074.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.664 I ggml_metal_init: simdgroup reduction   = true
0.00.074.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.664 I ggml_metal_init: has bfloat            = true
0.00.074.664 I ggml_metal_init: use bfloat            = true
0.00.074.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.720 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.136 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.157 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.208 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.210 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.210 I llama_new_context_with_model: graph nodes  = 967
0.00.107.210 I llama_new_context_with_model: graph splits = 2
0.00.107.235 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.240 I main: llama threadpool init, n_threads = 4
0.00.774.278 I 
0.00.774.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.305 I 
0.00.774.521 I sampler seed: 1234
0.00.774.525 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.576 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.578 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.578 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.506.407 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.506.408 I llama_perf_context_print:        load time =     765.45 ms
0.01.506.409 I llama_perf_context_print: prompt eval time =      43.47 ms /     7 tokens (    6.21 ms per token,   161.02 tokens per second)
0.01.506.409 I llama_perf_context_print:        eval time =     685.32 ms /    63 runs   (   10.88 ms per token,    91.93 tokens per second)
0.01.506.410 I llama_perf_context_print:       total time =     732.17 ms /    70 tokens
0.01.506.585 I ggml_metal_free: deallocating

real	0m1.522s
user	0m0.116s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.332 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.988 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.524 I llama_model_loader: - type  f32:  194 tensors
0.00.026.524 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.651 I llm_load_vocab: special tokens cache size = 25
0.00.053.500 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.503 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.504 I llm_load_print_meta: arch             = gptneox
0.00.053.504 I llm_load_print_meta: vocab type       = BPE
0.00.053.504 I llm_load_print_meta: n_vocab          = 50304
0.00.053.504 I llm_load_print_meta: n_merges         = 50009
0.00.053.505 I llm_load_print_meta: vocab_only       = 0
0.00.053.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.505 I llm_load_print_meta: n_embd           = 2048
0.00.053.505 I llm_load_print_meta: n_layer          = 24
0.00.053.508 I llm_load_print_meta: n_head           = 16
0.00.053.509 I llm_load_print_meta: n_head_kv        = 16
0.00.053.509 I llm_load_print_meta: n_rot            = 32
0.00.053.509 I llm_load_print_meta: n_swa            = 0
0.00.053.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.510 I llm_load_print_meta: n_gqa            = 1
0.00.053.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.512 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.514 I llm_load_print_meta: n_ff             = 8192
0.00.053.514 I llm_load_print_meta: n_expert         = 0
0.00.053.514 I llm_load_print_meta: n_expert_used    = 0
0.00.053.514 I llm_load_print_meta: causal attn      = 1
0.00.053.515 I llm_load_print_meta: pooling type     = 0
0.00.053.516 I llm_load_print_meta: rope type        = 2
0.00.053.517 I llm_load_print_meta: rope scaling     = linear
0.00.053.517 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.517 I llm_load_print_meta: freq_scale_train = 1
0.00.053.517 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.518 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.518 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.518 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.518 I llm_load_print_meta: model type       = 1.4B
0.00.053.519 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.519 I llm_load_print_meta: model params     = 1.41 B
0.00.053.520 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.520 I llm_load_print_meta: general.name     = 1.4B
0.00.053.520 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.521 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.521 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.521 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.525 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.525 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.525 I llm_load_print_meta: max token length = 1024
0.00.055.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.197 I llm_load_tensors: offloading output layer to GPU
0.00.055.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.207 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.208 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.049 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.050 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.050 I llama_new_context_with_model: n_batch       = 2048
0.00.056.050 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.050 I llama_new_context_with_model: flash_attn    = 0
0.00.056.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.051 I llama_new_context_with_model: freq_scale    = 1
0.00.056.051 I ggml_metal_init: allocating
0.00.056.055 I ggml_metal_init: found device: Apple M4
0.00.056.057 I ggml_metal_init: picking default device: Apple M4
0.00.056.653 I ggml_metal_init: using embedded metal library
0.00.059.088 I ggml_metal_init: GPU name:   Apple M4
0.00.059.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.091 I ggml_metal_init: simdgroup reduction   = true
0.00.059.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.091 I ggml_metal_init: has bfloat            = true
0.00.059.091 I ggml_metal_init: use bfloat            = true
0.00.059.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.251 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.344 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.350 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.459 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.460 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.461 I llama_new_context_with_model: graph nodes  = 967
0.00.091.461 I llama_new_context_with_model: graph splits = 2
0.00.091.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.870 I main: llama threadpool init, n_threads = 4
0.00.763.909 I 
0.00.763.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.953 I 
0.00.764.170 I sampler seed: 1234
0.00.764.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.188 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.188 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.557.338 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.557.339 I llama_perf_context_print:        load time =     753.53 ms
0.01.557.340 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.67 tokens per second)
0.01.557.340 I llama_perf_context_print:        eval time =     743.12 ms /    63 runs   (   11.80 ms per token,    84.78 tokens per second)
0.01.557.344 I llama_perf_context_print:       total time =     793.47 ms /    70 tokens
0.01.557.534 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.740 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.906 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.265 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.351 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.352 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.352 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.353 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.353 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.353 I llama_model_loader: - type  f32:  194 tensors
0.00.025.354 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.354 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.973 I llm_load_vocab: special tokens cache size = 25
0.00.051.880 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.885 I llm_load_print_meta: arch             = gptneox
0.00.051.885 I llm_load_print_meta: vocab type       = BPE
0.00.051.885 I llm_load_print_meta: n_vocab          = 50304
0.00.051.888 I llm_load_print_meta: n_merges         = 50009
0.00.051.888 I llm_load_print_meta: vocab_only       = 0
0.00.051.888 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.888 I llm_load_print_meta: n_embd           = 2048
0.00.051.889 I llm_load_print_meta: n_layer          = 24
0.00.051.892 I llm_load_print_meta: n_head           = 16
0.00.051.892 I llm_load_print_meta: n_head_kv        = 16
0.00.051.894 I llm_load_print_meta: n_rot            = 32
0.00.051.894 I llm_load_print_meta: n_swa            = 0
0.00.051.894 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.894 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.895 I llm_load_print_meta: n_gqa            = 1
0.00.051.896 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.896 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.897 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.897 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.898 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.898 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.898 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.899 I llm_load_print_meta: n_ff             = 8192
0.00.051.899 I llm_load_print_meta: n_expert         = 0
0.00.051.899 I llm_load_print_meta: n_expert_used    = 0
0.00.051.901 I llm_load_print_meta: causal attn      = 1
0.00.051.901 I llm_load_print_meta: pooling type     = 0
0.00.051.902 I llm_load_print_meta: rope type        = 2
0.00.051.902 I llm_load_print_meta: rope scaling     = linear
0.00.051.902 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.903 I llm_load_print_meta: freq_scale_train = 1
0.00.051.903 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.903 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.904 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.904 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.904 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.904 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.904 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.904 I llm_load_print_meta: model type       = 1.4B
0.00.051.907 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.907 I llm_load_print_meta: model params     = 1.41 B
0.00.051.908 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.909 I llm_load_print_meta: general.name     = 1.4B
0.00.051.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.909 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.910 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.910 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.910 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.910 I llm_load_print_meta: max token length = 1024
0.00.054.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.007 I llm_load_tensors: offloading output layer to GPU
0.00.054.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.017 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.019 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.951 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.952 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.952 I llama_new_context_with_model: n_batch       = 2048
0.00.054.952 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.952 I llama_new_context_with_model: flash_attn    = 0
0.00.054.953 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.953 I llama_new_context_with_model: freq_scale    = 1
0.00.054.953 I ggml_metal_init: allocating
0.00.054.959 I ggml_metal_init: found device: Apple M4
0.00.054.961 I ggml_metal_init: picking default device: Apple M4
0.00.055.558 I ggml_metal_init: using embedded metal library
0.00.057.896 I ggml_metal_init: GPU name:   Apple M4
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.898 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.899 I ggml_metal_init: simdgroup reduction   = true
0.00.057.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.900 I ggml_metal_init: has bfloat            = true
0.00.057.901 I ggml_metal_init: use bfloat            = true
0.00.057.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.622 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.842 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.850 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.956 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.958 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.958 I llama_new_context_with_model: graph nodes  = 967
0.00.087.958 I llama_new_context_with_model: graph splits = 2
0.00.087.983 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.563 I main: llama threadpool init, n_threads = 4
0.00.794.604 I 
0.00.794.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.633 I 
0.00.794.788 I sampler seed: 1234
0.00.794.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.812 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.812 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.813 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.635.195 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.635.196 I llama_perf_context_print:        load time =     785.82 ms
0.01.635.196 I llama_perf_context_print: prompt eval time =      42.18 ms /     7 tokens (    6.03 ms per token,   165.96 tokens per second)
0.01.635.197 I llama_perf_context_print:        eval time =     795.03 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.635.197 I llama_perf_context_print:       total time =     840.64 ms /    70 tokens
0.01.635.362 I ggml_metal_free: deallocating

real	0m1.651s
user	0m0.110s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.125 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.683 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.957 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.957 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.958 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.958 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.652 I llm_load_vocab: special tokens cache size = 25
0.00.051.649 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.653 I llm_load_print_meta: arch             = gptneox
0.00.051.653 I llm_load_print_meta: vocab type       = BPE
0.00.051.653 I llm_load_print_meta: n_vocab          = 50304
0.00.051.653 I llm_load_print_meta: n_merges         = 50009
0.00.051.654 I llm_load_print_meta: vocab_only       = 0
0.00.051.654 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.654 I llm_load_print_meta: n_embd           = 2048
0.00.051.654 I llm_load_print_meta: n_layer          = 24
0.00.051.657 I llm_load_print_meta: n_head           = 16
0.00.051.659 I llm_load_print_meta: n_head_kv        = 16
0.00.051.660 I llm_load_print_meta: n_rot            = 32
0.00.051.660 I llm_load_print_meta: n_swa            = 0
0.00.051.660 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.660 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.661 I llm_load_print_meta: n_gqa            = 1
0.00.051.662 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.664 I llm_load_print_meta: n_ff             = 8192
0.00.051.665 I llm_load_print_meta: n_expert         = 0
0.00.051.665 I llm_load_print_meta: n_expert_used    = 0
0.00.051.665 I llm_load_print_meta: causal attn      = 1
0.00.051.665 I llm_load_print_meta: pooling type     = 0
0.00.051.665 I llm_load_print_meta: rope type        = 2
0.00.051.665 I llm_load_print_meta: rope scaling     = linear
0.00.051.667 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.667 I llm_load_print_meta: freq_scale_train = 1
0.00.051.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.668 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.668 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.668 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.672 I llm_load_print_meta: model type       = 1.4B
0.00.051.673 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.673 I llm_load_print_meta: model params     = 1.41 B
0.00.051.674 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.674 I llm_load_print_meta: general.name     = 1.4B
0.00.051.675 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.676 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.676 I llm_load_print_meta: max token length = 1024
0.00.053.608 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.608 I llm_load_tensors: offloading output layer to GPU
0.00.053.608 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.619 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.620 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.518 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.518 I llama_new_context_with_model: n_batch       = 2048
0.00.054.518 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.518 I llama_new_context_with_model: flash_attn    = 0
0.00.054.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.519 I llama_new_context_with_model: freq_scale    = 1
0.00.054.519 I ggml_metal_init: allocating
0.00.054.523 I ggml_metal_init: found device: Apple M4
0.00.054.525 I ggml_metal_init: picking default device: Apple M4
0.00.055.096 I ggml_metal_init: using embedded metal library
0.00.057.419 I ggml_metal_init: GPU name:   Apple M4
0.00.057.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.422 I ggml_metal_init: simdgroup reduction   = true
0.00.057.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.422 I ggml_metal_init: has bfloat            = true
0.00.057.422 I ggml_metal_init: use bfloat            = true
0.00.057.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.366 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.961 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.966 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.987 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.039 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.041 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.041 I llama_new_context_with_model: graph nodes  = 967
0.00.088.041 I llama_new_context_with_model: graph splits = 2
0.00.088.054 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.473 I main: llama threadpool init, n_threads = 4
0.00.487.514 I 
0.00.487.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.544 I 
0.00.487.758 I sampler seed: 1234
0.00.487.762 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.798 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.798 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.166.218 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.166.219 I llama_perf_context_print:        load time =     477.34 ms
0.01.166.220 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.166.221 I llama_perf_context_print:        eval time =     639.64 ms /    63 runs   (   10.15 ms per token,    98.49 tokens per second)
0.01.166.222 I llama_perf_context_print:       total time =     678.75 ms /    70 tokens
0.01.166.407 I ggml_metal_free: deallocating

real	0m1.185s
user	0m0.111s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.708 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.754 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.616 I llama_model_loader: - type  f32:  194 tensors
0.00.026.616 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.616 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.617 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.617 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.735 I llm_load_vocab: special tokens cache size = 25
0.00.054.897 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.901 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.901 I llm_load_print_meta: arch             = gptneox
0.00.054.902 I llm_load_print_meta: vocab type       = BPE
0.00.054.902 I llm_load_print_meta: n_vocab          = 50304
0.00.054.902 I llm_load_print_meta: n_merges         = 50009
0.00.054.902 I llm_load_print_meta: vocab_only       = 0
0.00.054.903 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.907 I llm_load_print_meta: n_embd           = 2048
0.00.054.908 I llm_load_print_meta: n_layer          = 24
0.00.054.911 I llm_load_print_meta: n_head           = 16
0.00.054.912 I llm_load_print_meta: n_head_kv        = 16
0.00.054.912 I llm_load_print_meta: n_rot            = 32
0.00.054.912 I llm_load_print_meta: n_swa            = 0
0.00.054.912 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.912 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.913 I llm_load_print_meta: n_gqa            = 1
0.00.054.914 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.914 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.915 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.915 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.916 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.916 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.916 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.917 I llm_load_print_meta: n_ff             = 8192
0.00.054.917 I llm_load_print_meta: n_expert         = 0
0.00.054.917 I llm_load_print_meta: n_expert_used    = 0
0.00.054.918 I llm_load_print_meta: causal attn      = 1
0.00.054.918 I llm_load_print_meta: pooling type     = 0
0.00.054.918 I llm_load_print_meta: rope type        = 2
0.00.054.918 I llm_load_print_meta: rope scaling     = linear
0.00.054.919 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.919 I llm_load_print_meta: freq_scale_train = 1
0.00.054.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.920 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.920 I llm_load_print_meta: model type       = 1.4B
0.00.054.921 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.054.921 I llm_load_print_meta: model params     = 1.41 B
0.00.054.922 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.054.922 I llm_load_print_meta: general.name     = 1.4B
0.00.054.922 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.923 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.923 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.923 I llm_load_print_meta: max token length = 1024
0.00.056.850 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.850 I llm_load_tensors: offloading output layer to GPU
0.00.056.850 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.861 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.056.862 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.057.834 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.834 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.835 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.835 I llama_new_context_with_model: n_batch       = 2048
0.00.057.835 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.835 I llama_new_context_with_model: flash_attn    = 0
0.00.057.836 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.836 I llama_new_context_with_model: freq_scale    = 1
0.00.057.836 I ggml_metal_init: allocating
0.00.057.840 I ggml_metal_init: found device: Apple M4
0.00.057.842 I ggml_metal_init: picking default device: Apple M4
0.00.058.504 I ggml_metal_init: using embedded metal library
0.00.060.977 I ggml_metal_init: GPU name:   Apple M4
0.00.060.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.980 I ggml_metal_init: simdgroup reduction   = true
0.00.060.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.980 I ggml_metal_init: has bfloat            = true
0.00.060.980 I ggml_metal_init: use bfloat            = true
0.00.060.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.558 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.091.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.751 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.705 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.706 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.707 I llama_new_context_with_model: graph nodes  = 967
0.00.092.707 I llama_new_context_with_model: graph splits = 2
0.00.092.720 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.995.269 I main: llama threadpool init, n_threads = 4
0.00.995.398 I 
0.00.995.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.995.538 I 
0.00.996.077 I sampler seed: 1234
0.00.996.088 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.996.108 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.996.109 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.996.109 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.748.252 I llama_perf_sampler_print:    sampling time =       1.59 ms /    71 runs   (    0.02 ms per token, 44626.02 tokens per second)
0.01.748.253 I llama_perf_context_print:        load time =     985.55 ms
0.01.748.254 I llama_perf_context_print: prompt eval time =      41.02 ms /     7 tokens (    5.86 ms per token,   170.67 tokens per second)
0.01.748.255 I llama_perf_context_print:        eval time =     707.67 ms /    63 runs   (   11.23 ms per token,    89.02 tokens per second)
0.01.748.255 I llama_perf_context_print:       total time =     752.99 ms /    70 tokens
0.01.748.427 I ggml_metal_free: deallocating

real	0m1.766s
user	0m0.135s
sys	0m0.130s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.330 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.976 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.993 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.370 I llama_model_loader: - type  f32:  194 tensors
0.00.025.370 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.370 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.370 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.972 I llm_load_vocab: special tokens cache size = 25
0.00.052.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.861 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.863 I llm_load_print_meta: arch             = gptneox
0.00.052.868 I llm_load_print_meta: vocab type       = BPE
0.00.052.868 I llm_load_print_meta: n_vocab          = 50304
0.00.052.868 I llm_load_print_meta: n_merges         = 50009
0.00.052.869 I llm_load_print_meta: vocab_only       = 0
0.00.052.869 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.869 I llm_load_print_meta: n_embd           = 2048
0.00.052.869 I llm_load_print_meta: n_layer          = 24
0.00.052.873 I llm_load_print_meta: n_head           = 16
0.00.052.879 I llm_load_print_meta: n_head_kv        = 16
0.00.052.879 I llm_load_print_meta: n_rot            = 32
0.00.052.879 I llm_load_print_meta: n_swa            = 0
0.00.052.879 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.879 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.880 I llm_load_print_meta: n_gqa            = 1
0.00.052.881 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.882 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.882 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.883 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.883 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.883 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.884 I llm_load_print_meta: n_ff             = 8192
0.00.052.884 I llm_load_print_meta: n_expert         = 0
0.00.052.886 I llm_load_print_meta: n_expert_used    = 0
0.00.052.887 I llm_load_print_meta: causal attn      = 1
0.00.052.887 I llm_load_print_meta: pooling type     = 0
0.00.052.887 I llm_load_print_meta: rope type        = 2
0.00.052.887 I llm_load_print_meta: rope scaling     = linear
0.00.052.888 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.888 I llm_load_print_meta: freq_scale_train = 1
0.00.052.888 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.888 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.889 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.889 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.889 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.890 I llm_load_print_meta: model type       = 1.4B
0.00.052.891 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.891 I llm_load_print_meta: model params     = 1.41 B
0.00.052.891 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.892 I llm_load_print_meta: general.name     = 1.4B
0.00.052.892 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.895 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.895 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.896 I llm_load_print_meta: max token length = 1024
0.00.055.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.007 I llm_load_tensors: offloading output layer to GPU
0.00.055.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.018 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.019 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.044 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.044 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.044 I llama_new_context_with_model: n_batch       = 2048
0.00.056.045 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.045 I llama_new_context_with_model: flash_attn    = 0
0.00.056.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.046 I llama_new_context_with_model: freq_scale    = 1
0.00.056.046 I ggml_metal_init: allocating
0.00.056.049 I ggml_metal_init: found device: Apple M4
0.00.056.051 I ggml_metal_init: picking default device: Apple M4
0.00.056.666 I ggml_metal_init: using embedded metal library
0.00.059.101 I ggml_metal_init: GPU name:   Apple M4
0.00.059.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.104 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.105 I ggml_metal_init: simdgroup reduction   = true
0.00.059.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.106 I ggml_metal_init: has bfloat            = true
0.00.059.106 I ggml_metal_init: use bfloat            = true
0.00.059.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.229 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.844 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.869 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.898 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.900 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.900 I llama_new_context_with_model: graph nodes  = 967
0.00.090.900 I llama_new_context_with_model: graph splits = 2
0.00.090.924 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.827 I main: llama threadpool init, n_threads = 4
0.00.621.866 I 
0.00.621.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.894 I 
0.00.622.119 I sampler seed: 1234
0.00.622.124 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.138 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.138 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.138 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.384.111 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.384.112 I llama_perf_context_print:        load time =     612.49 ms
0.01.384.112 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.40 tokens per second)
0.01.384.116 I llama_perf_context_print:        eval time =     711.63 ms /    63 runs   (   11.30 ms per token,    88.53 tokens per second)
0.01.384.116 I llama_perf_context_print:       total time =     762.29 ms /    70 tokens
0.01.384.293 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.112s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.382 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.387 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.840 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.841 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.842 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.842 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.842 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.843 I llama_model_loader: - type  f32:  194 tensors
0.00.024.843 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.843 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.437 I llm_load_vocab: special tokens cache size = 25
0.00.051.495 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.498 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.499 I llm_load_print_meta: arch             = gptneox
0.00.051.499 I llm_load_print_meta: vocab type       = BPE
0.00.051.499 I llm_load_print_meta: n_vocab          = 50304
0.00.051.499 I llm_load_print_meta: n_merges         = 50009
0.00.051.500 I llm_load_print_meta: vocab_only       = 0
0.00.051.500 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.500 I llm_load_print_meta: n_embd           = 2048
0.00.051.500 I llm_load_print_meta: n_layer          = 24
0.00.051.503 I llm_load_print_meta: n_head           = 16
0.00.051.504 I llm_load_print_meta: n_head_kv        = 16
0.00.051.504 I llm_load_print_meta: n_rot            = 32
0.00.051.504 I llm_load_print_meta: n_swa            = 0
0.00.051.505 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.505 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.505 I llm_load_print_meta: n_gqa            = 1
0.00.051.506 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.509 I llm_load_print_meta: n_ff             = 8192
0.00.051.509 I llm_load_print_meta: n_expert         = 0
0.00.051.509 I llm_load_print_meta: n_expert_used    = 0
0.00.051.511 I llm_load_print_meta: causal attn      = 1
0.00.051.513 I llm_load_print_meta: pooling type     = 0
0.00.051.513 I llm_load_print_meta: rope type        = 2
0.00.051.513 I llm_load_print_meta: rope scaling     = linear
0.00.051.514 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.514 I llm_load_print_meta: freq_scale_train = 1
0.00.051.514 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.514 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.515 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.515 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.515 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.515 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.515 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.515 I llm_load_print_meta: model type       = 1.4B
0.00.051.516 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.516 I llm_load_print_meta: model params     = 1.41 B
0.00.051.517 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.517 I llm_load_print_meta: general.name     = 1.4B
0.00.051.517 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.517 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.518 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.518 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.518 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.518 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.519 I llm_load_print_meta: max token length = 1024
0.00.053.535 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.535 I llm_load_tensors: offloading output layer to GPU
0.00.053.535 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.546 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.547 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.491 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.491 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.491 I llama_new_context_with_model: n_batch       = 2048
0.00.054.491 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.492 I llama_new_context_with_model: flash_attn    = 0
0.00.054.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.492 I llama_new_context_with_model: freq_scale    = 1
0.00.054.493 I ggml_metal_init: allocating
0.00.054.496 I ggml_metal_init: found device: Apple M4
0.00.054.498 I ggml_metal_init: picking default device: Apple M4
0.00.055.093 I ggml_metal_init: using embedded metal library
0.00.057.425 I ggml_metal_init: GPU name:   Apple M4
0.00.057.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.428 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.429 I ggml_metal_init: simdgroup reduction   = true
0.00.057.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.429 I ggml_metal_init: has bfloat            = true
0.00.057.429 I ggml_metal_init: use bfloat            = true
0.00.057.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.319 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.785 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.800 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.827 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.810 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.812 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.812 I llama_new_context_with_model: graph nodes  = 967
0.00.087.812 I llama_new_context_with_model: graph splits = 2
0.00.087.837 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.978 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.840 I main: llama threadpool init, n_threads = 4
0.00.703.878 I 
0.00.703.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.906 I 
0.00.704.141 I sampler seed: 1234
0.00.704.145 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.159 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.159 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.159 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.554.278 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.554.278 I llama_perf_context_print:        load time =     695.09 ms
0.01.554.279 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.57 tokens per second)
0.01.554.280 I llama_perf_context_print:        eval time =     795.44 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.554.280 I llama_perf_context_print:       total time =     850.44 ms /    70 tokens
0.01.554.492 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.111s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.044 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.470 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.470 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.546 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.815 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.816 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.817 I llama_model_loader: - type  f32:  194 tensors
0.00.025.817 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.485 I llm_load_vocab: special tokens cache size = 25
0.00.052.260 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.263 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.265 I llm_load_print_meta: arch             = gptneox
0.00.052.265 I llm_load_print_meta: vocab type       = BPE
0.00.052.265 I llm_load_print_meta: n_vocab          = 50304
0.00.052.266 I llm_load_print_meta: n_merges         = 50009
0.00.052.266 I llm_load_print_meta: vocab_only       = 0
0.00.052.271 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.271 I llm_load_print_meta: n_embd           = 2048
0.00.052.271 I llm_load_print_meta: n_layer          = 24
0.00.052.274 I llm_load_print_meta: n_head           = 16
0.00.052.275 I llm_load_print_meta: n_head_kv        = 16
0.00.052.275 I llm_load_print_meta: n_rot            = 32
0.00.052.275 I llm_load_print_meta: n_swa            = 0
0.00.052.275 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.276 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.276 I llm_load_print_meta: n_gqa            = 1
0.00.052.277 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.279 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.280 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.281 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.281 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.281 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.281 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.282 I llm_load_print_meta: n_ff             = 8192
0.00.052.282 I llm_load_print_meta: n_expert         = 0
0.00.052.282 I llm_load_print_meta: n_expert_used    = 0
0.00.052.283 I llm_load_print_meta: causal attn      = 1
0.00.052.284 I llm_load_print_meta: pooling type     = 0
0.00.052.285 I llm_load_print_meta: rope type        = 2
0.00.052.285 I llm_load_print_meta: rope scaling     = linear
0.00.052.286 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.286 I llm_load_print_meta: freq_scale_train = 1
0.00.052.286 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.286 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.286 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.287 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.287 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.287 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.287 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.289 I llm_load_print_meta: model type       = 1.4B
0.00.052.290 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.292 I llm_load_print_meta: model params     = 1.41 B
0.00.052.292 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.293 I llm_load_print_meta: general.name     = 1.4B
0.00.052.293 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.293 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.294 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.294 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.294 I llm_load_print_meta: max token length = 1024
0.00.054.162 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.162 I llm_load_tensors: offloading output layer to GPU
0.00.054.162 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.168 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.168 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.186 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.187 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.187 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.187 I llama_new_context_with_model: n_batch       = 2048
0.00.055.187 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.187 I llama_new_context_with_model: flash_attn    = 0
0.00.055.188 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.188 I llama_new_context_with_model: freq_scale    = 1
0.00.055.189 I ggml_metal_init: allocating
0.00.055.194 I ggml_metal_init: found device: Apple M4
0.00.055.197 I ggml_metal_init: picking default device: Apple M4
0.00.055.770 I ggml_metal_init: using embedded metal library
0.00.058.098 I ggml_metal_init: GPU name:   Apple M4
0.00.058.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.100 I ggml_metal_init: simdgroup reduction   = true
0.00.058.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.102 I ggml_metal_init: has bfloat            = true
0.00.058.102 I ggml_metal_init: use bfloat            = true
0.00.058.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.987 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.578 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.603 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.620 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.622 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.622 I llama_new_context_with_model: graph nodes  = 967
0.00.088.623 I llama_new_context_with_model: graph splits = 2
0.00.088.647 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.103 I main: llama threadpool init, n_threads = 4
0.00.751.136 I 
0.00.751.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.166 I 
0.00.751.385 I sampler seed: 1234
0.00.751.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.455 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.637.933 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.637.935 I llama_perf_context_print:        load time =     741.06 ms
0.01.637.936 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.59 tokens per second)
0.01.637.936 I llama_perf_context_print:        eval time =     829.11 ms /    63 runs   (   13.16 ms per token,    75.99 tokens per second)
0.01.637.936 I llama_perf_context_print:       total time =     886.83 ms /    70 tokens
0.01.638.193 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.429 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.364 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.384 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.385 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.386 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.228 I llama_model_loader: - type  f32:  194 tensors
0.00.039.229 I llama_model_loader: - type  f16:   98 tensors
0.00.060.530 I llm_load_vocab: special tokens cache size = 25
0.00.066.591 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.596 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.596 I llm_load_print_meta: arch             = gptneox
0.00.066.597 I llm_load_print_meta: vocab type       = BPE
0.00.066.597 I llm_load_print_meta: n_vocab          = 50304
0.00.066.598 I llm_load_print_meta: n_merges         = 50009
0.00.066.598 I llm_load_print_meta: vocab_only       = 0
0.00.066.599 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.601 I llm_load_print_meta: n_embd           = 2048
0.00.066.601 I llm_load_print_meta: n_layer          = 24
0.00.066.605 I llm_load_print_meta: n_head           = 16
0.00.066.606 I llm_load_print_meta: n_head_kv        = 16
0.00.066.606 I llm_load_print_meta: n_rot            = 32
0.00.066.606 I llm_load_print_meta: n_swa            = 0
0.00.066.607 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.607 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.607 I llm_load_print_meta: n_gqa            = 1
0.00.066.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.610 I llm_load_print_meta: n_ff             = 8192
0.00.066.611 I llm_load_print_meta: n_expert         = 0
0.00.066.611 I llm_load_print_meta: n_expert_used    = 0
0.00.066.611 I llm_load_print_meta: causal attn      = 1
0.00.066.611 I llm_load_print_meta: pooling type     = 0
0.00.066.611 I llm_load_print_meta: rope type        = 2
0.00.066.611 I llm_load_print_meta: rope scaling     = linear
0.00.066.612 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.612 I llm_load_print_meta: freq_scale_train = 1
0.00.066.612 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.612 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.613 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.613 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.613 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.613 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.613 I llm_load_print_meta: model type       = 1.4B
0.00.066.614 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.066.614 I llm_load_print_meta: model params     = 1.41 B
0.00.066.615 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.066.615 I llm_load_print_meta: general.name     = 1.4B
0.00.066.616 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.616 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.616 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.617 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.617 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.617 I llm_load_print_meta: max token length = 1024
0.00.068.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.238 I llm_load_tensors: offloading output layer to GPU
0.00.068.238 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.248 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.068.250 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.069.086 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.087 I llama_new_context_with_model: n_ctx         = 128
0.00.069.087 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.087 I llama_new_context_with_model: n_batch       = 128
0.00.069.087 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.087 I llama_new_context_with_model: flash_attn    = 0
0.00.069.088 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.088 I llama_new_context_with_model: freq_scale    = 1
0.00.069.089 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.089 I ggml_metal_init: allocating
0.00.069.099 I ggml_metal_init: found device: Apple M4
0.00.069.102 I ggml_metal_init: picking default device: Apple M4
0.00.069.711 I ggml_metal_init: using embedded metal library
0.00.072.047 I ggml_metal_init: GPU name:   Apple M4
0.00.072.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.049 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.050 I ggml_metal_init: simdgroup reduction   = true
0.00.072.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.050 I ggml_metal_init: has bfloat            = true
0.00.072.050 I ggml_metal_init: use bfloat            = true
0.00.072.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.272 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.784 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.798 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.699 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.700 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.700 I llama_new_context_with_model: graph nodes  = 967
0.00.084.701 I llama_new_context_with_model: graph splits = 2
0.00.084.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.981.222 I 
0.00.981.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.981.299 I perplexity: tokenizing the input ..
0.00.992.522 I perplexity: tokenization took 11.22 ms
0.00.992.526 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.113.972 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.115.640 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.115.668 I llama_perf_context_print:        load time =     963.43 ms
0.01.115.671 I llama_perf_context_print: prompt eval time =     121.06 ms /   128 tokens (    0.95 ms per token,  1057.28 tokens per second)
0.01.115.673 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.115.673 I llama_perf_context_print:       total time =     134.45 ms /   129 tokens
0.01.116.204 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.103s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.141 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.158 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.285 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.285 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.290 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.825 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.828 I llama_model_loader: - type  f32:  194 tensors
0.00.033.828 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.687 I llm_load_vocab: special tokens cache size = 25
0.00.066.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.086 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.090 I llm_load_print_meta: arch             = gptneox
0.00.066.090 I llm_load_print_meta: vocab type       = BPE
0.00.066.090 I llm_load_print_meta: n_vocab          = 50304
0.00.066.090 I llm_load_print_meta: n_merges         = 50009
0.00.066.091 I llm_load_print_meta: vocab_only       = 0
0.00.066.091 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.091 I llm_load_print_meta: n_embd           = 2048
0.00.066.092 I llm_load_print_meta: n_layer          = 24
0.00.066.095 I llm_load_print_meta: n_head           = 16
0.00.066.096 I llm_load_print_meta: n_head_kv        = 16
0.00.066.096 I llm_load_print_meta: n_rot            = 32
0.00.066.096 I llm_load_print_meta: n_swa            = 0
0.00.066.096 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.097 I llm_load_print_meta: n_gqa            = 1
0.00.066.098 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.099 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.099 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.101 I llm_load_print_meta: n_ff             = 8192
0.00.066.101 I llm_load_print_meta: n_expert         = 0
0.00.066.101 I llm_load_print_meta: n_expert_used    = 0
0.00.066.101 I llm_load_print_meta: causal attn      = 1
0.00.066.101 I llm_load_print_meta: pooling type     = 0
0.00.066.101 I llm_load_print_meta: rope type        = 2
0.00.066.102 I llm_load_print_meta: rope scaling     = linear
0.00.066.102 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.102 I llm_load_print_meta: freq_scale_train = 1
0.00.066.102 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.103 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.103 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.103 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.103 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.103 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.104 I llm_load_print_meta: model type       = 1.4B
0.00.066.104 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.106 I llm_load_print_meta: model params     = 1.41 B
0.00.066.106 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.107 I llm_load_print_meta: general.name     = 1.4B
0.00.066.107 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.107 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.108 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.108 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.108 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.108 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.109 I llm_load_print_meta: max token length = 1024
0.00.068.518 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.518 I llm_load_tensors: offloading output layer to GPU
0.00.068.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.529 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.530 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.466 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.466 I llama_new_context_with_model: n_ctx         = 128
0.00.069.467 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.467 I llama_new_context_with_model: n_batch       = 128
0.00.069.467 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.467 I llama_new_context_with_model: flash_attn    = 0
0.00.069.468 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.468 I llama_new_context_with_model: freq_scale    = 1
0.00.069.468 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.469 I ggml_metal_init: allocating
0.00.069.472 I ggml_metal_init: found device: Apple M4
0.00.069.474 I ggml_metal_init: picking default device: Apple M4
0.00.070.133 I ggml_metal_init: using embedded metal library
0.00.072.534 I ggml_metal_init: GPU name:   Apple M4
0.00.072.536 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.537 I ggml_metal_init: simdgroup reduction   = true
0.00.072.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.537 I ggml_metal_init: has bfloat            = true
0.00.072.537 I ggml_metal_init: use bfloat            = true
0.00.072.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.065 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.473 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.489 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.413 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.414 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.415 I llama_new_context_with_model: graph nodes  = 967
0.00.084.415 I llama_new_context_with_model: graph splits = 2
0.00.084.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.873.546 I 
0.00.873.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.873.615 I perplexity: tokenizing the input ..
0.00.881.547 I perplexity: tokenization took 7.93 ms
0.00.881.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.005.758 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.006.941 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.006.963 I llama_perf_context_print:        load time =     861.38 ms
0.01.006.964 I llama_perf_context_print: prompt eval time =     123.98 ms /   128 tokens (    0.97 ms per token,  1032.42 tokens per second)
0.01.006.965 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.006.965 I llama_perf_context_print:       total time =     133.42 ms /   129 tokens
0.01.007.411 I ggml_metal_free: deallocating

real	0m1.026s
user	0m0.095s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.313 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.187 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.196 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.197 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.582 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.583 I llama_model_loader: - type  f32:  194 tensors
0.00.024.583 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.583 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.171 I llm_load_vocab: special tokens cache size = 25
0.00.051.096 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.098 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.099 I llm_load_print_meta: arch             = gptneox
0.00.051.099 I llm_load_print_meta: vocab type       = BPE
0.00.051.099 I llm_load_print_meta: n_vocab          = 50304
0.00.051.099 I llm_load_print_meta: n_merges         = 50009
0.00.051.100 I llm_load_print_meta: vocab_only       = 0
0.00.051.100 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.100 I llm_load_print_meta: n_embd           = 2048
0.00.051.100 I llm_load_print_meta: n_layer          = 24
0.00.051.103 I llm_load_print_meta: n_head           = 16
0.00.051.104 I llm_load_print_meta: n_head_kv        = 16
0.00.051.104 I llm_load_print_meta: n_rot            = 32
0.00.051.104 I llm_load_print_meta: n_swa            = 0
0.00.051.105 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.105 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.105 I llm_load_print_meta: n_gqa            = 1
0.00.051.106 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.110 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.111 I llm_load_print_meta: n_ff             = 8192
0.00.051.111 I llm_load_print_meta: n_expert         = 0
0.00.051.112 I llm_load_print_meta: n_expert_used    = 0
0.00.051.112 I llm_load_print_meta: causal attn      = 1
0.00.051.112 I llm_load_print_meta: pooling type     = 0
0.00.051.112 I llm_load_print_meta: rope type        = 2
0.00.051.112 I llm_load_print_meta: rope scaling     = linear
0.00.051.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.113 I llm_load_print_meta: freq_scale_train = 1
0.00.051.113 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.115 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.115 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.115 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.116 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.116 I llm_load_print_meta: model type       = 1.4B
0.00.051.116 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.117 I llm_load_print_meta: model params     = 1.41 B
0.00.051.117 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.117 I llm_load_print_meta: general.name     = 1.4B
0.00.051.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.118 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.119 I llm_load_print_meta: max token length = 1024
0.00.053.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.079 I llm_load_tensors: offloading output layer to GPU
0.00.053.079 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.090 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.091 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.993 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.993 I llama_new_context_with_model: n_ctx         = 128
0.00.053.994 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.994 I llama_new_context_with_model: n_batch       = 128
0.00.053.994 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.994 I llama_new_context_with_model: flash_attn    = 0
0.00.053.994 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.995 I llama_new_context_with_model: freq_scale    = 1
0.00.053.995 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.995 I ggml_metal_init: allocating
0.00.053.999 I ggml_metal_init: found device: Apple M4
0.00.054.001 I ggml_metal_init: picking default device: Apple M4
0.00.054.562 I ggml_metal_init: using embedded metal library
0.00.056.905 I ggml_metal_init: GPU name:   Apple M4
0.00.056.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.907 I ggml_metal_init: simdgroup reduction   = true
0.00.056.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.908 I ggml_metal_init: has bfloat            = true
0.00.056.908 I ggml_metal_init: use bfloat            = true
0.00.056.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.998 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.285 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.291 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.305 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.208 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.209 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.209 I llama_new_context_with_model: graph nodes  = 967
0.00.069.209 I llama_new_context_with_model: graph splits = 2
0.00.069.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.459 I 
0.00.603.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.507 I perplexity: tokenizing the input ..
0.00.611.164 I perplexity: tokenization took 7.655 ms
0.00.611.173 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.733.906 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.735.104 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.735.119 I llama_perf_context_print:        load time =     594.14 ms
0.00.735.120 I llama_perf_context_print: prompt eval time =     122.47 ms /   128 tokens (    0.96 ms per token,  1045.16 tokens per second)
0.00.735.121 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.122 I llama_perf_context_print:       total time =     131.66 ms /   129 tokens
0.00.735.536 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.079s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.943 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.943 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.944 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.946 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.947 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.369 I llama_model_loader: - type  f32:  194 tensors
0.00.024.370 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.370 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.866 I llm_load_vocab: special tokens cache size = 25
0.00.050.801 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.803 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.805 I llm_load_print_meta: arch             = gptneox
0.00.050.805 I llm_load_print_meta: vocab type       = BPE
0.00.050.806 I llm_load_print_meta: n_vocab          = 50304
0.00.050.806 I llm_load_print_meta: n_merges         = 50009
0.00.050.806 I llm_load_print_meta: vocab_only       = 0
0.00.050.807 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.808 I llm_load_print_meta: n_embd           = 2048
0.00.050.808 I llm_load_print_meta: n_layer          = 24
0.00.050.810 I llm_load_print_meta: n_head           = 16
0.00.050.811 I llm_load_print_meta: n_head_kv        = 16
0.00.050.811 I llm_load_print_meta: n_rot            = 32
0.00.050.811 I llm_load_print_meta: n_swa            = 0
0.00.050.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.813 I llm_load_print_meta: n_gqa            = 1
0.00.050.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.814 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.816 I llm_load_print_meta: n_ff             = 8192
0.00.050.816 I llm_load_print_meta: n_expert         = 0
0.00.050.816 I llm_load_print_meta: n_expert_used    = 0
0.00.050.817 I llm_load_print_meta: causal attn      = 1
0.00.050.817 I llm_load_print_meta: pooling type     = 0
0.00.050.817 I llm_load_print_meta: rope type        = 2
0.00.050.817 I llm_load_print_meta: rope scaling     = linear
0.00.050.817 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.818 I llm_load_print_meta: freq_scale_train = 1
0.00.050.818 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.818 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.818 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.818 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.819 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.819 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.819 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.821 I llm_load_print_meta: model type       = 1.4B
0.00.050.821 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.822 I llm_load_print_meta: model params     = 1.41 B
0.00.050.822 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.822 I llm_load_print_meta: general.name     = 1.4B
0.00.050.823 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.824 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.824 I llm_load_print_meta: max token length = 1024
0.00.052.811 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.811 I llm_load_tensors: offloading output layer to GPU
0.00.052.811 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.821 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.823 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.724 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.724 I llama_new_context_with_model: n_ctx         = 128
0.00.053.725 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.725 I llama_new_context_with_model: n_batch       = 128
0.00.053.725 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.725 I llama_new_context_with_model: flash_attn    = 0
0.00.053.725 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.726 I llama_new_context_with_model: freq_scale    = 1
0.00.053.726 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.727 I ggml_metal_init: allocating
0.00.053.730 I ggml_metal_init: found device: Apple M4
0.00.053.731 I ggml_metal_init: picking default device: Apple M4
0.00.054.320 I ggml_metal_init: using embedded metal library
0.00.056.639 I ggml_metal_init: GPU name:   Apple M4
0.00.056.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.641 I ggml_metal_init: simdgroup reduction   = true
0.00.056.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.641 I ggml_metal_init: has bfloat            = true
0.00.056.641 I ggml_metal_init: use bfloat            = true
0.00.056.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.608 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.478 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.479 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.480 I llama_new_context_with_model: graph nodes  = 967
0.00.068.480 I llama_new_context_with_model: graph splits = 2
0.00.068.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.556 I 
0.00.640.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.616 I perplexity: tokenizing the input ..
0.00.648.522 I perplexity: tokenization took 7.905 ms
0.00.648.525 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.675 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.772.824 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.772.837 I llama_perf_context_print:        load time =     631.66 ms
0.00.772.838 I llama_perf_context_print: prompt eval time =     122.92 ms /   128 tokens (    0.96 ms per token,  1041.30 tokens per second)
0.00.772.839 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.840 I llama_perf_context_print:       total time =     132.28 ms /   129 tokens
0.00.773.212 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.217 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.115 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.116 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.116 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.117 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.121 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.497 I llama_model_loader: - type  f32:  194 tensors
0.00.025.497 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.497 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.708 I llm_load_vocab: special tokens cache size = 25
0.00.052.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.763 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.763 I llm_load_print_meta: arch             = gptneox
0.00.052.764 I llm_load_print_meta: vocab type       = BPE
0.00.052.764 I llm_load_print_meta: n_vocab          = 50304
0.00.052.764 I llm_load_print_meta: n_merges         = 50009
0.00.052.764 I llm_load_print_meta: vocab_only       = 0
0.00.052.765 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.765 I llm_load_print_meta: n_embd           = 2048
0.00.052.765 I llm_load_print_meta: n_layer          = 24
0.00.052.768 I llm_load_print_meta: n_head           = 16
0.00.052.770 I llm_load_print_meta: n_head_kv        = 16
0.00.052.770 I llm_load_print_meta: n_rot            = 32
0.00.052.770 I llm_load_print_meta: n_swa            = 0
0.00.052.771 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.771 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.771 I llm_load_print_meta: n_gqa            = 1
0.00.052.772 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.773 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.773 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.774 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.774 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.774 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.774 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.775 I llm_load_print_meta: n_ff             = 8192
0.00.052.776 I llm_load_print_meta: n_expert         = 0
0.00.052.778 I llm_load_print_meta: n_expert_used    = 0
0.00.052.778 I llm_load_print_meta: causal attn      = 1
0.00.052.778 I llm_load_print_meta: pooling type     = 0
0.00.052.778 I llm_load_print_meta: rope type        = 2
0.00.052.778 I llm_load_print_meta: rope scaling     = linear
0.00.052.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.779 I llm_load_print_meta: freq_scale_train = 1
0.00.052.779 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.779 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.781 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.781 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.782 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.782 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.782 I llm_load_print_meta: model type       = 1.4B
0.00.052.783 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.783 I llm_load_print_meta: model params     = 1.41 B
0.00.052.784 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.784 I llm_load_print_meta: general.name     = 1.4B
0.00.052.784 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.784 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.785 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: max token length = 1024
0.00.054.361 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.361 I llm_load_tensors: offloading output layer to GPU
0.00.054.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.371 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.372 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.223 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.224 I llama_new_context_with_model: n_ctx         = 128
0.00.055.224 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.225 I llama_new_context_with_model: n_batch       = 128
0.00.055.225 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.225 I llama_new_context_with_model: flash_attn    = 0
0.00.055.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.226 I llama_new_context_with_model: freq_scale    = 1
0.00.055.226 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.227 I ggml_metal_init: allocating
0.00.055.233 I ggml_metal_init: found device: Apple M4
0.00.055.236 I ggml_metal_init: picking default device: Apple M4
0.00.055.772 I ggml_metal_init: using embedded metal library
0.00.058.103 I ggml_metal_init: GPU name:   Apple M4
0.00.058.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.105 I ggml_metal_init: simdgroup reduction   = true
0.00.058.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.105 I ggml_metal_init: has bfloat            = true
0.00.058.106 I ggml_metal_init: use bfloat            = true
0.00.058.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.697 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.903 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.786 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.787 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.788 I llama_new_context_with_model: graph nodes  = 967
0.00.069.788 I llama_new_context_with_model: graph splits = 2
0.00.069.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.237 I 
0.00.720.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.294 I perplexity: tokenizing the input ..
0.00.728.755 I perplexity: tokenization took 8.459 ms
0.00.728.759 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.791 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.864.938 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.960 I llama_perf_context_print:        load time =     710.01 ms
0.00.864.961 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.51 tokens per second)
0.00.864.962 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.962 I llama_perf_context_print:       total time =     144.73 ms /   129 tokens
0.00.865.400 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.081s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.892 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.255 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.257 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.257 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.258 I llama_model_loader: - type  f32:  194 tensors
0.00.024.258 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.258 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.701 I llm_load_vocab: special tokens cache size = 25
0.00.050.547 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.550 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.552 I llm_load_print_meta: arch             = gptneox
0.00.050.552 I llm_load_print_meta: vocab type       = BPE
0.00.050.552 I llm_load_print_meta: n_vocab          = 50304
0.00.050.552 I llm_load_print_meta: n_merges         = 50009
0.00.050.553 I llm_load_print_meta: vocab_only       = 0
0.00.050.554 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.555 I llm_load_print_meta: n_embd           = 2048
0.00.050.555 I llm_load_print_meta: n_layer          = 24
0.00.050.558 I llm_load_print_meta: n_head           = 16
0.00.050.559 I llm_load_print_meta: n_head_kv        = 16
0.00.050.560 I llm_load_print_meta: n_rot            = 32
0.00.050.560 I llm_load_print_meta: n_swa            = 0
0.00.050.561 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.561 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.562 I llm_load_print_meta: n_gqa            = 1
0.00.050.563 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.563 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.564 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.565 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.565 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.565 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.565 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.566 I llm_load_print_meta: n_ff             = 8192
0.00.050.566 I llm_load_print_meta: n_expert         = 0
0.00.050.566 I llm_load_print_meta: n_expert_used    = 0
0.00.050.566 I llm_load_print_meta: causal attn      = 1
0.00.050.566 I llm_load_print_meta: pooling type     = 0
0.00.050.567 I llm_load_print_meta: rope type        = 2
0.00.050.567 I llm_load_print_meta: rope scaling     = linear
0.00.050.567 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.567 I llm_load_print_meta: freq_scale_train = 1
0.00.050.568 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.568 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.568 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.568 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.568 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.569 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.570 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.570 I llm_load_print_meta: model type       = 1.4B
0.00.050.570 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.571 I llm_load_print_meta: model params     = 1.41 B
0.00.050.571 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.571 I llm_load_print_meta: general.name     = 1.4B
0.00.050.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.573 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: max token length = 1024
0.00.052.581 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.581 I llm_load_tensors: offloading output layer to GPU
0.00.052.581 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.591 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.593 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.531 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.532 I llama_new_context_with_model: n_ctx         = 128
0.00.053.532 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.532 I llama_new_context_with_model: n_batch       = 128
0.00.053.532 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.533 I llama_new_context_with_model: flash_attn    = 0
0.00.053.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.533 I llama_new_context_with_model: freq_scale    = 1
0.00.053.534 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.534 I ggml_metal_init: allocating
0.00.053.540 I ggml_metal_init: found device: Apple M4
0.00.053.542 I ggml_metal_init: picking default device: Apple M4
0.00.054.137 I ggml_metal_init: using embedded metal library
0.00.056.471 I ggml_metal_init: GPU name:   Apple M4
0.00.056.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.473 I ggml_metal_init: simdgroup reduction   = true
0.00.056.473 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.473 I ggml_metal_init: has bfloat            = true
0.00.056.473 I ggml_metal_init: use bfloat            = true
0.00.056.474 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.133 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.450 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.464 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.367 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.368 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.369 I llama_new_context_with_model: graph nodes  = 967
0.00.068.369 I llama_new_context_with_model: graph splits = 2
0.00.068.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.505 I 
0.00.738.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.560 I perplexity: tokenizing the input ..
0.00.746.269 I perplexity: tokenization took 7.708 ms
0.00.746.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.302 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.882.549 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.882.562 I llama_perf_context_print:        load time =     729.69 ms
0.00.882.563 I llama_perf_context_print: prompt eval time =     134.80 ms /   128 tokens (    1.05 ms per token,   949.55 tokens per second)
0.00.882.564 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.564 I llama_perf_context_print:       total time =     144.06 ms /   129 tokens
0.00.883.119 I ggml_metal_free: deallocating

real	0m0.897s
user	0m0.079s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.404 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.037 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.037 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.401 I llama_model_loader: - type  f32:  194 tensors
0.00.024.401 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.401 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.401 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.942 I llm_load_vocab: special tokens cache size = 25
0.00.051.006 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.009 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.009 I llm_load_print_meta: arch             = gptneox
0.00.051.010 I llm_load_print_meta: vocab type       = BPE
0.00.051.010 I llm_load_print_meta: n_vocab          = 50304
0.00.051.010 I llm_load_print_meta: n_merges         = 50009
0.00.051.010 I llm_load_print_meta: vocab_only       = 0
0.00.051.011 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.011 I llm_load_print_meta: n_embd           = 2048
0.00.051.011 I llm_load_print_meta: n_layer          = 24
0.00.051.014 I llm_load_print_meta: n_head           = 16
0.00.051.015 I llm_load_print_meta: n_head_kv        = 16
0.00.051.015 I llm_load_print_meta: n_rot            = 32
0.00.051.016 I llm_load_print_meta: n_swa            = 0
0.00.051.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.018 I llm_load_print_meta: n_gqa            = 1
0.00.051.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.020 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.022 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.022 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.022 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.022 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.023 I llm_load_print_meta: n_ff             = 8192
0.00.051.023 I llm_load_print_meta: n_expert         = 0
0.00.051.023 I llm_load_print_meta: n_expert_used    = 0
0.00.051.023 I llm_load_print_meta: causal attn      = 1
0.00.051.024 I llm_load_print_meta: pooling type     = 0
0.00.051.024 I llm_load_print_meta: rope type        = 2
0.00.051.024 I llm_load_print_meta: rope scaling     = linear
0.00.051.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.025 I llm_load_print_meta: freq_scale_train = 1
0.00.051.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.026 I llm_load_print_meta: model type       = 1.4B
0.00.051.026 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.028 I llm_load_print_meta: model params     = 1.41 B
0.00.051.028 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.029 I llm_load_print_meta: general.name     = 1.4B
0.00.051.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.033 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.034 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.034 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.035 I llm_load_print_meta: max token length = 1024
0.00.052.902 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.902 I llm_load_tensors: offloading output layer to GPU
0.00.052.902 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.913 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.914 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.846 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.847 I llama_new_context_with_model: n_ctx         = 128
0.00.053.847 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.848 I llama_new_context_with_model: n_batch       = 128
0.00.053.848 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.848 I llama_new_context_with_model: flash_attn    = 0
0.00.053.848 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.849 I llama_new_context_with_model: freq_scale    = 1
0.00.053.849 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.849 I ggml_metal_init: allocating
0.00.053.855 I ggml_metal_init: found device: Apple M4
0.00.053.857 I ggml_metal_init: picking default device: Apple M4
0.00.054.421 I ggml_metal_init: using embedded metal library
0.00.056.748 I ggml_metal_init: GPU name:   Apple M4
0.00.056.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.750 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.750 I ggml_metal_init: simdgroup reduction   = true
0.00.056.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.751 I ggml_metal_init: has bfloat            = true
0.00.056.751 I ggml_metal_init: use bfloat            = true
0.00.056.751 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.752 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.319 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.564 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.567 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.582 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.490 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.490 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.491 I llama_new_context_with_model: graph nodes  = 967
0.00.068.491 I llama_new_context_with_model: graph splits = 2
0.00.068.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.974 I 
0.00.414.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.041 I perplexity: tokenizing the input ..
0.00.422.172 I perplexity: tokenization took 8.129 ms
0.00.422.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.424 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.604 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.617 I llama_perf_context_print:        load time =     404.56 ms
0.00.555.618 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.55 tokens per second)
0.00.555.619 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.619 I llama_perf_context_print:       total time =     141.65 ms /   129 tokens
0.00.556.122 I ggml_metal_free: deallocating

real	0m0.572s
user	0m0.079s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.695 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.602 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.963 I llama_model_loader: - type  f32:  194 tensors
0.00.023.963 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.964 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.964 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.432 I llm_load_vocab: special tokens cache size = 25
0.00.050.652 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.655 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.657 I llm_load_print_meta: arch             = gptneox
0.00.050.658 I llm_load_print_meta: vocab type       = BPE
0.00.050.660 I llm_load_print_meta: n_vocab          = 50304
0.00.050.660 I llm_load_print_meta: n_merges         = 50009
0.00.050.660 I llm_load_print_meta: vocab_only       = 0
0.00.050.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.661 I llm_load_print_meta: n_embd           = 2048
0.00.050.665 I llm_load_print_meta: n_layer          = 24
0.00.050.668 I llm_load_print_meta: n_head           = 16
0.00.050.669 I llm_load_print_meta: n_head_kv        = 16
0.00.050.669 I llm_load_print_meta: n_rot            = 32
0.00.050.669 I llm_load_print_meta: n_swa            = 0
0.00.050.670 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.673 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.673 I llm_load_print_meta: n_gqa            = 1
0.00.050.674 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.675 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.675 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.677 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.677 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.677 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.677 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.678 I llm_load_print_meta: n_ff             = 8192
0.00.050.678 I llm_load_print_meta: n_expert         = 0
0.00.050.679 I llm_load_print_meta: n_expert_used    = 0
0.00.050.681 I llm_load_print_meta: causal attn      = 1
0.00.050.682 I llm_load_print_meta: pooling type     = 0
0.00.050.682 I llm_load_print_meta: rope type        = 2
0.00.050.682 I llm_load_print_meta: rope scaling     = linear
0.00.050.682 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.683 I llm_load_print_meta: freq_scale_train = 1
0.00.050.683 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.683 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.683 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.683 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.683 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.683 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.684 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.684 I llm_load_print_meta: model type       = 1.4B
0.00.050.685 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.686 I llm_load_print_meta: model params     = 1.41 B
0.00.050.686 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.686 I llm_load_print_meta: general.name     = 1.4B
0.00.050.687 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.687 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.687 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.689 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.689 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.689 I llm_load_print_meta: max token length = 1024
0.00.052.625 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.626 I llm_load_tensors: offloading output layer to GPU
0.00.052.626 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.636 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.637 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.547 I llama_new_context_with_model: n_ctx         = 128
0.00.053.547 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.547 I llama_new_context_with_model: n_batch       = 128
0.00.053.547 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.547 I llama_new_context_with_model: flash_attn    = 0
0.00.053.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.548 I llama_new_context_with_model: freq_scale    = 1
0.00.053.548 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.549 I ggml_metal_init: allocating
0.00.053.554 I ggml_metal_init: found device: Apple M4
0.00.053.558 I ggml_metal_init: picking default device: Apple M4
0.00.054.105 I ggml_metal_init: using embedded metal library
0.00.056.395 I ggml_metal_init: GPU name:   Apple M4
0.00.056.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.397 I ggml_metal_init: simdgroup reduction   = true
0.00.056.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.397 I ggml_metal_init: has bfloat            = true
0.00.056.397 I ggml_metal_init: use bfloat            = true
0.00.056.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.063 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.385 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.387 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.400 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.275 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.276 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.276 I llama_new_context_with_model: graph nodes  = 967
0.00.068.277 I llama_new_context_with_model: graph splits = 2
0.00.068.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.243 I 
0.00.490.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.316 I perplexity: tokenizing the input ..
0.00.498.402 I perplexity: tokenization took 8.085 ms
0.00.498.405 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.629.725 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.630.901 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.630.918 I llama_perf_context_print:        load time =     481.54 ms
0.00.630.919 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.41 tokens per second)
0.00.630.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.630.920 I llama_perf_context_print:       total time =     140.68 ms /   129 tokens
0.00.631.472 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.080s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.718 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.620 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.621 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.623 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.624 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.624 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.088 I llama_model_loader: - type  f32:  194 tensors
0.00.024.089 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.089 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.089 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.258 I llm_load_vocab: special tokens cache size = 25
0.00.051.278 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.281 I llm_load_print_meta: arch             = gptneox
0.00.051.281 I llm_load_print_meta: vocab type       = BPE
0.00.051.282 I llm_load_print_meta: n_vocab          = 50304
0.00.051.282 I llm_load_print_meta: n_merges         = 50009
0.00.051.282 I llm_load_print_meta: vocab_only       = 0
0.00.051.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.282 I llm_load_print_meta: n_embd           = 2048
0.00.051.283 I llm_load_print_meta: n_layer          = 24
0.00.051.285 I llm_load_print_meta: n_head           = 16
0.00.051.286 I llm_load_print_meta: n_head_kv        = 16
0.00.051.286 I llm_load_print_meta: n_rot            = 32
0.00.051.286 I llm_load_print_meta: n_swa            = 0
0.00.051.287 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.287 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.287 I llm_load_print_meta: n_gqa            = 1
0.00.051.288 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.289 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.290 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.290 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.290 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.290 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.291 I llm_load_print_meta: n_ff             = 8192
0.00.051.291 I llm_load_print_meta: n_expert         = 0
0.00.051.291 I llm_load_print_meta: n_expert_used    = 0
0.00.051.291 I llm_load_print_meta: causal attn      = 1
0.00.051.292 I llm_load_print_meta: pooling type     = 0
0.00.051.294 I llm_load_print_meta: rope type        = 2
0.00.051.294 I llm_load_print_meta: rope scaling     = linear
0.00.051.294 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.295 I llm_load_print_meta: freq_scale_train = 1
0.00.051.295 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.295 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.295 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.295 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.296 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.296 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.296 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.296 I llm_load_print_meta: model type       = 1.4B
0.00.051.296 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.297 I llm_load_print_meta: model params     = 1.41 B
0.00.051.297 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.298 I llm_load_print_meta: general.name     = 1.4B
0.00.051.298 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.298 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.298 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.299 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.299 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.299 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.299 I llm_load_print_meta: max token length = 1024
0.00.053.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.324 I llm_load_tensors: offloading output layer to GPU
0.00.053.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.335 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.336 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.237 I llama_new_context_with_model: n_ctx         = 128
0.00.054.237 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.238 I llama_new_context_with_model: n_batch       = 128
0.00.054.238 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.238 I llama_new_context_with_model: flash_attn    = 0
0.00.054.238 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.239 I llama_new_context_with_model: freq_scale    = 1
0.00.054.239 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.239 I ggml_metal_init: allocating
0.00.054.245 I ggml_metal_init: found device: Apple M4
0.00.054.248 I ggml_metal_init: picking default device: Apple M4
0.00.054.793 I ggml_metal_init: using embedded metal library
0.00.057.134 I ggml_metal_init: GPU name:   Apple M4
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.136 I ggml_metal_init: simdgroup reduction   = true
0.00.057.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.137 I ggml_metal_init: has bfloat            = true
0.00.057.137 I ggml_metal_init: use bfloat            = true
0.00.057.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.618 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.889 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.891 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.906 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.752 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.752 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.753 I llama_new_context_with_model: graph nodes  = 967
0.00.068.753 I llama_new_context_with_model: graph splits = 2
0.00.068.765 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.637 I 
0.00.557.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.718 I perplexity: tokenizing the input ..
0.00.565.504 I perplexity: tokenization took 7.784 ms
0.00.565.508 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.778 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.701.018 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.701.037 I llama_perf_context_print:        load time =     548.91 ms
0.00.701.039 I llama_perf_context_print: prompt eval time =     134.05 ms /   128 tokens (    1.05 ms per token,   954.90 tokens per second)
0.00.701.040 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.701.041 I llama_perf_context_print:       total time =     143.40 ms /   129 tokens
0.00.701.474 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.080s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.565 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.726 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.726 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.375 I llm_load_vocab: special tokens cache size = 25
0.00.052.461 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.464 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.466 I llm_load_print_meta: arch             = gptneox
0.00.052.467 I llm_load_print_meta: vocab type       = BPE
0.00.052.472 I llm_load_print_meta: n_vocab          = 50304
0.00.052.472 I llm_load_print_meta: n_merges         = 50009
0.00.052.473 I llm_load_print_meta: vocab_only       = 0
0.00.052.473 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.473 I llm_load_print_meta: n_embd           = 2048
0.00.052.473 I llm_load_print_meta: n_layer          = 24
0.00.052.476 I llm_load_print_meta: n_head           = 16
0.00.052.477 I llm_load_print_meta: n_head_kv        = 16
0.00.052.477 I llm_load_print_meta: n_rot            = 32
0.00.052.477 I llm_load_print_meta: n_swa            = 0
0.00.052.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.478 I llm_load_print_meta: n_gqa            = 1
0.00.052.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.482 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.482 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.483 I llm_load_print_meta: n_ff             = 8192
0.00.052.484 I llm_load_print_meta: n_expert         = 0
0.00.052.484 I llm_load_print_meta: n_expert_used    = 0
0.00.052.484 I llm_load_print_meta: causal attn      = 1
0.00.052.484 I llm_load_print_meta: pooling type     = 0
0.00.052.484 I llm_load_print_meta: rope type        = 2
0.00.052.484 I llm_load_print_meta: rope scaling     = linear
0.00.052.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.488 I llm_load_print_meta: freq_scale_train = 1
0.00.052.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.489 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.490 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.490 I llm_load_print_meta: model type       = 1.4B
0.00.052.491 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.491 I llm_load_print_meta: model params     = 1.41 B
0.00.052.492 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.492 I llm_load_print_meta: general.name     = 1.4B
0.00.052.492 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.496 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.496 I llm_load_print_meta: max token length = 1024
0.00.054.526 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.526 I llm_load_tensors: offloading output layer to GPU
0.00.054.526 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.536 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.537 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.494 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.495 I llama_new_context_with_model: n_ctx         = 128
0.00.055.495 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.495 I llama_new_context_with_model: n_batch       = 128
0.00.055.495 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.496 I llama_new_context_with_model: flash_attn    = 0
0.00.055.496 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.496 I llama_new_context_with_model: freq_scale    = 1
0.00.055.497 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.497 I ggml_metal_init: allocating
0.00.055.500 I ggml_metal_init: found device: Apple M4
0.00.055.502 I ggml_metal_init: picking default device: Apple M4
0.00.056.072 I ggml_metal_init: using embedded metal library
0.00.058.356 I ggml_metal_init: GPU name:   Apple M4
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.359 I ggml_metal_init: simdgroup reduction   = true
0.00.058.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.359 I ggml_metal_init: has bfloat            = true
0.00.058.359 I ggml_metal_init: use bfloat            = true
0.00.058.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.130 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.376 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.378 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.393 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.334 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.335 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.336 I llama_new_context_with_model: graph nodes  = 967
0.00.070.336 I llama_new_context_with_model: graph splits = 2
0.00.070.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.912 I 
0.00.643.947 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.961 I perplexity: tokenizing the input ..
0.00.652.153 I perplexity: tokenization took 8.19 ms
0.00.652.161 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.866 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.794.033 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.794.055 I llama_perf_context_print:        load time =     633.34 ms
0.00.794.056 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.17 tokens per second)
0.00.794.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.057 I llama_perf_context_print:       total time =     150.14 ms /   129 tokens
0.00.794.474 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.080s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.628 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.629 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.629 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.632 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.632 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.061 I llama_model_loader: - type  f32:  194 tensors
0.00.024.061 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.538 I llm_load_vocab: special tokens cache size = 25
0.00.050.606 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.610 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.610 I llm_load_print_meta: arch             = gptneox
0.00.050.611 I llm_load_print_meta: vocab type       = BPE
0.00.050.611 I llm_load_print_meta: n_vocab          = 50304
0.00.050.611 I llm_load_print_meta: n_merges         = 50009
0.00.050.613 I llm_load_print_meta: vocab_only       = 0
0.00.050.613 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.613 I llm_load_print_meta: n_embd           = 2048
0.00.050.613 I llm_load_print_meta: n_layer          = 24
0.00.050.616 I llm_load_print_meta: n_head           = 16
0.00.050.616 I llm_load_print_meta: n_head_kv        = 16
0.00.050.617 I llm_load_print_meta: n_rot            = 32
0.00.050.617 I llm_load_print_meta: n_swa            = 0
0.00.050.617 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.617 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.618 I llm_load_print_meta: n_gqa            = 1
0.00.050.619 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.620 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.620 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.621 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.622 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.622 I llm_load_print_meta: n_ff             = 8192
0.00.050.623 I llm_load_print_meta: n_expert         = 0
0.00.050.623 I llm_load_print_meta: n_expert_used    = 0
0.00.050.623 I llm_load_print_meta: causal attn      = 1
0.00.050.624 I llm_load_print_meta: pooling type     = 0
0.00.050.625 I llm_load_print_meta: rope type        = 2
0.00.050.625 I llm_load_print_meta: rope scaling     = linear
0.00.050.625 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.625 I llm_load_print_meta: freq_scale_train = 1
0.00.050.626 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.626 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.626 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.626 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.626 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.626 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.627 I llm_load_print_meta: model type       = 1.4B
0.00.050.628 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.628 I llm_load_print_meta: model params     = 1.41 B
0.00.050.628 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.629 I llm_load_print_meta: general.name     = 1.4B
0.00.050.629 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.634 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.635 I llm_load_print_meta: max token length = 1024
0.00.052.724 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.724 I llm_load_tensors: offloading output layer to GPU
0.00.052.725 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.735 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.736 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.665 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.666 I llama_new_context_with_model: n_ctx         = 128
0.00.053.666 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.666 I llama_new_context_with_model: n_batch       = 128
0.00.053.666 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.666 I llama_new_context_with_model: flash_attn    = 0
0.00.053.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.667 I llama_new_context_with_model: freq_scale    = 1
0.00.053.667 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.668 I ggml_metal_init: allocating
0.00.053.674 I ggml_metal_init: found device: Apple M4
0.00.053.676 I ggml_metal_init: picking default device: Apple M4
0.00.054.254 I ggml_metal_init: using embedded metal library
0.00.057.769 I ggml_metal_init: GPU name:   Apple M4
0.00.057.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.771 I ggml_metal_init: simdgroup reduction   = true
0.00.057.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.772 I ggml_metal_init: has bfloat            = true
0.00.057.772 I ggml_metal_init: use bfloat            = true
0.00.057.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.413 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.698 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.701 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.718 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.610 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.611 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.611 I llama_new_context_with_model: graph nodes  = 967
0.00.070.612 I llama_new_context_with_model: graph splits = 2
0.00.070.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.305.077 I 
0.00.305.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.305.123 I perplexity: tokenizing the input ..
0.00.312.884 I perplexity: tokenization took 7.759 ms
0.00.312.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.452.413 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.453.962 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.453.976 I llama_perf_context_print:        load time =     296.22 ms
0.00.453.977 I llama_perf_context_print: prompt eval time =     139.27 ms /   128 tokens (    1.09 ms per token,   919.08 tokens per second)
0.00.453.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.453.978 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.454.341 I ggml_metal_free: deallocating

real	0m0.468s
user	0m0.078s
sys	0m0.057s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.242 I build: 4368 (7ab08d52) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.641 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.225 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.893 I llama_model_loader: - type  f32:  194 tensors
0.00.054.893 I llama_model_loader: - type  f16:   98 tensors
0.00.083.104 I llm_load_vocab: special tokens cache size = 25
0.00.089.676 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.679 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.679 I llm_load_print_meta: arch             = gptneox
0.00.089.680 I llm_load_print_meta: vocab type       = BPE
0.00.089.680 I llm_load_print_meta: n_vocab          = 50304
0.00.089.680 I llm_load_print_meta: n_merges         = 50009
0.00.089.680 I llm_load_print_meta: vocab_only       = 0
0.00.089.680 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.681 I llm_load_print_meta: n_embd           = 2048
0.00.089.681 I llm_load_print_meta: n_layer          = 24
0.00.089.684 I llm_load_print_meta: n_head           = 16
0.00.089.685 I llm_load_print_meta: n_head_kv        = 16
0.00.089.685 I llm_load_print_meta: n_rot            = 32
0.00.089.685 I llm_load_print_meta: n_swa            = 0
0.00.089.685 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.686 I llm_load_print_meta: n_gqa            = 1
0.00.089.687 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.688 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.689 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.689 I llm_load_print_meta: n_ff             = 8192
0.00.089.689 I llm_load_print_meta: n_expert         = 0
0.00.089.690 I llm_load_print_meta: n_expert_used    = 0
0.00.089.690 I llm_load_print_meta: causal attn      = 1
0.00.089.690 I llm_load_print_meta: pooling type     = 0
0.00.089.690 I llm_load_print_meta: rope type        = 2
0.00.089.690 I llm_load_print_meta: rope scaling     = linear
0.00.089.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.691 I llm_load_print_meta: freq_scale_train = 1
0.00.089.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.693 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.693 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.693 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.694 I llm_load_print_meta: model type       = 1.4B
0.00.089.694 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.695 I llm_load_print_meta: model params     = 1.41 B
0.00.089.695 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.695 I llm_load_print_meta: general.name     = 1.4B
0.00.089.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.696 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.696 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.699 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.700 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.700 I llm_load_print_meta: max token length = 1024
0.00.092.254 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.254 I llm_load_tensors: offloading output layer to GPU
0.00.092.254 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.265 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.266 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.194 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.195 I llama_new_context_with_model: n_ctx         = 128
0.00.093.195 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.196 I llama_new_context_with_model: n_batch       = 128
0.00.093.196 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.196 I llama_new_context_with_model: flash_attn    = 0
0.00.093.196 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.197 I llama_new_context_with_model: freq_scale    = 1
0.00.093.197 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.197 I ggml_metal_init: allocating
0.00.093.201 I ggml_metal_init: found device: Apple M4
0.00.093.203 I ggml_metal_init: picking default device: Apple M4
0.00.093.815 I ggml_metal_init: using embedded metal library
0.00.096.308 I ggml_metal_init: GPU name:   Apple M4
0.00.096.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.310 I ggml_metal_init: simdgroup reduction   = true
0.00.096.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.310 I ggml_metal_init: has bfloat            = true
0.00.096.310 I ggml_metal_init: use bfloat            = true
0.00.096.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.412 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.718 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.719 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.733 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.573 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.574 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.574 I llama_new_context_with_model: graph nodes  = 967
0.00.107.574 I llama_new_context_with_model: graph splits = 2
0.00.107.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.587 I 
0.00.107.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.107.631 I compute_imatrix: tokenizing the input ..
0.00.114.578 I compute_imatrix: tokenization took 6.947 ms
0.00.114.580 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.576.655 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.578.989 I llama_perf_context_print:        load time =    1553.01 ms
0.01.578.991 I llama_perf_context_print: prompt eval time =    1461.44 ms /   128 tokens (   11.42 ms per token,    87.58 tokens per second)
0.01.578.992 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.578.993 I llama_perf_context_print:       total time =    1555.34 ms /   129 tokens
0.01.579.547 I ggml_metal_free: deallocating

real	0m1.790s
user	0m0.166s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4368 (7ab08d52)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13670a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13670a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13670aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13670b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13670ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13670bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13670c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13670cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13670d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13670d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13670daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13670dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13670eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13670f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13670fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1367101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1367168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1367176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1367182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1367199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13671a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13671a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13671abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13671b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13671bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13671c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13671c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13671cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13671d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13671d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13671df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13671e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13671ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13671f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13671f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13671f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1367208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1367216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1367240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1367250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1367260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1367270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1367280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1367290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1367295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13672a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13672a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13672ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13672b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13672b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13672bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13671b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13672bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13672c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13672cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13672d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13672d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13672dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13672e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13672e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13672ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13672f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13672f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13672fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1367301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1367310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1367335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1367343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1367351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1367368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1367376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13673a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13673a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13673a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13673ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13673b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13673b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13673bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13673c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13673c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13673c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13673ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13673d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13673d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13673dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13673e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13673e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13673ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13673eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13673f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13673f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13673fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1367413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1367421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1367438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1367446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1367454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1367479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1367483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1367488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13674a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13674a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13674b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13674b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13674b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13674bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13674c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13674cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13674d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13674d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13674d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13674e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13674e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13674ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13674f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13674f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13674fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1367506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1367560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1367570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1367580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1367590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13675a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13675a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13675ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13675b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13675b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13675bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13675c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13675c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13675cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13675d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13675d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13675db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13675e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13675e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13675eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13675f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13675f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13675fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1367605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1367618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1367626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1367655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1367663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1367674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1367685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136604e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1366052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136605710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136605b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136605ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136606460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1366068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136606d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1366071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136607620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136607a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136608150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136608c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136609420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136609c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13660a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13660aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13660b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13660b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13660c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13660c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13660cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13660d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13660dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13660e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13660e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13660e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13660ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13660f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13660f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13660fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136610100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136610570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136610830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136610ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136611110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136611670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136612a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136613470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136613970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136613e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1366142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136614750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136614bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1366154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136615910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136615d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1366161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136616660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136616ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1366172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136617740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136618010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136618800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136618ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136619140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1366195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136619f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13661a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13661a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13661ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13661b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13661b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13661bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13661bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13661c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13661c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13661cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13661d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13661d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13661deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13661e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13661e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13661eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13661f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13661f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13661fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1366203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1366213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136621920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136621e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1366223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136622910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136622e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1366233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136623e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1366243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1366248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1366258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1366268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1366278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1366288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1366298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136629d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13662a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13662a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13662ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13662afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13662b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13662b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13662bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13662c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13662c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13662cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13662d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13662d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13662d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13662de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13662e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13662e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13662ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13662f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13662f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13662f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13662fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1366307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136630c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1366310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136631580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136631ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136632360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136632800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136632ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136633140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1366335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136633a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136633f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1366343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136634860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136634d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1366351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136635640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136635ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136636420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1366368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136636d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136637200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1366376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136637b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136637fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136638480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136638920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136638dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136639260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136639700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136639ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13663a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13663a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13663a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13663ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13663b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13663b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13663bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13663c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13663c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13663c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13663ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13663d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13663d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13663dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13663e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13663e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13663ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13663eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13663f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13663f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13663fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136640160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136640600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136640aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136640ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136641540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136641fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1366422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1366428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1366434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136644160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136644420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136645040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136645830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136645cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136646610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136646dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136647860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136647db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136648300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1366492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136649840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136649d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13664a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13664a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13664ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13664b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13664b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13664bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13664c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13664c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13664cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13664d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13664d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13664dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13664e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13664e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13664ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13664f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13664f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13664fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136650280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1366507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136650d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136651270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1366517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136651d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136652260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1366527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136652d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1366537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136653cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136654240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136654790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136654ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136655780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136655cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136656770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136656cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136657210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136657760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1337044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1337075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133707ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133708310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133708780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133708bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133709060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1337094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133709940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133709db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13370a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13370af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13370b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13370bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13370c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13370c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13370ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13370d090 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136604ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136605150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1366055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136605a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136605ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136606310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136606780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136606bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136607060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1366074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136607940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136607f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136608f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136609770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136609e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13660a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13660ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13660b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13660bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13660c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13660ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13660d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13660d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13660df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13660e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13660e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13660ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13660f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13660f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13660fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13660fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1366102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1366105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1366112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1366124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1366143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1366159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1366162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136616bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136617900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1366181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136618650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136618ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136618f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1366193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13661a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13661a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13661a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13661ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13661b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13661b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13661bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13661c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13661c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13661c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13661cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13661d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13661d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13661daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13661df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13661e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13661e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13661ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13661f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13661f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13661f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13661fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136621450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1366218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1366221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136622a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136622ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1366237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136623c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1366240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136624520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1366256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136625fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136626430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1366268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1366275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136627a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136627ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136628340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1366287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136629090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136629500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136629970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136629de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13662a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13662a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13662ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13662afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13662b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13662b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13662bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13662c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13662c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13662ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13662ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13662d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13662d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13662dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13662e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13662e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13662e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13662edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13662f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13662f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13662fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13662ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1366303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136630860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136631140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1366315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136632300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136632770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136632be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136633050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1366334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136633930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136633da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136634680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136634af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136634f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1366353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136635840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136636a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1366372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136638030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1366384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136638910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136638d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1366391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136639ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136639f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13663a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13663a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13663ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13663b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13663b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13663b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13663be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13663c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13663c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13663cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13663d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13663d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13663d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13663dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13663e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13663e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13663eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13663ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13663f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13663f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13663fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1366400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136640550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1366409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136640e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1366412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1366434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136643da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136644210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1366453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136645840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1366472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136647750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136647bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136648030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1366484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136648910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1366491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136649660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136649f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13664a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13664a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13664ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13664b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13664b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13664b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13664be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13664c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13664c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13664cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13664d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13664d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13664d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13664dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13664e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13664e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13664eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13664ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13664f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13664f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13664fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1366500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1366509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136650e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1366512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136651b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136651ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1366528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1366531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136653620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136653a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136653f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1366547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1366550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136655530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1366559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136656200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1366568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136656fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1366576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136657bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136658040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136658640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136658c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.293s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4368 (7ab08d52)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e0ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e0d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e0e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e10d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e16c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e17b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e1a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e22000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e26e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e27370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e28360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e2a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e2d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e2e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e30460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e48310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e4f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e60300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e60850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e63010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e64730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e64bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e65510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e68f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e69230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e69840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119f07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119f077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119f07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119f08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119f09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119f09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119f0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119f0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119f0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119f0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119f0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119f0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119f0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119f0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119f0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119f0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119f0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119f0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119f0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119f0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119f10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119f10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119f11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119f123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119f14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119f16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119f17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119f17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119f19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119f1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119f1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119f1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119f1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119f1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119f1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119f1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119f1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119f1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119f1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119f20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119f207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119f20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119f21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119f21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119f21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119f21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119f22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119f226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119f22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119f22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119f23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119f23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119f23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119f24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119f24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119f24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119f25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119f25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119f25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119f26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119f264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119f26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119f26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119f27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119f276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119f27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119f27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119f283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119f28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119f28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119f29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119f295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119f29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119f2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119f2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119f2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119f2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119f2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119f2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119f2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119f2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119f2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119f2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119f2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119f2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119f2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119f2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119f2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119f2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119f2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119f2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119f2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119f304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119f30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119f30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119f311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119f31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119f31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119f31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119f323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119f32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119f32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119f33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119f33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119f339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119f33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119f342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119f34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119f35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119f35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119f358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119f35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119f361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119f36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119f36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119f36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119f37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119f37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119f37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119f380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119f38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119f389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119f38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119f392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119f39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119f39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119f3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119f3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119f3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119f3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119f3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119f3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119f3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119f3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119f3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119f3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119f3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119f3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119f3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119f3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119f3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119f3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119f3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119f3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119f3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119f3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119f40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119f40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119f40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119f41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119f41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119f41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119f42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119f42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119f429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119f42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119f43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119f43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119f43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119f44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119f448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119f44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119f451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119f45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119f45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119f45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119f467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119f46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119f470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119f47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119f47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119f48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119f486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119f48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119f49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119f498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119f49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119f4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119f4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119f4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119f4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119f4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119f4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119f4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119f4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119f4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119f4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119f4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119f4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119f4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119f4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119f4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119f4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119f4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119f4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119f50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119f50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119f50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119f51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119f514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119f51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119f51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119f526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119f52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119f52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119f533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119f53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119f53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119f54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119f545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119f54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119f55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119f55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119f56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119f56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119f57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119f57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119f57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119f582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119f588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119f58ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119f072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119f07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119f07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119f08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119f08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119f0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119f0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119f0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119f10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119f11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119f119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119f11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119f122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119f12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119f13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119f13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119f138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119f14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119f14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119f14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119f15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119f160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119f16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119f17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119f17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119f35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119f35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119f36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119f36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119f370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119f37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119f379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119f37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119f38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119f38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119f39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119f398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119f39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119f3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119f3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119f3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119f3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119f3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119f3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119f3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119f3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119f3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119f3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119f3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119f3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119f3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119f3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119f3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119f3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119f3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119f3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119f3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119f40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119f40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119f410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119f41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119f42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119f429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119f42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119f432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119f43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119f44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119f44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119f448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119f451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119f45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119f46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119f470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119f47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119f48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119f48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119f49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119f498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119f49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119f4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119f4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119f4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119f4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119f4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119f4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119f4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119f4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119f4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119f4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119f4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119f4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119f4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119f4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119f4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119f4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119f4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119f4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119f4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119f50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119f50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119f510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119f51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119f51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119f52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119f526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119f52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119f52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119f53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119f53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119f53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119f54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119f545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119f54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119f54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119f55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119f557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119f56000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119f566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119f56de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119f574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119f57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119f58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119f58690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.942s
user	0m0.246s
sys	0m0.148s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
