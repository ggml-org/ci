### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.39 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.66 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.31 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.36 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.12 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.24 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.84 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.71 sec*proc (29 tests)

Total Test time (real) = 165.72 sec

real	2m45.728s
user	4m39.645s
sys	0m5.611s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.89 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.39 sec*proc (29 tests)

Total Test time (real) =  48.40 sec

real	0m48.415s
user	0m54.289s
sys	0m5.185s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.137 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.754 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.187 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.192 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.195 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.195 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.195 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.196 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.197 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.197 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.198 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.198 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.198 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.201 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.201 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.202 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.202 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.202 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.203 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.203 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.022.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.022.895 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.896 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.022.897 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.022.897 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.022.897 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.022.897 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.022.898 I llama_model_loader: - type  f32:  124 tensors
0.00.022.898 I llama_model_loader: - type  f16:   73 tensors
0.00.022.899 I print_info: file format = GGUF V3 (latest)
0.00.022.899 I print_info: file type   = F16
0.00.022.900 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.025.249 I load: special tokens cache size = 5
0.00.026.457 I load: token to piece cache size = 0.2032 MB
0.00.026.480 I print_info: arch             = bert
0.00.026.481 I print_info: vocab_only       = 0
0.00.026.481 I print_info: n_ctx_train      = 512
0.00.026.482 I print_info: n_embd           = 384
0.00.026.482 I print_info: n_layer          = 12
0.00.026.485 I print_info: n_head           = 12
0.00.026.486 I print_info: n_head_kv        = 12
0.00.026.486 I print_info: n_rot            = 32
0.00.026.486 I print_info: n_swa            = 0
0.00.026.486 I print_info: n_embd_head_k    = 32
0.00.026.486 I print_info: n_embd_head_v    = 32
0.00.026.487 I print_info: n_gqa            = 1
0.00.026.487 I print_info: n_embd_k_gqa     = 384
0.00.026.488 I print_info: n_embd_v_gqa     = 384
0.00.026.488 I print_info: f_norm_eps       = 1.0e-12
0.00.026.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.026.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.026.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.026.492 I print_info: f_logit_scale    = 0.0e+00
0.00.026.492 I print_info: n_ff             = 1536
0.00.026.492 I print_info: n_expert         = 0
0.00.026.493 I print_info: n_expert_used    = 0
0.00.026.493 I print_info: causal attn      = 0
0.00.026.493 I print_info: pooling type     = 2
0.00.026.493 I print_info: rope type        = 2
0.00.026.493 I print_info: rope scaling     = linear
0.00.026.494 I print_info: freq_base_train  = 10000.0
0.00.026.494 I print_info: freq_scale_train = 1
0.00.026.494 I print_info: n_ctx_orig_yarn  = 512
0.00.026.494 I print_info: rope_finetuned   = unknown
0.00.026.494 I print_info: ssm_d_conv       = 0
0.00.026.494 I print_info: ssm_d_inner      = 0
0.00.026.494 I print_info: ssm_d_state      = 0
0.00.026.495 I print_info: ssm_dt_rank      = 0
0.00.026.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.026.496 I print_info: model type       = 33M
0.00.026.496 I print_info: model params     = 33.21 M
0.00.026.496 I print_info: general.name     = Bge Small
0.00.026.497 I print_info: vocab type       = WPM
0.00.026.497 I print_info: n_vocab          = 30522
0.00.026.497 I print_info: n_merges         = 0
0.00.026.497 I print_info: BOS token        = 101 '[CLS]'
0.00.026.498 I print_info: UNK token        = 100 '[UNK]'
0.00.026.498 I print_info: SEP token        = 102 '[SEP]'
0.00.026.498 I print_info: PAD token        = 0 '[PAD]'
0.00.026.498 I print_info: MASK token       = 103 '[MASK]'
0.00.026.498 I print_info: LF token         = 0 '[PAD]'
0.00.026.499 I print_info: max token length = 21
0.00.026.499 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.028.869 I load_tensors: offloading 12 repeating layers to GPU
0.00.028.870 I load_tensors: offloading output layer to GPU
0.00.028.871 I load_tensors: offloaded 13/13 layers to GPU
0.00.028.891 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.892 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.029.140 I llama_init_from_model: n_seq_max     = 1
0.00.029.141 I llama_init_from_model: n_ctx         = 512
0.00.029.141 I llama_init_from_model: n_ctx_per_seq = 512
0.00.029.141 I llama_init_from_model: n_batch       = 2048
0.00.029.142 I llama_init_from_model: n_ubatch      = 2048
0.00.029.142 I llama_init_from_model: flash_attn    = 0
0.00.029.142 I llama_init_from_model: freq_base     = 10000.0
0.00.029.142 I llama_init_from_model: freq_scale    = 1
0.00.029.143 I ggml_metal_init: allocating
0.00.029.147 I ggml_metal_init: found device: Apple M4
0.00.029.151 I ggml_metal_init: picking default device: Apple M4
0.00.029.639 I ggml_metal_init: using embedded metal library
0.00.032.213 I ggml_metal_init: GPU name:   Apple M4
0.00.032.216 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.217 I ggml_metal_init: simdgroup reduction   = true
0.00.032.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.217 I ggml_metal_init: has residency sets    = true
0.00.032.217 I ggml_metal_init: has bfloat            = true
0.00.032.217 I ggml_metal_init: use bfloat            = true
0.00.032.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.042.584 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.228 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.043.230 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.233 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.044.295 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.044.296 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.044.296 I llama_init_from_model: graph nodes  = 429
0.00.044.296 I llama_init_from_model: graph splits = 2
0.00.044.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.044.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.863 I 
0.00.048.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.488 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.847 I llama_perf_context_print:        load time =      31.10 ms
0.00.053.848 I llama_perf_context_print: prompt eval time =       4.24 ms /     9 tokens (    0.47 ms per token,  2121.64 tokens per second)
0.00.053.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.849 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.054.041 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.035s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.094 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.453 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.458 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.459 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.459 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.460 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.461 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.461 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.462 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.463 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.463 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.465 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.466 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.466 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.467 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.467 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.467 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.649 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.241 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.242 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.242 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.243 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.243 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.243 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.244 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.244 I llama_model_loader: - type  f32:  124 tensors
0.00.014.244 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.245 I print_info: file format = GGUF V3 (latest)
0.00.014.246 I print_info: file type   = Q8_0
0.00.014.247 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.647 I load: special tokens cache size = 5
0.00.017.904 I load: token to piece cache size = 0.2032 MB
0.00.017.913 I print_info: arch             = bert
0.00.017.914 I print_info: vocab_only       = 0
0.00.017.914 I print_info: n_ctx_train      = 512
0.00.017.914 I print_info: n_embd           = 384
0.00.017.914 I print_info: n_layer          = 12
0.00.017.918 I print_info: n_head           = 12
0.00.017.918 I print_info: n_head_kv        = 12
0.00.017.919 I print_info: n_rot            = 32
0.00.017.919 I print_info: n_swa            = 0
0.00.017.919 I print_info: n_embd_head_k    = 32
0.00.017.919 I print_info: n_embd_head_v    = 32
0.00.017.920 I print_info: n_gqa            = 1
0.00.017.921 I print_info: n_embd_k_gqa     = 384
0.00.017.921 I print_info: n_embd_v_gqa     = 384
0.00.017.922 I print_info: f_norm_eps       = 1.0e-12
0.00.017.922 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.923 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.923 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.923 I print_info: f_logit_scale    = 0.0e+00
0.00.017.924 I print_info: n_ff             = 1536
0.00.017.924 I print_info: n_expert         = 0
0.00.017.924 I print_info: n_expert_used    = 0
0.00.017.924 I print_info: causal attn      = 0
0.00.017.924 I print_info: pooling type     = 2
0.00.017.925 I print_info: rope type        = 2
0.00.017.925 I print_info: rope scaling     = linear
0.00.017.925 I print_info: freq_base_train  = 10000.0
0.00.017.926 I print_info: freq_scale_train = 1
0.00.017.926 I print_info: n_ctx_orig_yarn  = 512
0.00.017.926 I print_info: rope_finetuned   = unknown
0.00.017.926 I print_info: ssm_d_conv       = 0
0.00.017.926 I print_info: ssm_d_inner      = 0
0.00.017.926 I print_info: ssm_d_state      = 0
0.00.017.927 I print_info: ssm_dt_rank      = 0
0.00.017.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.927 I print_info: model type       = 33M
0.00.017.928 I print_info: model params     = 33.21 M
0.00.017.928 I print_info: general.name     = Bge Small
0.00.017.929 I print_info: vocab type       = WPM
0.00.017.930 I print_info: n_vocab          = 30522
0.00.017.930 I print_info: n_merges         = 0
0.00.017.930 I print_info: BOS token        = 101 '[CLS]'
0.00.017.930 I print_info: UNK token        = 100 '[UNK]'
0.00.017.931 I print_info: SEP token        = 102 '[SEP]'
0.00.017.931 I print_info: PAD token        = 0 '[PAD]'
0.00.017.931 I print_info: MASK token       = 103 '[MASK]'
0.00.017.931 I print_info: LF token         = 0 '[PAD]'
0.00.017.932 I print_info: max token length = 21
0.00.017.932 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.709 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.710 I load_tensors: offloading output layer to GPU
0.00.019.711 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.717 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.718 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.950 I llama_init_from_model: n_seq_max     = 1
0.00.019.951 I llama_init_from_model: n_ctx         = 512
0.00.019.951 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.951 I llama_init_from_model: n_batch       = 2048
0.00.019.951 I llama_init_from_model: n_ubatch      = 2048
0.00.019.952 I llama_init_from_model: flash_attn    = 0
0.00.019.952 I llama_init_from_model: freq_base     = 10000.0
0.00.019.952 I llama_init_from_model: freq_scale    = 1
0.00.019.953 I ggml_metal_init: allocating
0.00.019.956 I ggml_metal_init: found device: Apple M4
0.00.019.959 I ggml_metal_init: picking default device: Apple M4
0.00.020.453 I ggml_metal_init: using embedded metal library
0.00.023.980 I ggml_metal_init: GPU name:   Apple M4
0.00.023.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.983 I ggml_metal_init: simdgroup reduction   = true
0.00.023.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.984 I ggml_metal_init: has residency sets    = true
0.00.023.984 I ggml_metal_init: has bfloat            = true
0.00.023.984 I ggml_metal_init: use bfloat            = true
0.00.023.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.542 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.036.168 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.036.171 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.172 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.037.229 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.037.230 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.037.230 I llama_init_from_model: graph nodes  = 429
0.00.037.230 I llama_init_from_model: graph splits = 2
0.00.037.232 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.279 I 
0.00.041.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.914 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.373 I llama_perf_context_print:        load time =      32.18 ms
0.00.046.374 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2069.92 tokens per second)
0.00.046.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.375 I llama_perf_context_print:       total time =       5.09 ms /    10 tokens
0.00.046.600 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.318 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.182 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.761 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.767 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.768 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.769 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.770 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.770 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.772 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.773 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.774 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.774 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.775 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.778 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.779 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.779 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.019 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.041.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.256 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.256 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.257 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.257 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.257 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.258 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.258 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.046.258 I llama_model_loader: - type  f32:   40 tensors
0.00.046.259 I llama_model_loader: - type  f16:   30 tensors
0.00.046.259 I print_info: file format = GGUF V3 (latest)
0.00.046.260 I print_info: file type   = F16
0.00.046.261 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.050.557 W load: empty token at index 5
0.00.055.566 W load: model vocab missing newline token, using special_pad_id instead
0.00.056.988 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.057.023 I load: special tokens cache size = 5
0.00.320.646 I load: token to piece cache size = 1.5060 MB
0.00.320.675 I print_info: arch             = jina-bert-v2
0.00.320.676 I print_info: vocab_only       = 0
0.00.320.676 I print_info: n_ctx_train      = 8192
0.00.320.677 I print_info: n_embd           = 384
0.00.320.677 I print_info: n_layer          = 4
0.00.320.683 I print_info: n_head           = 12
0.00.320.684 I print_info: n_head_kv        = 12
0.00.320.684 I print_info: n_rot            = 32
0.00.320.686 I print_info: n_swa            = 0
0.00.320.687 I print_info: n_embd_head_k    = 32
0.00.320.687 I print_info: n_embd_head_v    = 32
0.00.320.687 I print_info: n_gqa            = 1
0.00.320.688 I print_info: n_embd_k_gqa     = 384
0.00.320.688 I print_info: n_embd_v_gqa     = 384
0.00.320.689 I print_info: f_norm_eps       = 1.0e-12
0.00.320.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.320.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.320.690 I print_info: f_max_alibi_bias = 8.0e+00
0.00.320.691 I print_info: f_logit_scale    = 0.0e+00
0.00.320.692 I print_info: n_ff             = 1536
0.00.320.692 I print_info: n_expert         = 0
0.00.320.692 I print_info: n_expert_used    = 0
0.00.320.703 I print_info: causal attn      = 0
0.00.320.705 I print_info: pooling type     = -1
0.00.320.705 I print_info: rope type        = -1
0.00.320.706 I print_info: rope scaling     = linear
0.00.320.711 I print_info: freq_base_train  = 10000.0
0.00.320.711 I print_info: freq_scale_train = 1
0.00.320.711 I print_info: n_ctx_orig_yarn  = 8192
0.00.320.712 I print_info: rope_finetuned   = unknown
0.00.320.712 I print_info: ssm_d_conv       = 0
0.00.320.712 I print_info: ssm_d_inner      = 0
0.00.320.712 I print_info: ssm_d_state      = 0
0.00.320.712 I print_info: ssm_dt_rank      = 0
0.00.320.712 I print_info: ssm_dt_b_c_rms   = 0
0.00.320.713 I print_info: model type       = 33M
0.00.320.713 I print_info: model params     = 32.90 M
0.00.320.716 I print_info: general.name     = Jina Bert Implementation
0.00.320.718 I print_info: vocab type       = BPE
0.00.320.718 I print_info: n_vocab          = 61056
0.00.320.718 I print_info: n_merges         = 39382
0.00.320.718 I print_info: BOS token        = 0 '<s>'
0.00.320.719 I print_info: EOS token        = 2 '</s>'
0.00.320.719 I print_info: UNK token        = 3 '<unk>'
0.00.320.719 I print_info: SEP token        = 2 '</s>'
0.00.320.719 I print_info: PAD token        = 1 '<pad>'
0.00.320.720 I print_info: MASK token       = 4 '<mask>'
0.00.320.720 I print_info: EOG token        = 2 '</s>'
0.00.320.720 I print_info: max token length = 45
0.00.320.721 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.323.088 I load_tensors: offloading 4 repeating layers to GPU
0.00.323.089 I load_tensors: offloading output layer to GPU
0.00.323.090 I load_tensors: offloaded 5/5 layers to GPU
0.00.323.117 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.323.118 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.323.675 I llama_init_from_model: n_seq_max     = 1
0.00.323.676 I llama_init_from_model: n_ctx         = 8192
0.00.323.676 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.323.676 I llama_init_from_model: n_batch       = 2048
0.00.323.676 I llama_init_from_model: n_ubatch      = 2048
0.00.323.677 I llama_init_from_model: flash_attn    = 0
0.00.323.677 I llama_init_from_model: freq_base     = 10000.0
0.00.323.677 I llama_init_from_model: freq_scale    = 1
0.00.323.678 I ggml_metal_init: allocating
0.00.323.682 I ggml_metal_init: found device: Apple M4
0.00.323.685 I ggml_metal_init: picking default device: Apple M4
0.00.324.464 I ggml_metal_init: using embedded metal library
0.00.327.451 I ggml_metal_init: GPU name:   Apple M4
0.00.327.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.453 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.453 I ggml_metal_init: simdgroup reduction   = true
0.00.327.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.454 I ggml_metal_init: has residency sets    = true
0.00.327.454 I ggml_metal_init: has bfloat            = true
0.00.327.454 I ggml_metal_init: use bfloat            = true
0.00.327.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.254 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.378 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.380 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.382 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.531 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.533 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.533 I llama_init_from_model: graph nodes  = 154
0.00.347.533 I llama_init_from_model: graph splits = 2
0.00.347.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.355.123 I 
0.00.355.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.355.554 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.355.556 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.355.563 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.355.565 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.355.569 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.355.569 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.120 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.616 I llama_perf_context_print:        load time =     334.93 ms
0.00.359.617 I llama_perf_context_print: prompt eval time =       3.49 ms /    62 tokens (    0.06 ms per token, 17780.33 tokens per second)
0.00.359.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.618 I llama_perf_context_print:       total time =       4.49 ms /    63 tokens
0.00.359.861 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.332s
sys	0m0.055s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.253 I main: llama backend init
0.00.000.259 I main: load the model and apply lora adapter, if any
0.00.062.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.075.246 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.075.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.075.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.075.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.075.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.075.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.075.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.075.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.075.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.075.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.075.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.075.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.075.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.075.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.075.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.075.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.075.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.082.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.084.462 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.091.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.091.372 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.091.373 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.091.373 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.091.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.091.375 I llama_model_loader: - type  f32:  194 tensors
0.00.091.375 I llama_model_loader: - type  f16:   98 tensors
0.00.091.376 I print_info: file format = GGUF V3 (latest)
0.00.091.377 I print_info: file type   = all F32 (guessed)
0.00.091.379 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.099.199 I load: special tokens cache size = 25
0.00.105.235 I load: token to piece cache size = 0.2984 MB
0.00.105.261 I print_info: arch             = gptneox
0.00.105.262 I print_info: vocab_only       = 0
0.00.105.262 I print_info: n_ctx_train      = 2048
0.00.105.262 I print_info: n_embd           = 2048
0.00.105.262 I print_info: n_layer          = 24
0.00.105.267 I print_info: n_head           = 16
0.00.105.268 I print_info: n_head_kv        = 16
0.00.105.268 I print_info: n_rot            = 32
0.00.105.268 I print_info: n_swa            = 0
0.00.105.268 I print_info: n_embd_head_k    = 128
0.00.105.268 I print_info: n_embd_head_v    = 128
0.00.105.269 I print_info: n_gqa            = 1
0.00.105.269 I print_info: n_embd_k_gqa     = 2048
0.00.105.273 I print_info: n_embd_v_gqa     = 2048
0.00.105.273 I print_info: f_norm_eps       = 1.0e-05
0.00.105.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.274 I print_info: f_logit_scale    = 0.0e+00
0.00.105.275 I print_info: n_ff             = 8192
0.00.105.275 I print_info: n_expert         = 0
0.00.105.275 I print_info: n_expert_used    = 0
0.00.105.275 I print_info: causal attn      = 1
0.00.105.275 I print_info: pooling type     = 0
0.00.105.276 I print_info: rope type        = 2
0.00.105.277 I print_info: rope scaling     = linear
0.00.105.277 I print_info: freq_base_train  = 10000.0
0.00.105.277 I print_info: freq_scale_train = 1
0.00.105.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.278 I print_info: rope_finetuned   = unknown
0.00.105.278 I print_info: ssm_d_conv       = 0
0.00.105.278 I print_info: ssm_d_inner      = 0
0.00.105.278 I print_info: ssm_d_state      = 0
0.00.105.278 I print_info: ssm_dt_rank      = 0
0.00.105.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.278 I print_info: model type       = 1.4B
0.00.105.279 I print_info: model params     = 1.41 B
0.00.105.279 I print_info: general.name     = 1.4B
0.00.105.279 I print_info: vocab type       = BPE
0.00.105.279 I print_info: n_vocab          = 50304
0.00.105.280 I print_info: n_merges         = 50009
0.00.105.280 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.280 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.280 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.280 I print_info: LF token         = 187 ''
0.00.105.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.281 I print_info: max token length = 1024
0.00.105.281 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.217.607 I load_tensors: offloading 24 repeating layers to GPU
0.00.217.611 I load_tensors: offloading output layer to GPU
0.00.217.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.217.641 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.217.643 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.218.294 I llama_init_from_model: n_seq_max     = 1
0.00.218.295 I llama_init_from_model: n_ctx         = 2048
0.00.218.295 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.218.295 I llama_init_from_model: n_batch       = 2048
0.00.218.295 I llama_init_from_model: n_ubatch      = 512
0.00.218.296 I llama_init_from_model: flash_attn    = 0
0.00.218.296 I llama_init_from_model: freq_base     = 10000.0
0.00.218.296 I llama_init_from_model: freq_scale    = 1
0.00.218.299 I ggml_metal_init: allocating
0.00.218.426 I ggml_metal_init: found device: Apple M4
0.00.218.432 I ggml_metal_init: picking default device: Apple M4
0.00.219.139 I ggml_metal_init: using embedded metal library
0.00.238.262 I ggml_metal_init: GPU name:   Apple M4
0.00.238.265 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.238.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.238.266 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.238.267 I ggml_metal_init: simdgroup reduction   = true
0.00.238.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.238.267 I ggml_metal_init: has residency sets    = true
0.00.238.267 I ggml_metal_init: has bfloat            = true
0.00.238.267 I ggml_metal_init: use bfloat            = true
0.00.238.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.238.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.315.582 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.343.934 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.343.941 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.343.964 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.347.781 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.347.783 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.347.784 I llama_init_from_model: graph nodes  = 967
0.00.347.784 I llama_init_from_model: graph splits = 2
0.00.347.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.347.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.347.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.049 I main: llama threadpool init, n_threads = 4
0.00.413.087 I 
0.00.413.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.117 I 
0.00.413.303 I sampler seed: 1234
0.00.413.307 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.413.343 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.413.344 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.413.345 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.239.963 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.02.239.964 I llama_perf_context_print:        load time =     349.48 ms
0.02.239.965 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.55 tokens per second)
0.02.239.965 I llama_perf_context_print:        eval time =    1779.86 ms /    63 runs   (   28.25 ms per token,    35.40 tokens per second)
0.02.239.966 I llama_perf_context_print:       total time =    1827.77 ms /    70 tokens
0.02.240.252 I ggml_metal_free: deallocating

real	0m2.628s
user	0m0.120s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.846 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.303 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.673 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.692 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.694 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.695 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.695 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.696 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.696 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.984 I llama_model_loader: - type  f32:  194 tensors
0.00.056.985 I llama_model_loader: - type  f16:   98 tensors
0.00.056.985 I print_info: file format = GGUF V3 (latest)
0.00.056.986 I print_info: file type   = all F32 (guessed)
0.00.056.987 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.817 I load: special tokens cache size = 25
0.00.076.484 I load: token to piece cache size = 0.2984 MB
0.00.076.499 I print_info: arch             = gptneox
0.00.076.500 I print_info: vocab_only       = 0
0.00.076.500 I print_info: n_ctx_train      = 2048
0.00.076.500 I print_info: n_embd           = 2048
0.00.076.500 I print_info: n_layer          = 24
0.00.076.503 I print_info: n_head           = 16
0.00.076.504 I print_info: n_head_kv        = 16
0.00.076.504 I print_info: n_rot            = 32
0.00.076.504 I print_info: n_swa            = 0
0.00.076.505 I print_info: n_embd_head_k    = 128
0.00.076.505 I print_info: n_embd_head_v    = 128
0.00.076.506 I print_info: n_gqa            = 1
0.00.076.506 I print_info: n_embd_k_gqa     = 2048
0.00.076.507 I print_info: n_embd_v_gqa     = 2048
0.00.076.508 I print_info: f_norm_eps       = 1.0e-05
0.00.076.508 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.508 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.508 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.509 I print_info: f_logit_scale    = 0.0e+00
0.00.076.509 I print_info: n_ff             = 8192
0.00.076.509 I print_info: n_expert         = 0
0.00.076.510 I print_info: n_expert_used    = 0
0.00.076.510 I print_info: causal attn      = 1
0.00.076.510 I print_info: pooling type     = 0
0.00.076.510 I print_info: rope type        = 2
0.00.076.510 I print_info: rope scaling     = linear
0.00.076.510 I print_info: freq_base_train  = 10000.0
0.00.076.511 I print_info: freq_scale_train = 1
0.00.076.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.511 I print_info: rope_finetuned   = unknown
0.00.076.511 I print_info: ssm_d_conv       = 0
0.00.076.511 I print_info: ssm_d_inner      = 0
0.00.076.512 I print_info: ssm_d_state      = 0
0.00.076.512 I print_info: ssm_dt_rank      = 0
0.00.076.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.512 I print_info: model type       = 1.4B
0.00.076.512 I print_info: model params     = 1.41 B
0.00.076.513 I print_info: general.name     = 1.4B
0.00.076.513 I print_info: vocab type       = BPE
0.00.076.513 I print_info: n_vocab          = 50304
0.00.076.514 I print_info: n_merges         = 50009
0.00.076.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.514 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.514 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.514 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.515 I print_info: LF token         = 187 ''
0.00.076.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.515 I print_info: max token length = 1024
0.00.076.515 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.415.149 I load_tensors: offloading 24 repeating layers to GPU
0.01.415.153 I load_tensors: offloading output layer to GPU
0.01.415.153 I load_tensors: offloaded 25/25 layers to GPU
0.01.415.181 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.415.183 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.416.249 I llama_init_from_model: n_seq_max     = 1
0.01.416.250 I llama_init_from_model: n_ctx         = 128
0.01.416.250 I llama_init_from_model: n_ctx_per_seq = 128
0.01.416.250 I llama_init_from_model: n_batch       = 128
0.01.416.251 I llama_init_from_model: n_ubatch      = 128
0.01.416.251 I llama_init_from_model: flash_attn    = 0
0.01.416.252 I llama_init_from_model: freq_base     = 10000.0
0.01.416.252 I llama_init_from_model: freq_scale    = 1
0.01.416.252 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.416.256 I ggml_metal_init: allocating
0.01.416.303 I ggml_metal_init: found device: Apple M4
0.01.416.308 I ggml_metal_init: picking default device: Apple M4
0.01.417.220 I ggml_metal_init: using embedded metal library
0.01.421.142 I ggml_metal_init: GPU name:   Apple M4
0.01.421.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.421.145 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.421.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.421.145 I ggml_metal_init: simdgroup reduction   = true
0.01.421.146 I ggml_metal_init: simdgroup matrix mul. = true
0.01.421.146 I ggml_metal_init: has residency sets    = true
0.01.421.146 I ggml_metal_init: has bfloat            = true
0.01.421.146 I ggml_metal_init: use bfloat            = true
0.01.421.147 I ggml_metal_init: hasUnifiedMemory      = true
0.01.421.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.433.675 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.435.359 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.435.361 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.435.388 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.436.983 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.436.984 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.436.985 I llama_init_from_model: graph nodes  = 967
0.01.436.985 I llama_init_from_model: graph splits = 2
0.01.436.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.436.987 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.472.677 I 
0.01.472.719 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.472.724 I perplexity: tokenizing the input ..
0.01.478.080 I perplexity: tokenization took 5.355 ms
0.01.478.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.596.823 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.598.168 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.598.197 I llama_perf_context_print:        load time =    1447.37 ms
0.01.598.198 I llama_perf_context_print: prompt eval time =     118.43 ms /   128 tokens (    0.93 ms per token,  1080.84 tokens per second)
0.01.598.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.598.200 I llama_perf_context_print:       total time =     125.52 ms /   129 tokens
0.01.598.532 I ggml_metal_free: deallocating

real	0m1.819s
user	0m0.098s
sys	0m0.273s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.011.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.046 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.511 I llama_model_loader: - type  f32:  194 tensors
0.00.027.512 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.512 I print_info: file format = GGUF V3 (latest)
0.00.027.513 I print_info: file type   = Q8_0
0.00.027.514 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.593 I load: special tokens cache size = 25
0.00.042.034 I load: token to piece cache size = 0.2984 MB
0.00.042.053 I print_info: arch             = gptneox
0.00.042.054 I print_info: vocab_only       = 0
0.00.042.054 I print_info: n_ctx_train      = 2048
0.00.042.055 I print_info: n_embd           = 2048
0.00.042.055 I print_info: n_layer          = 24
0.00.042.061 I print_info: n_head           = 16
0.00.042.062 I print_info: n_head_kv        = 16
0.00.042.062 I print_info: n_rot            = 32
0.00.042.062 I print_info: n_swa            = 0
0.00.042.062 I print_info: n_embd_head_k    = 128
0.00.042.062 I print_info: n_embd_head_v    = 128
0.00.042.063 I print_info: n_gqa            = 1
0.00.042.064 I print_info: n_embd_k_gqa     = 2048
0.00.042.065 I print_info: n_embd_v_gqa     = 2048
0.00.042.065 I print_info: f_norm_eps       = 1.0e-05
0.00.042.066 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.066 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.066 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.069 I print_info: f_logit_scale    = 0.0e+00
0.00.042.070 I print_info: n_ff             = 8192
0.00.042.070 I print_info: n_expert         = 0
0.00.042.070 I print_info: n_expert_used    = 0
0.00.042.071 I print_info: causal attn      = 1
0.00.042.071 I print_info: pooling type     = 0
0.00.042.071 I print_info: rope type        = 2
0.00.042.076 I print_info: rope scaling     = linear
0.00.042.077 I print_info: freq_base_train  = 10000.0
0.00.042.077 I print_info: freq_scale_train = 1
0.00.042.077 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.078 I print_info: rope_finetuned   = unknown
0.00.042.078 I print_info: ssm_d_conv       = 0
0.00.042.078 I print_info: ssm_d_inner      = 0
0.00.042.081 I print_info: ssm_d_state      = 0
0.00.042.081 I print_info: ssm_dt_rank      = 0
0.00.042.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.081 I print_info: model type       = 1.4B
0.00.042.082 I print_info: model params     = 1.41 B
0.00.042.082 I print_info: general.name     = 1.4B
0.00.042.083 I print_info: vocab type       = BPE
0.00.042.083 I print_info: n_vocab          = 50304
0.00.042.083 I print_info: n_merges         = 50009
0.00.042.083 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.084 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.084 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.084 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.084 I print_info: LF token         = 187 ''
0.00.042.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.085 I print_info: max token length = 1024
0.00.042.085 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.922.658 I load_tensors: offloading 24 repeating layers to GPU
0.00.922.663 I load_tensors: offloading output layer to GPU
0.00.922.665 I load_tensors: offloaded 25/25 layers to GPU
0.00.922.689 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.922.690 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.923.475 I llama_init_from_model: n_seq_max     = 1
0.00.923.476 I llama_init_from_model: n_ctx         = 2048
0.00.923.477 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.923.477 I llama_init_from_model: n_batch       = 2048
0.00.923.477 I llama_init_from_model: n_ubatch      = 512
0.00.923.478 I llama_init_from_model: flash_attn    = 0
0.00.923.478 I llama_init_from_model: freq_base     = 10000.0
0.00.923.479 I llama_init_from_model: freq_scale    = 1
0.00.923.480 I ggml_metal_init: allocating
0.00.923.487 I ggml_metal_init: found device: Apple M4
0.00.923.494 I ggml_metal_init: picking default device: Apple M4
0.00.924.494 I ggml_metal_init: using embedded metal library
0.00.929.790 I ggml_metal_init: GPU name:   Apple M4
0.00.929.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.929.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.929.795 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.929.795 I ggml_metal_init: simdgroup reduction   = true
0.00.929.796 I ggml_metal_init: simdgroup matrix mul. = true
0.00.929.796 I ggml_metal_init: has residency sets    = true
0.00.929.796 I ggml_metal_init: has bfloat            = true
0.00.929.796 I ggml_metal_init: use bfloat            = true
0.00.929.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.929.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.945.823 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.988.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.988.829 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.988.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.993.687 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.993.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.993.689 I llama_init_from_model: graph nodes  = 967
0.00.993.689 I llama_init_from_model: graph splits = 2
0.00.993.694 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.993.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.993.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.050.503 I main: llama threadpool init, n_threads = 4
0.01.050.550 I 
0.01.050.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.050.573 I 
0.01.050.724 I sampler seed: 1234
0.01.050.728 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.050.767 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.050.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.050.770 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.138.456 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.02.138.456 I llama_perf_context_print:        load time =    1038.51 ms
0.02.138.457 I llama_perf_context_print: prompt eval time =      49.33 ms /     7 tokens (    7.05 ms per token,   141.90 tokens per second)
0.02.138.458 I llama_perf_context_print:        eval time =    1035.35 ms /    63 runs   (   16.43 ms per token,    60.85 tokens per second)
0.02.138.459 I llama_perf_context_print:       total time =    1088.68 ms /    70 tokens
0.02.138.703 I ggml_metal_free: deallocating

real	0m2.157s
user	0m0.106s
sys	0m0.252s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.576 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.879 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.602 I llama_model_loader: - type  f32:  194 tensors
0.00.026.602 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.603 I print_info: file format = GGUF V3 (latest)
0.00.026.604 I print_info: file type   = Q8_0
0.00.026.605 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.192 I load: special tokens cache size = 25
0.00.041.854 I load: token to piece cache size = 0.2984 MB
0.00.041.874 I print_info: arch             = gptneox
0.00.041.874 I print_info: vocab_only       = 0
0.00.041.875 I print_info: n_ctx_train      = 2048
0.00.041.875 I print_info: n_embd           = 2048
0.00.041.875 I print_info: n_layer          = 24
0.00.041.879 I print_info: n_head           = 16
0.00.041.880 I print_info: n_head_kv        = 16
0.00.041.880 I print_info: n_rot            = 32
0.00.041.880 I print_info: n_swa            = 0
0.00.041.880 I print_info: n_embd_head_k    = 128
0.00.041.880 I print_info: n_embd_head_v    = 128
0.00.041.881 I print_info: n_gqa            = 1
0.00.041.881 I print_info: n_embd_k_gqa     = 2048
0.00.041.882 I print_info: n_embd_v_gqa     = 2048
0.00.041.883 I print_info: f_norm_eps       = 1.0e-05
0.00.041.883 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.883 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.886 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.886 I print_info: f_logit_scale    = 0.0e+00
0.00.041.886 I print_info: n_ff             = 8192
0.00.041.887 I print_info: n_expert         = 0
0.00.041.887 I print_info: n_expert_used    = 0
0.00.041.887 I print_info: causal attn      = 1
0.00.041.887 I print_info: pooling type     = 0
0.00.041.887 I print_info: rope type        = 2
0.00.041.887 I print_info: rope scaling     = linear
0.00.041.888 I print_info: freq_base_train  = 10000.0
0.00.041.888 I print_info: freq_scale_train = 1
0.00.041.888 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.888 I print_info: rope_finetuned   = unknown
0.00.041.889 I print_info: ssm_d_conv       = 0
0.00.041.889 I print_info: ssm_d_inner      = 0
0.00.041.889 I print_info: ssm_d_state      = 0
0.00.041.889 I print_info: ssm_dt_rank      = 0
0.00.041.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.889 I print_info: model type       = 1.4B
0.00.041.890 I print_info: model params     = 1.41 B
0.00.041.891 I print_info: general.name     = 1.4B
0.00.041.892 I print_info: vocab type       = BPE
0.00.041.892 I print_info: n_vocab          = 50304
0.00.041.892 I print_info: n_merges         = 50009
0.00.041.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.893 I print_info: LF token         = 187 ''
0.00.041.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.893 I print_info: max token length = 1024
0.00.041.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.811.638 I load_tensors: offloading 24 repeating layers to GPU
0.00.811.645 I load_tensors: offloading output layer to GPU
0.00.811.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.811.673 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.811.674 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.812.969 I llama_init_from_model: n_seq_max     = 1
0.00.812.970 I llama_init_from_model: n_ctx         = 128
0.00.812.971 I llama_init_from_model: n_ctx_per_seq = 128
0.00.812.971 I llama_init_from_model: n_batch       = 128
0.00.812.971 I llama_init_from_model: n_ubatch      = 128
0.00.812.971 I llama_init_from_model: flash_attn    = 0
0.00.812.972 I llama_init_from_model: freq_base     = 10000.0
0.00.812.973 I llama_init_from_model: freq_scale    = 1
0.00.812.973 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.812.974 I ggml_metal_init: allocating
0.00.813.038 I ggml_metal_init: found device: Apple M4
0.00.813.046 I ggml_metal_init: picking default device: Apple M4
0.00.814.208 I ggml_metal_init: using embedded metal library
0.00.819.627 I ggml_metal_init: GPU name:   Apple M4
0.00.819.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.819.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.819.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.819.632 I ggml_metal_init: simdgroup reduction   = true
0.00.819.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.819.632 I ggml_metal_init: has residency sets    = true
0.00.819.633 I ggml_metal_init: has bfloat            = true
0.00.819.633 I ggml_metal_init: use bfloat            = true
0.00.819.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.819.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.834.662 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.837.946 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.837.956 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.837.994 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.841.124 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.841.126 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.841.126 I llama_init_from_model: graph nodes  = 967
0.00.841.127 I llama_init_from_model: graph splits = 2
0.00.841.129 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.841.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.867.130 I 
0.00.867.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.867.218 I perplexity: tokenizing the input ..
0.00.873.954 I perplexity: tokenization took 6.733 ms
0.00.873.959 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.011.692 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.013.031 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.013.057 I llama_perf_context_print:        load time =     856.54 ms
0.01.013.058 I llama_perf_context_print: prompt eval time =     136.87 ms /   128 tokens (    1.07 ms per token,   935.21 tokens per second)
0.01.013.058 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.013.059 I llama_perf_context_print:       total time =     145.93 ms /   129 tokens
0.01.013.455 I ggml_metal_free: deallocating

real	0m1.029s
user	0m0.077s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.011.143 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.372 I llama_model_loader: - type  f32:  194 tensors
0.00.028.372 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.372 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.373 I print_info: file format = GGUF V3 (latest)
0.00.028.373 I print_info: file type   = Q4_0
0.00.028.375 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.464 I load: special tokens cache size = 25
0.00.042.775 I load: token to piece cache size = 0.2984 MB
0.00.042.790 I print_info: arch             = gptneox
0.00.042.791 I print_info: vocab_only       = 0
0.00.042.791 I print_info: n_ctx_train      = 2048
0.00.042.791 I print_info: n_embd           = 2048
0.00.042.791 I print_info: n_layer          = 24
0.00.042.796 I print_info: n_head           = 16
0.00.042.797 I print_info: n_head_kv        = 16
0.00.042.797 I print_info: n_rot            = 32
0.00.042.797 I print_info: n_swa            = 0
0.00.042.797 I print_info: n_embd_head_k    = 128
0.00.042.798 I print_info: n_embd_head_v    = 128
0.00.042.798 I print_info: n_gqa            = 1
0.00.042.799 I print_info: n_embd_k_gqa     = 2048
0.00.042.800 I print_info: n_embd_v_gqa     = 2048
0.00.042.801 I print_info: f_norm_eps       = 1.0e-05
0.00.042.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.801 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.802 I print_info: f_logit_scale    = 0.0e+00
0.00.042.802 I print_info: n_ff             = 8192
0.00.042.802 I print_info: n_expert         = 0
0.00.042.803 I print_info: n_expert_used    = 0
0.00.042.803 I print_info: causal attn      = 1
0.00.042.803 I print_info: pooling type     = 0
0.00.042.803 I print_info: rope type        = 2
0.00.042.804 I print_info: rope scaling     = linear
0.00.042.805 I print_info: freq_base_train  = 10000.0
0.00.042.805 I print_info: freq_scale_train = 1
0.00.042.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.805 I print_info: rope_finetuned   = unknown
0.00.042.805 I print_info: ssm_d_conv       = 0
0.00.042.806 I print_info: ssm_d_inner      = 0
0.00.042.807 I print_info: ssm_d_state      = 0
0.00.042.808 I print_info: ssm_dt_rank      = 0
0.00.042.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.808 I print_info: model type       = 1.4B
0.00.042.808 I print_info: model params     = 1.41 B
0.00.042.808 I print_info: general.name     = 1.4B
0.00.042.809 I print_info: vocab type       = BPE
0.00.042.809 I print_info: n_vocab          = 50304
0.00.042.810 I print_info: n_merges         = 50009
0.00.042.810 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.812 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.812 I print_info: LF token         = 187 ''
0.00.042.813 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.813 I print_info: max token length = 1024
0.00.042.813 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.558.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.558.137 I load_tensors: offloading output layer to GPU
0.00.558.138 I load_tensors: offloaded 25/25 layers to GPU
0.00.558.170 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.558.171 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.559.752 I llama_init_from_model: n_seq_max     = 1
0.00.559.755 I llama_init_from_model: n_ctx         = 2048
0.00.559.756 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.559.756 I llama_init_from_model: n_batch       = 2048
0.00.559.757 I llama_init_from_model: n_ubatch      = 512
0.00.559.757 I llama_init_from_model: flash_attn    = 0
0.00.559.760 I llama_init_from_model: freq_base     = 10000.0
0.00.559.761 I llama_init_from_model: freq_scale    = 1
0.00.559.763 I ggml_metal_init: allocating
0.00.559.833 I ggml_metal_init: found device: Apple M4
0.00.559.845 I ggml_metal_init: picking default device: Apple M4
0.00.561.394 I ggml_metal_init: using embedded metal library
0.00.567.945 I ggml_metal_init: GPU name:   Apple M4
0.00.567.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.567.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.567.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.567.953 I ggml_metal_init: simdgroup reduction   = true
0.00.567.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.567.954 I ggml_metal_init: has residency sets    = true
0.00.567.954 I ggml_metal_init: has bfloat            = true
0.00.567.954 I ggml_metal_init: use bfloat            = true
0.00.567.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.567.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.587.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.960 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.640.966 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.640.993 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.297 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.645.299 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.645.300 I llama_init_from_model: graph nodes  = 967
0.00.645.300 I llama_init_from_model: graph splits = 2
0.00.645.305 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.645.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.645.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.553 I main: llama threadpool init, n_threads = 4
0.00.697.596 I 
0.00.697.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.616 I 
0.00.697.725 I sampler seed: 1234
0.00.697.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.742 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.747 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.747 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.384.415 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.01.384.416 I llama_perf_context_print:        load time =     685.67 ms
0.01.384.416 I llama_perf_context_print: prompt eval time =      49.09 ms /     7 tokens (    7.01 ms per token,   142.59 tokens per second)
0.01.384.417 I llama_perf_context_print:        eval time =     634.68 ms /    63 runs   (   10.07 ms per token,    99.26 tokens per second)
0.01.384.418 I llama_perf_context_print:       total time =     687.60 ms /    70 tokens
0.01.384.702 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.109s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.272 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.278 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.280 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.288 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.068 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.882 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.883 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.884 I llama_model_loader: - type  f32:  194 tensors
0.00.025.884 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.885 I print_info: file format = GGUF V3 (latest)
0.00.025.885 I print_info: file type   = Q4_0
0.00.025.886 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.844 I load: special tokens cache size = 25
0.00.040.336 I load: token to piece cache size = 0.2984 MB
0.00.040.354 I print_info: arch             = gptneox
0.00.040.355 I print_info: vocab_only       = 0
0.00.040.355 I print_info: n_ctx_train      = 2048
0.00.040.355 I print_info: n_embd           = 2048
0.00.040.355 I print_info: n_layer          = 24
0.00.040.360 I print_info: n_head           = 16
0.00.040.363 I print_info: n_head_kv        = 16
0.00.040.363 I print_info: n_rot            = 32
0.00.040.363 I print_info: n_swa            = 0
0.00.040.364 I print_info: n_embd_head_k    = 128
0.00.040.364 I print_info: n_embd_head_v    = 128
0.00.040.365 I print_info: n_gqa            = 1
0.00.040.367 I print_info: n_embd_k_gqa     = 2048
0.00.040.367 I print_info: n_embd_v_gqa     = 2048
0.00.040.368 I print_info: f_norm_eps       = 1.0e-05
0.00.040.368 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.368 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.368 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.368 I print_info: f_logit_scale    = 0.0e+00
0.00.040.369 I print_info: n_ff             = 8192
0.00.040.369 I print_info: n_expert         = 0
0.00.040.369 I print_info: n_expert_used    = 0
0.00.040.370 I print_info: causal attn      = 1
0.00.040.370 I print_info: pooling type     = 0
0.00.040.370 I print_info: rope type        = 2
0.00.040.370 I print_info: rope scaling     = linear
0.00.040.370 I print_info: freq_base_train  = 10000.0
0.00.040.371 I print_info: freq_scale_train = 1
0.00.040.371 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.372 I print_info: rope_finetuned   = unknown
0.00.040.372 I print_info: ssm_d_conv       = 0
0.00.040.372 I print_info: ssm_d_inner      = 0
0.00.040.372 I print_info: ssm_d_state      = 0
0.00.040.372 I print_info: ssm_dt_rank      = 0
0.00.040.372 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.372 I print_info: model type       = 1.4B
0.00.040.373 I print_info: model params     = 1.41 B
0.00.040.373 I print_info: general.name     = 1.4B
0.00.040.375 I print_info: vocab type       = BPE
0.00.040.375 I print_info: n_vocab          = 50304
0.00.040.375 I print_info: n_merges         = 50009
0.00.040.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.375 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.375 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.376 I print_info: LF token         = 187 ''
0.00.040.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.376 I print_info: max token length = 1024
0.00.040.380 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.549.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.789 I load_tensors: offloading output layer to GPU
0.00.549.789 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.821 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.549.823 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.551.334 I llama_init_from_model: n_seq_max     = 1
0.00.551.337 I llama_init_from_model: n_ctx         = 128
0.00.551.337 I llama_init_from_model: n_ctx_per_seq = 128
0.00.551.338 I llama_init_from_model: n_batch       = 128
0.00.551.338 I llama_init_from_model: n_ubatch      = 128
0.00.551.338 I llama_init_from_model: flash_attn    = 0
0.00.551.340 I llama_init_from_model: freq_base     = 10000.0
0.00.551.341 I llama_init_from_model: freq_scale    = 1
0.00.551.341 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.551.343 I ggml_metal_init: allocating
0.00.551.407 I ggml_metal_init: found device: Apple M4
0.00.551.421 I ggml_metal_init: picking default device: Apple M4
0.00.552.910 I ggml_metal_init: using embedded metal library
0.00.558.753 I ggml_metal_init: GPU name:   Apple M4
0.00.558.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.766 I ggml_metal_init: simdgroup reduction   = true
0.00.558.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.767 I ggml_metal_init: has residency sets    = true
0.00.558.767 I ggml_metal_init: has bfloat            = true
0.00.558.767 I ggml_metal_init: use bfloat            = true
0.00.558.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.774 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.584.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.584.543 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.584.594 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.587.966 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.587.968 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.587.969 I llama_init_from_model: graph nodes  = 967
0.00.587.969 I llama_init_from_model: graph splits = 2
0.00.587.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.587.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.814 I 
0.00.616.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.911 I perplexity: tokenizing the input ..
0.00.623.900 I perplexity: tokenization took 6.987 ms
0.00.623.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.461 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.756 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.779 I llama_perf_context_print:        load time =     606.78 ms
0.00.761.780 I llama_perf_context_print: prompt eval time =     135.54 ms /   128 tokens (    1.06 ms per token,   944.36 tokens per second)
0.00.761.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.781 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.762.124 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.082s
sys	0m0.122s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.277 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.775 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.147 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.149 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.149 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.150 I llama_model_loader: - type  f32:  194 tensors
0.00.026.150 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.151 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.151 I print_info: file format = GGUF V3 (latest)
0.00.026.152 I print_info: file type   = Q4_1
0.00.026.156 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.928 I load: special tokens cache size = 25
0.00.040.186 I load: token to piece cache size = 0.2984 MB
0.00.040.199 I print_info: arch             = gptneox
0.00.040.200 I print_info: vocab_only       = 0
0.00.040.201 I print_info: n_ctx_train      = 2048
0.00.040.201 I print_info: n_embd           = 2048
0.00.040.201 I print_info: n_layer          = 24
0.00.040.204 I print_info: n_head           = 16
0.00.040.204 I print_info: n_head_kv        = 16
0.00.040.205 I print_info: n_rot            = 32
0.00.040.205 I print_info: n_swa            = 0
0.00.040.205 I print_info: n_embd_head_k    = 128
0.00.040.205 I print_info: n_embd_head_v    = 128
0.00.040.206 I print_info: n_gqa            = 1
0.00.040.207 I print_info: n_embd_k_gqa     = 2048
0.00.040.208 I print_info: n_embd_v_gqa     = 2048
0.00.040.209 I print_info: f_norm_eps       = 1.0e-05
0.00.040.209 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.209 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.211 I print_info: f_logit_scale    = 0.0e+00
0.00.040.212 I print_info: n_ff             = 8192
0.00.040.212 I print_info: n_expert         = 0
0.00.040.212 I print_info: n_expert_used    = 0
0.00.040.212 I print_info: causal attn      = 1
0.00.040.212 I print_info: pooling type     = 0
0.00.040.214 I print_info: rope type        = 2
0.00.040.214 I print_info: rope scaling     = linear
0.00.040.217 I print_info: freq_base_train  = 10000.0
0.00.040.218 I print_info: freq_scale_train = 1
0.00.040.218 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.218 I print_info: rope_finetuned   = unknown
0.00.040.218 I print_info: ssm_d_conv       = 0
0.00.040.219 I print_info: ssm_d_inner      = 0
0.00.040.220 I print_info: ssm_d_state      = 0
0.00.040.220 I print_info: ssm_dt_rank      = 0
0.00.040.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.220 I print_info: model type       = 1.4B
0.00.040.220 I print_info: model params     = 1.41 B
0.00.040.220 I print_info: general.name     = 1.4B
0.00.040.221 I print_info: vocab type       = BPE
0.00.040.221 I print_info: n_vocab          = 50304
0.00.040.221 I print_info: n_merges         = 50009
0.00.040.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: LF token         = 187 ''
0.00.040.224 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.225 I print_info: max token length = 1024
0.00.040.225 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.567.365 I load_tensors: offloading 24 repeating layers to GPU
0.00.567.381 I load_tensors: offloading output layer to GPU
0.00.567.382 I load_tensors: offloaded 25/25 layers to GPU
0.00.567.414 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.567.416 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.569.105 I llama_init_from_model: n_seq_max     = 1
0.00.569.108 I llama_init_from_model: n_ctx         = 2048
0.00.569.109 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.569.109 I llama_init_from_model: n_batch       = 2048
0.00.569.110 I llama_init_from_model: n_ubatch      = 512
0.00.569.111 I llama_init_from_model: flash_attn    = 0
0.00.569.113 I llama_init_from_model: freq_base     = 10000.0
0.00.569.113 I llama_init_from_model: freq_scale    = 1
0.00.569.115 I ggml_metal_init: allocating
0.00.569.188 I ggml_metal_init: found device: Apple M4
0.00.569.202 I ggml_metal_init: picking default device: Apple M4
0.00.570.767 I ggml_metal_init: using embedded metal library
0.00.577.408 I ggml_metal_init: GPU name:   Apple M4
0.00.577.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.577.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.577.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.577.416 I ggml_metal_init: simdgroup reduction   = true
0.00.577.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.577.416 I ggml_metal_init: has residency sets    = true
0.00.577.416 I ggml_metal_init: has bfloat            = true
0.00.577.417 I ggml_metal_init: use bfloat            = true
0.00.577.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.577.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.596.063 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.787 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.651.795 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.651.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.075 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.656.077 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.656.078 I llama_init_from_model: graph nodes  = 967
0.00.656.078 I llama_init_from_model: graph splits = 2
0.00.656.083 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.656.200 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.656.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.632 I main: llama threadpool init, n_threads = 4
0.00.713.679 I 
0.00.713.699 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.700 I 
0.00.713.868 I sampler seed: 1234
0.00.713.873 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.889 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.453.296 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.453.297 I llama_perf_context_print:        load time =     702.63 ms
0.01.453.298 I llama_perf_context_print: prompt eval time =      49.05 ms /     7 tokens (    7.01 ms per token,   142.72 tokens per second)
0.01.453.298 I llama_perf_context_print:        eval time =     687.69 ms /    63 runs   (   10.92 ms per token,    91.61 tokens per second)
0.01.453.300 I llama_perf_context_print:       total time =     740.39 ms /    70 tokens
0.01.453.590 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.110s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.507 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.508 I llama_model_loader: - type  f32:  194 tensors
0.00.024.509 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.510 I print_info: file format = GGUF V3 (latest)
0.00.024.514 I print_info: file type   = Q4_1
0.00.024.516 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.449 I load: special tokens cache size = 25
0.00.038.682 I load: token to piece cache size = 0.2984 MB
0.00.038.695 I print_info: arch             = gptneox
0.00.038.696 I print_info: vocab_only       = 0
0.00.038.696 I print_info: n_ctx_train      = 2048
0.00.038.696 I print_info: n_embd           = 2048
0.00.038.696 I print_info: n_layer          = 24
0.00.038.700 I print_info: n_head           = 16
0.00.038.701 I print_info: n_head_kv        = 16
0.00.038.701 I print_info: n_rot            = 32
0.00.038.701 I print_info: n_swa            = 0
0.00.038.702 I print_info: n_embd_head_k    = 128
0.00.038.702 I print_info: n_embd_head_v    = 128
0.00.038.702 I print_info: n_gqa            = 1
0.00.038.703 I print_info: n_embd_k_gqa     = 2048
0.00.038.704 I print_info: n_embd_v_gqa     = 2048
0.00.038.704 I print_info: f_norm_eps       = 1.0e-05
0.00.038.710 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.710 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.711 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.711 I print_info: f_logit_scale    = 0.0e+00
0.00.038.711 I print_info: n_ff             = 8192
0.00.038.712 I print_info: n_expert         = 0
0.00.038.712 I print_info: n_expert_used    = 0
0.00.038.712 I print_info: causal attn      = 1
0.00.038.712 I print_info: pooling type     = 0
0.00.038.716 I print_info: rope type        = 2
0.00.038.716 I print_info: rope scaling     = linear
0.00.038.716 I print_info: freq_base_train  = 10000.0
0.00.038.717 I print_info: freq_scale_train = 1
0.00.038.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.717 I print_info: rope_finetuned   = unknown
0.00.038.717 I print_info: ssm_d_conv       = 0
0.00.038.717 I print_info: ssm_d_inner      = 0
0.00.038.717 I print_info: ssm_d_state      = 0
0.00.038.717 I print_info: ssm_dt_rank      = 0
0.00.038.719 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.719 I print_info: model type       = 1.4B
0.00.038.719 I print_info: model params     = 1.41 B
0.00.038.720 I print_info: general.name     = 1.4B
0.00.038.720 I print_info: vocab type       = BPE
0.00.038.720 I print_info: n_vocab          = 50304
0.00.038.720 I print_info: n_merges         = 50009
0.00.038.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: LF token         = 187 ''
0.00.038.721 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: max token length = 1024
0.00.038.722 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.558.637 I load_tensors: offloading 24 repeating layers to GPU
0.00.558.654 I load_tensors: offloading output layer to GPU
0.00.558.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.558.690 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.558.692 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.560.388 I llama_init_from_model: n_seq_max     = 1
0.00.560.391 I llama_init_from_model: n_ctx         = 128
0.00.560.391 I llama_init_from_model: n_ctx_per_seq = 128
0.00.560.392 I llama_init_from_model: n_batch       = 128
0.00.560.392 I llama_init_from_model: n_ubatch      = 128
0.00.560.393 I llama_init_from_model: flash_attn    = 0
0.00.560.396 I llama_init_from_model: freq_base     = 10000.0
0.00.560.396 I llama_init_from_model: freq_scale    = 1
0.00.560.399 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.560.402 I ggml_metal_init: allocating
0.00.560.486 I ggml_metal_init: found device: Apple M4
0.00.560.500 I ggml_metal_init: picking default device: Apple M4
0.00.562.183 I ggml_metal_init: using embedded metal library
0.00.568.890 I ggml_metal_init: GPU name:   Apple M4
0.00.568.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.568.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.568.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.568.898 I ggml_metal_init: simdgroup reduction   = true
0.00.568.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.568.899 I ggml_metal_init: has residency sets    = true
0.00.568.899 I ggml_metal_init: has bfloat            = true
0.00.568.899 I ggml_metal_init: use bfloat            = true
0.00.568.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.568.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.587.021 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.590.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.590.683 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.590.718 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.593.959 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.593.961 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.593.962 I llama_init_from_model: graph nodes  = 967
0.00.593.962 I llama_init_from_model: graph splits = 2
0.00.593.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.593.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.275 I 
0.00.622.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.365 I perplexity: tokenizing the input ..
0.00.629.513 I perplexity: tokenization took 7.146 ms
0.00.629.526 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.914 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.766.254 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.766.277 I llama_perf_context_print:        load time =     613.26 ms
0.00.766.278 I llama_perf_context_print: prompt eval time =     134.46 ms /   128 tokens (    1.05 ms per token,   951.96 tokens per second)
0.00.766.278 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.279 I llama_perf_context_print:       total time =     144.01 ms /   129 tokens
0.00.766.639 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.118s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.068 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.774 I llama_model_loader: - type  f32:  194 tensors
0.00.027.774 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.775 I print_info: file format = GGUF V3 (latest)
0.00.027.776 I print_info: file type   = Q5_0
0.00.027.776 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.686 I load: special tokens cache size = 25
0.00.041.654 I load: token to piece cache size = 0.2984 MB
0.00.041.669 I print_info: arch             = gptneox
0.00.041.670 I print_info: vocab_only       = 0
0.00.041.670 I print_info: n_ctx_train      = 2048
0.00.041.670 I print_info: n_embd           = 2048
0.00.041.670 I print_info: n_layer          = 24
0.00.041.673 I print_info: n_head           = 16
0.00.041.674 I print_info: n_head_kv        = 16
0.00.041.674 I print_info: n_rot            = 32
0.00.041.674 I print_info: n_swa            = 0
0.00.041.674 I print_info: n_embd_head_k    = 128
0.00.041.675 I print_info: n_embd_head_v    = 128
0.00.041.675 I print_info: n_gqa            = 1
0.00.041.676 I print_info: n_embd_k_gqa     = 2048
0.00.041.677 I print_info: n_embd_v_gqa     = 2048
0.00.041.678 I print_info: f_norm_eps       = 1.0e-05
0.00.041.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.679 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.679 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.679 I print_info: f_logit_scale    = 0.0e+00
0.00.041.680 I print_info: n_ff             = 8192
0.00.041.680 I print_info: n_expert         = 0
0.00.041.680 I print_info: n_expert_used    = 0
0.00.041.680 I print_info: causal attn      = 1
0.00.041.681 I print_info: pooling type     = 0
0.00.041.682 I print_info: rope type        = 2
0.00.041.683 I print_info: rope scaling     = linear
0.00.041.683 I print_info: freq_base_train  = 10000.0
0.00.041.684 I print_info: freq_scale_train = 1
0.00.041.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.684 I print_info: rope_finetuned   = unknown
0.00.041.684 I print_info: ssm_d_conv       = 0
0.00.041.684 I print_info: ssm_d_inner      = 0
0.00.041.684 I print_info: ssm_d_state      = 0
0.00.041.684 I print_info: ssm_dt_rank      = 0
0.00.041.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.685 I print_info: model type       = 1.4B
0.00.041.685 I print_info: model params     = 1.41 B
0.00.041.686 I print_info: general.name     = 1.4B
0.00.041.686 I print_info: vocab type       = BPE
0.00.041.687 I print_info: n_vocab          = 50304
0.00.041.687 I print_info: n_merges         = 50009
0.00.041.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.687 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.688 I print_info: LF token         = 187 ''
0.00.041.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.688 I print_info: max token length = 1024
0.00.041.688 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.986 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.002 I load_tensors: offloading output layer to GPU
0.00.638.003 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.039 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.638.041 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.639.487 I llama_init_from_model: n_seq_max     = 1
0.00.639.490 I llama_init_from_model: n_ctx         = 2048
0.00.639.491 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.491 I llama_init_from_model: n_batch       = 2048
0.00.639.491 I llama_init_from_model: n_ubatch      = 512
0.00.639.493 I llama_init_from_model: flash_attn    = 0
0.00.639.495 I llama_init_from_model: freq_base     = 10000.0
0.00.639.496 I llama_init_from_model: freq_scale    = 1
0.00.639.499 I ggml_metal_init: allocating
0.00.639.572 I ggml_metal_init: found device: Apple M4
0.00.639.585 I ggml_metal_init: picking default device: Apple M4
0.00.640.983 I ggml_metal_init: using embedded metal library
0.00.647.501 I ggml_metal_init: GPU name:   Apple M4
0.00.647.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.507 I ggml_metal_init: simdgroup reduction   = true
0.00.647.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.508 I ggml_metal_init: has residency sets    = true
0.00.647.508 I ggml_metal_init: has bfloat            = true
0.00.647.508 I ggml_metal_init: use bfloat            = true
0.00.647.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.812 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.363 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.718.369 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.392 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.528 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.530 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.531 I llama_init_from_model: graph nodes  = 967
0.00.722.531 I llama_init_from_model: graph splits = 2
0.00.722.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.122 I main: llama threadpool init, n_threads = 4
0.00.780.170 I 
0.00.780.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.191 I 
0.00.780.339 I sampler seed: 1234
0.00.780.343 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.358 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.358 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.359 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.568.068 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.568.069 I llama_perf_context_print:        load time =     767.69 ms
0.01.568.070 I llama_perf_context_print: prompt eval time =      53.27 ms /     7 tokens (    7.61 ms per token,   131.40 tokens per second)
0.01.568.070 I llama_perf_context_print:        eval time =     731.50 ms /    63 runs   (   11.61 ms per token,    86.12 tokens per second)
0.01.568.071 I llama_perf_context_print:       total time =     788.66 ms /    70 tokens
0.01.568.333 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.107s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.887 I llama_model_loader: - type  f32:  194 tensors
0.00.025.888 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.888 I print_info: file format = GGUF V3 (latest)
0.00.025.889 I print_info: file type   = Q5_0
0.00.025.890 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.760 I load: special tokens cache size = 25
0.00.039.847 I load: token to piece cache size = 0.2984 MB
0.00.039.864 I print_info: arch             = gptneox
0.00.039.865 I print_info: vocab_only       = 0
0.00.039.865 I print_info: n_ctx_train      = 2048
0.00.039.865 I print_info: n_embd           = 2048
0.00.039.865 I print_info: n_layer          = 24
0.00.039.869 I print_info: n_head           = 16
0.00.039.870 I print_info: n_head_kv        = 16
0.00.039.870 I print_info: n_rot            = 32
0.00.039.870 I print_info: n_swa            = 0
0.00.039.871 I print_info: n_embd_head_k    = 128
0.00.039.871 I print_info: n_embd_head_v    = 128
0.00.039.871 I print_info: n_gqa            = 1
0.00.039.872 I print_info: n_embd_k_gqa     = 2048
0.00.039.872 I print_info: n_embd_v_gqa     = 2048
0.00.039.879 I print_info: f_norm_eps       = 1.0e-05
0.00.039.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.879 I print_info: f_logit_scale    = 0.0e+00
0.00.039.880 I print_info: n_ff             = 8192
0.00.039.880 I print_info: n_expert         = 0
0.00.039.880 I print_info: n_expert_used    = 0
0.00.039.880 I print_info: causal attn      = 1
0.00.039.880 I print_info: pooling type     = 0
0.00.039.881 I print_info: rope type        = 2
0.00.039.881 I print_info: rope scaling     = linear
0.00.039.881 I print_info: freq_base_train  = 10000.0
0.00.039.881 I print_info: freq_scale_train = 1
0.00.039.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.882 I print_info: rope_finetuned   = unknown
0.00.039.882 I print_info: ssm_d_conv       = 0
0.00.039.882 I print_info: ssm_d_inner      = 0
0.00.039.884 I print_info: ssm_d_state      = 0
0.00.039.884 I print_info: ssm_dt_rank      = 0
0.00.039.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.884 I print_info: model type       = 1.4B
0.00.039.884 I print_info: model params     = 1.41 B
0.00.039.885 I print_info: general.name     = 1.4B
0.00.039.885 I print_info: vocab type       = BPE
0.00.039.885 I print_info: n_vocab          = 50304
0.00.039.886 I print_info: n_merges         = 50009
0.00.039.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: LF token         = 187 ''
0.00.039.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: max token length = 1024
0.00.039.888 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.984 I load_tensors: offloading output layer to GPU
0.00.609.985 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.019 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.610.021 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.611.739 I llama_init_from_model: n_seq_max     = 1
0.00.611.742 I llama_init_from_model: n_ctx         = 128
0.00.611.742 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.743 I llama_init_from_model: n_batch       = 128
0.00.611.743 I llama_init_from_model: n_ubatch      = 128
0.00.611.743 I llama_init_from_model: flash_attn    = 0
0.00.611.746 I llama_init_from_model: freq_base     = 10000.0
0.00.611.746 I llama_init_from_model: freq_scale    = 1
0.00.611.747 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.752 I ggml_metal_init: allocating
0.00.611.880 I ggml_metal_init: found device: Apple M4
0.00.611.895 I ggml_metal_init: picking default device: Apple M4
0.00.613.497 I ggml_metal_init: using embedded metal library
0.00.620.427 I ggml_metal_init: GPU name:   Apple M4
0.00.620.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.436 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.438 I ggml_metal_init: simdgroup reduction   = true
0.00.620.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.438 I ggml_metal_init: has residency sets    = true
0.00.620.438 I ggml_metal_init: has bfloat            = true
0.00.620.439 I ggml_metal_init: use bfloat            = true
0.00.620.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.847 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.452 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.482 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.642 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.644 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.644 I llama_init_from_model: graph nodes  = 967
0.00.645.644 I llama_init_from_model: graph splits = 2
0.00.645.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.509 I 
0.00.676.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.616 I perplexity: tokenizing the input ..
0.00.683.930 I perplexity: tokenization took 7.309 ms
0.00.683.937 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.487 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.833.836 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.833.865 I llama_perf_context_print:        load time =     666.45 ms
0.00.833.866 I llama_perf_context_print: prompt eval time =     147.65 ms /   128 tokens (    1.15 ms per token,   866.90 tokens per second)
0.00.833.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.867 I llama_perf_context_print:       total time =     157.36 ms /   129 tokens
0.00.834.300 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.080s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.913 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.916 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.917 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.918 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.918 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.920 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.924 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.924 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.348 I llama_model_loader: - type  f32:  194 tensors
0.00.025.349 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.349 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.350 I print_info: file format = GGUF V3 (latest)
0.00.025.350 I print_info: file type   = Q5_1
0.00.025.351 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.077 I load: special tokens cache size = 25
0.00.039.442 I load: token to piece cache size = 0.2984 MB
0.00.039.456 I print_info: arch             = gptneox
0.00.039.457 I print_info: vocab_only       = 0
0.00.039.457 I print_info: n_ctx_train      = 2048
0.00.039.457 I print_info: n_embd           = 2048
0.00.039.457 I print_info: n_layer          = 24
0.00.039.464 I print_info: n_head           = 16
0.00.039.465 I print_info: n_head_kv        = 16
0.00.039.465 I print_info: n_rot            = 32
0.00.039.466 I print_info: n_swa            = 0
0.00.039.467 I print_info: n_embd_head_k    = 128
0.00.039.467 I print_info: n_embd_head_v    = 128
0.00.039.468 I print_info: n_gqa            = 1
0.00.039.469 I print_info: n_embd_k_gqa     = 2048
0.00.039.469 I print_info: n_embd_v_gqa     = 2048
0.00.039.470 I print_info: f_norm_eps       = 1.0e-05
0.00.039.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.470 I print_info: f_logit_scale    = 0.0e+00
0.00.039.471 I print_info: n_ff             = 8192
0.00.039.471 I print_info: n_expert         = 0
0.00.039.471 I print_info: n_expert_used    = 0
0.00.039.471 I print_info: causal attn      = 1
0.00.039.471 I print_info: pooling type     = 0
0.00.039.473 I print_info: rope type        = 2
0.00.039.474 I print_info: rope scaling     = linear
0.00.039.474 I print_info: freq_base_train  = 10000.0
0.00.039.475 I print_info: freq_scale_train = 1
0.00.039.475 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.478 I print_info: rope_finetuned   = unknown
0.00.039.478 I print_info: ssm_d_conv       = 0
0.00.039.479 I print_info: ssm_d_inner      = 0
0.00.039.479 I print_info: ssm_d_state      = 0
0.00.039.479 I print_info: ssm_dt_rank      = 0
0.00.039.479 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.479 I print_info: model type       = 1.4B
0.00.039.479 I print_info: model params     = 1.41 B
0.00.039.480 I print_info: general.name     = 1.4B
0.00.039.480 I print_info: vocab type       = BPE
0.00.039.480 I print_info: n_vocab          = 50304
0.00.039.481 I print_info: n_merges         = 50009
0.00.039.481 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.481 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.481 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.481 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.482 I print_info: LF token         = 187 ''
0.00.039.482 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.482 I print_info: max token length = 1024
0.00.039.483 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.700.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.145 I load_tensors: offloading output layer to GPU
0.00.700.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.180 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.700.182 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.701.879 I llama_init_from_model: n_seq_max     = 1
0.00.701.881 I llama_init_from_model: n_ctx         = 2048
0.00.701.882 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.701.882 I llama_init_from_model: n_batch       = 2048
0.00.701.883 I llama_init_from_model: n_ubatch      = 512
0.00.701.883 I llama_init_from_model: flash_attn    = 0
0.00.701.885 I llama_init_from_model: freq_base     = 10000.0
0.00.701.885 I llama_init_from_model: freq_scale    = 1
0.00.701.886 I ggml_metal_init: allocating
0.00.701.902 I ggml_metal_init: found device: Apple M4
0.00.701.912 I ggml_metal_init: picking default device: Apple M4
0.00.703.211 I ggml_metal_init: using embedded metal library
0.00.709.509 I ggml_metal_init: GPU name:   Apple M4
0.00.709.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.709.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.709.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.709.515 I ggml_metal_init: simdgroup reduction   = true
0.00.709.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.709.516 I ggml_metal_init: has residency sets    = true
0.00.709.516 I ggml_metal_init: has bfloat            = true
0.00.709.516 I ggml_metal_init: use bfloat            = true
0.00.709.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.709.526 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.726.574 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.778.249 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.778.255 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.778.278 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.782.377 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.782.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.782.380 I llama_init_from_model: graph nodes  = 967
0.00.782.380 I llama_init_from_model: graph splits = 2
0.00.782.390 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.782.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.782.520 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.946 I main: llama threadpool init, n_threads = 4
0.00.841.997 I 
0.00.842.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.021 I 
0.00.842.180 I sampler seed: 1234
0.00.842.184 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.200 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.200 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.680.048 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.680.049 I llama_perf_context_print:        load time =     832.36 ms
0.01.680.050 I llama_perf_context_print: prompt eval time =      51.94 ms /     7 tokens (    7.42 ms per token,   134.77 tokens per second)
0.01.680.050 I llama_perf_context_print:        eval time =     783.15 ms /    63 runs   (   12.43 ms per token,    80.44 tokens per second)
0.01.680.051 I llama_perf_context_print:       total time =     838.84 ms /    70 tokens
0.01.680.342 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.107s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.828 I llama_model_loader: - type  f32:  194 tensors
0.00.024.828 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.828 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.829 I print_info: file format = GGUF V3 (latest)
0.00.024.830 I print_info: file type   = Q5_1
0.00.024.831 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.423 I load: special tokens cache size = 25
0.00.039.813 I load: token to piece cache size = 0.2984 MB
0.00.039.832 I print_info: arch             = gptneox
0.00.039.832 I print_info: vocab_only       = 0
0.00.039.833 I print_info: n_ctx_train      = 2048
0.00.039.833 I print_info: n_embd           = 2048
0.00.039.833 I print_info: n_layer          = 24
0.00.039.837 I print_info: n_head           = 16
0.00.039.841 I print_info: n_head_kv        = 16
0.00.039.841 I print_info: n_rot            = 32
0.00.039.841 I print_info: n_swa            = 0
0.00.039.841 I print_info: n_embd_head_k    = 128
0.00.039.841 I print_info: n_embd_head_v    = 128
0.00.039.842 I print_info: n_gqa            = 1
0.00.039.842 I print_info: n_embd_k_gqa     = 2048
0.00.039.843 I print_info: n_embd_v_gqa     = 2048
0.00.039.844 I print_info: f_norm_eps       = 1.0e-05
0.00.039.844 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.844 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.844 I print_info: f_logit_scale    = 0.0e+00
0.00.039.845 I print_info: n_ff             = 8192
0.00.039.845 I print_info: n_expert         = 0
0.00.039.845 I print_info: n_expert_used    = 0
0.00.039.845 I print_info: causal attn      = 1
0.00.039.846 I print_info: pooling type     = 0
0.00.039.846 I print_info: rope type        = 2
0.00.039.846 I print_info: rope scaling     = linear
0.00.039.846 I print_info: freq_base_train  = 10000.0
0.00.039.846 I print_info: freq_scale_train = 1
0.00.039.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.847 I print_info: rope_finetuned   = unknown
0.00.039.847 I print_info: ssm_d_conv       = 0
0.00.039.847 I print_info: ssm_d_inner      = 0
0.00.039.847 I print_info: ssm_d_state      = 0
0.00.039.847 I print_info: ssm_dt_rank      = 0
0.00.039.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.847 I print_info: model type       = 1.4B
0.00.039.848 I print_info: model params     = 1.41 B
0.00.039.848 I print_info: general.name     = 1.4B
0.00.039.848 I print_info: vocab type       = BPE
0.00.039.849 I print_info: n_vocab          = 50304
0.00.039.849 I print_info: n_merges         = 50009
0.00.039.849 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.849 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.849 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: LF token         = 187 ''
0.00.039.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: max token length = 1024
0.00.039.850 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.675.540 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.552 I load_tensors: offloading output layer to GPU
0.00.675.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.586 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.675.587 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.677.207 I llama_init_from_model: n_seq_max     = 1
0.00.677.213 I llama_init_from_model: n_ctx         = 128
0.00.677.214 I llama_init_from_model: n_ctx_per_seq = 128
0.00.677.215 I llama_init_from_model: n_batch       = 128
0.00.677.215 I llama_init_from_model: n_ubatch      = 128
0.00.677.215 I llama_init_from_model: flash_attn    = 0
0.00.677.216 I llama_init_from_model: freq_base     = 10000.0
0.00.677.216 I llama_init_from_model: freq_scale    = 1
0.00.677.217 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.677.219 I ggml_metal_init: allocating
0.00.677.288 I ggml_metal_init: found device: Apple M4
0.00.677.304 I ggml_metal_init: picking default device: Apple M4
0.00.679.030 I ggml_metal_init: using embedded metal library
0.00.685.675 I ggml_metal_init: GPU name:   Apple M4
0.00.685.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.685.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.685.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.685.682 I ggml_metal_init: simdgroup reduction   = true
0.00.685.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.685.682 I ggml_metal_init: has residency sets    = true
0.00.685.683 I ggml_metal_init: has bfloat            = true
0.00.685.683 I ggml_metal_init: use bfloat            = true
0.00.685.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.685.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.703.029 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.706.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.706.599 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.706.626 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.729 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.709.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.709.732 I llama_init_from_model: graph nodes  = 967
0.00.709.732 I llama_init_from_model: graph splits = 2
0.00.709.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.709.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.746 I 
0.00.742.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.846 I perplexity: tokenizing the input ..
0.00.749.763 I perplexity: tokenization took 6.915 ms
0.00.749.769 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.898.565 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.900.050 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.900.075 I llama_perf_context_print:        load time =     733.83 ms
0.00.900.076 I llama_perf_context_print: prompt eval time =     147.90 ms /   128 tokens (    1.16 ms per token,   865.46 tokens per second)
0.00.900.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.077 I llama_perf_context_print:       total time =     157.33 ms /   129 tokens
0.00.900.479 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.079s
sys	0m0.144s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.173 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.716 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.720 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.112 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.114 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.115 I llama_model_loader: - type  f32:  194 tensors
0.00.026.116 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.116 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.116 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.117 I print_info: file format = GGUF V3 (latest)
0.00.026.117 I print_info: file type   = Q2_K - Medium
0.00.026.118 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.910 I load: special tokens cache size = 25
0.00.040.281 I load: token to piece cache size = 0.2984 MB
0.00.040.295 I print_info: arch             = gptneox
0.00.040.296 I print_info: vocab_only       = 0
0.00.040.296 I print_info: n_ctx_train      = 2048
0.00.040.296 I print_info: n_embd           = 2048
0.00.040.296 I print_info: n_layer          = 24
0.00.040.299 I print_info: n_head           = 16
0.00.040.300 I print_info: n_head_kv        = 16
0.00.040.300 I print_info: n_rot            = 32
0.00.040.300 I print_info: n_swa            = 0
0.00.040.300 I print_info: n_embd_head_k    = 128
0.00.040.300 I print_info: n_embd_head_v    = 128
0.00.040.301 I print_info: n_gqa            = 1
0.00.040.302 I print_info: n_embd_k_gqa     = 2048
0.00.040.303 I print_info: n_embd_v_gqa     = 2048
0.00.040.303 I print_info: f_norm_eps       = 1.0e-05
0.00.040.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.304 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.304 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.304 I print_info: f_logit_scale    = 0.0e+00
0.00.040.305 I print_info: n_ff             = 8192
0.00.040.305 I print_info: n_expert         = 0
0.00.040.305 I print_info: n_expert_used    = 0
0.00.040.305 I print_info: causal attn      = 1
0.00.040.307 I print_info: pooling type     = 0
0.00.040.308 I print_info: rope type        = 2
0.00.040.308 I print_info: rope scaling     = linear
0.00.040.309 I print_info: freq_base_train  = 10000.0
0.00.040.309 I print_info: freq_scale_train = 1
0.00.040.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.309 I print_info: rope_finetuned   = unknown
0.00.040.309 I print_info: ssm_d_conv       = 0
0.00.040.309 I print_info: ssm_d_inner      = 0
0.00.040.310 I print_info: ssm_d_state      = 0
0.00.040.310 I print_info: ssm_dt_rank      = 0
0.00.040.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.310 I print_info: model type       = 1.4B
0.00.040.310 I print_info: model params     = 1.41 B
0.00.040.310 I print_info: general.name     = 1.4B
0.00.040.311 I print_info: vocab type       = BPE
0.00.040.311 I print_info: n_vocab          = 50304
0.00.040.311 I print_info: n_merges         = 50009
0.00.040.311 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: LF token         = 187 ''
0.00.040.312 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: max token length = 1024
0.00.040.313 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.374.036 I load_tensors: offloading 24 repeating layers to GPU
0.00.374.053 I load_tensors: offloading output layer to GPU
0.00.374.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.374.086 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.374.088 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.375.650 I llama_init_from_model: n_seq_max     = 1
0.00.375.653 I llama_init_from_model: n_ctx         = 2048
0.00.375.654 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.375.654 I llama_init_from_model: n_batch       = 2048
0.00.375.655 I llama_init_from_model: n_ubatch      = 512
0.00.375.655 I llama_init_from_model: flash_attn    = 0
0.00.375.658 I llama_init_from_model: freq_base     = 10000.0
0.00.375.658 I llama_init_from_model: freq_scale    = 1
0.00.375.661 I ggml_metal_init: allocating
0.00.375.733 I ggml_metal_init: found device: Apple M4
0.00.375.746 I ggml_metal_init: picking default device: Apple M4
0.00.377.323 I ggml_metal_init: using embedded metal library
0.00.383.157 I ggml_metal_init: GPU name:   Apple M4
0.00.383.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.170 I ggml_metal_init: simdgroup reduction   = true
0.00.383.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.171 I ggml_metal_init: has residency sets    = true
0.00.383.171 I ggml_metal_init: has bfloat            = true
0.00.383.171 I ggml_metal_init: use bfloat            = true
0.00.383.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.192 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.625 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.469.631 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.469.653 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.856 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.474.858 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.474.858 I llama_init_from_model: graph nodes  = 967
0.00.474.858 I llama_init_from_model: graph splits = 2
0.00.474.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.474.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.474.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.530.635 I main: llama threadpool init, n_threads = 4
0.00.530.679 I 
0.00.530.696 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.530.697 I 
0.00.530.804 I sampler seed: 1234
0.00.530.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.530.843 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.530.846 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.530.846 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.201.069 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.201.069 I llama_perf_context_print:        load time =     519.76 ms
0.01.201.071 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.63 tokens per second)
0.01.201.072 I llama_perf_context_print:        eval time =     631.83 ms /    63 runs   (   10.03 ms per token,    99.71 tokens per second)
0.01.201.072 I llama_perf_context_print:       total time =     671.13 ms /    70 tokens
0.01.201.295 I ggml_metal_free: deallocating

real	0m1.221s
user	0m0.113s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.117 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.982 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.983 I llama_model_loader: - type  f32:  194 tensors
0.00.025.983 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.983 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.983 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.984 I print_info: file format = GGUF V3 (latest)
0.00.025.985 I print_info: file type   = Q2_K - Medium
0.00.025.986 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.382 I load: special tokens cache size = 25
0.00.040.874 I load: token to piece cache size = 0.2984 MB
0.00.040.892 I print_info: arch             = gptneox
0.00.040.893 I print_info: vocab_only       = 0
0.00.040.893 I print_info: n_ctx_train      = 2048
0.00.040.893 I print_info: n_embd           = 2048
0.00.040.894 I print_info: n_layer          = 24
0.00.040.897 I print_info: n_head           = 16
0.00.040.898 I print_info: n_head_kv        = 16
0.00.040.898 I print_info: n_rot            = 32
0.00.040.898 I print_info: n_swa            = 0
0.00.040.899 I print_info: n_embd_head_k    = 128
0.00.040.901 I print_info: n_embd_head_v    = 128
0.00.040.901 I print_info: n_gqa            = 1
0.00.040.902 I print_info: n_embd_k_gqa     = 2048
0.00.040.902 I print_info: n_embd_v_gqa     = 2048
0.00.040.903 I print_info: f_norm_eps       = 1.0e-05
0.00.040.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.904 I print_info: f_logit_scale    = 0.0e+00
0.00.040.904 I print_info: n_ff             = 8192
0.00.040.904 I print_info: n_expert         = 0
0.00.040.904 I print_info: n_expert_used    = 0
0.00.040.905 I print_info: causal attn      = 1
0.00.040.905 I print_info: pooling type     = 0
0.00.040.905 I print_info: rope type        = 2
0.00.040.905 I print_info: rope scaling     = linear
0.00.040.905 I print_info: freq_base_train  = 10000.0
0.00.040.906 I print_info: freq_scale_train = 1
0.00.040.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.906 I print_info: rope_finetuned   = unknown
0.00.040.906 I print_info: ssm_d_conv       = 0
0.00.040.906 I print_info: ssm_d_inner      = 0
0.00.040.906 I print_info: ssm_d_state      = 0
0.00.040.906 I print_info: ssm_dt_rank      = 0
0.00.040.907 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.907 I print_info: model type       = 1.4B
0.00.040.907 I print_info: model params     = 1.41 B
0.00.040.907 I print_info: general.name     = 1.4B
0.00.040.908 I print_info: vocab type       = BPE
0.00.040.908 I print_info: n_vocab          = 50304
0.00.040.908 I print_info: n_merges         = 50009
0.00.040.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.909 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.909 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: LF token         = 187 ''
0.00.040.910 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: max token length = 1024
0.00.040.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.363.974 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.982 I load_tensors: offloading output layer to GPU
0.00.363.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.364.000 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.364.001 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.364.953 I llama_init_from_model: n_seq_max     = 1
0.00.364.958 I llama_init_from_model: n_ctx         = 128
0.00.364.959 I llama_init_from_model: n_ctx_per_seq = 128
0.00.364.959 I llama_init_from_model: n_batch       = 128
0.00.364.959 I llama_init_from_model: n_ubatch      = 128
0.00.364.960 I llama_init_from_model: flash_attn    = 0
0.00.364.961 I llama_init_from_model: freq_base     = 10000.0
0.00.364.962 I llama_init_from_model: freq_scale    = 1
0.00.364.962 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.964 I ggml_metal_init: allocating
0.00.365.001 I ggml_metal_init: found device: Apple M4
0.00.365.010 I ggml_metal_init: picking default device: Apple M4
0.00.365.967 I ggml_metal_init: using embedded metal library
0.00.370.403 I ggml_metal_init: GPU name:   Apple M4
0.00.370.414 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.415 I ggml_metal_init: simdgroup reduction   = true
0.00.370.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.416 I ggml_metal_init: has residency sets    = true
0.00.370.416 I ggml_metal_init: has bfloat            = true
0.00.370.416 I ggml_metal_init: use bfloat            = true
0.00.370.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.388.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.390.579 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.390.584 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.390.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.235 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.392.237 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.392.237 I llama_init_from_model: graph nodes  = 967
0.00.392.237 I llama_init_from_model: graph splits = 2
0.00.392.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.392.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.082 I 
0.00.416.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.125 I perplexity: tokenizing the input ..
0.00.419.992 I perplexity: tokenization took 3.866 ms
0.00.419.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.550.898 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.183 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.206 I llama_perf_context_print:        load time =     405.96 ms
0.00.552.208 I llama_perf_context_print: prompt eval time =     130.67 ms /   128 tokens (    1.02 ms per token,   979.57 tokens per second)
0.00.552.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.212 I llama_perf_context_print:       total time =     136.12 ms /   129 tokens
0.00.552.574 I ggml_metal_free: deallocating

real	0m0.568s
user	0m0.073s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.617 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.305 I llama_model_loader: - type  f32:  194 tensors
0.00.025.305 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.305 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.306 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.306 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.307 I print_info: file format = GGUF V3 (latest)
0.00.025.312 I print_info: file type   = Q3_K - Medium
0.00.025.313 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.860 I load: special tokens cache size = 25
0.00.040.386 I load: token to piece cache size = 0.2984 MB
0.00.040.406 I print_info: arch             = gptneox
0.00.040.407 I print_info: vocab_only       = 0
0.00.040.407 I print_info: n_ctx_train      = 2048
0.00.040.407 I print_info: n_embd           = 2048
0.00.040.407 I print_info: n_layer          = 24
0.00.040.411 I print_info: n_head           = 16
0.00.040.412 I print_info: n_head_kv        = 16
0.00.040.412 I print_info: n_rot            = 32
0.00.040.412 I print_info: n_swa            = 0
0.00.040.412 I print_info: n_embd_head_k    = 128
0.00.040.412 I print_info: n_embd_head_v    = 128
0.00.040.413 I print_info: n_gqa            = 1
0.00.040.414 I print_info: n_embd_k_gqa     = 2048
0.00.040.414 I print_info: n_embd_v_gqa     = 2048
0.00.040.415 I print_info: f_norm_eps       = 1.0e-05
0.00.040.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.419 I print_info: f_logit_scale    = 0.0e+00
0.00.040.420 I print_info: n_ff             = 8192
0.00.040.420 I print_info: n_expert         = 0
0.00.040.420 I print_info: n_expert_used    = 0
0.00.040.420 I print_info: causal attn      = 1
0.00.040.420 I print_info: pooling type     = 0
0.00.040.420 I print_info: rope type        = 2
0.00.040.421 I print_info: rope scaling     = linear
0.00.040.421 I print_info: freq_base_train  = 10000.0
0.00.040.421 I print_info: freq_scale_train = 1
0.00.040.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.422 I print_info: rope_finetuned   = unknown
0.00.040.422 I print_info: ssm_d_conv       = 0
0.00.040.423 I print_info: ssm_d_inner      = 0
0.00.040.423 I print_info: ssm_d_state      = 0
0.00.040.424 I print_info: ssm_dt_rank      = 0
0.00.040.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.424 I print_info: model type       = 1.4B
0.00.040.424 I print_info: model params     = 1.41 B
0.00.040.424 I print_info: general.name     = 1.4B
0.00.040.425 I print_info: vocab type       = BPE
0.00.040.425 I print_info: n_vocab          = 50304
0.00.040.425 I print_info: n_merges         = 50009
0.00.040.426 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: LF token         = 187 ''
0.00.040.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.428 I print_info: max token length = 1024
0.00.040.429 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.421.405 I load_tensors: offloading 24 repeating layers to GPU
0.00.421.413 I load_tensors: offloading output layer to GPU
0.00.421.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.421.433 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.421.434 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.422.366 I llama_init_from_model: n_seq_max     = 1
0.00.422.371 I llama_init_from_model: n_ctx         = 2048
0.00.422.371 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.422.372 I llama_init_from_model: n_batch       = 2048
0.00.422.372 I llama_init_from_model: n_ubatch      = 512
0.00.422.372 I llama_init_from_model: flash_attn    = 0
0.00.422.374 I llama_init_from_model: freq_base     = 10000.0
0.00.422.374 I llama_init_from_model: freq_scale    = 1
0.00.422.375 I ggml_metal_init: allocating
0.00.422.420 I ggml_metal_init: found device: Apple M4
0.00.422.432 I ggml_metal_init: picking default device: Apple M4
0.00.423.402 I ggml_metal_init: using embedded metal library
0.00.427.774 I ggml_metal_init: GPU name:   Apple M4
0.00.427.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.427.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.427.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.427.785 I ggml_metal_init: simdgroup reduction   = true
0.00.427.785 I ggml_metal_init: simdgroup matrix mul. = true
0.00.427.785 I ggml_metal_init: has residency sets    = true
0.00.427.786 I ggml_metal_init: has bfloat            = true
0.00.427.786 I ggml_metal_init: use bfloat            = true
0.00.427.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.427.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.443.935 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.476.039 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.476.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.481.878 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.481.880 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.481.880 I llama_init_from_model: graph nodes  = 967
0.00.481.881 I llama_init_from_model: graph splits = 2
0.00.481.888 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.482.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.482.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.669 I main: llama threadpool init, n_threads = 4
0.00.538.712 I 
0.00.538.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.732 I 
0.00.538.894 I sampler seed: 1234
0.00.538.898 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.928 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.928 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.929 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.287.534 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.287.534 I llama_perf_context_print:        load time =     528.88 ms
0.01.287.535 I llama_perf_context_print: prompt eval time =      49.89 ms /     7 tokens (    7.13 ms per token,   140.30 tokens per second)
0.01.287.539 I llama_perf_context_print:        eval time =     695.84 ms /    63 runs   (   11.05 ms per token,    90.54 tokens per second)
0.01.287.539 I llama_perf_context_print:       total time =     749.60 ms /    70 tokens
0.01.287.741 I ggml_metal_free: deallocating

real	0m1.305s
user	0m0.107s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.029.301 I llama_model_loader: - type  f32:  194 tensors
0.00.029.301 I llama_model_loader: - type q3_K:   25 tensors
0.00.029.301 I llama_model_loader: - type q4_K:   71 tensors
0.00.029.302 I llama_model_loader: - type q5_K:    1 tensors
0.00.029.302 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.303 I print_info: file format = GGUF V3 (latest)
0.00.029.303 I print_info: file type   = Q3_K - Medium
0.00.029.304 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.037.676 I load: special tokens cache size = 25
0.00.044.008 I load: token to piece cache size = 0.2984 MB
0.00.044.031 I print_info: arch             = gptneox
0.00.044.032 I print_info: vocab_only       = 0
0.00.044.032 I print_info: n_ctx_train      = 2048
0.00.044.032 I print_info: n_embd           = 2048
0.00.044.032 I print_info: n_layer          = 24
0.00.044.036 I print_info: n_head           = 16
0.00.044.037 I print_info: n_head_kv        = 16
0.00.044.037 I print_info: n_rot            = 32
0.00.044.037 I print_info: n_swa            = 0
0.00.044.037 I print_info: n_embd_head_k    = 128
0.00.044.037 I print_info: n_embd_head_v    = 128
0.00.044.038 I print_info: n_gqa            = 1
0.00.044.038 I print_info: n_embd_k_gqa     = 2048
0.00.044.039 I print_info: n_embd_v_gqa     = 2048
0.00.044.040 I print_info: f_norm_eps       = 1.0e-05
0.00.044.040 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.040 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.040 I print_info: f_logit_scale    = 0.0e+00
0.00.044.041 I print_info: n_ff             = 8192
0.00.044.041 I print_info: n_expert         = 0
0.00.044.041 I print_info: n_expert_used    = 0
0.00.044.041 I print_info: causal attn      = 1
0.00.044.041 I print_info: pooling type     = 0
0.00.044.048 I print_info: rope type        = 2
0.00.044.050 I print_info: rope scaling     = linear
0.00.044.050 I print_info: freq_base_train  = 10000.0
0.00.044.050 I print_info: freq_scale_train = 1
0.00.044.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.051 I print_info: rope_finetuned   = unknown
0.00.044.052 I print_info: ssm_d_conv       = 0
0.00.044.052 I print_info: ssm_d_inner      = 0
0.00.044.052 I print_info: ssm_d_state      = 0
0.00.044.052 I print_info: ssm_dt_rank      = 0
0.00.044.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.052 I print_info: model type       = 1.4B
0.00.044.053 I print_info: model params     = 1.41 B
0.00.044.053 I print_info: general.name     = 1.4B
0.00.044.053 I print_info: vocab type       = BPE
0.00.044.054 I print_info: n_vocab          = 50304
0.00.044.054 I print_info: n_merges         = 50009
0.00.044.054 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.054 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.054 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.054 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.055 I print_info: LF token         = 187 ''
0.00.044.055 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.055 I print_info: max token length = 1024
0.00.044.055 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.426.492 I load_tensors: offloading 24 repeating layers to GPU
0.00.426.511 I load_tensors: offloading output layer to GPU
0.00.426.512 I load_tensors: offloaded 25/25 layers to GPU
0.00.426.546 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.426.547 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.428.163 I llama_init_from_model: n_seq_max     = 1
0.00.428.171 I llama_init_from_model: n_ctx         = 128
0.00.428.172 I llama_init_from_model: n_ctx_per_seq = 128
0.00.428.172 I llama_init_from_model: n_batch       = 128
0.00.428.172 I llama_init_from_model: n_ubatch      = 128
0.00.428.173 I llama_init_from_model: flash_attn    = 0
0.00.428.174 I llama_init_from_model: freq_base     = 10000.0
0.00.428.175 I llama_init_from_model: freq_scale    = 1
0.00.428.176 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.428.178 I ggml_metal_init: allocating
0.00.428.253 I ggml_metal_init: found device: Apple M4
0.00.428.267 I ggml_metal_init: picking default device: Apple M4
0.00.429.788 I ggml_metal_init: using embedded metal library
0.00.435.150 I ggml_metal_init: GPU name:   Apple M4
0.00.435.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.435.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.435.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.435.170 I ggml_metal_init: simdgroup reduction   = true
0.00.435.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.435.171 I ggml_metal_init: has residency sets    = true
0.00.435.171 I ggml_metal_init: has bfloat            = true
0.00.435.172 I ggml_metal_init: use bfloat            = true
0.00.435.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.435.177 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.455.680 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.459.285 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.459.289 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.459.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.462.654 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.462.656 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.462.656 I llama_init_from_model: graph nodes  = 967
0.00.462.657 I llama_init_from_model: graph splits = 2
0.00.462.660 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.462.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.966 I 
0.00.492.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.069 I perplexity: tokenizing the input ..
0.00.498.521 I perplexity: tokenization took 6.45 ms
0.00.498.529 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.642.348 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.643.690 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.643.710 I llama_perf_context_print:        load time =     483.10 ms
0.00.643.711 I llama_perf_context_print: prompt eval time =     143.44 ms /   128 tokens (    1.12 ms per token,   892.39 tokens per second)
0.00.643.712 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.643.712 I llama_perf_context_print:       total time =     151.75 ms /   129 tokens
0.00.644.064 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.080s
sys	0m0.099s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.962 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.963 I llama_model_loader: - type  f32:  194 tensors
0.00.024.963 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.963 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.964 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.964 I print_info: file format = GGUF V3 (latest)
0.00.024.965 I print_info: file type   = Q4_K - Medium
0.00.024.965 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.788 I load: special tokens cache size = 25
0.00.039.089 I load: token to piece cache size = 0.2984 MB
0.00.039.103 I print_info: arch             = gptneox
0.00.039.104 I print_info: vocab_only       = 0
0.00.039.104 I print_info: n_ctx_train      = 2048
0.00.039.105 I print_info: n_embd           = 2048
0.00.039.105 I print_info: n_layer          = 24
0.00.039.108 I print_info: n_head           = 16
0.00.039.108 I print_info: n_head_kv        = 16
0.00.039.109 I print_info: n_rot            = 32
0.00.039.109 I print_info: n_swa            = 0
0.00.039.109 I print_info: n_embd_head_k    = 128
0.00.039.109 I print_info: n_embd_head_v    = 128
0.00.039.110 I print_info: n_gqa            = 1
0.00.039.110 I print_info: n_embd_k_gqa     = 2048
0.00.039.111 I print_info: n_embd_v_gqa     = 2048
0.00.039.113 I print_info: f_norm_eps       = 1.0e-05
0.00.039.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.114 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.114 I print_info: f_logit_scale    = 0.0e+00
0.00.039.115 I print_info: n_ff             = 8192
0.00.039.115 I print_info: n_expert         = 0
0.00.039.115 I print_info: n_expert_used    = 0
0.00.039.115 I print_info: causal attn      = 1
0.00.039.115 I print_info: pooling type     = 0
0.00.039.116 I print_info: rope type        = 2
0.00.039.116 I print_info: rope scaling     = linear
0.00.039.116 I print_info: freq_base_train  = 10000.0
0.00.039.120 I print_info: freq_scale_train = 1
0.00.039.121 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.121 I print_info: rope_finetuned   = unknown
0.00.039.121 I print_info: ssm_d_conv       = 0
0.00.039.121 I print_info: ssm_d_inner      = 0
0.00.039.121 I print_info: ssm_d_state      = 0
0.00.039.121 I print_info: ssm_dt_rank      = 0
0.00.039.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.122 I print_info: model type       = 1.4B
0.00.039.122 I print_info: model params     = 1.41 B
0.00.039.122 I print_info: general.name     = 1.4B
0.00.039.123 I print_info: vocab type       = BPE
0.00.039.123 I print_info: n_vocab          = 50304
0.00.039.123 I print_info: n_merges         = 50009
0.00.039.124 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: LF token         = 187 ''
0.00.039.125 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: max token length = 1024
0.00.039.126 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.513.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.513.982 I load_tensors: offloading output layer to GPU
0.00.513.983 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.016 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.023 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.546 I llama_init_from_model: n_seq_max     = 1
0.00.515.549 I llama_init_from_model: n_ctx         = 2048
0.00.515.550 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.515.550 I llama_init_from_model: n_batch       = 2048
0.00.515.550 I llama_init_from_model: n_ubatch      = 512
0.00.515.551 I llama_init_from_model: flash_attn    = 0
0.00.515.553 I llama_init_from_model: freq_base     = 10000.0
0.00.515.554 I llama_init_from_model: freq_scale    = 1
0.00.515.556 I ggml_metal_init: allocating
0.00.515.619 I ggml_metal_init: found device: Apple M4
0.00.515.632 I ggml_metal_init: picking default device: Apple M4
0.00.517.266 I ggml_metal_init: using embedded metal library
0.00.524.030 I ggml_metal_init: GPU name:   Apple M4
0.00.524.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.037 I ggml_metal_init: simdgroup reduction   = true
0.00.524.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.038 I ggml_metal_init: has residency sets    = true
0.00.524.038 I ggml_metal_init: has bfloat            = true
0.00.524.038 I ggml_metal_init: use bfloat            = true
0.00.524.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.599.095 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.599.100 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.599.123 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.418 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.604.421 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.604.421 I llama_init_from_model: graph nodes  = 967
0.00.604.421 I llama_init_from_model: graph splits = 2
0.00.604.428 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.604.552 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.604.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.302 I main: llama threadpool init, n_threads = 4
0.00.664.346 I 
0.00.664.366 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.368 I 
0.00.664.523 I sampler seed: 1234
0.00.664.527 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.541 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.542 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.542 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.955 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50932.57 tokens per second)
0.01.427.956 I llama_perf_context_print:        load time =     654.50 ms
0.01.427.956 I llama_perf_context_print: prompt eval time =      58.02 ms /     7 tokens (    8.29 ms per token,   120.64 tokens per second)
0.01.427.957 I llama_perf_context_print:        eval time =     702.53 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.427.957 I llama_perf_context_print:       total time =     764.43 ms /    70 tokens
0.01.428.251 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.109s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.007 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.013 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.024 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.024 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.024 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.025 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.631 I llama_model_loader: - type  f32:  194 tensors
0.00.024.631 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.631 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.632 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.632 I print_info: file format = GGUF V3 (latest)
0.00.024.633 I print_info: file type   = Q4_K - Medium
0.00.024.634 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.894 I load: special tokens cache size = 25
0.00.039.327 I load: token to piece cache size = 0.2984 MB
0.00.039.344 I print_info: arch             = gptneox
0.00.039.344 I print_info: vocab_only       = 0
0.00.039.345 I print_info: n_ctx_train      = 2048
0.00.039.345 I print_info: n_embd           = 2048
0.00.039.345 I print_info: n_layer          = 24
0.00.039.349 I print_info: n_head           = 16
0.00.039.350 I print_info: n_head_kv        = 16
0.00.039.350 I print_info: n_rot            = 32
0.00.039.350 I print_info: n_swa            = 0
0.00.039.350 I print_info: n_embd_head_k    = 128
0.00.039.350 I print_info: n_embd_head_v    = 128
0.00.039.351 I print_info: n_gqa            = 1
0.00.039.351 I print_info: n_embd_k_gqa     = 2048
0.00.039.353 I print_info: n_embd_v_gqa     = 2048
0.00.039.353 I print_info: f_norm_eps       = 1.0e-05
0.00.039.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.354 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.354 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.354 I print_info: f_logit_scale    = 0.0e+00
0.00.039.355 I print_info: n_ff             = 8192
0.00.039.355 I print_info: n_expert         = 0
0.00.039.355 I print_info: n_expert_used    = 0
0.00.039.355 I print_info: causal attn      = 1
0.00.039.355 I print_info: pooling type     = 0
0.00.039.355 I print_info: rope type        = 2
0.00.039.355 I print_info: rope scaling     = linear
0.00.039.356 I print_info: freq_base_train  = 10000.0
0.00.039.356 I print_info: freq_scale_train = 1
0.00.039.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.356 I print_info: rope_finetuned   = unknown
0.00.039.357 I print_info: ssm_d_conv       = 0
0.00.039.357 I print_info: ssm_d_inner      = 0
0.00.039.357 I print_info: ssm_d_state      = 0
0.00.039.357 I print_info: ssm_dt_rank      = 0
0.00.039.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.357 I print_info: model type       = 1.4B
0.00.039.357 I print_info: model params     = 1.41 B
0.00.039.358 I print_info: general.name     = 1.4B
0.00.039.358 I print_info: vocab type       = BPE
0.00.039.358 I print_info: n_vocab          = 50304
0.00.039.358 I print_info: n_merges         = 50009
0.00.039.359 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.359 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.359 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.359 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.359 I print_info: LF token         = 187 ''
0.00.039.360 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: max token length = 1024
0.00.039.360 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.539.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.539.169 I load_tensors: offloading output layer to GPU
0.00.539.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.539.202 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.539.203 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.918 I llama_init_from_model: n_seq_max     = 1
0.00.540.921 I llama_init_from_model: n_ctx         = 128
0.00.540.922 I llama_init_from_model: n_ctx_per_seq = 128
0.00.540.923 I llama_init_from_model: n_batch       = 128
0.00.540.923 I llama_init_from_model: n_ubatch      = 128
0.00.540.923 I llama_init_from_model: flash_attn    = 0
0.00.540.925 I llama_init_from_model: freq_base     = 10000.0
0.00.540.926 I llama_init_from_model: freq_scale    = 1
0.00.540.926 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.540.929 I ggml_metal_init: allocating
0.00.541.012 I ggml_metal_init: found device: Apple M4
0.00.541.026 I ggml_metal_init: picking default device: Apple M4
0.00.542.590 I ggml_metal_init: using embedded metal library
0.00.549.686 I ggml_metal_init: GPU name:   Apple M4
0.00.549.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.700 I ggml_metal_init: simdgroup reduction   = true
0.00.549.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.700 I ggml_metal_init: has residency sets    = true
0.00.549.700 I ggml_metal_init: has bfloat            = true
0.00.549.701 I ggml_metal_init: use bfloat            = true
0.00.549.702 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.567.993 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.571.459 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.571.462 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.571.492 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.825 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.574.827 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.574.828 I llama_init_from_model: graph nodes  = 967
0.00.574.828 I llama_init_from_model: graph splits = 2
0.00.574.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.574.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.768 I 
0.00.603.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.880 I perplexity: tokenizing the input ..
0.00.611.014 I perplexity: tokenization took 7.13 ms
0.00.611.022 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.835 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.759.173 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.759.201 I llama_perf_context_print:        load time =     594.82 ms
0.00.759.202 I llama_perf_context_print: prompt eval time =     145.95 ms /   128 tokens (    1.14 ms per token,   877.00 tokens per second)
0.00.759.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.203 I llama_perf_context_print:       total time =     155.44 ms /   129 tokens
0.00.759.625 I ggml_metal_free: deallocating

real	0m0.773s
user	0m0.080s
sys	0m0.141s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.300 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.954 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.969 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.971 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.971 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.316 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.317 I llama_model_loader: - type  f32:  194 tensors
0.00.026.317 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.317 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.318 I print_info: file format = GGUF V3 (latest)
0.00.026.318 I print_info: file type   = Q5_K - Medium
0.00.026.322 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.213 I load: special tokens cache size = 25
0.00.040.476 I load: token to piece cache size = 0.2984 MB
0.00.040.489 I print_info: arch             = gptneox
0.00.040.490 I print_info: vocab_only       = 0
0.00.040.491 I print_info: n_ctx_train      = 2048
0.00.040.491 I print_info: n_embd           = 2048
0.00.040.491 I print_info: n_layer          = 24
0.00.040.494 I print_info: n_head           = 16
0.00.040.494 I print_info: n_head_kv        = 16
0.00.040.495 I print_info: n_rot            = 32
0.00.040.495 I print_info: n_swa            = 0
0.00.040.495 I print_info: n_embd_head_k    = 128
0.00.040.495 I print_info: n_embd_head_v    = 128
0.00.040.496 I print_info: n_gqa            = 1
0.00.040.496 I print_info: n_embd_k_gqa     = 2048
0.00.040.497 I print_info: n_embd_v_gqa     = 2048
0.00.040.498 I print_info: f_norm_eps       = 1.0e-05
0.00.040.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.500 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.500 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.500 I print_info: f_logit_scale    = 0.0e+00
0.00.040.501 I print_info: n_ff             = 8192
0.00.040.501 I print_info: n_expert         = 0
0.00.040.501 I print_info: n_expert_used    = 0
0.00.040.502 I print_info: causal attn      = 1
0.00.040.502 I print_info: pooling type     = 0
0.00.040.502 I print_info: rope type        = 2
0.00.040.502 I print_info: rope scaling     = linear
0.00.040.502 I print_info: freq_base_train  = 10000.0
0.00.040.503 I print_info: freq_scale_train = 1
0.00.040.503 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.503 I print_info: rope_finetuned   = unknown
0.00.040.503 I print_info: ssm_d_conv       = 0
0.00.040.503 I print_info: ssm_d_inner      = 0
0.00.040.504 I print_info: ssm_d_state      = 0
0.00.040.504 I print_info: ssm_dt_rank      = 0
0.00.040.504 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.504 I print_info: model type       = 1.4B
0.00.040.505 I print_info: model params     = 1.41 B
0.00.040.505 I print_info: general.name     = 1.4B
0.00.040.505 I print_info: vocab type       = BPE
0.00.040.505 I print_info: n_vocab          = 50304
0.00.040.506 I print_info: n_merges         = 50009
0.00.040.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.506 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.507 I print_info: LF token         = 187 ''
0.00.040.507 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.507 I print_info: max token length = 1024
0.00.040.508 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.197 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.214 I load_tensors: offloading output layer to GPU
0.00.600.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.248 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.249 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.582 I llama_init_from_model: n_seq_max     = 1
0.00.601.586 I llama_init_from_model: n_ctx         = 2048
0.00.601.586 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.587 I llama_init_from_model: n_batch       = 2048
0.00.601.588 I llama_init_from_model: n_ubatch      = 512
0.00.601.588 I llama_init_from_model: flash_attn    = 0
0.00.601.590 I llama_init_from_model: freq_base     = 10000.0
0.00.601.590 I llama_init_from_model: freq_scale    = 1
0.00.601.592 I ggml_metal_init: allocating
0.00.601.611 I ggml_metal_init: found device: Apple M4
0.00.601.622 I ggml_metal_init: picking default device: Apple M4
0.00.602.981 I ggml_metal_init: using embedded metal library
0.00.609.399 I ggml_metal_init: GPU name:   Apple M4
0.00.609.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.405 I ggml_metal_init: simdgroup reduction   = true
0.00.609.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.405 I ggml_metal_init: has residency sets    = true
0.00.609.406 I ggml_metal_init: has bfloat            = true
0.00.609.406 I ggml_metal_init: use bfloat            = true
0.00.609.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.595 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.678.303 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.325 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.121 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.683.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.683.124 I llama_init_from_model: graph nodes  = 967
0.00.683.124 I llama_init_from_model: graph splits = 2
0.00.683.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.015 I main: llama threadpool init, n_threads = 4
0.00.745.063 I 
0.00.745.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.087 I 
0.00.745.243 I sampler seed: 1234
0.00.745.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.263 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.263 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.263 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.585.630 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.585.631 I llama_perf_context_print:        load time =     733.99 ms
0.01.585.631 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.05 tokens per second)
0.01.585.632 I llama_perf_context_print:        eval time =     784.94 ms /    63 runs   (   12.46 ms per token,    80.26 tokens per second)
0.01.585.633 I llama_perf_context_print:       total time =     841.34 ms /    70 tokens
0.01.585.857 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.108s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.159 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.169 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.655 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.656 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.657 I llama_model_loader: - type  f32:  194 tensors
0.00.025.657 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.658 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.658 I print_info: file format = GGUF V3 (latest)
0.00.025.659 I print_info: file type   = Q5_K - Medium
0.00.025.660 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.034 I load: special tokens cache size = 25
0.00.040.446 I load: token to piece cache size = 0.2984 MB
0.00.040.464 I print_info: arch             = gptneox
0.00.040.465 I print_info: vocab_only       = 0
0.00.040.465 I print_info: n_ctx_train      = 2048
0.00.040.465 I print_info: n_embd           = 2048
0.00.040.465 I print_info: n_layer          = 24
0.00.040.469 I print_info: n_head           = 16
0.00.040.471 I print_info: n_head_kv        = 16
0.00.040.471 I print_info: n_rot            = 32
0.00.040.471 I print_info: n_swa            = 0
0.00.040.471 I print_info: n_embd_head_k    = 128
0.00.040.471 I print_info: n_embd_head_v    = 128
0.00.040.472 I print_info: n_gqa            = 1
0.00.040.473 I print_info: n_embd_k_gqa     = 2048
0.00.040.473 I print_info: n_embd_v_gqa     = 2048
0.00.040.474 I print_info: f_norm_eps       = 1.0e-05
0.00.040.474 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.474 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.475 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.475 I print_info: f_logit_scale    = 0.0e+00
0.00.040.475 I print_info: n_ff             = 8192
0.00.040.475 I print_info: n_expert         = 0
0.00.040.476 I print_info: n_expert_used    = 0
0.00.040.476 I print_info: causal attn      = 1
0.00.040.476 I print_info: pooling type     = 0
0.00.040.476 I print_info: rope type        = 2
0.00.040.476 I print_info: rope scaling     = linear
0.00.040.477 I print_info: freq_base_train  = 10000.0
0.00.040.477 I print_info: freq_scale_train = 1
0.00.040.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.477 I print_info: rope_finetuned   = unknown
0.00.040.477 I print_info: ssm_d_conv       = 0
0.00.040.478 I print_info: ssm_d_inner      = 0
0.00.040.478 I print_info: ssm_d_state      = 0
0.00.040.478 I print_info: ssm_dt_rank      = 0
0.00.040.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.478 I print_info: model type       = 1.4B
0.00.040.478 I print_info: model params     = 1.41 B
0.00.040.479 I print_info: general.name     = 1.4B
0.00.040.479 I print_info: vocab type       = BPE
0.00.040.479 I print_info: n_vocab          = 50304
0.00.040.479 I print_info: n_merges         = 50009
0.00.040.480 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: LF token         = 187 ''
0.00.040.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: max token length = 1024
0.00.040.481 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.909 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.917 I load_tensors: offloading output layer to GPU
0.00.589.917 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.949 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.953 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.612 I llama_init_from_model: n_seq_max     = 1
0.00.591.615 I llama_init_from_model: n_ctx         = 128
0.00.591.615 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.616 I llama_init_from_model: n_batch       = 128
0.00.591.616 I llama_init_from_model: n_ubatch      = 128
0.00.591.616 I llama_init_from_model: flash_attn    = 0
0.00.591.617 I llama_init_from_model: freq_base     = 10000.0
0.00.591.618 I llama_init_from_model: freq_scale    = 1
0.00.591.618 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.620 I ggml_metal_init: allocating
0.00.591.718 I ggml_metal_init: found device: Apple M4
0.00.591.731 I ggml_metal_init: picking default device: Apple M4
0.00.593.123 I ggml_metal_init: using embedded metal library
0.00.599.278 I ggml_metal_init: GPU name:   Apple M4
0.00.599.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.284 I ggml_metal_init: simdgroup reduction   = true
0.00.599.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.285 I ggml_metal_init: has residency sets    = true
0.00.599.285 I ggml_metal_init: has bfloat            = true
0.00.599.285 I ggml_metal_init: use bfloat            = true
0.00.599.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.288 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.787 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.343 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.347 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.521 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.523 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.524 I llama_init_from_model: graph nodes  = 967
0.00.623.524 I llama_init_from_model: graph splits = 2
0.00.623.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.056 I 
0.00.660.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.156 I perplexity: tokenizing the input ..
0.00.667.132 I perplexity: tokenization took 6.973 ms
0.00.667.140 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.615 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.817.949 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.817.973 I llama_perf_context_print:        load time =     650.05 ms
0.00.817.974 I llama_perf_context_print: prompt eval time =     148.80 ms /   128 tokens (    1.16 ms per token,   860.22 tokens per second)
0.00.817.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.975 I llama_perf_context_print:       total time =     157.92 ms /   129 tokens
0.00.818.345 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.079s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.983 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.539 I llama_model_loader: - type  f32:  194 tensors
0.00.025.539 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.540 I print_info: file format = GGUF V3 (latest)
0.00.025.540 I print_info: file type   = Q6_K
0.00.025.541 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.750 I load: special tokens cache size = 25
0.00.040.055 I load: token to piece cache size = 0.2984 MB
0.00.040.069 I print_info: arch             = gptneox
0.00.040.070 I print_info: vocab_only       = 0
0.00.040.070 I print_info: n_ctx_train      = 2048
0.00.040.070 I print_info: n_embd           = 2048
0.00.040.071 I print_info: n_layer          = 24
0.00.040.078 I print_info: n_head           = 16
0.00.040.078 I print_info: n_head_kv        = 16
0.00.040.079 I print_info: n_rot            = 32
0.00.040.079 I print_info: n_swa            = 0
0.00.040.079 I print_info: n_embd_head_k    = 128
0.00.040.079 I print_info: n_embd_head_v    = 128
0.00.040.080 I print_info: n_gqa            = 1
0.00.040.081 I print_info: n_embd_k_gqa     = 2048
0.00.040.081 I print_info: n_embd_v_gqa     = 2048
0.00.040.082 I print_info: f_norm_eps       = 1.0e-05
0.00.040.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.085 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.085 I print_info: f_logit_scale    = 0.0e+00
0.00.040.086 I print_info: n_ff             = 8192
0.00.040.086 I print_info: n_expert         = 0
0.00.040.086 I print_info: n_expert_used    = 0
0.00.040.086 I print_info: causal attn      = 1
0.00.040.086 I print_info: pooling type     = 0
0.00.040.086 I print_info: rope type        = 2
0.00.040.087 I print_info: rope scaling     = linear
0.00.040.087 I print_info: freq_base_train  = 10000.0
0.00.040.087 I print_info: freq_scale_train = 1
0.00.040.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.087 I print_info: rope_finetuned   = unknown
0.00.040.088 I print_info: ssm_d_conv       = 0
0.00.040.088 I print_info: ssm_d_inner      = 0
0.00.040.089 I print_info: ssm_d_state      = 0
0.00.040.089 I print_info: ssm_dt_rank      = 0
0.00.040.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.090 I print_info: model type       = 1.4B
0.00.040.090 I print_info: model params     = 1.41 B
0.00.040.090 I print_info: general.name     = 1.4B
0.00.040.091 I print_info: vocab type       = BPE
0.00.040.091 I print_info: n_vocab          = 50304
0.00.040.091 I print_info: n_merges         = 50009
0.00.040.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.093 I print_info: LF token         = 187 ''
0.00.040.093 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: max token length = 1024
0.00.040.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.459 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.470 I load_tensors: offloading output layer to GPU
0.00.663.471 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.497 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.663.501 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.664.929 I llama_init_from_model: n_seq_max     = 1
0.00.664.932 I llama_init_from_model: n_ctx         = 2048
0.00.664.933 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.664.933 I llama_init_from_model: n_batch       = 2048
0.00.664.934 I llama_init_from_model: n_ubatch      = 512
0.00.664.934 I llama_init_from_model: flash_attn    = 0
0.00.664.936 I llama_init_from_model: freq_base     = 10000.0
0.00.664.936 I llama_init_from_model: freq_scale    = 1
0.00.664.939 I ggml_metal_init: allocating
0.00.664.990 I ggml_metal_init: found device: Apple M4
0.00.665.003 I ggml_metal_init: picking default device: Apple M4
0.00.666.520 I ggml_metal_init: using embedded metal library
0.00.673.274 I ggml_metal_init: GPU name:   Apple M4
0.00.673.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.280 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.281 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.281 I ggml_metal_init: simdgroup reduction   = true
0.00.673.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.282 I ggml_metal_init: has residency sets    = true
0.00.673.282 I ggml_metal_init: has bfloat            = true
0.00.673.283 I ggml_metal_init: use bfloat            = true
0.00.673.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.265 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.935 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.747.942 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.752.007 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.752.009 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.752.009 I llama_init_from_model: graph nodes  = 967
0.00.752.009 I llama_init_from_model: graph splits = 2
0.00.752.016 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.752.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.919 I main: llama threadpool init, n_threads = 4
0.00.821.965 I 
0.00.821.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.988 I 
0.00.822.146 I sampler seed: 1234
0.00.822.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.166 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.167 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.167 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.695.038 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.695.039 I llama_perf_context_print:        load time =     812.18 ms
0.01.695.039 I llama_perf_context_print: prompt eval time =      57.55 ms /     7 tokens (    8.22 ms per token,   121.63 tokens per second)
0.01.695.040 I llama_perf_context_print:        eval time =     812.31 ms /    63 runs   (   12.89 ms per token,    77.56 tokens per second)
0.01.695.040 I llama_perf_context_print:       total time =     873.90 ms /    70 tokens
0.01.695.270 I ggml_metal_free: deallocating

real	0m1.712s
user	0m0.111s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4855 (7ab36439) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.469 I llama_model_loader: - type  f32:  194 tensors
0.00.024.470 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.470 I print_info: file format = GGUF V3 (latest)
0.00.024.471 I print_info: file type   = Q6_K
0.00.024.472 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.670 I load: special tokens cache size = 25
0.00.039.002 I load: token to piece cache size = 0.2984 MB
0.00.039.019 I print_info: arch             = gptneox
0.00.039.020 I print_info: vocab_only       = 0
0.00.039.020 I print_info: n_ctx_train      = 2048
0.00.039.021 I print_info: n_embd           = 2048
0.00.039.021 I print_info: n_layer          = 24
0.00.039.025 I print_info: n_head           = 16
0.00.039.026 I print_info: n_head_kv        = 16
0.00.039.026 I print_info: n_rot            = 32
0.00.039.026 I print_info: n_swa            = 0
0.00.039.026 I print_info: n_embd_head_k    = 128
0.00.039.026 I print_info: n_embd_head_v    = 128
0.00.039.027 I print_info: n_gqa            = 1
0.00.039.027 I print_info: n_embd_k_gqa     = 2048
0.00.039.028 I print_info: n_embd_v_gqa     = 2048
0.00.039.028 I print_info: f_norm_eps       = 1.0e-05
0.00.039.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.029 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.029 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.029 I print_info: f_logit_scale    = 0.0e+00
0.00.039.030 I print_info: n_ff             = 8192
0.00.039.030 I print_info: n_expert         = 0
0.00.039.031 I print_info: n_expert_used    = 0
0.00.039.031 I print_info: causal attn      = 1
0.00.039.032 I print_info: pooling type     = 0
0.00.039.032 I print_info: rope type        = 2
0.00.039.032 I print_info: rope scaling     = linear
0.00.039.032 I print_info: freq_base_train  = 10000.0
0.00.039.033 I print_info: freq_scale_train = 1
0.00.039.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.033 I print_info: rope_finetuned   = unknown
0.00.039.033 I print_info: ssm_d_conv       = 0
0.00.039.033 I print_info: ssm_d_inner      = 0
0.00.039.033 I print_info: ssm_d_state      = 0
0.00.039.033 I print_info: ssm_dt_rank      = 0
0.00.039.034 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.034 I print_info: model type       = 1.4B
0.00.039.034 I print_info: model params     = 1.41 B
0.00.039.034 I print_info: general.name     = 1.4B
0.00.039.035 I print_info: vocab type       = BPE
0.00.039.035 I print_info: n_vocab          = 50304
0.00.039.035 I print_info: n_merges         = 50009
0.00.039.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.036 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.036 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.036 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.036 I print_info: LF token         = 187 ''
0.00.039.036 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.037 I print_info: max token length = 1024
0.00.039.037 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.715 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.723 I load_tensors: offloading output layer to GPU
0.00.599.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.750 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.599.753 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.601.288 I llama_init_from_model: n_seq_max     = 1
0.00.601.290 I llama_init_from_model: n_ctx         = 128
0.00.601.290 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.291 I llama_init_from_model: n_batch       = 128
0.00.601.291 I llama_init_from_model: n_ubatch      = 128
0.00.601.291 I llama_init_from_model: flash_attn    = 0
0.00.601.292 I llama_init_from_model: freq_base     = 10000.0
0.00.601.293 I llama_init_from_model: freq_scale    = 1
0.00.601.294 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.295 I ggml_metal_init: allocating
0.00.601.321 I ggml_metal_init: found device: Apple M4
0.00.601.330 I ggml_metal_init: picking default device: Apple M4
0.00.602.593 I ggml_metal_init: using embedded metal library
0.00.608.833 I ggml_metal_init: GPU name:   Apple M4
0.00.608.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.840 I ggml_metal_init: simdgroup reduction   = true
0.00.608.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.840 I ggml_metal_init: has residency sets    = true
0.00.608.840 I ggml_metal_init: has bfloat            = true
0.00.608.840 I ggml_metal_init: use bfloat            = true
0.00.608.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.562 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.040 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.043 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.069 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.388 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.389 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.390 I llama_init_from_model: graph nodes  = 967
0.00.632.390 I llama_init_from_model: graph splits = 2
0.00.632.393 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.001 I 
0.00.666.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.104 I perplexity: tokenizing the input ..
0.00.673.427 I perplexity: tokenization took 7.319 ms
0.00.673.435 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.807.549 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.807.575 I llama_perf_context_print:        load time =     657.10 ms
0.00.807.577 I llama_perf_context_print: prompt eval time =     131.87 ms /   128 tokens (    1.03 ms per token,   970.67 tokens per second)
0.00.807.579 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.579 I llama_perf_context_print:       total time =     141.58 ms /   129 tokens
0.00.807.966 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.079s
sys	0m0.135s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4855 (7ab36439)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1319053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131905b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1319060b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131906660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131906c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1319071c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131907770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131907d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1319082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1319087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131908cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1319091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131909cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13190a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13190acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13190b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13190baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13190c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13190c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13190d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13190d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13190df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13190e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13190ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13190f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13190fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13190ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131910600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131910aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131910f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131911200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1319118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131911bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131912050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1319124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131912990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131912e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1319132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131913770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131913c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1319140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131914550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1319149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131914e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131915150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131915660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131915b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131916570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131916a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131916eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131917350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1319177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131917c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131918130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1319185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131918a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131918f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1319193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131919900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131919da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13191a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13191a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13191a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13191ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13191b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13191b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13191bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13191c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13191c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13191ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13191cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13191d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13191d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13191dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13191e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13191e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13191ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13191f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13191f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13191fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131920260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1319207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131920d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131921250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1319217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131921cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131922240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131922790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131922ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131923230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131923780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131923cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131924220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131924770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131924cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131925210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131925760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131916080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131925bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131926380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1319268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131926e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131927370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1319278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131927e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131928360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1319288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131928e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131929350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1319298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131929df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13192a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13192a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13192ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13192b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13192b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13192bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13192bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13192c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13192c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13192cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13192d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13192d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13192db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13192e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13192e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13192e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13192edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13192f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13192f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13192fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131930070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131930510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1319309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131930e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1319312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131931790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131931c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1319320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131932570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131932a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131932eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131933350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1319337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131933c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131934130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1319345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131934a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131934f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1319353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131935850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131935cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131936190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131936630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131936ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131936f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131937410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1319378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131937d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1319381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131938690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131938b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131938fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131939470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131939910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131939db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13193a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13193a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13193ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13193b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13193b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13193b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13193be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13193c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13193c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13193cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13193d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13193d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13193d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13193de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13193e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13193e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13193ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13193f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13193f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13193fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13193fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131940370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131940810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131940cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131941150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1319415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131941a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131941fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131942530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131942a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131942fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131943470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131943910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131943db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131944250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1319446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131944b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1319450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131945580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131945a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131945ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131946360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131946800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131946ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1319474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131947a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131947d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131948280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131948830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131948de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131949390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131949940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131949ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13194a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13194aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13194b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13194b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13194bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13194c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13194c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13194cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13194d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13194d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13194dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13194e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13194e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13194ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13194f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13194f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13194ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131950550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131950b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1319510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131951660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131951c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1319521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131952770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131952d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1319532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131953880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131953e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1319543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131954990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131954f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1319554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131955aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131956050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131956600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131956bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131957160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131957710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131957cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131958270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131958820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131958dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131959380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131959930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131959ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13195a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13195aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13195aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13195b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13195baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13195bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13195c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13195c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13195cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13195d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13195d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13195dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13195e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13195e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13195eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13195f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13195f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13195fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1319600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1319605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131960aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x131960fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1319614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1319619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131961ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1319623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1319628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x131962da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1319632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131963cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1319643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131964af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131965210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1319654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131965c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131966100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1319665a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.693.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.693.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116f04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116f05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116f054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116f05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116f05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116f06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116f06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116f06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116f06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116f073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116f07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116f07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116f08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116f091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116f09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116f0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116f0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116f0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116f0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116f0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116f0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116f0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116f0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116f0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116f0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116f0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116f0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116f0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116f0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116f0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116f0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116f0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116f105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116f10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116f10f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116f113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116f11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116f11cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116f12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116f12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116f12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116f13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116f138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116f141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116f14690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116f14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116f14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116f15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116f15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116f16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116f166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116f16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116f17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116f174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116f17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116f17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116f17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116f18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116f187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116f18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116f19080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116f194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116f19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131947fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131951370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131950260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13194cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13194a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131959bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131957420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131955200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131952fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13194b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131948af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13194da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13194eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1319540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131950dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131958ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13194b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13194c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131953b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131955d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13194e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13194f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131954c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131951920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131951ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13194c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13194d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13195a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1319579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131949650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131952a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131948540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13194a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13195a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13194fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131963560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131957f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13194e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131950810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1319546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13194be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131956310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13194ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131959090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1319568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131952480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13195b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131949c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13195ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1319490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131959640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131953590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1319557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131958530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131956e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13194f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13190eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131909490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131925e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131965790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1319114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131966860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131966b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131966de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1319670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131967360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131967620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1319678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131967ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131967e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131968120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1319683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1319686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131968960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131968c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131968ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1319691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131969460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131969720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1319699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131969ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131969f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13196a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13196a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13196a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13196aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13196ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13196afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13196b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13196b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13196b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13196bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13196bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13196c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13196c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13196c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13196c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13196cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13196ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13196d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13196d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13196d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13196d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13196dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13196dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13196e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13196e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13196e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13196e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13196ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13196ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13196f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13196f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13196f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13196fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13196fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13196ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131970260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131970520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1319707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131970aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131970d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131971020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1319712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1319715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131971860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131971b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131971de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1319720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131972360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131972620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1319728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131972ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131972e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131973120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1319733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1319736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131973960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131973c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131973ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1319741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131974460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131974720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1319749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131975050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131975310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1319755d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131975890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131975b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131975e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1319760d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131976390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131976650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131976910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131976bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131976e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131977150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131977410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1319776d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131977990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131977c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131977f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1319781d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131978490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131978750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131978a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131978cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131978f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131979250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131979510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1319797d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131979a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131979d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13197a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13197a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13197a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13197a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13197ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13197add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13197b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13197b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13197b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13197b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13197bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13197be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13197c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13197c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13197c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13197c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13197cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13197ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13197d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13197d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13197d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13197d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13197df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13197e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13197e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13197ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13197f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13197f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13197fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13197ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1319801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131980660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131980ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131980f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1319813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131981820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131981c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131982100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131982570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1319829e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131982e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1319832c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131983730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x131983ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x131984010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131984480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1319848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x131984d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1319851d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131985640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131985ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131985f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x131986390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131986800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131987260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131987980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1319880a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1319887c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131988a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131989210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1319896b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131989b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131b07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131b07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131b08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131b086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131b08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131b08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131b09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131b09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131b09cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131b0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131b0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131b0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131b0b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131b0bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131b0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131b0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131b0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131b0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131b0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131b0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131b0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131b0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131b100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131b10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131b10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131b113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131b11860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131b11d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131b121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131b12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131b12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131b12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131b13240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131b136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131b13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131b14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131b144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131b14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131b14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131b152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131b15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131b15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131b16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131b16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131b169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131b16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131b17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131b177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131b17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131b180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131b18580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131b18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131b18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131b19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131b19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131b19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131b1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131b1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131b1a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131b1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131b1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131b1b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131b1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131b1bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131b1c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131b1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131b1ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131b1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131b1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131b1d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131b1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131b1e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116f19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116f0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116f081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116f04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116f101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116f0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116f1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116f1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116f1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116f1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116f1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116f1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116f1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116f1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116f1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116f1bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116f1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116f1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116f1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116f1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116f1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116f1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116f1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116f1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116f1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116f1d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116f1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116f1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116f1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116f1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116f1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116f1ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116f1eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116f1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116f1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116f1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116f1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116f1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116f1ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116f201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116f204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116f20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116f20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116f20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116f21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116f21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116f217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116f21ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116f21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116f22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116f222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116f225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116f22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116f22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116f22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116f23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116f23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116f23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116f243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116f24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116f24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116f25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116f25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116f259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116f25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116f262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116f26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116f26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116f27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116f27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116f278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116f27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116f28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116f28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116f28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116f29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116f29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116f29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116f2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116f2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116f2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116f2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116f2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116f2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116f2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116f2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116f2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116f2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116f2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116f2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116f2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116f2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116f2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116f2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116f2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116f2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116f2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116f2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116f2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116f30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116f306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116f30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116f30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116f31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116f318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116f31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116f32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116f32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116f32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116f32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116f33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116f337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116f33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116f340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116f34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116f34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116f34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116f35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116f356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116f35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116f360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116f36530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116f369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116f37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116f37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116f379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116f37ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116f385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116f38a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116f38f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116f393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116f39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116f39ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116f3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116f3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116f3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116f3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116f3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116f3c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116f3c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116f3cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116f3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116f3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116f3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116f3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116f3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116f3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116f3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116f3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116f40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116f40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116f41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116f41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116f41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116f42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116f42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116f42cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116f432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116f43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116f43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116f443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116f44960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116f44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116f454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116f45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116f46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116f465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116f47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116f476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116f47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116f48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116f487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116f48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116f49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116f49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116f49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116f4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116f4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116f4afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116f4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116f4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116f4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116f4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116f4d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116f4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116f4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116f4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116f4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116f4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116f4f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116f4f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116f4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116f50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116f50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116f50a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116f50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116f51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116f51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116f51e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116f52340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x116f52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x116f52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x116f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x116f53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x116f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x116f54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x116f54640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x116f54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x116f55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x116f55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116f55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116f56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116f56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116f57290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116f579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116f57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116f58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116f588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116f58d40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.768s
user	0m0.282s
sys	0m0.331s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4855 (7ab36439)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff0feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff10460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff10fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff12520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff13cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff14c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff15340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff15a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff18e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff19e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff1b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff1b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff1fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff24e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff30a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff35ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff36d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff37fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff39700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff3ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff3b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff3b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff3bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff3c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff3e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff3e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff3ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff3f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff3fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff43d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff45de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff46720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff48780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff49560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff49ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff4b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff4cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff50330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff51390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff51910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff53580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff53b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff55d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff56300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff59080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff5a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff5b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff5be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff5c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff5c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff5cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff5d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff5da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff5e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff5e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff61350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff61900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff61eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff62460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff62fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff63570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff64680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff65630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff65b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff66030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff66530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff66a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff66f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff67930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff67e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff68330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff68830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff68d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff69230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ff69730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ff69c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ff6a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ff6a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ff6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ff6b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ff6b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ff6ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ff6bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ff6c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff6d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff6da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff6e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff6e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff6eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff6f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff6f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff6fc30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.102.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff63280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff60ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff58230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff5d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff5a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff62170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff56010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff5d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff5afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff5b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff55a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff63830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff5c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff51bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff53840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff5dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff5f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff5ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff5bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff64940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff52730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff62cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff5cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff61bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff6ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff6fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff70470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff70730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff70cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff70f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff71230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff71a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff71d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff71ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff72570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff72830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff72af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff72db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff73070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff73330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff73b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff73e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff74670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff74930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff74bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff74eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff75170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff75430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff75c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff75f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff76770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff76a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff76cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff76fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff77270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff77530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff77ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff77d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff78030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff78870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff78b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff78df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff79370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff79630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff79bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff79e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff7a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff7a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff7a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff7a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff7ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff7aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff7b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff7b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff7b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff7b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff7bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff7bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff7c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff7c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff7c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff7ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff7cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff7cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff7d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff7d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff7d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff7daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff7ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff7e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff7e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff7e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff7e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff7eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff7ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff7f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff7f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff7f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff7f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff7fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff7feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff80170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff80430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff80c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff80f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff81770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff81a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff81cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff81fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff82270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff82530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff82ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff82d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff83030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff83870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff83b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff83df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff84370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff84630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff84bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff84e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff85130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff85970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff85c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff86030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff86cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff871f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff87bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff880f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff885f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff88af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff89650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff89c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff8a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff8a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff8abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff8b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff8b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff8bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff8bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff8c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff8c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff8d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff8d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff8d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff8de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff8e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff8e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff8ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff8f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff8fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff90030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff905e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff90b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff91140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff916f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff91ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff92250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff92800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff92db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff93360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff93910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff93ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff94470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff94a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff94fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff95580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff95b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff960e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff96690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff96c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff971f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff977a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff97d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff98300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff988b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff98e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff99410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff999c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff99f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff9a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff9aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff9b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff9b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff9bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff9c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff9c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff9ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff9d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff9d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff9de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff9e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff9e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff9ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff9f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff9fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ffa0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ffa05d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ffa0b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ffa1130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ffa16e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ffa1c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ffa2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ffa27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ffa2cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ffa31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ffa36f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ffa3bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ffa40f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ffa45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ffa4af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ffa4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ffa54f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ffa59f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ffa5ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ffa63f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ffa68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ffa6df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ffa72f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ffa77f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ffa7cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ffa81f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ffa86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ffa8bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ffa90f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ffa95f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ffa9af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ffa9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ffaa4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ffaaf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ffab620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ffabd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ffac460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ffac720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ffaceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ffad350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ffad7f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff985c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff8c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff94180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff919b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ffa0e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff9e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff9c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff9a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff92510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff8fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff94ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff95df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff89910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff9b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff98010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff9fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff92ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff93bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff9ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff9cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff95840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff96950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff9bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff8ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff98b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff99120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff93620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff94730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ffa13f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff89360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff9ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff908a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff99c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff8f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff8f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff88db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff91400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ffa19a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff96f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ffaa7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff9f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff95290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff97a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff9b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff93070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff9d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff91f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ffa02e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff9db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff996d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ffa2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff90e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ffa1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff89ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ffa0890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff9a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff9ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff9f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff9e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ffadab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ffadd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ffae030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ffae2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ffae5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ffae870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ffaeb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ffaedf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ffaf0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ffaf370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ffaf630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ffaf8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ffafbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ffafe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ffb0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ffb03f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ffb06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ffb0970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ffb0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ffb0ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ffb11b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ffb1470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ffb1730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ffb19f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ffb1cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ffb1f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ffb2230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ffb24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ffb27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ffb2a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ffb2d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ffb2ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ffb32b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ffb3570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ffb3830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ffb3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ffb3db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ffb4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ffb4330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ffb45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ffb48b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ffb4b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ffb4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ffb50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ffb53b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ffb5670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ffb5930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ffb5bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ffb5eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ffb6170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ffb6430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ffb66f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ffb69b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ffb6c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ffb6f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ffb71f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ffb74b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ffb7770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ffb7a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ffb7cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ffb7fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ffb8270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ffb8530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ffb87f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ffb8ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ffb8d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ffb9030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ffb92f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ffb95b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ffb9870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ffb9b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ffb9df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ffba0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ffba370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ffba630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ffba8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ffbabb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ffbae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ffbb130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ffbb3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ffbb6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ffbb970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ffbbc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ffbbef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ffbc1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ffbc470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ffbc730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ffbc9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ffbccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ffbcf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ffbd230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ffbd4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ffbd7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ffbda70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ffbdd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ffbdff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ffbe2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ffbe570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ffbe830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ffbeaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ffbedb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ffbf070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ffbf330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ffbf5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ffbf8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ffbfb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ffbfe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ffc00f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ffc03b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ffc0670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ffc0930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ffc0bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ffc0eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ffc1170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ffc1430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ffc16f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ffc19b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ffc1c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ffc1f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ffc21f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ffc24b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ffc2770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ffc2a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ffc2cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ffc2fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ffc3270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ffc3530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ffc37f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ffc3ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ffc3d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ffc4030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ffc42f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ffc45b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ffc4870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ffc4b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ffc4df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ffc50b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ffc5370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ffc5630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ffc58f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ffc5bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ffc5e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ffc6130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ffc63f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ffc66b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ffc6970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ffc6c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ffc6ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ffc7560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ffc7820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ffc7ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ffc7da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ffc8060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ffc8320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ffc85e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ffc88a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ffc8b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ffc8e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ffc90e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ffc93a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ffc98f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ffc9e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ffca390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ffca8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ffcae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ffcb380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ffcb8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ffcbe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ffcc370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ffcc8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ffcce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ffcd360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ffcd8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ffcde00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ffce350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ffce8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ffcedf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ffcf340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ffcf890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ffcfde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ffd0330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ffd0880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ffd0dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ffd1320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ffd1870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ffd1dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ffd2310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ffd2860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ffd2db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ffd3300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ffd3850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ffd3da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ffd42f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ffd4840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ffd4d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ffd52e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ffd5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ffd5d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ffd62d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ffd6820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ffd6d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ffd72c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ffd7810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ffd7d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ffd82b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ffd8570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ffd8830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ffd8d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ffd9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ffd9730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ffd9c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ffda130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ffda630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ffdab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ffdb030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ffdb530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ffdba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ffdbf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ffdc430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ffdc930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ffdce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ffdd330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ffdd830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ffddd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ffde230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ffde730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ffdec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ffdf130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ffdf630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ffdfb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ffe0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ffe0a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ffe1160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ffe1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ffe1fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ffe2260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ffe29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ffe2e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ffe3330 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.228s
sys	0m0.185s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.50 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.46 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.96 sec*proc (2 tests)

Total Test time (real) =   1.97 sec
        1.99 real         0.53 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
