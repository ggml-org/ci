### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.64 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.28 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.92 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.03 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  196.38 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.70 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 238.95 sec*proc (28 tests)

Total Test time (real) = 238.96 sec

real	3m58.991s
user	8m25.752s
sys	0m6.654s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.41 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.83 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.20 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.87 sec*proc (28 tests)

Total Test time (real) =  52.89 sec

real	0m52.897s
user	1m16.241s
sys	0m6.086s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.081 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.611 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.552 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.561 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.562 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.563 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.564 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.565 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.566 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.566 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.567 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.567 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.571 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.571 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.572 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.573 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.573 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.574 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.574 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.969 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.193 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.195 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.196 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.196 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.197 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.197 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.198 I llama_model_loader: - type  f32:  124 tensors
0.00.026.198 I llama_model_loader: - type  f16:   73 tensors
0.00.026.199 I print_info: file format = GGUF V3 (latest)
0.00.026.200 I print_info: file type   = F16
0.00.026.202 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.389 I load: special tokens cache size = 5
0.00.032.237 I load: token to piece cache size = 0.2032 MB
0.00.032.241 I print_info: arch             = bert
0.00.032.241 I print_info: vocab_only       = 0
0.00.032.242 I print_info: n_ctx_train      = 512
0.00.032.242 I print_info: n_embd           = 384
0.00.032.242 I print_info: n_layer          = 12
0.00.032.246 I print_info: n_head           = 12
0.00.032.247 I print_info: n_head_kv        = 12
0.00.032.249 I print_info: n_rot            = 32
0.00.032.249 I print_info: n_swa            = 0
0.00.032.249 I print_info: n_embd_head_k    = 32
0.00.032.249 I print_info: n_embd_head_v    = 32
0.00.032.250 I print_info: n_gqa            = 1
0.00.032.251 I print_info: n_embd_k_gqa     = 384
0.00.032.251 I print_info: n_embd_v_gqa     = 384
0.00.032.258 I print_info: f_norm_eps       = 1.0e-12
0.00.032.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.261 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.261 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.261 I print_info: f_logit_scale    = 0.0e+00
0.00.032.262 I print_info: n_ff             = 1536
0.00.032.264 I print_info: n_expert         = 0
0.00.032.264 I print_info: n_expert_used    = 0
0.00.032.264 I print_info: causal attn      = 0
0.00.032.265 I print_info: pooling type     = 2
0.00.032.265 I print_info: rope type        = 2
0.00.032.265 I print_info: rope scaling     = linear
0.00.032.266 I print_info: freq_base_train  = 10000.0
0.00.032.266 I print_info: freq_scale_train = 1
0.00.032.266 I print_info: n_ctx_orig_yarn  = 512
0.00.032.267 I print_info: rope_finetuned   = unknown
0.00.032.267 I print_info: ssm_d_conv       = 0
0.00.032.267 I print_info: ssm_d_inner      = 0
0.00.032.267 I print_info: ssm_d_state      = 0
0.00.032.267 I print_info: ssm_dt_rank      = 0
0.00.032.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.268 I print_info: model type       = 33M
0.00.032.269 I print_info: model params     = 33.21 M
0.00.032.269 I print_info: general.name     = Bge Small
0.00.032.270 I print_info: vocab type       = WPM
0.00.032.270 I print_info: n_vocab          = 30522
0.00.032.270 I print_info: n_merges         = 0
0.00.032.271 I print_info: BOS token        = 101 '[CLS]'
0.00.032.271 I print_info: UNK token        = 100 '[UNK]'
0.00.032.271 I print_info: SEP token        = 102 '[SEP]'
0.00.032.271 I print_info: PAD token        = 0 '[PAD]'
0.00.032.271 I print_info: MASK token       = 103 '[MASK]'
0.00.032.272 I print_info: LF token         = 0 '[PAD]'
0.00.032.272 I print_info: max token length = 21
0.00.034.145 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.145 I load_tensors: offloading output layer to GPU
0.00.034.147 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.174 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.175 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.402 I llama_init_from_model: n_seq_max     = 1
0.00.034.404 I llama_init_from_model: n_ctx         = 512
0.00.034.404 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.404 I llama_init_from_model: n_batch       = 2048
0.00.034.405 I llama_init_from_model: n_ubatch      = 2048
0.00.034.405 I llama_init_from_model: flash_attn    = 0
0.00.034.405 I llama_init_from_model: freq_base     = 10000.0
0.00.034.406 I llama_init_from_model: freq_scale    = 1
0.00.034.406 I ggml_metal_init: allocating
0.00.034.410 I ggml_metal_init: found device: Apple M4
0.00.034.413 I ggml_metal_init: picking default device: Apple M4
0.00.035.216 I ggml_metal_init: using embedded metal library
0.00.039.269 I ggml_metal_init: GPU name:   Apple M4
0.00.039.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.273 I ggml_metal_init: simdgroup reduction   = true
0.00.039.273 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.273 I ggml_metal_init: has bfloat            = true
0.00.039.273 I ggml_metal_init: use bfloat            = true
0.00.039.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.415 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.964 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.966 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.969 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.052.724 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.052.726 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.052.726 I llama_init_from_model: graph nodes  = 429
0.00.052.726 I llama_init_from_model: graph splits = 2
0.00.052.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.054 I 
0.00.059.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.709 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.428 I llama_perf_context_print:        load time =      43.44 ms
0.00.064.429 I llama_perf_context_print: prompt eval time =       4.57 ms /     9 tokens (    0.51 ms per token,  1969.37 tokens per second)
0.00.064.430 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.431 I llama_perf_context_print:       total time =       5.38 ms /    10 tokens
0.00.064.568 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.047s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.435 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.104 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.109 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.110 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.114 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.114 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.115 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.115 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.116 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.117 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.118 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.120 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.120 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.121 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.121 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.121 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.122 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.575 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.215 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.216 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.217 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.217 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.217 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.218 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.218 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.218 I llama_model_loader: - type  f32:  124 tensors
0.00.015.219 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.219 I print_info: file format = GGUF V3 (latest)
0.00.015.220 I print_info: file type   = Q8_0
0.00.015.221 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.604 I load: special tokens cache size = 5
0.00.018.901 I load: token to piece cache size = 0.2032 MB
0.00.018.904 I print_info: arch             = bert
0.00.018.905 I print_info: vocab_only       = 0
0.00.018.905 I print_info: n_ctx_train      = 512
0.00.018.905 I print_info: n_embd           = 384
0.00.018.905 I print_info: n_layer          = 12
0.00.018.908 I print_info: n_head           = 12
0.00.018.909 I print_info: n_head_kv        = 12
0.00.018.909 I print_info: n_rot            = 32
0.00.018.909 I print_info: n_swa            = 0
0.00.018.909 I print_info: n_embd_head_k    = 32
0.00.018.909 I print_info: n_embd_head_v    = 32
0.00.018.910 I print_info: n_gqa            = 1
0.00.018.910 I print_info: n_embd_k_gqa     = 384
0.00.018.911 I print_info: n_embd_v_gqa     = 384
0.00.018.912 I print_info: f_norm_eps       = 1.0e-12
0.00.018.912 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.913 I print_info: f_logit_scale    = 0.0e+00
0.00.018.913 I print_info: n_ff             = 1536
0.00.018.914 I print_info: n_expert         = 0
0.00.018.914 I print_info: n_expert_used    = 0
0.00.018.914 I print_info: causal attn      = 0
0.00.018.914 I print_info: pooling type     = 2
0.00.018.914 I print_info: rope type        = 2
0.00.018.914 I print_info: rope scaling     = linear
0.00.018.915 I print_info: freq_base_train  = 10000.0
0.00.018.915 I print_info: freq_scale_train = 1
0.00.018.915 I print_info: n_ctx_orig_yarn  = 512
0.00.018.915 I print_info: rope_finetuned   = unknown
0.00.018.915 I print_info: ssm_d_conv       = 0
0.00.018.916 I print_info: ssm_d_inner      = 0
0.00.018.916 I print_info: ssm_d_state      = 0
0.00.018.916 I print_info: ssm_dt_rank      = 0
0.00.018.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.916 I print_info: model type       = 33M
0.00.018.917 I print_info: model params     = 33.21 M
0.00.018.917 I print_info: general.name     = Bge Small
0.00.018.918 I print_info: vocab type       = WPM
0.00.018.918 I print_info: n_vocab          = 30522
0.00.018.918 I print_info: n_merges         = 0
0.00.018.918 I print_info: BOS token        = 101 '[CLS]'
0.00.018.919 I print_info: UNK token        = 100 '[UNK]'
0.00.018.919 I print_info: SEP token        = 102 '[SEP]'
0.00.018.919 I print_info: PAD token        = 0 '[PAD]'
0.00.018.919 I print_info: MASK token       = 103 '[MASK]'
0.00.018.919 I print_info: LF token         = 0 '[PAD]'
0.00.018.920 I print_info: max token length = 21
0.00.020.233 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.233 I load_tensors: offloading output layer to GPU
0.00.020.234 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.242 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.243 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.391 I llama_init_from_model: n_seq_max     = 1
0.00.020.392 I llama_init_from_model: n_ctx         = 512
0.00.020.392 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.393 I llama_init_from_model: n_batch       = 2048
0.00.020.393 I llama_init_from_model: n_ubatch      = 2048
0.00.020.393 I llama_init_from_model: flash_attn    = 0
0.00.020.393 I llama_init_from_model: freq_base     = 10000.0
0.00.020.394 I llama_init_from_model: freq_scale    = 1
0.00.020.394 I ggml_metal_init: allocating
0.00.020.398 I ggml_metal_init: found device: Apple M4
0.00.020.400 I ggml_metal_init: picking default device: Apple M4
0.00.021.048 I ggml_metal_init: using embedded metal library
0.00.023.589 I ggml_metal_init: GPU name:   Apple M4
0.00.023.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.593 I ggml_metal_init: simdgroup reduction   = true
0.00.023.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.593 I ggml_metal_init: has bfloat            = true
0.00.023.593 I ggml_metal_init: use bfloat            = true
0.00.023.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.922 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.414 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.416 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.417 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.982 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.983 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.983 I llama_init_from_model: graph nodes  = 429
0.00.034.983 I llama_init_from_model: graph splits = 2
0.00.034.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.412 I 
0.00.039.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.968 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.215 I llama_perf_context_print:        load time =      29.97 ms
0.00.044.217 I llama_perf_context_print: prompt eval time =       4.12 ms /     9 tokens (    0.46 ms per token,  2185.00 tokens per second)
0.00.044.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.218 I llama_perf_context_print:       total time =       4.80 ms /    10 tokens
0.00.044.427 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.205 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.653 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.343 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.351 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.359 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.360 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.361 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.362 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.363 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.363 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.364 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.364 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.368 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.368 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.369 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.333 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.333 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.334 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.334 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.334 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.335 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.335 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.336 I llama_model_loader: - type  f32:   40 tensors
0.00.052.336 I llama_model_loader: - type  f16:   30 tensors
0.00.052.337 I print_info: file format = GGUF V3 (latest)
0.00.052.338 I print_info: file type   = F16
0.00.052.339 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.068.466 W load: empty token at index 5
0.00.072.827 W load: model vocab missing newline token, using special_pad_id instead
0.00.074.090 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.120 I load: special tokens cache size = 5
0.00.335.001 I load: token to piece cache size = 1.5060 MB
0.00.335.007 I print_info: arch             = jina-bert-v2
0.00.335.007 I print_info: vocab_only       = 0
0.00.335.007 I print_info: n_ctx_train      = 8192
0.00.335.010 I print_info: n_embd           = 384
0.00.335.011 I print_info: n_layer          = 4
0.00.335.017 I print_info: n_head           = 12
0.00.335.017 I print_info: n_head_kv        = 12
0.00.335.017 I print_info: n_rot            = 32
0.00.335.018 I print_info: n_swa            = 0
0.00.335.018 I print_info: n_embd_head_k    = 32
0.00.335.018 I print_info: n_embd_head_v    = 32
0.00.335.019 I print_info: n_gqa            = 1
0.00.335.019 I print_info: n_embd_k_gqa     = 384
0.00.335.020 I print_info: n_embd_v_gqa     = 384
0.00.335.021 I print_info: f_norm_eps       = 1.0e-12
0.00.335.021 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.335.022 I print_info: f_clamp_kqv      = 0.0e+00
0.00.335.022 I print_info: f_max_alibi_bias = 8.0e+00
0.00.335.022 I print_info: f_logit_scale    = 0.0e+00
0.00.335.023 I print_info: n_ff             = 1536
0.00.335.023 I print_info: n_expert         = 0
0.00.335.023 I print_info: n_expert_used    = 0
0.00.335.025 I print_info: causal attn      = 0
0.00.335.026 I print_info: pooling type     = -1
0.00.335.026 I print_info: rope type        = -1
0.00.335.026 I print_info: rope scaling     = linear
0.00.335.026 I print_info: freq_base_train  = 10000.0
0.00.335.027 I print_info: freq_scale_train = 1
0.00.335.027 I print_info: n_ctx_orig_yarn  = 8192
0.00.335.027 I print_info: rope_finetuned   = unknown
0.00.335.027 I print_info: ssm_d_conv       = 0
0.00.335.027 I print_info: ssm_d_inner      = 0
0.00.335.028 I print_info: ssm_d_state      = 0
0.00.335.028 I print_info: ssm_dt_rank      = 0
0.00.335.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.335.028 I print_info: model type       = 33M
0.00.335.028 I print_info: model params     = 32.90 M
0.00.335.029 I print_info: general.name     = Jina Bert Implementation
0.00.335.029 I print_info: vocab type       = BPE
0.00.335.029 I print_info: n_vocab          = 61056
0.00.335.030 I print_info: n_merges         = 39382
0.00.335.030 I print_info: BOS token        = 0 '<s>'
0.00.335.030 I print_info: EOS token        = 2 '</s>'
0.00.335.030 I print_info: UNK token        = 3 '<unk>'
0.00.335.031 I print_info: SEP token        = 2 '</s>'
0.00.335.031 I print_info: PAD token        = 1 '<pad>'
0.00.335.031 I print_info: MASK token       = 4 '<mask>'
0.00.335.032 I print_info: EOG token        = 2 '</s>'
0.00.335.033 I print_info: max token length = 45
0.00.336.312 I load_tensors: offloading 4 repeating layers to GPU
0.00.336.312 I load_tensors: offloading output layer to GPU
0.00.336.312 I load_tensors: offloaded 5/5 layers to GPU
0.00.336.337 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.338 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.336.647 I llama_init_from_model: n_seq_max     = 1
0.00.336.648 I llama_init_from_model: n_ctx         = 8192
0.00.336.648 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.336.648 I llama_init_from_model: n_batch       = 2048
0.00.336.649 I llama_init_from_model: n_ubatch      = 2048
0.00.336.649 I llama_init_from_model: flash_attn    = 0
0.00.336.649 I llama_init_from_model: freq_base     = 10000.0
0.00.336.649 I llama_init_from_model: freq_scale    = 1
0.00.336.650 I ggml_metal_init: allocating
0.00.336.653 I ggml_metal_init: found device: Apple M4
0.00.336.655 I ggml_metal_init: picking default device: Apple M4
0.00.337.657 I ggml_metal_init: using embedded metal library
0.00.340.347 I ggml_metal_init: GPU name:   Apple M4
0.00.340.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.349 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.349 I ggml_metal_init: simdgroup reduction   = true
0.00.340.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.349 I ggml_metal_init: has bfloat            = true
0.00.340.350 I ggml_metal_init: use bfloat            = true
0.00.340.350 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.822 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.352.237 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.352.239 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.241 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.795 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.796 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.796 I llama_init_from_model: graph nodes  = 154
0.00.352.796 I llama_init_from_model: graph splits = 2
0.00.352.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.573 I 
0.00.362.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.740 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.362.741 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.362.744 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.362.744 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.362.748 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.362.749 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.232 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.366.952 I llama_perf_context_print:        load time =     336.91 ms
0.00.366.953 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16707.09 tokens per second)
0.00.366.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.956 I llama_perf_context_print:       total time =       4.38 ms /    63 tokens
0.00.367.167 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.340s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.174 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.284 I main: llama backend init
0.00.000.290 I main: load the model and apply lora adapter, if any
0.00.063.757 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.076.667 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.076.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.076.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.076.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.076.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.076.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.076.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.076.691 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.076.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.076.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.076.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.076.694 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.076.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.076.695 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.076.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.076.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.076.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.083.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.085.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.094.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.094.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.094.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.094.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.094.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.094.263 I llama_model_loader: - type  f32:  194 tensors
0.00.094.264 I llama_model_loader: - type  f16:   98 tensors
0.00.094.265 I print_info: file format = GGUF V3 (latest)
0.00.094.269 I print_info: file type   = all F32 (guessed)
0.00.094.270 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.128.519 I load: special tokens cache size = 25
0.00.136.262 I load: token to piece cache size = 0.2984 MB
0.00.136.265 I print_info: arch             = gptneox
0.00.136.265 I print_info: vocab_only       = 0
0.00.136.265 I print_info: n_ctx_train      = 2048
0.00.136.266 I print_info: n_embd           = 2048
0.00.136.266 I print_info: n_layer          = 24
0.00.136.270 I print_info: n_head           = 16
0.00.136.273 I print_info: n_head_kv        = 16
0.00.136.273 I print_info: n_rot            = 32
0.00.136.273 I print_info: n_swa            = 0
0.00.136.273 I print_info: n_embd_head_k    = 128
0.00.136.273 I print_info: n_embd_head_v    = 128
0.00.136.274 I print_info: n_gqa            = 1
0.00.136.279 I print_info: n_embd_k_gqa     = 2048
0.00.136.280 I print_info: n_embd_v_gqa     = 2048
0.00.136.281 I print_info: f_norm_eps       = 1.0e-05
0.00.136.281 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.136.283 I print_info: f_clamp_kqv      = 0.0e+00
0.00.136.283 I print_info: f_max_alibi_bias = 0.0e+00
0.00.136.283 I print_info: f_logit_scale    = 0.0e+00
0.00.136.284 I print_info: n_ff             = 8192
0.00.136.284 I print_info: n_expert         = 0
0.00.136.284 I print_info: n_expert_used    = 0
0.00.136.284 I print_info: causal attn      = 1
0.00.136.285 I print_info: pooling type     = 0
0.00.136.285 I print_info: rope type        = 2
0.00.136.286 I print_info: rope scaling     = linear
0.00.136.286 I print_info: freq_base_train  = 10000.0
0.00.136.286 I print_info: freq_scale_train = 1
0.00.136.287 I print_info: n_ctx_orig_yarn  = 2048
0.00.136.287 I print_info: rope_finetuned   = unknown
0.00.136.287 I print_info: ssm_d_conv       = 0
0.00.136.288 I print_info: ssm_d_inner      = 0
0.00.136.289 I print_info: ssm_d_state      = 0
0.00.136.289 I print_info: ssm_dt_rank      = 0
0.00.136.289 I print_info: ssm_dt_b_c_rms   = 0
0.00.136.289 I print_info: model type       = 1.4B
0.00.136.290 I print_info: model params     = 1.41 B
0.00.136.290 I print_info: general.name     = 1.4B
0.00.136.290 I print_info: vocab type       = BPE
0.00.136.291 I print_info: n_vocab          = 50304
0.00.136.291 I print_info: n_merges         = 50009
0.00.136.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.136.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.136.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.136.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.136.292 I print_info: LF token         = 128 'Ä'
0.00.136.292 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.136.292 I print_info: max token length = 1024
0.00.138.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.138.999 I load_tensors: offloading output layer to GPU
0.00.138.999 I load_tensors: offloaded 25/25 layers to GPU
0.00.139.018 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.139.020 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.139.365 I llama_init_from_model: n_seq_max     = 1
0.00.139.366 I llama_init_from_model: n_ctx         = 2048
0.00.139.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.139.366 I llama_init_from_model: n_batch       = 2048
0.00.139.367 I llama_init_from_model: n_ubatch      = 512
0.00.139.367 I llama_init_from_model: flash_attn    = 0
0.00.139.367 I llama_init_from_model: freq_base     = 10000.0
0.00.139.368 I llama_init_from_model: freq_scale    = 1
0.00.139.368 I ggml_metal_init: allocating
0.00.139.371 I ggml_metal_init: found device: Apple M4
0.00.139.373 I ggml_metal_init: picking default device: Apple M4
0.00.140.062 I ggml_metal_init: using embedded metal library
0.00.162.005 I ggml_metal_init: GPU name:   Apple M4
0.00.162.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.162.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.162.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.162.008 I ggml_metal_init: simdgroup reduction   = true
0.00.162.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.162.008 I ggml_metal_init: has bfloat            = true
0.00.162.008 I ggml_metal_init: use bfloat            = true
0.00.162.009 I ggml_metal_init: hasUnifiedMemory      = true
0.00.162.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.256.480 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.277.436 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.277.443 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.277.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.278.431 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.278.433 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.278.434 I llama_init_from_model: graph nodes  = 967
0.00.278.434 I llama_init_from_model: graph splits = 2
0.00.278.438 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.278.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.278.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.627 I main: llama threadpool init, n_threads = 4
0.00.362.674 I 
0.00.362.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.717 I 
0.00.362.784 I sampler seed: 1234
0.00.362.789 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.362.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.362.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.362.815 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.197.029 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.02.197.030 I llama_perf_context_print:        load time =     298.86 ms
0.02.197.033 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.16 tokens per second)
0.02.197.033 I llama_perf_context_print:        eval time =    1787.60 ms /    63 runs   (   28.37 ms per token,    35.24 tokens per second)
0.02.197.034 I llama_perf_context_print:       total time =    1834.40 ms /    70 tokens
0.02.197.271 I ggml_metal_free: deallocating

real	0m2.498s
user	0m0.150s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.858 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.020 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.071 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.084 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.087 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.091 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.092 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.092 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.095 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.096 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.172 I llama_model_loader: - type  f32:  194 tensors
0.00.056.172 I llama_model_loader: - type  f16:   98 tensors
0.00.056.173 I print_info: file format = GGUF V3 (latest)
0.00.056.174 I print_info: file type   = all F32 (guessed)
0.00.056.177 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.988 I load: special tokens cache size = 25
0.00.089.771 I load: token to piece cache size = 0.2984 MB
0.00.089.775 I print_info: arch             = gptneox
0.00.089.776 I print_info: vocab_only       = 0
0.00.089.776 I print_info: n_ctx_train      = 2048
0.00.089.776 I print_info: n_embd           = 2048
0.00.089.776 I print_info: n_layer          = 24
0.00.089.779 I print_info: n_head           = 16
0.00.089.780 I print_info: n_head_kv        = 16
0.00.089.780 I print_info: n_rot            = 32
0.00.089.780 I print_info: n_swa            = 0
0.00.089.780 I print_info: n_embd_head_k    = 128
0.00.089.781 I print_info: n_embd_head_v    = 128
0.00.089.781 I print_info: n_gqa            = 1
0.00.089.782 I print_info: n_embd_k_gqa     = 2048
0.00.089.782 I print_info: n_embd_v_gqa     = 2048
0.00.089.783 I print_info: f_norm_eps       = 1.0e-05
0.00.089.783 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.783 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.784 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.784 I print_info: f_logit_scale    = 0.0e+00
0.00.089.784 I print_info: n_ff             = 8192
0.00.089.785 I print_info: n_expert         = 0
0.00.089.785 I print_info: n_expert_used    = 0
0.00.089.785 I print_info: causal attn      = 1
0.00.089.785 I print_info: pooling type     = 0
0.00.089.786 I print_info: rope type        = 2
0.00.089.786 I print_info: rope scaling     = linear
0.00.089.786 I print_info: freq_base_train  = 10000.0
0.00.089.787 I print_info: freq_scale_train = 1
0.00.089.787 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.787 I print_info: rope_finetuned   = unknown
0.00.089.787 I print_info: ssm_d_conv       = 0
0.00.089.787 I print_info: ssm_d_inner      = 0
0.00.089.787 I print_info: ssm_d_state      = 0
0.00.089.788 I print_info: ssm_dt_rank      = 0
0.00.089.788 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.788 I print_info: model type       = 1.4B
0.00.089.788 I print_info: model params     = 1.41 B
0.00.089.788 I print_info: general.name     = 1.4B
0.00.089.789 I print_info: vocab type       = BPE
0.00.089.789 I print_info: n_vocab          = 50304
0.00.089.789 I print_info: n_merges         = 50009
0.00.089.789 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.790 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.790 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.792 I print_info: LF token         = 128 'Ä'
0.00.089.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.792 I print_info: max token length = 1024
0.00.092.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.354 I load_tensors: offloading output layer to GPU
0.00.092.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.364 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.365 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.663 I llama_init_from_model: n_seq_max     = 1
0.00.092.663 I llama_init_from_model: n_ctx         = 128
0.00.092.664 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.664 I llama_init_from_model: n_batch       = 128
0.00.092.664 I llama_init_from_model: n_ubatch      = 128
0.00.092.664 I llama_init_from_model: flash_attn    = 0
0.00.092.664 I llama_init_from_model: freq_base     = 10000.0
0.00.092.665 I llama_init_from_model: freq_scale    = 1
0.00.092.665 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.665 I ggml_metal_init: allocating
0.00.092.668 I ggml_metal_init: found device: Apple M4
0.00.092.670 I ggml_metal_init: picking default device: Apple M4
0.00.093.297 I ggml_metal_init: using embedded metal library
0.00.095.786 I ggml_metal_init: GPU name:   Apple M4
0.00.095.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.789 I ggml_metal_init: simdgroup reduction   = true
0.00.095.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.789 I ggml_metal_init: has bfloat            = true
0.00.095.789 I ggml_metal_init: use bfloat            = true
0.00.095.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.080 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.083 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.097 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.882 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.106.883 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.106.883 I llama_init_from_model: graph nodes  = 967
0.00.106.883 I llama_init_from_model: graph splits = 2
0.00.106.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.006.153 I 
0.01.006.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.006.198 I perplexity: tokenizing the input ..
0.01.018.539 I perplexity: tokenization took 12.338 ms
0.01.018.547 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.140.001 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.141.902 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.141.960 I llama_perf_context_print:        load time =     982.12 ms
0.01.141.962 I llama_perf_context_print: prompt eval time =     121.07 ms /   128 tokens (    0.95 ms per token,  1057.22 tokens per second)
0.01.141.963 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.141.964 I llama_perf_context_print:       total time =     135.81 ms /   129 tokens
0.01.142.682 I ggml_metal_free: deallocating

real	0m1.329s
user	0m0.121s
sys	0m0.187s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.195 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.313 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.319 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.319 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.852 I llama_model_loader: - type  f32:  194 tensors
0.00.028.852 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.853 I print_info: file format = GGUF V3 (latest)
0.00.028.854 I print_info: file type   = Q8_0
0.00.028.855 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.681 I load: special tokens cache size = 25
0.00.054.930 I load: token to piece cache size = 0.2984 MB
0.00.054.935 I print_info: arch             = gptneox
0.00.054.935 I print_info: vocab_only       = 0
0.00.054.936 I print_info: n_ctx_train      = 2048
0.00.054.936 I print_info: n_embd           = 2048
0.00.054.936 I print_info: n_layer          = 24
0.00.054.941 I print_info: n_head           = 16
0.00.054.942 I print_info: n_head_kv        = 16
0.00.054.942 I print_info: n_rot            = 32
0.00.054.944 I print_info: n_swa            = 0
0.00.054.944 I print_info: n_embd_head_k    = 128
0.00.054.944 I print_info: n_embd_head_v    = 128
0.00.054.945 I print_info: n_gqa            = 1
0.00.054.946 I print_info: n_embd_k_gqa     = 2048
0.00.054.947 I print_info: n_embd_v_gqa     = 2048
0.00.054.947 I print_info: f_norm_eps       = 1.0e-05
0.00.054.948 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.948 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.949 I print_info: f_logit_scale    = 0.0e+00
0.00.054.950 I print_info: n_ff             = 8192
0.00.054.950 I print_info: n_expert         = 0
0.00.054.950 I print_info: n_expert_used    = 0
0.00.054.950 I print_info: causal attn      = 1
0.00.054.951 I print_info: pooling type     = 0
0.00.054.953 I print_info: rope type        = 2
0.00.054.953 I print_info: rope scaling     = linear
0.00.054.954 I print_info: freq_base_train  = 10000.0
0.00.054.954 I print_info: freq_scale_train = 1
0.00.054.954 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.955 I print_info: rope_finetuned   = unknown
0.00.054.955 I print_info: ssm_d_conv       = 0
0.00.054.955 I print_info: ssm_d_inner      = 0
0.00.054.955 I print_info: ssm_d_state      = 0
0.00.054.955 I print_info: ssm_dt_rank      = 0
0.00.054.955 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.956 I print_info: model type       = 1.4B
0.00.054.956 I print_info: model params     = 1.41 B
0.00.054.956 I print_info: general.name     = 1.4B
0.00.054.957 I print_info: vocab type       = BPE
0.00.054.957 I print_info: n_vocab          = 50304
0.00.054.958 I print_info: n_merges         = 50009
0.00.054.958 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.958 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.958 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.958 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.959 I print_info: LF token         = 128 'Ä'
0.00.054.959 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.959 I print_info: max token length = 1024
0.00.057.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.039 I load_tensors: offloading output layer to GPU
0.00.057.039 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.046 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.047 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.057.376 I llama_init_from_model: n_seq_max     = 1
0.00.057.377 I llama_init_from_model: n_ctx         = 2048
0.00.057.377 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.377 I llama_init_from_model: n_batch       = 2048
0.00.057.377 I llama_init_from_model: n_ubatch      = 512
0.00.057.377 I llama_init_from_model: flash_attn    = 0
0.00.057.378 I llama_init_from_model: freq_base     = 10000.0
0.00.057.378 I llama_init_from_model: freq_scale    = 1
0.00.057.379 I ggml_metal_init: allocating
0.00.057.382 I ggml_metal_init: found device: Apple M4
0.00.057.384 I ggml_metal_init: picking default device: Apple M4
0.00.058.137 I ggml_metal_init: using embedded metal library
0.00.060.732 I ggml_metal_init: GPU name:   Apple M4
0.00.060.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.734 I ggml_metal_init: simdgroup reduction   = true
0.00.060.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.735 I ggml_metal_init: has bfloat            = true
0.00.060.735 I ggml_metal_init: use bfloat            = true
0.00.060.735 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.988 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.569 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.593 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.913 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.914 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.915 I llama_init_from_model: graph nodes  = 967
0.00.095.915 I llama_init_from_model: graph splits = 2
0.00.095.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.673.376 I main: llama threadpool init, n_threads = 4
0.01.673.423 I 
0.01.673.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.673.455 I 
0.01.673.617 I sampler seed: 1234
0.01.673.621 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.673.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.673.659 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.673.659 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.767.618 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50390.35 tokens per second)
0.02.767.619 I llama_perf_context_print:        load time =    1663.18 ms
0.02.767.620 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.74 tokens per second)
0.02.767.621 I llama_perf_context_print:        eval time =    1051.46 ms /    63 runs   (   16.69 ms per token,    59.92 tokens per second)
0.02.767.621 I llama_perf_context_print:       total time =    1094.24 ms /    70 tokens
0.02.767.888 I ggml_metal_free: deallocating

real	0m2.788s
user	0m0.112s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.217 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.069 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.079 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.715 I llama_model_loader: - type  f32:  194 tensors
0.00.034.716 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.716 I print_info: file format = GGUF V3 (latest)
0.00.034.717 I print_info: file type   = Q8_0
0.00.034.718 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.347 I load: special tokens cache size = 25
0.00.063.449 I load: token to piece cache size = 0.2984 MB
0.00.063.452 I print_info: arch             = gptneox
0.00.063.453 I print_info: vocab_only       = 0
0.00.063.453 I print_info: n_ctx_train      = 2048
0.00.063.453 I print_info: n_embd           = 2048
0.00.063.453 I print_info: n_layer          = 24
0.00.063.457 I print_info: n_head           = 16
0.00.063.458 I print_info: n_head_kv        = 16
0.00.063.458 I print_info: n_rot            = 32
0.00.063.458 I print_info: n_swa            = 0
0.00.063.458 I print_info: n_embd_head_k    = 128
0.00.063.460 I print_info: n_embd_head_v    = 128
0.00.063.460 I print_info: n_gqa            = 1
0.00.063.461 I print_info: n_embd_k_gqa     = 2048
0.00.063.462 I print_info: n_embd_v_gqa     = 2048
0.00.063.462 I print_info: f_norm_eps       = 1.0e-05
0.00.063.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.463 I print_info: f_logit_scale    = 0.0e+00
0.00.063.464 I print_info: n_ff             = 8192
0.00.063.465 I print_info: n_expert         = 0
0.00.063.466 I print_info: n_expert_used    = 0
0.00.063.466 I print_info: causal attn      = 1
0.00.063.466 I print_info: pooling type     = 0
0.00.063.466 I print_info: rope type        = 2
0.00.063.466 I print_info: rope scaling     = linear
0.00.063.467 I print_info: freq_base_train  = 10000.0
0.00.063.468 I print_info: freq_scale_train = 1
0.00.063.468 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.468 I print_info: rope_finetuned   = unknown
0.00.063.469 I print_info: ssm_d_conv       = 0
0.00.063.469 I print_info: ssm_d_inner      = 0
0.00.063.469 I print_info: ssm_d_state      = 0
0.00.063.469 I print_info: ssm_dt_rank      = 0
0.00.063.469 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.470 I print_info: model type       = 1.4B
0.00.063.470 I print_info: model params     = 1.41 B
0.00.063.470 I print_info: general.name     = 1.4B
0.00.063.470 I print_info: vocab type       = BPE
0.00.063.471 I print_info: n_vocab          = 50304
0.00.063.471 I print_info: n_merges         = 50009
0.00.063.475 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.475 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.475 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.476 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.476 I print_info: LF token         = 128 'Ä'
0.00.063.476 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.476 I print_info: max token length = 1024
0.00.065.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.726 I load_tensors: offloading output layer to GPU
0.00.065.726 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.737 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.738 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.027 I llama_init_from_model: n_seq_max     = 1
0.00.066.028 I llama_init_from_model: n_ctx         = 128
0.00.066.028 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.029 I llama_init_from_model: n_batch       = 128
0.00.066.029 I llama_init_from_model: n_ubatch      = 128
0.00.066.029 I llama_init_from_model: flash_attn    = 0
0.00.066.029 I llama_init_from_model: freq_base     = 10000.0
0.00.066.029 I llama_init_from_model: freq_scale    = 1
0.00.066.030 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.030 I ggml_metal_init: allocating
0.00.066.033 I ggml_metal_init: found device: Apple M4
0.00.066.035 I ggml_metal_init: picking default device: Apple M4
0.00.066.653 I ggml_metal_init: using embedded metal library
0.00.069.206 I ggml_metal_init: GPU name:   Apple M4
0.00.069.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.209 I ggml_metal_init: simdgroup reduction   = true
0.00.069.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.209 I ggml_metal_init: has bfloat            = true
0.00.069.209 I ggml_metal_init: use bfloat            = true
0.00.069.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.952 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.954 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.968 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.921 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.922 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.923 I llama_init_from_model: graph nodes  = 967
0.00.080.923 I llama_init_from_model: graph splits = 2
0.00.080.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.914.467 I 
0.00.914.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.914.560 I perplexity: tokenizing the input ..
0.00.922.515 I perplexity: tokenization took 7.953 ms
0.00.922.518 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.046.648 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.047.828 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.047.855 I llama_perf_context_print:        load time =     901.24 ms
0.01.047.857 I llama_perf_context_print: prompt eval time =     123.90 ms /   128 tokens (    0.97 ms per token,  1033.06 tokens per second)
0.01.047.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.047.858 I llama_perf_context_print:       total time =     133.39 ms /   129 tokens
0.01.048.338 I ggml_metal_free: deallocating

real	0m1.067s
user	0m0.089s
sys	0m0.149s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.016.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.374 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.988 I llama_model_loader: - type  f32:  194 tensors
0.00.044.988 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.988 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.989 I print_info: file format = GGUF V3 (latest)
0.00.044.989 I print_info: file type   = Q4_0
0.00.044.990 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.069.204 I load: special tokens cache size = 25
0.00.077.004 I load: token to piece cache size = 0.2984 MB
0.00.077.008 I print_info: arch             = gptneox
0.00.077.008 I print_info: vocab_only       = 0
0.00.077.009 I print_info: n_ctx_train      = 2048
0.00.077.009 I print_info: n_embd           = 2048
0.00.077.009 I print_info: n_layer          = 24
0.00.077.013 I print_info: n_head           = 16
0.00.077.014 I print_info: n_head_kv        = 16
0.00.077.014 I print_info: n_rot            = 32
0.00.077.014 I print_info: n_swa            = 0
0.00.077.015 I print_info: n_embd_head_k    = 128
0.00.077.015 I print_info: n_embd_head_v    = 128
0.00.077.016 I print_info: n_gqa            = 1
0.00.077.016 I print_info: n_embd_k_gqa     = 2048
0.00.077.017 I print_info: n_embd_v_gqa     = 2048
0.00.077.018 I print_info: f_norm_eps       = 1.0e-05
0.00.077.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.021 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.021 I print_info: f_logit_scale    = 0.0e+00
0.00.077.022 I print_info: n_ff             = 8192
0.00.077.023 I print_info: n_expert         = 0
0.00.077.023 I print_info: n_expert_used    = 0
0.00.077.023 I print_info: causal attn      = 1
0.00.077.024 I print_info: pooling type     = 0
0.00.077.026 I print_info: rope type        = 2
0.00.077.026 I print_info: rope scaling     = linear
0.00.077.026 I print_info: freq_base_train  = 10000.0
0.00.077.027 I print_info: freq_scale_train = 1
0.00.077.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.031 I print_info: rope_finetuned   = unknown
0.00.077.031 I print_info: ssm_d_conv       = 0
0.00.077.031 I print_info: ssm_d_inner      = 0
0.00.077.031 I print_info: ssm_d_state      = 0
0.00.077.032 I print_info: ssm_dt_rank      = 0
0.00.077.032 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.032 I print_info: model type       = 1.4B
0.00.077.033 I print_info: model params     = 1.41 B
0.00.077.033 I print_info: general.name     = 1.4B
0.00.077.034 I print_info: vocab type       = BPE
0.00.077.034 I print_info: n_vocab          = 50304
0.00.077.035 I print_info: n_merges         = 50009
0.00.077.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.036 I print_info: LF token         = 128 'Ä'
0.00.077.037 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.037 I print_info: max token length = 1024
0.00.079.517 I load_tensors: offloading 24 repeating layers to GPU
0.00.079.517 I load_tensors: offloading output layer to GPU
0.00.079.518 I load_tensors: offloaded 25/25 layers to GPU
0.00.079.530 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.531 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.079.951 I llama_init_from_model: n_seq_max     = 1
0.00.079.952 I llama_init_from_model: n_ctx         = 2048
0.00.079.952 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.952 I llama_init_from_model: n_batch       = 2048
0.00.079.953 I llama_init_from_model: n_ubatch      = 512
0.00.079.953 I llama_init_from_model: flash_attn    = 0
0.00.079.953 I llama_init_from_model: freq_base     = 10000.0
0.00.079.954 I llama_init_from_model: freq_scale    = 1
0.00.079.954 I ggml_metal_init: allocating
0.00.079.958 I ggml_metal_init: found device: Apple M4
0.00.079.961 I ggml_metal_init: picking default device: Apple M4
0.00.080.942 I ggml_metal_init: using embedded metal library
0.00.084.821 I ggml_metal_init: GPU name:   Apple M4
0.00.084.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.825 I ggml_metal_init: simdgroup reduction   = true
0.00.084.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.825 I ggml_metal_init: has bfloat            = true
0.00.084.825 I ggml_metal_init: use bfloat            = true
0.00.084.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.912 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.124.427 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.442 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.471 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.125.563 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.125.565 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.125.566 I llama_init_from_model: graph nodes  = 967
0.00.125.566 I llama_init_from_model: graph splits = 2
0.00.125.571 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.840 I main: llama threadpool init, n_threads = 4
0.00.844.882 I 
0.00.844.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.916 I 
0.00.845.154 I sampler seed: 1234
0.00.845.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.845.169 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.845.170 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.845.170 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.517.993 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.517.994 I llama_perf_context_print:        load time =     828.58 ms
0.01.517.994 I llama_perf_context_print: prompt eval time =      42.97 ms /     7 tokens (    6.14 ms per token,   162.89 tokens per second)
0.01.517.995 I llama_perf_context_print:        eval time =     626.88 ms /    63 runs   (    9.95 ms per token,   100.50 tokens per second)
0.01.517.995 I llama_perf_context_print:       total time =     673.16 ms /    70 tokens
0.01.518.189 I ggml_metal_free: deallocating

real	0m1.536s
user	0m0.122s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.036 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.747 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.760 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.760 I llama_model_loader: - type  f32:  194 tensors
0.00.025.761 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.761 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.762 I print_info: file format = GGUF V3 (latest)
0.00.025.762 I print_info: file type   = Q4_0
0.00.025.766 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.001 I load: special tokens cache size = 25
0.00.049.997 I load: token to piece cache size = 0.2984 MB
0.00.050.000 I print_info: arch             = gptneox
0.00.050.000 I print_info: vocab_only       = 0
0.00.050.000 I print_info: n_ctx_train      = 2048
0.00.050.001 I print_info: n_embd           = 2048
0.00.050.001 I print_info: n_layer          = 24
0.00.050.003 I print_info: n_head           = 16
0.00.050.004 I print_info: n_head_kv        = 16
0.00.050.004 I print_info: n_rot            = 32
0.00.050.004 I print_info: n_swa            = 0
0.00.050.005 I print_info: n_embd_head_k    = 128
0.00.050.006 I print_info: n_embd_head_v    = 128
0.00.050.006 I print_info: n_gqa            = 1
0.00.050.007 I print_info: n_embd_k_gqa     = 2048
0.00.050.008 I print_info: n_embd_v_gqa     = 2048
0.00.050.008 I print_info: f_norm_eps       = 1.0e-05
0.00.050.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.009 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.009 I print_info: f_logit_scale    = 0.0e+00
0.00.050.010 I print_info: n_ff             = 8192
0.00.050.010 I print_info: n_expert         = 0
0.00.050.010 I print_info: n_expert_used    = 0
0.00.050.010 I print_info: causal attn      = 1
0.00.050.011 I print_info: pooling type     = 0
0.00.050.011 I print_info: rope type        = 2
0.00.050.013 I print_info: rope scaling     = linear
0.00.050.013 I print_info: freq_base_train  = 10000.0
0.00.050.013 I print_info: freq_scale_train = 1
0.00.050.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.014 I print_info: rope_finetuned   = unknown
0.00.050.014 I print_info: ssm_d_conv       = 0
0.00.050.014 I print_info: ssm_d_inner      = 0
0.00.050.014 I print_info: ssm_d_state      = 0
0.00.050.014 I print_info: ssm_dt_rank      = 0
0.00.050.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.015 I print_info: model type       = 1.4B
0.00.050.015 I print_info: model params     = 1.41 B
0.00.050.015 I print_info: general.name     = 1.4B
0.00.050.016 I print_info: vocab type       = BPE
0.00.050.016 I print_info: n_vocab          = 50304
0.00.050.016 I print_info: n_merges         = 50009
0.00.050.016 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.019 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.020 I print_info: LF token         = 128 'Ä'
0.00.050.020 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.020 I print_info: max token length = 1024
0.00.051.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.893 I load_tensors: offloading output layer to GPU
0.00.051.893 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.904 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.905 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.234 I llama_init_from_model: n_seq_max     = 1
0.00.052.235 I llama_init_from_model: n_ctx         = 128
0.00.052.235 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.235 I llama_init_from_model: n_batch       = 128
0.00.052.235 I llama_init_from_model: n_ubatch      = 128
0.00.052.236 I llama_init_from_model: flash_attn    = 0
0.00.052.236 I llama_init_from_model: freq_base     = 10000.0
0.00.052.236 I llama_init_from_model: freq_scale    = 1
0.00.052.236 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.237 I ggml_metal_init: allocating
0.00.052.239 I ggml_metal_init: found device: Apple M4
0.00.052.241 I ggml_metal_init: picking default device: Apple M4
0.00.052.800 I ggml_metal_init: using embedded metal library
0.00.055.142 I ggml_metal_init: GPU name:   Apple M4
0.00.055.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.145 I ggml_metal_init: simdgroup reduction   = true
0.00.055.145 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.145 I ggml_metal_init: has bfloat            = true
0.00.055.145 I ggml_metal_init: use bfloat            = true
0.00.055.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.587 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.840 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.844 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.867 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.809 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.810 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.810 I llama_init_from_model: graph nodes  = 967
0.00.066.811 I llama_init_from_model: graph splits = 2
0.00.066.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.201 I 
0.00.609.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.238 I perplexity: tokenizing the input ..
0.00.617.498 I perplexity: tokenization took 8.259 ms
0.00.617.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.125 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.741.286 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.319 I llama_perf_context_print:        load time =     598.16 ms
0.00.741.320 I llama_perf_context_print: prompt eval time =     122.39 ms /   128 tokens (    0.96 ms per token,  1045.86 tokens per second)
0.00.741.321 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.321 I llama_perf_context_print:       total time =     132.12 ms /   129 tokens
0.00.741.841 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.075s
sys	0m0.091s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.896 I llama_model_loader: - type  f32:  194 tensors
0.00.024.896 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.896 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.897 I print_info: file format = GGUF V3 (latest)
0.00.024.897 I print_info: file type   = Q4_1
0.00.024.898 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.315 I load: special tokens cache size = 25
0.00.049.283 I load: token to piece cache size = 0.2984 MB
0.00.049.286 I print_info: arch             = gptneox
0.00.049.286 I print_info: vocab_only       = 0
0.00.049.286 I print_info: n_ctx_train      = 2048
0.00.049.287 I print_info: n_embd           = 2048
0.00.049.287 I print_info: n_layer          = 24
0.00.049.290 I print_info: n_head           = 16
0.00.049.290 I print_info: n_head_kv        = 16
0.00.049.291 I print_info: n_rot            = 32
0.00.049.291 I print_info: n_swa            = 0
0.00.049.291 I print_info: n_embd_head_k    = 128
0.00.049.291 I print_info: n_embd_head_v    = 128
0.00.049.292 I print_info: n_gqa            = 1
0.00.049.293 I print_info: n_embd_k_gqa     = 2048
0.00.049.294 I print_info: n_embd_v_gqa     = 2048
0.00.049.295 I print_info: f_norm_eps       = 1.0e-05
0.00.049.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.297 I print_info: f_logit_scale    = 0.0e+00
0.00.049.298 I print_info: n_ff             = 8192
0.00.049.298 I print_info: n_expert         = 0
0.00.049.298 I print_info: n_expert_used    = 0
0.00.049.299 I print_info: causal attn      = 1
0.00.049.299 I print_info: pooling type     = 0
0.00.049.299 I print_info: rope type        = 2
0.00.049.301 I print_info: rope scaling     = linear
0.00.049.301 I print_info: freq_base_train  = 10000.0
0.00.049.302 I print_info: freq_scale_train = 1
0.00.049.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.302 I print_info: rope_finetuned   = unknown
0.00.049.302 I print_info: ssm_d_conv       = 0
0.00.049.302 I print_info: ssm_d_inner      = 0
0.00.049.302 I print_info: ssm_d_state      = 0
0.00.049.303 I print_info: ssm_dt_rank      = 0
0.00.049.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.303 I print_info: model type       = 1.4B
0.00.049.303 I print_info: model params     = 1.41 B
0.00.049.303 I print_info: general.name     = 1.4B
0.00.049.304 I print_info: vocab type       = BPE
0.00.049.304 I print_info: n_vocab          = 50304
0.00.049.304 I print_info: n_merges         = 50009
0.00.049.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.305 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.309 I print_info: LF token         = 128 'Ä'
0.00.049.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.309 I print_info: max token length = 1024
0.00.050.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.863 I load_tensors: offloading output layer to GPU
0.00.050.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.874 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.875 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.155 I llama_init_from_model: n_seq_max     = 1
0.00.051.156 I llama_init_from_model: n_ctx         = 2048
0.00.051.156 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.156 I llama_init_from_model: n_batch       = 2048
0.00.051.156 I llama_init_from_model: n_ubatch      = 512
0.00.051.156 I llama_init_from_model: flash_attn    = 0
0.00.051.157 I llama_init_from_model: freq_base     = 10000.0
0.00.051.157 I llama_init_from_model: freq_scale    = 1
0.00.051.157 I ggml_metal_init: allocating
0.00.051.160 I ggml_metal_init: found device: Apple M4
0.00.051.162 I ggml_metal_init: picking default device: Apple M4
0.00.051.760 I ggml_metal_init: using embedded metal library
0.00.054.107 I ggml_metal_init: GPU name:   Apple M4
0.00.054.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.110 I ggml_metal_init: simdgroup reduction   = true
0.00.054.110 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.110 I ggml_metal_init: has bfloat            = true
0.00.054.110 I ggml_metal_init: use bfloat            = true
0.00.054.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.573 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.529 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.539 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.544 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.546 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.546 I llama_init_from_model: graph nodes  = 967
0.00.084.546 I llama_init_from_model: graph splits = 2
0.00.084.549 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.668 I main: llama threadpool init, n_threads = 4
0.00.720.712 I 
0.00.720.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.741 I 
0.00.720.969 I sampler seed: 1234
0.00.720.974 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.017 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.017 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.017 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.441.056 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.441.057 I llama_perf_context_print:        load time =     711.93 ms
0.01.441.057 I llama_perf_context_print: prompt eval time =      44.25 ms /     7 tokens (    6.32 ms per token,   158.18 tokens per second)
0.01.441.058 I llama_perf_context_print:        eval time =     672.79 ms /    63 runs   (   10.68 ms per token,    93.64 tokens per second)
0.01.441.058 I llama_perf_context_print:       total time =     720.39 ms /    70 tokens
0.01.441.288 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.107s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.621 I llama_model_loader: - type  f32:  194 tensors
0.00.023.621 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.621 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.622 I print_info: file format = GGUF V3 (latest)
0.00.023.622 I print_info: file type   = Q4_1
0.00.023.623 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.917 I load: special tokens cache size = 25
0.00.047.986 I load: token to piece cache size = 0.2984 MB
0.00.047.989 I print_info: arch             = gptneox
0.00.047.989 I print_info: vocab_only       = 0
0.00.047.989 I print_info: n_ctx_train      = 2048
0.00.047.990 I print_info: n_embd           = 2048
0.00.047.990 I print_info: n_layer          = 24
0.00.047.993 I print_info: n_head           = 16
0.00.047.993 I print_info: n_head_kv        = 16
0.00.047.994 I print_info: n_rot            = 32
0.00.047.995 I print_info: n_swa            = 0
0.00.047.995 I print_info: n_embd_head_k    = 128
0.00.047.995 I print_info: n_embd_head_v    = 128
0.00.047.996 I print_info: n_gqa            = 1
0.00.047.998 I print_info: n_embd_k_gqa     = 2048
0.00.047.999 I print_info: n_embd_v_gqa     = 2048
0.00.048.000 I print_info: f_norm_eps       = 1.0e-05
0.00.048.000 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.000 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.000 I print_info: f_logit_scale    = 0.0e+00
0.00.048.002 I print_info: n_ff             = 8192
0.00.048.003 I print_info: n_expert         = 0
0.00.048.003 I print_info: n_expert_used    = 0
0.00.048.003 I print_info: causal attn      = 1
0.00.048.003 I print_info: pooling type     = 0
0.00.048.003 I print_info: rope type        = 2
0.00.048.003 I print_info: rope scaling     = linear
0.00.048.004 I print_info: freq_base_train  = 10000.0
0.00.048.004 I print_info: freq_scale_train = 1
0.00.048.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.004 I print_info: rope_finetuned   = unknown
0.00.048.005 I print_info: ssm_d_conv       = 0
0.00.048.005 I print_info: ssm_d_inner      = 0
0.00.048.005 I print_info: ssm_d_state      = 0
0.00.048.005 I print_info: ssm_dt_rank      = 0
0.00.048.005 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.006 I print_info: model type       = 1.4B
0.00.048.006 I print_info: model params     = 1.41 B
0.00.048.006 I print_info: general.name     = 1.4B
0.00.048.007 I print_info: vocab type       = BPE
0.00.048.007 I print_info: n_vocab          = 50304
0.00.048.007 I print_info: n_merges         = 50009
0.00.048.007 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.008 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.008 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.008 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: LF token         = 128 'Ä'
0.00.048.009 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: max token length = 1024
0.00.049.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.920 I load_tensors: offloading output layer to GPU
0.00.049.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.930 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.049.932 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.050.244 I llama_init_from_model: n_seq_max     = 1
0.00.050.244 I llama_init_from_model: n_ctx         = 128
0.00.050.245 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.245 I llama_init_from_model: n_batch       = 128
0.00.050.245 I llama_init_from_model: n_ubatch      = 128
0.00.050.245 I llama_init_from_model: flash_attn    = 0
0.00.050.245 I llama_init_from_model: freq_base     = 10000.0
0.00.050.246 I llama_init_from_model: freq_scale    = 1
0.00.050.246 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.246 I ggml_metal_init: allocating
0.00.050.248 I ggml_metal_init: found device: Apple M4
0.00.050.250 I ggml_metal_init: picking default device: Apple M4
0.00.050.809 I ggml_metal_init: using embedded metal library
0.00.053.132 I ggml_metal_init: GPU name:   Apple M4
0.00.053.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.135 I ggml_metal_init: simdgroup reduction   = true
0.00.053.135 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.135 I ggml_metal_init: has bfloat            = true
0.00.053.135 I ggml_metal_init: use bfloat            = true
0.00.053.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.377 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.737 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.753 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.637 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.638 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.638 I llama_init_from_model: graph nodes  = 967
0.00.064.638 I llama_init_from_model: graph splits = 2
0.00.064.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.726 I 
0.00.671.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.782 I perplexity: tokenizing the input ..
0.00.679.214 I perplexity: tokenization took 7.429 ms
0.00.679.218 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.036 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.802.379 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.802.400 I llama_perf_context_print:        load time =     662.92 ms
0.00.802.401 I llama_perf_context_print: prompt eval time =     121.57 ms /   128 tokens (    0.95 ms per token,  1052.86 tokens per second)
0.00.802.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.402 I llama_perf_context_print:       total time =     130.68 ms /   129 tokens
0.00.802.736 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.076s
sys	0m0.103s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.602 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.054 I llama_model_loader: - type  f32:  194 tensors
0.00.026.055 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.056 I print_info: file format = GGUF V3 (latest)
0.00.026.056 I print_info: file type   = Q5_0
0.00.026.057 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.370 I load: special tokens cache size = 25
0.00.050.371 I load: token to piece cache size = 0.2984 MB
0.00.050.374 I print_info: arch             = gptneox
0.00.050.374 I print_info: vocab_only       = 0
0.00.050.374 I print_info: n_ctx_train      = 2048
0.00.050.375 I print_info: n_embd           = 2048
0.00.050.375 I print_info: n_layer          = 24
0.00.050.378 I print_info: n_head           = 16
0.00.050.379 I print_info: n_head_kv        = 16
0.00.050.379 I print_info: n_rot            = 32
0.00.050.379 I print_info: n_swa            = 0
0.00.050.379 I print_info: n_embd_head_k    = 128
0.00.050.379 I print_info: n_embd_head_v    = 128
0.00.050.380 I print_info: n_gqa            = 1
0.00.050.381 I print_info: n_embd_k_gqa     = 2048
0.00.050.382 I print_info: n_embd_v_gqa     = 2048
0.00.050.382 I print_info: f_norm_eps       = 1.0e-05
0.00.050.383 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.383 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.383 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.383 I print_info: f_logit_scale    = 0.0e+00
0.00.050.384 I print_info: n_ff             = 8192
0.00.050.384 I print_info: n_expert         = 0
0.00.050.384 I print_info: n_expert_used    = 0
0.00.050.385 I print_info: causal attn      = 1
0.00.050.385 I print_info: pooling type     = 0
0.00.050.386 I print_info: rope type        = 2
0.00.050.388 I print_info: rope scaling     = linear
0.00.050.388 I print_info: freq_base_train  = 10000.0
0.00.050.389 I print_info: freq_scale_train = 1
0.00.050.389 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.389 I print_info: rope_finetuned   = unknown
0.00.050.389 I print_info: ssm_d_conv       = 0
0.00.050.389 I print_info: ssm_d_inner      = 0
0.00.050.389 I print_info: ssm_d_state      = 0
0.00.050.389 I print_info: ssm_dt_rank      = 0
0.00.050.390 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.390 I print_info: model type       = 1.4B
0.00.050.390 I print_info: model params     = 1.41 B
0.00.050.390 I print_info: general.name     = 1.4B
0.00.050.391 I print_info: vocab type       = BPE
0.00.050.391 I print_info: n_vocab          = 50304
0.00.050.391 I print_info: n_merges         = 50009
0.00.050.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.391 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.392 I print_info: LF token         = 128 'Ä'
0.00.050.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.393 I print_info: max token length = 1024
0.00.052.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.160 I load_tensors: offloading output layer to GPU
0.00.052.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.166 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.166 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.438 I llama_init_from_model: n_seq_max     = 1
0.00.052.438 I llama_init_from_model: n_ctx         = 2048
0.00.052.438 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.439 I llama_init_from_model: n_batch       = 2048
0.00.052.439 I llama_init_from_model: n_ubatch      = 512
0.00.052.439 I llama_init_from_model: flash_attn    = 0
0.00.052.439 I llama_init_from_model: freq_base     = 10000.0
0.00.052.440 I llama_init_from_model: freq_scale    = 1
0.00.052.440 I ggml_metal_init: allocating
0.00.052.443 I ggml_metal_init: found device: Apple M4
0.00.052.445 I ggml_metal_init: picking default device: Apple M4
0.00.053.029 I ggml_metal_init: using embedded metal library
0.00.055.344 I ggml_metal_init: GPU name:   Apple M4
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.347 I ggml_metal_init: simdgroup reduction   = true
0.00.055.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.347 I ggml_metal_init: has bfloat            = true
0.00.055.347 I ggml_metal_init: use bfloat            = true
0.00.055.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.851 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.436 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.441 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.461 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.661 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.663 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.663 I llama_init_from_model: graph nodes  = 967
0.00.085.663 I llama_init_from_model: graph splits = 2
0.00.085.669 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.407 I main: llama threadpool init, n_threads = 4
0.00.753.447 I 
0.00.753.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.481 I 
0.00.753.704 I sampler seed: 1234
0.00.753.708 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.719 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.719 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.719 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.542.993 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.542.993 I llama_perf_context_print:        load time =     743.46 ms
0.01.542.994 I llama_perf_context_print: prompt eval time =      49.90 ms /     7 tokens (    7.13 ms per token,   140.29 tokens per second)
0.01.542.995 I llama_perf_context_print:        eval time =     736.38 ms /    63 runs   (   11.69 ms per token,    85.55 tokens per second)
0.01.542.996 I llama_perf_context_print:       total time =     789.59 ms /    70 tokens
0.01.543.215 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.107s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.820 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.514 I llama_model_loader: - type  f32:  194 tensors
0.00.025.514 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.515 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.516 I print_info: file format = GGUF V3 (latest)
0.00.025.516 I print_info: file type   = Q5_0
0.00.025.522 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.968 I load: special tokens cache size = 25
0.00.052.302 I load: token to piece cache size = 0.2984 MB
0.00.052.308 I print_info: arch             = gptneox
0.00.052.308 I print_info: vocab_only       = 0
0.00.052.309 I print_info: n_ctx_train      = 2048
0.00.052.309 I print_info: n_embd           = 2048
0.00.052.309 I print_info: n_layer          = 24
0.00.052.313 I print_info: n_head           = 16
0.00.052.313 I print_info: n_head_kv        = 16
0.00.052.313 I print_info: n_rot            = 32
0.00.052.313 I print_info: n_swa            = 0
0.00.052.314 I print_info: n_embd_head_k    = 128
0.00.052.314 I print_info: n_embd_head_v    = 128
0.00.052.314 I print_info: n_gqa            = 1
0.00.052.315 I print_info: n_embd_k_gqa     = 2048
0.00.052.316 I print_info: n_embd_v_gqa     = 2048
0.00.052.316 I print_info: f_norm_eps       = 1.0e-05
0.00.052.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.317 I print_info: f_logit_scale    = 0.0e+00
0.00.052.317 I print_info: n_ff             = 8192
0.00.052.318 I print_info: n_expert         = 0
0.00.052.318 I print_info: n_expert_used    = 0
0.00.052.318 I print_info: causal attn      = 1
0.00.052.318 I print_info: pooling type     = 0
0.00.052.318 I print_info: rope type        = 2
0.00.052.318 I print_info: rope scaling     = linear
0.00.052.319 I print_info: freq_base_train  = 10000.0
0.00.052.319 I print_info: freq_scale_train = 1
0.00.052.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.319 I print_info: rope_finetuned   = unknown
0.00.052.319 I print_info: ssm_d_conv       = 0
0.00.052.321 I print_info: ssm_d_inner      = 0
0.00.052.321 I print_info: ssm_d_state      = 0
0.00.052.323 I print_info: ssm_dt_rank      = 0
0.00.052.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.323 I print_info: model type       = 1.4B
0.00.052.323 I print_info: model params     = 1.41 B
0.00.052.324 I print_info: general.name     = 1.4B
0.00.052.324 I print_info: vocab type       = BPE
0.00.052.324 I print_info: n_vocab          = 50304
0.00.052.324 I print_info: n_merges         = 50009
0.00.052.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.325 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.325 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.325 I print_info: LF token         = 128 'Ä'
0.00.052.325 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.326 I print_info: max token length = 1024
0.00.054.292 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.293 I load_tensors: offloading output layer to GPU
0.00.054.294 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.304 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.305 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.622 I llama_init_from_model: n_seq_max     = 1
0.00.054.623 I llama_init_from_model: n_ctx         = 128
0.00.054.623 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.623 I llama_init_from_model: n_batch       = 128
0.00.054.623 I llama_init_from_model: n_ubatch      = 128
0.00.054.623 I llama_init_from_model: flash_attn    = 0
0.00.054.624 I llama_init_from_model: freq_base     = 10000.0
0.00.054.625 I llama_init_from_model: freq_scale    = 1
0.00.054.625 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.626 I ggml_metal_init: allocating
0.00.054.629 I ggml_metal_init: found device: Apple M4
0.00.054.631 I ggml_metal_init: picking default device: Apple M4
0.00.055.268 I ggml_metal_init: using embedded metal library
0.00.057.692 I ggml_metal_init: GPU name:   Apple M4
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.695 I ggml_metal_init: simdgroup reduction   = true
0.00.057.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.695 I ggml_metal_init: has bfloat            = true
0.00.057.695 I ggml_metal_init: use bfloat            = true
0.00.057.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.828 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.164 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.167 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.182 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.156 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.158 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.158 I llama_init_from_model: graph nodes  = 967
0.00.069.158 I llama_init_from_model: graph splits = 2
0.00.069.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.062 I 
0.00.685.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.115 I perplexity: tokenizing the input ..
0.00.692.858 I perplexity: tokenization took 7.742 ms
0.00.692.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.953 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.829.090 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.829.116 I llama_perf_context_print:        load time =     675.24 ms
0.00.829.119 I llama_perf_context_print: prompt eval time =     134.86 ms /   128 tokens (    1.05 ms per token,   949.13 tokens per second)
0.00.829.120 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.120 I llama_perf_context_print:       total time =     144.06 ms /   129 tokens
0.00.829.606 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.079s
sys	0m0.103s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.241 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.245 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.985 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.730 I llama_model_loader: - type  f32:  194 tensors
0.00.024.731 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.732 I print_info: file format = GGUF V3 (latest)
0.00.024.732 I print_info: file type   = Q5_1
0.00.024.733 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.065 I load: special tokens cache size = 25
0.00.050.141 I load: token to piece cache size = 0.2984 MB
0.00.050.145 I print_info: arch             = gptneox
0.00.050.145 I print_info: vocab_only       = 0
0.00.050.145 I print_info: n_ctx_train      = 2048
0.00.050.145 I print_info: n_embd           = 2048
0.00.050.146 I print_info: n_layer          = 24
0.00.050.148 I print_info: n_head           = 16
0.00.050.149 I print_info: n_head_kv        = 16
0.00.050.149 I print_info: n_rot            = 32
0.00.050.149 I print_info: n_swa            = 0
0.00.050.150 I print_info: n_embd_head_k    = 128
0.00.050.150 I print_info: n_embd_head_v    = 128
0.00.050.150 I print_info: n_gqa            = 1
0.00.050.151 I print_info: n_embd_k_gqa     = 2048
0.00.050.153 I print_info: n_embd_v_gqa     = 2048
0.00.050.153 I print_info: f_norm_eps       = 1.0e-05
0.00.050.153 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.155 I print_info: f_logit_scale    = 0.0e+00
0.00.050.156 I print_info: n_ff             = 8192
0.00.050.156 I print_info: n_expert         = 0
0.00.050.156 I print_info: n_expert_used    = 0
0.00.050.156 I print_info: causal attn      = 1
0.00.050.157 I print_info: pooling type     = 0
0.00.050.157 I print_info: rope type        = 2
0.00.050.159 I print_info: rope scaling     = linear
0.00.050.159 I print_info: freq_base_train  = 10000.0
0.00.050.159 I print_info: freq_scale_train = 1
0.00.050.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.160 I print_info: rope_finetuned   = unknown
0.00.050.160 I print_info: ssm_d_conv       = 0
0.00.050.160 I print_info: ssm_d_inner      = 0
0.00.050.160 I print_info: ssm_d_state      = 0
0.00.050.160 I print_info: ssm_dt_rank      = 0
0.00.050.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.161 I print_info: model type       = 1.4B
0.00.050.161 I print_info: model params     = 1.41 B
0.00.050.161 I print_info: general.name     = 1.4B
0.00.050.162 I print_info: vocab type       = BPE
0.00.050.162 I print_info: n_vocab          = 50304
0.00.050.162 I print_info: n_merges         = 50009
0.00.050.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.163 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.163 I print_info: LF token         = 128 'Ä'
0.00.050.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: max token length = 1024
0.00.052.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.218 I load_tensors: offloading output layer to GPU
0.00.052.218 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.229 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.230 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.549 I llama_init_from_model: n_seq_max     = 1
0.00.052.550 I llama_init_from_model: n_ctx         = 2048
0.00.052.550 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.550 I llama_init_from_model: n_batch       = 2048
0.00.052.550 I llama_init_from_model: n_ubatch      = 512
0.00.052.550 I llama_init_from_model: flash_attn    = 0
0.00.052.551 I llama_init_from_model: freq_base     = 10000.0
0.00.052.551 I llama_init_from_model: freq_scale    = 1
0.00.052.551 I ggml_metal_init: allocating
0.00.052.554 I ggml_metal_init: found device: Apple M4
0.00.052.558 I ggml_metal_init: picking default device: Apple M4
0.00.053.160 I ggml_metal_init: using embedded metal library
0.00.055.479 I ggml_metal_init: GPU name:   Apple M4
0.00.055.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.481 I ggml_metal_init: simdgroup reduction   = true
0.00.055.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.481 I ggml_metal_init: has bfloat            = true
0.00.055.482 I ggml_metal_init: use bfloat            = true
0.00.055.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.693 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.708 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.734 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.736 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.736 I llama_init_from_model: graph nodes  = 967
0.00.086.736 I llama_init_from_model: graph splits = 2
0.00.086.739 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.740 I main: llama threadpool init, n_threads = 4
0.00.699.781 I 
0.00.699.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.828 I 
0.00.700.070 I sampler seed: 1234
0.00.700.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.123 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.127 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.539.851 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.539.852 I llama_perf_context_print:        load time =     690.92 ms
0.01.539.853 I llama_perf_context_print: prompt eval time =      42.20 ms /     7 tokens (    6.03 ms per token,   165.90 tokens per second)
0.01.539.854 I llama_perf_context_print:        eval time =     794.68 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.539.854 I llama_perf_context_print:       total time =     840.12 ms /    70 tokens
0.01.540.101 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.517 I llama_model_loader: - type  f32:  194 tensors
0.00.023.517 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.518 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.518 I print_info: file format = GGUF V3 (latest)
0.00.023.519 I print_info: file type   = Q5_1
0.00.023.519 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.874 I load: special tokens cache size = 25
0.00.047.753 I load: token to piece cache size = 0.2984 MB
0.00.047.756 I print_info: arch             = gptneox
0.00.047.756 I print_info: vocab_only       = 0
0.00.047.756 I print_info: n_ctx_train      = 2048
0.00.047.756 I print_info: n_embd           = 2048
0.00.047.756 I print_info: n_layer          = 24
0.00.047.759 I print_info: n_head           = 16
0.00.047.760 I print_info: n_head_kv        = 16
0.00.047.760 I print_info: n_rot            = 32
0.00.047.760 I print_info: n_swa            = 0
0.00.047.762 I print_info: n_embd_head_k    = 128
0.00.047.763 I print_info: n_embd_head_v    = 128
0.00.047.763 I print_info: n_gqa            = 1
0.00.047.764 I print_info: n_embd_k_gqa     = 2048
0.00.047.765 I print_info: n_embd_v_gqa     = 2048
0.00.047.765 I print_info: f_norm_eps       = 1.0e-05
0.00.047.765 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.766 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.766 I print_info: f_logit_scale    = 0.0e+00
0.00.047.767 I print_info: n_ff             = 8192
0.00.047.767 I print_info: n_expert         = 0
0.00.047.767 I print_info: n_expert_used    = 0
0.00.047.767 I print_info: causal attn      = 1
0.00.047.767 I print_info: pooling type     = 0
0.00.047.767 I print_info: rope type        = 2
0.00.047.767 I print_info: rope scaling     = linear
0.00.047.768 I print_info: freq_base_train  = 10000.0
0.00.047.772 I print_info: freq_scale_train = 1
0.00.047.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.773 I print_info: rope_finetuned   = unknown
0.00.047.774 I print_info: ssm_d_conv       = 0
0.00.047.774 I print_info: ssm_d_inner      = 0
0.00.047.774 I print_info: ssm_d_state      = 0
0.00.047.775 I print_info: ssm_dt_rank      = 0
0.00.047.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.775 I print_info: model type       = 1.4B
0.00.047.775 I print_info: model params     = 1.41 B
0.00.047.775 I print_info: general.name     = 1.4B
0.00.047.776 I print_info: vocab type       = BPE
0.00.047.776 I print_info: n_vocab          = 50304
0.00.047.776 I print_info: n_merges         = 50009
0.00.047.777 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.777 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.777 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.777 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.777 I print_info: LF token         = 128 'Ä'
0.00.047.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.780 I print_info: max token length = 1024
0.00.049.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.764 I load_tensors: offloading output layer to GPU
0.00.049.764 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.774 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.049.776 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.062 I llama_init_from_model: n_seq_max     = 1
0.00.050.063 I llama_init_from_model: n_ctx         = 128
0.00.050.063 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.063 I llama_init_from_model: n_batch       = 128
0.00.050.063 I llama_init_from_model: n_ubatch      = 128
0.00.050.064 I llama_init_from_model: flash_attn    = 0
0.00.050.064 I llama_init_from_model: freq_base     = 10000.0
0.00.050.064 I llama_init_from_model: freq_scale    = 1
0.00.050.064 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.065 I ggml_metal_init: allocating
0.00.050.067 I ggml_metal_init: found device: Apple M4
0.00.050.069 I ggml_metal_init: picking default device: Apple M4
0.00.050.631 I ggml_metal_init: using embedded metal library
0.00.052.961 I ggml_metal_init: GPU name:   Apple M4
0.00.052.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.963 I ggml_metal_init: simdgroup reduction   = true
0.00.052.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.963 I ggml_metal_init: has bfloat            = true
0.00.052.963 I ggml_metal_init: use bfloat            = true
0.00.052.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.374 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.689 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.704 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.648 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.649 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.649 I llama_init_from_model: graph nodes  = 967
0.00.064.649 I llama_init_from_model: graph splits = 2
0.00.064.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.651 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.600 I 
0.00.685.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.640 I perplexity: tokenizing the input ..
0.00.693.382 I perplexity: tokenization took 7.74 ms
0.00.693.392 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.253 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.829.425 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.829.459 I llama_perf_context_print:        load time =     676.82 ms
0.00.829.460 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.73 tokens per second)
0.00.829.461 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.462 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.829.886 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.076s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.443 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.553 I llama_model_loader: - type  f32:  194 tensors
0.00.024.553 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.553 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.553 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.554 I print_info: file format = GGUF V3 (latest)
0.00.024.554 I print_info: file type   = Q2_K - Medium
0.00.024.555 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.826 I load: special tokens cache size = 25
0.00.049.920 I load: token to piece cache size = 0.2984 MB
0.00.049.923 I print_info: arch             = gptneox
0.00.049.923 I print_info: vocab_only       = 0
0.00.049.924 I print_info: n_ctx_train      = 2048
0.00.049.924 I print_info: n_embd           = 2048
0.00.049.924 I print_info: n_layer          = 24
0.00.049.927 I print_info: n_head           = 16
0.00.049.928 I print_info: n_head_kv        = 16
0.00.049.928 I print_info: n_rot            = 32
0.00.049.928 I print_info: n_swa            = 0
0.00.049.928 I print_info: n_embd_head_k    = 128
0.00.049.929 I print_info: n_embd_head_v    = 128
0.00.049.929 I print_info: n_gqa            = 1
0.00.049.930 I print_info: n_embd_k_gqa     = 2048
0.00.049.931 I print_info: n_embd_v_gqa     = 2048
0.00.049.931 I print_info: f_norm_eps       = 1.0e-05
0.00.049.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.933 I print_info: f_logit_scale    = 0.0e+00
0.00.049.933 I print_info: n_ff             = 8192
0.00.049.933 I print_info: n_expert         = 0
0.00.049.934 I print_info: n_expert_used    = 0
0.00.049.934 I print_info: causal attn      = 1
0.00.049.934 I print_info: pooling type     = 0
0.00.049.934 I print_info: rope type        = 2
0.00.049.934 I print_info: rope scaling     = linear
0.00.049.935 I print_info: freq_base_train  = 10000.0
0.00.049.935 I print_info: freq_scale_train = 1
0.00.049.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.936 I print_info: rope_finetuned   = unknown
0.00.049.938 I print_info: ssm_d_conv       = 0
0.00.049.938 I print_info: ssm_d_inner      = 0
0.00.049.939 I print_info: ssm_d_state      = 0
0.00.049.939 I print_info: ssm_dt_rank      = 0
0.00.049.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.939 I print_info: model type       = 1.4B
0.00.049.939 I print_info: model params     = 1.41 B
0.00.049.940 I print_info: general.name     = 1.4B
0.00.049.940 I print_info: vocab type       = BPE
0.00.049.940 I print_info: n_vocab          = 50304
0.00.049.940 I print_info: n_merges         = 50009
0.00.049.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: LF token         = 128 'Ä'
0.00.049.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: max token length = 1024
0.00.051.837 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.838 I load_tensors: offloading output layer to GPU
0.00.051.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.848 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.850 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.169 I llama_init_from_model: n_seq_max     = 1
0.00.052.169 I llama_init_from_model: n_ctx         = 2048
0.00.052.170 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.170 I llama_init_from_model: n_batch       = 2048
0.00.052.170 I llama_init_from_model: n_ubatch      = 512
0.00.052.170 I llama_init_from_model: flash_attn    = 0
0.00.052.170 I llama_init_from_model: freq_base     = 10000.0
0.00.052.171 I llama_init_from_model: freq_scale    = 1
0.00.052.171 I ggml_metal_init: allocating
0.00.052.175 I ggml_metal_init: found device: Apple M4
0.00.052.177 I ggml_metal_init: picking default device: Apple M4
0.00.052.780 I ggml_metal_init: using embedded metal library
0.00.055.122 I ggml_metal_init: GPU name:   Apple M4
0.00.055.123 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.124 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.124 I ggml_metal_init: simdgroup reduction   = true
0.00.055.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.125 I ggml_metal_init: has bfloat            = true
0.00.055.125 I ggml_metal_init: use bfloat            = true
0.00.055.125 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.877 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.485 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.493 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.512 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.509 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.510 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.510 I llama_init_from_model: graph nodes  = 967
0.00.084.511 I llama_init_from_model: graph splits = 2
0.00.084.513 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.644 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.644 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.430.739 I main: llama threadpool init, n_threads = 4
0.00.430.789 I 
0.00.430.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.430.827 I 
0.00.431.053 I sampler seed: 1234
0.00.431.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.431.070 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.431.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.431.070 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.326 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.113.326 I llama_perf_context_print:        load time =     421.29 ms
0.01.113.327 I llama_perf_context_print: prompt eval time =      38.95 ms /     7 tokens (    5.56 ms per token,   179.72 tokens per second)
0.01.113.327 I llama_perf_context_print:        eval time =     640.33 ms /    63 runs   (   10.16 ms per token,    98.39 tokens per second)
0.01.113.329 I llama_perf_context_print:       total time =     682.59 ms /    70 tokens
0.01.113.573 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.108s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.505 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.925 I llama_model_loader: - type  f32:  194 tensors
0.00.025.925 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.925 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.926 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.926 I print_info: file format = GGUF V3 (latest)
0.00.025.926 I print_info: file type   = Q2_K - Medium
0.00.025.927 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.835 I load: special tokens cache size = 25
0.00.050.898 I load: token to piece cache size = 0.2984 MB
0.00.050.901 I print_info: arch             = gptneox
0.00.050.901 I print_info: vocab_only       = 0
0.00.050.902 I print_info: n_ctx_train      = 2048
0.00.050.902 I print_info: n_embd           = 2048
0.00.050.902 I print_info: n_layer          = 24
0.00.050.905 I print_info: n_head           = 16
0.00.050.906 I print_info: n_head_kv        = 16
0.00.050.906 I print_info: n_rot            = 32
0.00.050.906 I print_info: n_swa            = 0
0.00.050.908 I print_info: n_embd_head_k    = 128
0.00.050.908 I print_info: n_embd_head_v    = 128
0.00.050.909 I print_info: n_gqa            = 1
0.00.050.910 I print_info: n_embd_k_gqa     = 2048
0.00.050.911 I print_info: n_embd_v_gqa     = 2048
0.00.050.911 I print_info: f_norm_eps       = 1.0e-05
0.00.050.912 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.912 I print_info: f_logit_scale    = 0.0e+00
0.00.050.913 I print_info: n_ff             = 8192
0.00.050.913 I print_info: n_expert         = 0
0.00.050.913 I print_info: n_expert_used    = 0
0.00.050.913 I print_info: causal attn      = 1
0.00.050.913 I print_info: pooling type     = 0
0.00.050.913 I print_info: rope type        = 2
0.00.050.918 I print_info: rope scaling     = linear
0.00.050.918 I print_info: freq_base_train  = 10000.0
0.00.050.919 I print_info: freq_scale_train = 1
0.00.050.919 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.919 I print_info: rope_finetuned   = unknown
0.00.050.920 I print_info: ssm_d_conv       = 0
0.00.050.920 I print_info: ssm_d_inner      = 0
0.00.050.920 I print_info: ssm_d_state      = 0
0.00.050.920 I print_info: ssm_dt_rank      = 0
0.00.050.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.921 I print_info: model type       = 1.4B
0.00.050.921 I print_info: model params     = 1.41 B
0.00.050.921 I print_info: general.name     = 1.4B
0.00.050.922 I print_info: vocab type       = BPE
0.00.050.923 I print_info: n_vocab          = 50304
0.00.050.923 I print_info: n_merges         = 50009
0.00.050.923 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.923 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.924 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.925 I print_info: LF token         = 128 'Ä'
0.00.050.925 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.925 I print_info: max token length = 1024
0.00.052.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.766 I load_tensors: offloading output layer to GPU
0.00.052.766 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.777 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.778 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.049 I llama_init_from_model: n_seq_max     = 1
0.00.053.050 I llama_init_from_model: n_ctx         = 128
0.00.053.050 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.050 I llama_init_from_model: n_batch       = 128
0.00.053.050 I llama_init_from_model: n_ubatch      = 128
0.00.053.050 I llama_init_from_model: flash_attn    = 0
0.00.053.051 I llama_init_from_model: freq_base     = 10000.0
0.00.053.051 I llama_init_from_model: freq_scale    = 1
0.00.053.051 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.052 I ggml_metal_init: allocating
0.00.053.054 I ggml_metal_init: found device: Apple M4
0.00.053.056 I ggml_metal_init: picking default device: Apple M4
0.00.053.615 I ggml_metal_init: using embedded metal library
0.00.055.953 I ggml_metal_init: GPU name:   Apple M4
0.00.055.954 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.955 I ggml_metal_init: simdgroup reduction   = true
0.00.055.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.955 I ggml_metal_init: has bfloat            = true
0.00.055.955 I ggml_metal_init: use bfloat            = true
0.00.055.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.359 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.703 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.706 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.723 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.578 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.579 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.579 I llama_init_from_model: graph nodes  = 967
0.00.067.580 I llama_init_from_model: graph splits = 2
0.00.067.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.422 I 
0.00.453.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.469 I perplexity: tokenizing the input ..
0.00.461.523 I perplexity: tokenization took 8.046 ms
0.00.461.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.593.308 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.594.523 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.594.548 I llama_perf_context_print:        load time =     442.29 ms
0.00.594.549 I llama_perf_context_print: prompt eval time =     131.55 ms /   128 tokens (    1.03 ms per token,   973.02 tokens per second)
0.00.594.550 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.594.550 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.595.006 I ggml_metal_free: deallocating

real	0m0.610s
user	0m0.077s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.025 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.887 I llama_model_loader: - type  f32:  194 tensors
0.00.024.888 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.888 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.888 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.889 I print_info: file format = GGUF V3 (latest)
0.00.024.889 I print_info: file type   = Q3_K - Medium
0.00.024.890 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.295 I load: special tokens cache size = 25
0.00.049.420 I load: token to piece cache size = 0.2984 MB
0.00.049.423 I print_info: arch             = gptneox
0.00.049.423 I print_info: vocab_only       = 0
0.00.049.423 I print_info: n_ctx_train      = 2048
0.00.049.424 I print_info: n_embd           = 2048
0.00.049.424 I print_info: n_layer          = 24
0.00.049.427 I print_info: n_head           = 16
0.00.049.428 I print_info: n_head_kv        = 16
0.00.049.428 I print_info: n_rot            = 32
0.00.049.428 I print_info: n_swa            = 0
0.00.049.428 I print_info: n_embd_head_k    = 128
0.00.049.428 I print_info: n_embd_head_v    = 128
0.00.049.429 I print_info: n_gqa            = 1
0.00.049.430 I print_info: n_embd_k_gqa     = 2048
0.00.049.431 I print_info: n_embd_v_gqa     = 2048
0.00.049.431 I print_info: f_norm_eps       = 1.0e-05
0.00.049.432 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.432 I print_info: f_logit_scale    = 0.0e+00
0.00.049.433 I print_info: n_ff             = 8192
0.00.049.433 I print_info: n_expert         = 0
0.00.049.433 I print_info: n_expert_used    = 0
0.00.049.435 I print_info: causal attn      = 1
0.00.049.436 I print_info: pooling type     = 0
0.00.049.436 I print_info: rope type        = 2
0.00.049.436 I print_info: rope scaling     = linear
0.00.049.437 I print_info: freq_base_train  = 10000.0
0.00.049.437 I print_info: freq_scale_train = 1
0.00.049.439 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.439 I print_info: rope_finetuned   = unknown
0.00.049.439 I print_info: ssm_d_conv       = 0
0.00.049.439 I print_info: ssm_d_inner      = 0
0.00.049.439 I print_info: ssm_d_state      = 0
0.00.049.440 I print_info: ssm_dt_rank      = 0
0.00.049.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.441 I print_info: model type       = 1.4B
0.00.049.441 I print_info: model params     = 1.41 B
0.00.049.441 I print_info: general.name     = 1.4B
0.00.049.442 I print_info: vocab type       = BPE
0.00.049.442 I print_info: n_vocab          = 50304
0.00.049.443 I print_info: n_merges         = 50009
0.00.049.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: LF token         = 128 'Ä'
0.00.049.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.445 I print_info: max token length = 1024
0.00.051.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.264 I load_tensors: offloading output layer to GPU
0.00.051.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.275 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.276 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.562 I llama_init_from_model: n_seq_max     = 1
0.00.051.563 I llama_init_from_model: n_ctx         = 2048
0.00.051.563 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.563 I llama_init_from_model: n_batch       = 2048
0.00.051.563 I llama_init_from_model: n_ubatch      = 512
0.00.051.564 I llama_init_from_model: flash_attn    = 0
0.00.051.564 I llama_init_from_model: freq_base     = 10000.0
0.00.051.564 I llama_init_from_model: freq_scale    = 1
0.00.051.565 I ggml_metal_init: allocating
0.00.051.567 I ggml_metal_init: found device: Apple M4
0.00.051.569 I ggml_metal_init: picking default device: Apple M4
0.00.052.152 I ggml_metal_init: using embedded metal library
0.00.054.826 I ggml_metal_init: GPU name:   Apple M4
0.00.054.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.829 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.829 I ggml_metal_init: simdgroup reduction   = true
0.00.054.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.829 I ggml_metal_init: has bfloat            = true
0.00.054.829 I ggml_metal_init: use bfloat            = true
0.00.054.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.402 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.985 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.994 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.078 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.080 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.080 I llama_init_from_model: graph nodes  = 967
0.00.085.080 I llama_init_from_model: graph splits = 2
0.00.085.083 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.216 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.217 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.816 I main: llama threadpool init, n_threads = 4
0.00.588.861 I 
0.00.588.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.897 I 
0.00.589.132 I sampler seed: 1234
0.00.589.137 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.148 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.148 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.328.132 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49271.34 tokens per second)
0.01.328.133 I llama_perf_context_print:        load time =     579.79 ms
0.01.328.133 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.87 tokens per second)
0.01.328.134 I llama_perf_context_print:        eval time =     696.01 ms /    63 runs   (   11.05 ms per token,    90.52 tokens per second)
0.01.328.134 I llama_perf_context_print:       total time =     739.32 ms /    70 tokens
0.01.328.367 I ggml_metal_free: deallocating

real	0m1.344s
user	0m0.107s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.785 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.787 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.793 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.794 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.794 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.197 I llama_model_loader: - type  f32:  194 tensors
0.00.024.198 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.198 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.198 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.199 I print_info: file format = GGUF V3 (latest)
0.00.024.200 I print_info: file type   = Q3_K - Medium
0.00.024.200 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.240 I load: special tokens cache size = 25
0.00.049.282 I load: token to piece cache size = 0.2984 MB
0.00.049.285 I print_info: arch             = gptneox
0.00.049.285 I print_info: vocab_only       = 0
0.00.049.285 I print_info: n_ctx_train      = 2048
0.00.049.285 I print_info: n_embd           = 2048
0.00.049.286 I print_info: n_layer          = 24
0.00.049.288 I print_info: n_head           = 16
0.00.049.289 I print_info: n_head_kv        = 16
0.00.049.289 I print_info: n_rot            = 32
0.00.049.289 I print_info: n_swa            = 0
0.00.049.292 I print_info: n_embd_head_k    = 128
0.00.049.292 I print_info: n_embd_head_v    = 128
0.00.049.292 I print_info: n_gqa            = 1
0.00.049.293 I print_info: n_embd_k_gqa     = 2048
0.00.049.294 I print_info: n_embd_v_gqa     = 2048
0.00.049.294 I print_info: f_norm_eps       = 1.0e-05
0.00.049.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.295 I print_info: f_logit_scale    = 0.0e+00
0.00.049.296 I print_info: n_ff             = 8192
0.00.049.296 I print_info: n_expert         = 0
0.00.049.296 I print_info: n_expert_used    = 0
0.00.049.296 I print_info: causal attn      = 1
0.00.049.296 I print_info: pooling type     = 0
0.00.049.297 I print_info: rope type        = 2
0.00.049.301 I print_info: rope scaling     = linear
0.00.049.302 I print_info: freq_base_train  = 10000.0
0.00.049.302 I print_info: freq_scale_train = 1
0.00.049.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.303 I print_info: rope_finetuned   = unknown
0.00.049.303 I print_info: ssm_d_conv       = 0
0.00.049.303 I print_info: ssm_d_inner      = 0
0.00.049.303 I print_info: ssm_d_state      = 0
0.00.049.304 I print_info: ssm_dt_rank      = 0
0.00.049.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.304 I print_info: model type       = 1.4B
0.00.049.306 I print_info: model params     = 1.41 B
0.00.049.306 I print_info: general.name     = 1.4B
0.00.049.307 I print_info: vocab type       = BPE
0.00.049.307 I print_info: n_vocab          = 50304
0.00.049.307 I print_info: n_merges         = 50009
0.00.049.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.308 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.308 I print_info: LF token         = 128 'Ä'
0.00.049.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.308 I print_info: max token length = 1024
0.00.051.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.028 I load_tensors: offloading output layer to GPU
0.00.051.028 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.034 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.034 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.303 I llama_init_from_model: n_seq_max     = 1
0.00.051.303 I llama_init_from_model: n_ctx         = 128
0.00.051.304 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.304 I llama_init_from_model: n_batch       = 128
0.00.051.304 I llama_init_from_model: n_ubatch      = 128
0.00.051.304 I llama_init_from_model: flash_attn    = 0
0.00.051.304 I llama_init_from_model: freq_base     = 10000.0
0.00.051.305 I llama_init_from_model: freq_scale    = 1
0.00.051.305 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.305 I ggml_metal_init: allocating
0.00.051.308 I ggml_metal_init: found device: Apple M4
0.00.051.310 I ggml_metal_init: picking default device: Apple M4
0.00.051.876 I ggml_metal_init: using embedded metal library
0.00.054.224 I ggml_metal_init: GPU name:   Apple M4
0.00.054.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.227 I ggml_metal_init: simdgroup reduction   = true
0.00.054.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.227 I ggml_metal_init: has bfloat            = true
0.00.054.227 I ggml_metal_init: use bfloat            = true
0.00.054.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.707 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.612 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.613 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.613 I llama_init_from_model: graph nodes  = 967
0.00.065.613 I llama_init_from_model: graph splits = 2
0.00.065.614 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.476 I 
0.00.473.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.534 I perplexity: tokenizing the input ..
0.00.481.272 I perplexity: tokenization took 7.736 ms
0.00.481.276 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.150 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.376 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.406 I llama_perf_context_print:        load time =     464.64 ms
0.00.614.407 I llama_perf_context_print: prompt eval time =     131.64 ms /   128 tokens (    1.03 ms per token,   972.32 tokens per second)
0.00.614.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.408 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.614.907 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.076s
sys	0m0.082s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.520 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.877 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.878 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.879 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.880 I llama_model_loader: - type  f32:  194 tensors
0.00.026.880 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.880 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.880 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.881 I print_info: file format = GGUF V3 (latest)
0.00.026.881 I print_info: file type   = Q4_K - Medium
0.00.026.882 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.839 I load: special tokens cache size = 25
0.00.051.952 I load: token to piece cache size = 0.2984 MB
0.00.051.956 I print_info: arch             = gptneox
0.00.051.957 I print_info: vocab_only       = 0
0.00.051.957 I print_info: n_ctx_train      = 2048
0.00.051.957 I print_info: n_embd           = 2048
0.00.051.957 I print_info: n_layer          = 24
0.00.051.961 I print_info: n_head           = 16
0.00.051.962 I print_info: n_head_kv        = 16
0.00.051.963 I print_info: n_rot            = 32
0.00.051.964 I print_info: n_swa            = 0
0.00.051.964 I print_info: n_embd_head_k    = 128
0.00.051.964 I print_info: n_embd_head_v    = 128
0.00.051.965 I print_info: n_gqa            = 1
0.00.051.965 I print_info: n_embd_k_gqa     = 2048
0.00.051.966 I print_info: n_embd_v_gqa     = 2048
0.00.051.967 I print_info: f_norm_eps       = 1.0e-05
0.00.051.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.967 I print_info: f_logit_scale    = 0.0e+00
0.00.051.968 I print_info: n_ff             = 8192
0.00.051.968 I print_info: n_expert         = 0
0.00.051.968 I print_info: n_expert_used    = 0
0.00.051.968 I print_info: causal attn      = 1
0.00.051.969 I print_info: pooling type     = 0
0.00.051.970 I print_info: rope type        = 2
0.00.051.970 I print_info: rope scaling     = linear
0.00.051.970 I print_info: freq_base_train  = 10000.0
0.00.051.971 I print_info: freq_scale_train = 1
0.00.051.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.971 I print_info: rope_finetuned   = unknown
0.00.051.971 I print_info: ssm_d_conv       = 0
0.00.051.971 I print_info: ssm_d_inner      = 0
0.00.051.971 I print_info: ssm_d_state      = 0
0.00.051.971 I print_info: ssm_dt_rank      = 0
0.00.051.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.972 I print_info: model type       = 1.4B
0.00.051.972 I print_info: model params     = 1.41 B
0.00.051.972 I print_info: general.name     = 1.4B
0.00.051.973 I print_info: vocab type       = BPE
0.00.051.973 I print_info: n_vocab          = 50304
0.00.051.973 I print_info: n_merges         = 50009
0.00.051.973 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: LF token         = 128 'Ä'
0.00.051.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: max token length = 1024
0.00.053.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.769 I load_tensors: offloading output layer to GPU
0.00.053.769 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.779 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.780 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.103 I llama_init_from_model: n_seq_max     = 1
0.00.054.104 I llama_init_from_model: n_ctx         = 2048
0.00.054.104 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.104 I llama_init_from_model: n_batch       = 2048
0.00.054.104 I llama_init_from_model: n_ubatch      = 512
0.00.054.104 I llama_init_from_model: flash_attn    = 0
0.00.054.105 I llama_init_from_model: freq_base     = 10000.0
0.00.054.105 I llama_init_from_model: freq_scale    = 1
0.00.054.105 I ggml_metal_init: allocating
0.00.054.109 I ggml_metal_init: found device: Apple M4
0.00.054.110 I ggml_metal_init: picking default device: Apple M4
0.00.054.743 I ggml_metal_init: using embedded metal library
0.00.057.324 I ggml_metal_init: GPU name:   Apple M4
0.00.057.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.326 I ggml_metal_init: simdgroup reduction   = true
0.00.057.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.326 I ggml_metal_init: has bfloat            = true
0.00.057.326 I ggml_metal_init: use bfloat            = true
0.00.057.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.866 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.934 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.959 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.933 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.935 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.935 I llama_init_from_model: graph nodes  = 967
0.00.087.935 I llama_init_from_model: graph splits = 2
0.00.087.943 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.111 I main: llama threadpool init, n_threads = 4
0.00.606.157 I 
0.00.606.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.192 I 
0.00.606.417 I sampler seed: 1234
0.00.606.423 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.433 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.434 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.434 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.370.134 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49477.35 tokens per second)
0.01.370.135 I llama_perf_context_print:        load time =     594.19 ms
0.01.370.136 I llama_perf_context_print: prompt eval time =      50.94 ms /     7 tokens (    7.28 ms per token,   137.41 tokens per second)
0.01.370.136 I llama_perf_context_print:        eval time =     709.73 ms /    63 runs   (   11.27 ms per token,    88.77 tokens per second)
0.01.370.136 I llama_perf_context_print:       total time =     764.03 ms /    70 tokens
0.01.370.377 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.108s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.012 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.491 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.225 I llama_model_loader: - type  f32:  194 tensors
0.00.024.225 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.226 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.226 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.226 I print_info: file format = GGUF V3 (latest)
0.00.024.227 I print_info: file type   = Q4_K - Medium
0.00.024.228 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.557 I load: special tokens cache size = 25
0.00.048.589 I load: token to piece cache size = 0.2984 MB
0.00.048.592 I print_info: arch             = gptneox
0.00.048.592 I print_info: vocab_only       = 0
0.00.048.592 I print_info: n_ctx_train      = 2048
0.00.048.592 I print_info: n_embd           = 2048
0.00.048.593 I print_info: n_layer          = 24
0.00.048.596 I print_info: n_head           = 16
0.00.048.596 I print_info: n_head_kv        = 16
0.00.048.597 I print_info: n_rot            = 32
0.00.048.597 I print_info: n_swa            = 0
0.00.048.597 I print_info: n_embd_head_k    = 128
0.00.048.597 I print_info: n_embd_head_v    = 128
0.00.048.598 I print_info: n_gqa            = 1
0.00.048.599 I print_info: n_embd_k_gqa     = 2048
0.00.048.599 I print_info: n_embd_v_gqa     = 2048
0.00.048.600 I print_info: f_norm_eps       = 1.0e-05
0.00.048.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.603 I print_info: f_logit_scale    = 0.0e+00
0.00.048.604 I print_info: n_ff             = 8192
0.00.048.606 I print_info: n_expert         = 0
0.00.048.606 I print_info: n_expert_used    = 0
0.00.048.606 I print_info: causal attn      = 1
0.00.048.606 I print_info: pooling type     = 0
0.00.048.606 I print_info: rope type        = 2
0.00.048.607 I print_info: rope scaling     = linear
0.00.048.607 I print_info: freq_base_train  = 10000.0
0.00.048.607 I print_info: freq_scale_train = 1
0.00.048.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.608 I print_info: rope_finetuned   = unknown
0.00.048.608 I print_info: ssm_d_conv       = 0
0.00.048.608 I print_info: ssm_d_inner      = 0
0.00.048.608 I print_info: ssm_d_state      = 0
0.00.048.609 I print_info: ssm_dt_rank      = 0
0.00.048.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.614 I print_info: model type       = 1.4B
0.00.048.616 I print_info: model params     = 1.41 B
0.00.048.616 I print_info: general.name     = 1.4B
0.00.048.617 I print_info: vocab type       = BPE
0.00.048.617 I print_info: n_vocab          = 50304
0.00.048.617 I print_info: n_merges         = 50009
0.00.048.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.618 I print_info: LF token         = 128 'Ä'
0.00.048.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.621 I print_info: max token length = 1024
0.00.050.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.621 I load_tensors: offloading output layer to GPU
0.00.050.621 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.632 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.633 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.916 I llama_init_from_model: n_seq_max     = 1
0.00.050.917 I llama_init_from_model: n_ctx         = 128
0.00.050.917 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.917 I llama_init_from_model: n_batch       = 128
0.00.050.917 I llama_init_from_model: n_ubatch      = 128
0.00.050.918 I llama_init_from_model: flash_attn    = 0
0.00.050.918 I llama_init_from_model: freq_base     = 10000.0
0.00.050.918 I llama_init_from_model: freq_scale    = 1
0.00.050.919 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.919 I ggml_metal_init: allocating
0.00.050.921 I ggml_metal_init: found device: Apple M4
0.00.050.923 I ggml_metal_init: picking default device: Apple M4
0.00.051.536 I ggml_metal_init: using embedded metal library
0.00.054.032 I ggml_metal_init: GPU name:   Apple M4
0.00.054.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.035 I ggml_metal_init: simdgroup reduction   = true
0.00.054.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.035 I ggml_metal_init: has bfloat            = true
0.00.054.035 I ggml_metal_init: use bfloat            = true
0.00.054.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.489 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.779 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.782 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.796 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.656 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.657 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.657 I llama_init_from_model: graph nodes  = 967
0.00.064.657 I llama_init_from_model: graph splits = 2
0.00.064.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.999 I 
0.00.546.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.043 I perplexity: tokenizing the input ..
0.00.554.096 I perplexity: tokenization took 8.051 ms
0.00.554.100 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.061 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.202 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.230 I llama_perf_context_print:        load time =     536.98 ms
0.00.689.231 I llama_perf_context_print: prompt eval time =     133.74 ms /   128 tokens (    1.04 ms per token,   957.10 tokens per second)
0.00.689.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.232 I llama_perf_context_print:       total time =     143.23 ms /   129 tokens
0.00.689.665 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.075s
sys	0m0.094s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.898 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.588 I llama_model_loader: - type  f32:  194 tensors
0.00.023.588 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.589 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.589 I print_info: file format = GGUF V3 (latest)
0.00.023.590 I print_info: file type   = Q5_K - Medium
0.00.023.592 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.022 I load: special tokens cache size = 25
0.00.048.027 I load: token to piece cache size = 0.2984 MB
0.00.048.030 I print_info: arch             = gptneox
0.00.048.031 I print_info: vocab_only       = 0
0.00.048.031 I print_info: n_ctx_train      = 2048
0.00.048.031 I print_info: n_embd           = 2048
0.00.048.031 I print_info: n_layer          = 24
0.00.048.035 I print_info: n_head           = 16
0.00.048.035 I print_info: n_head_kv        = 16
0.00.048.036 I print_info: n_rot            = 32
0.00.048.036 I print_info: n_swa            = 0
0.00.048.036 I print_info: n_embd_head_k    = 128
0.00.048.036 I print_info: n_embd_head_v    = 128
0.00.048.037 I print_info: n_gqa            = 1
0.00.048.037 I print_info: n_embd_k_gqa     = 2048
0.00.048.038 I print_info: n_embd_v_gqa     = 2048
0.00.048.039 I print_info: f_norm_eps       = 1.0e-05
0.00.048.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.041 I print_info: f_logit_scale    = 0.0e+00
0.00.048.042 I print_info: n_ff             = 8192
0.00.048.042 I print_info: n_expert         = 0
0.00.048.042 I print_info: n_expert_used    = 0
0.00.048.042 I print_info: causal attn      = 1
0.00.048.043 I print_info: pooling type     = 0
0.00.048.043 I print_info: rope type        = 2
0.00.048.043 I print_info: rope scaling     = linear
0.00.048.044 I print_info: freq_base_train  = 10000.0
0.00.048.044 I print_info: freq_scale_train = 1
0.00.048.044 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.044 I print_info: rope_finetuned   = unknown
0.00.048.044 I print_info: ssm_d_conv       = 0
0.00.048.045 I print_info: ssm_d_inner      = 0
0.00.048.045 I print_info: ssm_d_state      = 0
0.00.048.045 I print_info: ssm_dt_rank      = 0
0.00.048.045 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.045 I print_info: model type       = 1.4B
0.00.048.046 I print_info: model params     = 1.41 B
0.00.048.046 I print_info: general.name     = 1.4B
0.00.048.046 I print_info: vocab type       = BPE
0.00.048.047 I print_info: n_vocab          = 50304
0.00.048.047 I print_info: n_merges         = 50009
0.00.048.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.047 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.048 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.048 I print_info: LF token         = 128 'Ä'
0.00.048.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.050 I print_info: max token length = 1024
0.00.049.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.989 I load_tensors: offloading output layer to GPU
0.00.049.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.000 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.001 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.275 I llama_init_from_model: n_seq_max     = 1
0.00.050.276 I llama_init_from_model: n_ctx         = 2048
0.00.050.276 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.276 I llama_init_from_model: n_batch       = 2048
0.00.050.276 I llama_init_from_model: n_ubatch      = 512
0.00.050.276 I llama_init_from_model: flash_attn    = 0
0.00.050.277 I llama_init_from_model: freq_base     = 10000.0
0.00.050.277 I llama_init_from_model: freq_scale    = 1
0.00.050.277 I ggml_metal_init: allocating
0.00.050.280 I ggml_metal_init: found device: Apple M4
0.00.050.282 I ggml_metal_init: picking default device: Apple M4
0.00.050.844 I ggml_metal_init: using embedded metal library
0.00.053.168 I ggml_metal_init: GPU name:   Apple M4
0.00.053.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.170 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.171 I ggml_metal_init: simdgroup reduction   = true
0.00.053.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.171 I ggml_metal_init: has bfloat            = true
0.00.053.171 I ggml_metal_init: use bfloat            = true
0.00.053.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.593 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.598 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.642 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.082.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.082.644 I llama_init_from_model: graph nodes  = 967
0.00.082.644 I llama_init_from_model: graph splits = 2
0.00.082.647 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.082.776 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.082.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.545 I main: llama threadpool init, n_threads = 4
0.00.709.592 I 
0.00.709.648 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.650 I 
0.00.709.887 I sampler seed: 1234
0.00.709.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.709.953 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.709.958 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.709.958 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.555.476 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.555.477 I llama_perf_context_print:        load time =     700.81 ms
0.01.555.477 I llama_perf_context_print: prompt eval time =      51.72 ms /     7 tokens (    7.39 ms per token,   135.34 tokens per second)
0.01.555.478 I llama_perf_context_print:        eval time =     790.76 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.555.478 I llama_perf_context_print:       total time =     845.94 ms /    70 tokens
0.01.555.738 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.107s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.889 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.894 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.403 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.403 I llama_model_loader: - type  f32:  194 tensors
0.00.025.403 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.404 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.404 I print_info: file format = GGUF V3 (latest)
0.00.025.405 I print_info: file type   = Q5_K - Medium
0.00.025.405 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.412 I load: special tokens cache size = 25
0.00.050.463 I load: token to piece cache size = 0.2984 MB
0.00.050.466 I print_info: arch             = gptneox
0.00.050.466 I print_info: vocab_only       = 0
0.00.050.467 I print_info: n_ctx_train      = 2048
0.00.050.467 I print_info: n_embd           = 2048
0.00.050.467 I print_info: n_layer          = 24
0.00.050.469 I print_info: n_head           = 16
0.00.050.470 I print_info: n_head_kv        = 16
0.00.050.470 I print_info: n_rot            = 32
0.00.050.470 I print_info: n_swa            = 0
0.00.050.471 I print_info: n_embd_head_k    = 128
0.00.050.471 I print_info: n_embd_head_v    = 128
0.00.050.472 I print_info: n_gqa            = 1
0.00.050.472 I print_info: n_embd_k_gqa     = 2048
0.00.050.473 I print_info: n_embd_v_gqa     = 2048
0.00.050.475 I print_info: f_norm_eps       = 1.0e-05
0.00.050.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.478 I print_info: f_logit_scale    = 0.0e+00
0.00.050.479 I print_info: n_ff             = 8192
0.00.050.479 I print_info: n_expert         = 0
0.00.050.479 I print_info: n_expert_used    = 0
0.00.050.479 I print_info: causal attn      = 1
0.00.050.479 I print_info: pooling type     = 0
0.00.050.480 I print_info: rope type        = 2
0.00.050.480 I print_info: rope scaling     = linear
0.00.050.480 I print_info: freq_base_train  = 10000.0
0.00.050.481 I print_info: freq_scale_train = 1
0.00.050.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.481 I print_info: rope_finetuned   = unknown
0.00.050.481 I print_info: ssm_d_conv       = 0
0.00.050.481 I print_info: ssm_d_inner      = 0
0.00.050.482 I print_info: ssm_d_state      = 0
0.00.050.482 I print_info: ssm_dt_rank      = 0
0.00.050.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.482 I print_info: model type       = 1.4B
0.00.050.483 I print_info: model params     = 1.41 B
0.00.050.483 I print_info: general.name     = 1.4B
0.00.050.484 I print_info: vocab type       = BPE
0.00.050.485 I print_info: n_vocab          = 50304
0.00.050.485 I print_info: n_merges         = 50009
0.00.050.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: LF token         = 128 'Ä'
0.00.050.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: max token length = 1024
0.00.052.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.479 I load_tensors: offloading output layer to GPU
0.00.052.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.490 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.491 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.840 I llama_init_from_model: n_seq_max     = 1
0.00.052.841 I llama_init_from_model: n_ctx         = 128
0.00.052.841 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.841 I llama_init_from_model: n_batch       = 128
0.00.052.841 I llama_init_from_model: n_ubatch      = 128
0.00.052.842 I llama_init_from_model: flash_attn    = 0
0.00.052.842 I llama_init_from_model: freq_base     = 10000.0
0.00.052.842 I llama_init_from_model: freq_scale    = 1
0.00.052.843 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.843 I ggml_metal_init: allocating
0.00.052.846 I ggml_metal_init: found device: Apple M4
0.00.052.848 I ggml_metal_init: picking default device: Apple M4
0.00.053.389 I ggml_metal_init: using embedded metal library
0.00.055.733 I ggml_metal_init: GPU name:   Apple M4
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.736 I ggml_metal_init: simdgroup reduction   = true
0.00.055.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.736 I ggml_metal_init: has bfloat            = true
0.00.055.736 I ggml_metal_init: use bfloat            = true
0.00.055.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.327 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.332 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.349 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.222 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.223 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.223 I llama_init_from_model: graph nodes  = 967
0.00.067.223 I llama_init_from_model: graph splits = 2
0.00.067.224 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.019 I 
0.00.645.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.063 I perplexity: tokenizing the input ..
0.00.652.922 I perplexity: tokenization took 7.858 ms
0.00.652.926 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.441 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.794.592 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.794.618 I llama_perf_context_print:        load time =     634.83 ms
0.00.794.619 I llama_perf_context_print: prompt eval time =     140.29 ms /   128 tokens (    1.10 ms per token,   912.41 tokens per second)
0.00.794.620 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.621 I llama_perf_context_print:       total time =     149.60 ms /   129 tokens
0.00.795.105 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.077s
sys	0m0.120s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.672 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.144 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.159 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.159 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.162 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.846 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.605 I llama_model_loader: - type  f32:  194 tensors
0.00.024.605 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.606 I print_info: file format = GGUF V3 (latest)
0.00.024.606 I print_info: file type   = Q6_K
0.00.024.607 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.085 I load: special tokens cache size = 25
0.00.049.153 I load: token to piece cache size = 0.2984 MB
0.00.049.156 I print_info: arch             = gptneox
0.00.049.156 I print_info: vocab_only       = 0
0.00.049.156 I print_info: n_ctx_train      = 2048
0.00.049.156 I print_info: n_embd           = 2048
0.00.049.156 I print_info: n_layer          = 24
0.00.049.159 I print_info: n_head           = 16
0.00.049.160 I print_info: n_head_kv        = 16
0.00.049.160 I print_info: n_rot            = 32
0.00.049.163 I print_info: n_swa            = 0
0.00.049.163 I print_info: n_embd_head_k    = 128
0.00.049.163 I print_info: n_embd_head_v    = 128
0.00.049.164 I print_info: n_gqa            = 1
0.00.049.164 I print_info: n_embd_k_gqa     = 2048
0.00.049.165 I print_info: n_embd_v_gqa     = 2048
0.00.049.170 I print_info: f_norm_eps       = 1.0e-05
0.00.049.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.173 I print_info: f_logit_scale    = 0.0e+00
0.00.049.173 I print_info: n_ff             = 8192
0.00.049.173 I print_info: n_expert         = 0
0.00.049.174 I print_info: n_expert_used    = 0
0.00.049.174 I print_info: causal attn      = 1
0.00.049.174 I print_info: pooling type     = 0
0.00.049.174 I print_info: rope type        = 2
0.00.049.174 I print_info: rope scaling     = linear
0.00.049.178 I print_info: freq_base_train  = 10000.0
0.00.049.178 I print_info: freq_scale_train = 1
0.00.049.179 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.180 I print_info: rope_finetuned   = unknown
0.00.049.180 I print_info: ssm_d_conv       = 0
0.00.049.180 I print_info: ssm_d_inner      = 0
0.00.049.181 I print_info: ssm_d_state      = 0
0.00.049.181 I print_info: ssm_dt_rank      = 0
0.00.049.181 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.181 I print_info: model type       = 1.4B
0.00.049.181 I print_info: model params     = 1.41 B
0.00.049.181 I print_info: general.name     = 1.4B
0.00.049.182 I print_info: vocab type       = BPE
0.00.049.182 I print_info: n_vocab          = 50304
0.00.049.182 I print_info: n_merges         = 50009
0.00.049.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.183 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.183 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.185 I print_info: LF token         = 128 'Ä'
0.00.049.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.185 I print_info: max token length = 1024
0.00.051.128 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.128 I load_tensors: offloading output layer to GPU
0.00.051.128 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.139 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.140 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.468 I llama_init_from_model: n_seq_max     = 1
0.00.051.468 I llama_init_from_model: n_ctx         = 2048
0.00.051.469 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.469 I llama_init_from_model: n_batch       = 2048
0.00.051.469 I llama_init_from_model: n_ubatch      = 512
0.00.051.469 I llama_init_from_model: flash_attn    = 0
0.00.051.469 I llama_init_from_model: freq_base     = 10000.0
0.00.051.470 I llama_init_from_model: freq_scale    = 1
0.00.051.470 I ggml_metal_init: allocating
0.00.051.473 I ggml_metal_init: found device: Apple M4
0.00.051.475 I ggml_metal_init: picking default device: Apple M4
0.00.052.042 I ggml_metal_init: using embedded metal library
0.00.054.331 I ggml_metal_init: GPU name:   Apple M4
0.00.054.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.333 I ggml_metal_init: simdgroup reduction   = true
0.00.054.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.333 I ggml_metal_init: has bfloat            = true
0.00.054.334 I ggml_metal_init: use bfloat            = true
0.00.054.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.373 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.383 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.409 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.504 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.506 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.506 I llama_init_from_model: graph nodes  = 967
0.00.084.507 I llama_init_from_model: graph splits = 2
0.00.084.510 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.644 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.931 I main: llama threadpool init, n_threads = 4
0.00.768.975 I 
0.00.769.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.008 I 
0.00.769.238 I sampler seed: 1234
0.00.769.242 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.254 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.640.512 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.640.512 I llama_perf_context_print:        load time =     759.25 ms
0.01.640.513 I llama_perf_context_print: prompt eval time =      54.56 ms /     7 tokens (    7.79 ms per token,   128.31 tokens per second)
0.01.640.514 I llama_perf_context_print:        eval time =     813.80 ms /    63 runs   (   12.92 ms per token,    77.41 tokens per second)
0.01.640.514 I llama_perf_context_print:       total time =     871.58 ms /    70 tokens
0.01.640.713 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.107s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4512 (ae3c1db2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.219 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.968 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.720 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.721 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.722 I llama_model_loader: - type  f32:  194 tensors
0.00.023.723 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.723 I print_info: file format = GGUF V3 (latest)
0.00.023.724 I print_info: file type   = Q6_K
0.00.023.724 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.069 I load: special tokens cache size = 25
0.00.048.048 I load: token to piece cache size = 0.2984 MB
0.00.048.050 I print_info: arch             = gptneox
0.00.048.051 I print_info: vocab_only       = 0
0.00.048.051 I print_info: n_ctx_train      = 2048
0.00.048.051 I print_info: n_embd           = 2048
0.00.048.051 I print_info: n_layer          = 24
0.00.048.054 I print_info: n_head           = 16
0.00.048.055 I print_info: n_head_kv        = 16
0.00.048.055 I print_info: n_rot            = 32
0.00.048.055 I print_info: n_swa            = 0
0.00.048.055 I print_info: n_embd_head_k    = 128
0.00.048.055 I print_info: n_embd_head_v    = 128
0.00.048.056 I print_info: n_gqa            = 1
0.00.048.057 I print_info: n_embd_k_gqa     = 2048
0.00.048.057 I print_info: n_embd_v_gqa     = 2048
0.00.048.058 I print_info: f_norm_eps       = 1.0e-05
0.00.048.058 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.059 I print_info: f_logit_scale    = 0.0e+00
0.00.048.060 I print_info: n_ff             = 8192
0.00.048.060 I print_info: n_expert         = 0
0.00.048.062 I print_info: n_expert_used    = 0
0.00.048.062 I print_info: causal attn      = 1
0.00.048.063 I print_info: pooling type     = 0
0.00.048.063 I print_info: rope type        = 2
0.00.048.063 I print_info: rope scaling     = linear
0.00.048.063 I print_info: freq_base_train  = 10000.0
0.00.048.064 I print_info: freq_scale_train = 1
0.00.048.064 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.064 I print_info: rope_finetuned   = unknown
0.00.048.064 I print_info: ssm_d_conv       = 0
0.00.048.066 I print_info: ssm_d_inner      = 0
0.00.048.066 I print_info: ssm_d_state      = 0
0.00.048.066 I print_info: ssm_dt_rank      = 0
0.00.048.066 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.066 I print_info: model type       = 1.4B
0.00.048.067 I print_info: model params     = 1.41 B
0.00.048.067 I print_info: general.name     = 1.4B
0.00.048.067 I print_info: vocab type       = BPE
0.00.048.067 I print_info: n_vocab          = 50304
0.00.048.067 I print_info: n_merges         = 50009
0.00.048.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.073 I print_info: LF token         = 128 'Ä'
0.00.048.073 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.073 I print_info: max token length = 1024
0.00.050.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.054 I load_tensors: offloading output layer to GPU
0.00.050.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.064 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.066 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.353 I llama_init_from_model: n_seq_max     = 1
0.00.050.354 I llama_init_from_model: n_ctx         = 128
0.00.050.354 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.354 I llama_init_from_model: n_batch       = 128
0.00.050.354 I llama_init_from_model: n_ubatch      = 128
0.00.050.354 I llama_init_from_model: flash_attn    = 0
0.00.050.355 I llama_init_from_model: freq_base     = 10000.0
0.00.050.355 I llama_init_from_model: freq_scale    = 1
0.00.050.355 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.356 I ggml_metal_init: allocating
0.00.050.358 I ggml_metal_init: found device: Apple M4
0.00.050.360 I ggml_metal_init: picking default device: Apple M4
0.00.050.925 I ggml_metal_init: using embedded metal library
0.00.053.245 I ggml_metal_init: GPU name:   Apple M4
0.00.053.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.248 I ggml_metal_init: simdgroup reduction   = true
0.00.053.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.248 I ggml_metal_init: has bfloat            = true
0.00.053.248 I ggml_metal_init: use bfloat            = true
0.00.053.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.592 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.842 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.846 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.737 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.738 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.739 I llama_init_from_model: graph nodes  = 967
0.00.063.739 I llama_init_from_model: graph splits = 2
0.00.063.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.320.020 I 
0.00.320.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.320.075 I perplexity: tokenizing the input ..
0.00.327.240 I perplexity: tokenization took 7.163 ms
0.00.327.244 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.467.288 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.468.446 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.468.475 I llama_perf_context_print:        load time =     311.20 ms
0.00.468.476 I llama_perf_context_print: prompt eval time =     139.82 ms /   128 tokens (    1.09 ms per token,   915.48 tokens per second)
0.00.468.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.468.477 I llama_perf_context_print:       total time =     148.46 ms /   129 tokens
0.00.468.987 I ggml_metal_free: deallocating

real	0m0.482s
user	0m0.075s
sys	0m0.067s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4512 (ae3c1db2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152b0b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152b0b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152b0bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152b0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152b0ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152b0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152b0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152b0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152b0e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152b0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152b0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152b0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152b0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152b10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152b10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152b11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152b11960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152b12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152b127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152b12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152b13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152b13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152b144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152b14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152b15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152b15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152b15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152b169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152b16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152b171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152b17670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152b17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152b181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152b18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152b189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152b18e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152b19300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152b197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152b19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152b1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152b1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152b1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152b1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152b1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152b1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152b1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152b1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152b1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152b1d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152b1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152b1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152b1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152b1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152b1efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152b1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152b1fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152b200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152b203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152b209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152b211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152b21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152b21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152b21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152b22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152b226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152b22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152b23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152b234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152b23970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152b23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152b242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152b24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152b24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152b25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152b25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152b25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152b26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152b26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152b26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152b27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152b27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152b27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152b28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152b28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152b28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152b29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152b29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152b29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152b2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152b2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152b2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152b2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152b2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152b2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152b2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152b2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152b2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152b1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152b2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152b2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152b2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152b2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152b2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152b2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152b2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152b2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152b2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152b30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152b30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152b30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152b31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152b31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152b31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152b32140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152b325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152b32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152b32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152b333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152b33860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152b33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152b341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152b34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152b34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152b34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152b35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152b358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152b35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152b36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152b366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152b36b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152b36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152b37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152b37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152b37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152b38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152b38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152b38ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152b39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152b394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152b39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152b39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152b3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152b3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152b3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152b3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152b3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152b3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152b3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152b3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152b3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152b3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152b3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152b3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152b3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152b3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152b3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152b3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152b3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152b3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152b3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152b3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152b3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152b403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152b40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152b40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152b411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152b41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152b41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152b41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152b42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152b428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152b42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152b43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152b436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152b43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152b44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152b444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152b44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152b44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152b45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152b45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152b45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152b46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152b46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152b469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152b46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152b472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152b47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152b47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152b480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152b48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152b48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152b48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152b493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152b49940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152b49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152b4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152b4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152b4acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152b4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152b4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152b4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152b4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152b4c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152b4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152b4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152b4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152b4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152b4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152b4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152b4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152b4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152b4fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152b501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152b50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152b50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152b511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152b516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152b51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152b52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152b526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152b52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152b53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152b536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152b53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152b54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152b546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152b54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152b55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152b556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152b55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152b56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152b566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152b56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152b57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152b57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152b57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152b58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152b58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152b58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152b59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152b59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152b59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152b5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152b5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152b5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152b5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152b5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152b5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152b5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152b5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152b5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152b5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152b5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152b5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152b5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152b5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152b5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152b5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152b5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152b5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152b600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152b60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152b60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152b610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152b615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152b61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152b61fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152b62480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152b62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152b62dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152b63260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152b63700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152b63ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152b64040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152b644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152b64980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152b64e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152b652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152b65760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152b65c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152b660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152b665f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152b66d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152b67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152b67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152b68270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152b68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152b68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152b68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152b695f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.133.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.899 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152909650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152909ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15290a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15290a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15290ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15290b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15290b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15290bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15290c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15290c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15290cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15290ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15290d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15290e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15290e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15290ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15290f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15290fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1529104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152910e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152911570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152911c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1529123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152912ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1529131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1529134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152913ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1529140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1529146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152914ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152915370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152915630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152915ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152916400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1529166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152916b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152917000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1529174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152917940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152917de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152918280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152918720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152918bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152919060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152919320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152919930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152919f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15291a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15291ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15291b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15291b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15291bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15291c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15291c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15291d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15291d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15291dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15291dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15291e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15291eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15291f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15291f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15291f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15291fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1529202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152b4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152b4af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152b692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152b4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152b4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152b1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152b1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152b20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152b4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152b15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152b1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152b1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152b1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152b1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152b1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152b1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152b1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152b14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152b0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152b0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152b1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152b20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152b2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152b687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152b17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152b17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152b4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152b4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152b16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152b162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152b165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152b69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152b69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152b69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152b6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152b6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152b6a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152b6aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152b6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152b6b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152b6b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152b6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152b6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152b6bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152b6be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152b6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152b6c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152b6c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152b6c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152b6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152b6ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152b6d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152b6d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152b6d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152b6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152b6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152b6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152b6e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152b6e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152b6e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152b6ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152b6ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152b6ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152b6f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152b6f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152b6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152b6fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152b6fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152b70010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152b702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152b70590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152b70850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152b70b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152b70dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152b71090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152b71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152b71610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152b718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152b71b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152b71e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152b72110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152b723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152b72690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152b72950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152b72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152b72ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152b73190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152b73450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152b73710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152b739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152b73c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152b73f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152b74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152b744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152b74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152b74a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152b74d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152b74fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152b75290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152b75550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152b75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152b75ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152b75d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152b76050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152b76310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152b765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152b76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152b76b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152b76e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152b770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152b77390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152b77650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152b77910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152b77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152b77e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152b78150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152b78410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152b786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152b78990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152b78c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152b78f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152b791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152b79490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152b79750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152b79a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152b79cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152b79f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152b7a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152b7a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152b7a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152b7aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152b7ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152b7b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152b7b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152b7b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152b7b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152b7bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152b7bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152b7c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152b7c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152b7c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152b7c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152b7cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152b7ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152b7d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152b7d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152b7d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152b7dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152b7df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152b7e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152b7e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152b7e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152b7ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152b7ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152b7efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152b7f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152b7f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152b7f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152b7faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152b7fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152b80020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152b802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152b805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152b80860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152b80b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152b80de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152b810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152b81360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152b81620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152b818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152b81ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152b81e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152b82120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152b823e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152b826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152b82960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152b82c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152b82ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152b831a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152b83460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152b83720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152b839e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152b83ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152b83f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152b84220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152b844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152b847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152b84a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152b84d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152b84fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152b852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152b85560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152b85820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152b85ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152b85da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152b86060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152b86320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152b865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152b868a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152b86b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152b86e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152b870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152b873a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152b87660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152b87920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152b87be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152b87ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152b88160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152b88420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152b886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152b889a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152b88c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152b88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152b891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152b894a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152b89760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152b89a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152b89ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152b89fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152b8a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152b8a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152b8a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152b8aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152b8ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152b8b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152b8b2e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152b8b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152b8bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152b8bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152b8c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152b8c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152b8c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152b8ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152b8cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152b8cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152b8d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152b8d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152b8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152b8dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152b8e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152b8e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152b8eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152b8ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152b8f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152b8f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152b8f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152b8fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152b8fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152b8ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152b902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152b90560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152b90820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152b90ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152b90da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152b91060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152b91320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152b915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152b918a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152b91b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152b91e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152b920e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152b923a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152b92660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152b92920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152b92be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152b92ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152b93160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152b93420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152b936e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152b939a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152b93c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152b93f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152b941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152b944a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152b94760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152b94a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152b94ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152b94fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152b95260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152b95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152b957e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152b95aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152b95d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152b96020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152b962e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152b965a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152b96860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152b96b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152b96de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152b970a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152b97360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152b97620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152b978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152b97ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152b97e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152b98120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152b983e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152b986a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152b98960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152b98c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152b98ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152b991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152b99460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152b99720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152b999e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152b99ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152b99f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152b9a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152b9a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152b9a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152b9aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152b9ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152b9afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152b9b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152b9b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152b9b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152b9bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152b9bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152b9c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152b9c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152b9c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152b9c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152b9cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152b9ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152b9d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152b9d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152b9d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152b9d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152b9dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152b9dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152b9e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152b9e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152b9e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152b9e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152b9ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152b9ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152b9f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152b9f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152b9f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152b9fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152b9fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152b9ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152ba0260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152ba0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152ba07e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152ba0aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152ba0d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152ba1020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152ba12e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152ba15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152ba1860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152ba1b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152ba1de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152ba20a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152ba2360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152ba2620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152ba28e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152ba2ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152ba2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152ba3120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152ba33e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152ba36a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152ba3960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152ba3c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152ba3ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152ba41a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152ba4460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152ba4720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152ba49e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152ba4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152ba4f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152ba5220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152ba54e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152ba57a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152ba5a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152ba5d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152ba5fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152ba62a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152ba6560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152ba6820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152ba6ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152ba6da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152ba7060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152ba7320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152ba75e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152ba78a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152ba7b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152ba7e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152ba80e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152ba83a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152ba8660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152ba8920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152ba8be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152ba8ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152ba9160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152ba9420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152ba96e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152ba99a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152ba9c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152ba9f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152baa1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152baa4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152baa760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152baaa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152baace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152baafa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152bab260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152bab520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152bab7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152babaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152babd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152bac020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152bac2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152bac5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152bac860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152bacb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152bacde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152bad0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152bad360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152bad620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152bad8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152badba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152bade60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152bae120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152bae3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152bae6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152bae960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152baec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152baeee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152baf1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152baf460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152baf720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152baf9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152bafca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152baff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152bb0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152bb07f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152bb0ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152bb0d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152bb1030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152bb12f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152bb15b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152bb1870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152bb1b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152bb1df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152bb20b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152bb2370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152bb2630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152bb28f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152bb2bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152bb2e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152bb3130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152bb33f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152bb36b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152bb3970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152bb3c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152bb3ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152bb41b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152bb4470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152bb4730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152bb49f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152bb4cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152bb4f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152bb5230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152bb54f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152bb57b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152bb5a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152bb5d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152bb6280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152bb67d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152bb6d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152bb7270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152bb77c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152bb7d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152bb8260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152bb87b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152bb8d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152bb9250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152bb97a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152bb9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152bba240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152bba790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152bbace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152bbb230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152bbb780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152bbbcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152bbc220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152bbc770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152bbccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152bbd210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152bbd760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152bbdcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152bbe200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152bbe6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152bbeb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152bbefe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152bbf480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152bbf920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152bbfdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152bc0260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152bc0700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152bc0ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152bc1040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152bc14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152bc1980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152bc1e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152bc22c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152bc2760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152bc2cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152bc33d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152bc3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152bc4210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152bc4930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152bc4bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152bc53e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152bc56a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152bc5cb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.823s
user	0m0.288s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4512 (ae3c1db2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a70d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a70dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a70e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a70e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a70ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a70f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a70f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a70fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a712df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a715240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a7167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a717040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a717760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a7191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a7194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a71a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a71ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a71b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a71ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a71c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a71ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a71d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a71d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a71d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a71df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a71e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a71ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a71f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a71fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a721290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a7223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a723480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a7249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a7257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a7260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a7293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a72a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a72a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a72ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a72b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a72b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a72be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a72c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a72ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a72d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a72d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a72de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a72e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a72e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a72ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a71eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a72f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a72fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a72ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a7314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a732f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a7334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a733f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a7348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a7351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a736470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a736910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a736db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a7376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a7384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a7392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a739750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a73a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a73a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a73a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a73b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a73b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a73bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a73c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a73c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a73ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a73ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a73d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a73d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a73dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a73e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a73e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a73ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a73ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a73f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a73f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a73fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a7401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a740af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a740f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a7418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a742210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a7426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a742b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a744270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a744710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a745050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a7454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a7462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a7479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a748330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a7487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a7495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a74a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a74acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a74b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a74bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a74c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a74c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a74c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a74cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a74d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a74dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a74e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a74eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a74f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a74f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a74ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a7503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a7519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a7529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a7539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a7549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a7559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a75a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a75a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a75aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a75b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a75b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a75be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a75c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a75c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a75ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a75d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a75d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a75de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a75e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a75e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a75ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a75f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a75f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a75fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a7603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a7608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a7618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a7628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a762e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a7638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a763e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a7642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a764750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a7659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a7667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a7670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a7688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a769e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a76a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a76a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a76aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a76b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a76b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a76b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a74edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a74cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a74d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a720930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a74f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a71e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a71f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a71f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a71dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a71fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a716ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a722f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a72f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a76aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a719ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a71a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a74f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a74de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a7182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a7185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a76bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a76bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a76c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a76c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a76c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a76cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a76cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a76d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a76d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a76d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a76d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a76db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a76de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a76e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a76e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a76e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a76e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a76ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a76eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a76f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a76f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a76f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a76f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a76fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a76ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a7701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a7704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a770760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a770a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a770ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a770fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a771260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a771520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a7717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a771aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a771d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a772020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a7722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a7725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a772860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a772b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a772de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a7730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a773360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a773620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a7738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a773ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a773e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a774120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a7743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a7746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a774960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a774c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a774ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a7751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a775460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a775720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a7759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a775ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a775f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a776220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a7764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a7767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a776a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a776d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a776fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a7772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a777560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a777820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a777ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a777da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a778060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a778320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a7785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a7788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a778b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a778e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a7790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a7793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a779660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a779920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a779be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a779ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a77a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a77a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a77a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a77a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a77ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a77af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a77b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a77b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a77b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a77ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a77bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a77bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a77c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a77c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a77c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a77caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a77cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a77d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a77d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a77d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a77d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a77db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a77dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a77e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a77e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a77e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a77e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a77eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a77ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a77f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a77f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a77f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a77f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a77fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a77fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a7801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a780460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a780720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a7809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a780ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a780f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a781220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a7814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a7817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a781a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a781d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a781fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a7822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a782560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a782820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a782ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a782da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a783060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a783320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a7835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a7838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a783b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a783e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a7840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a7843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a784660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a784920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a784be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a784ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a785160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a785420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a7856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a7859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a785c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a785f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a7861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a7864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a786760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a786a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a786ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a786fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a787260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a787520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a7877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a787aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a787d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a788020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a7882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a7885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a788860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a788b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a788de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a7890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a789360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a789620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a7898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a789ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a789e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a78a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a78a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a78a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a78a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a78ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a78aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a78b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a78b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a78b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a78bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a78bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a78c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a78c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a78cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a78cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a78d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a78d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a78dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a78e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a78e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a78ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a78eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a78f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a78f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a78fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a790090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a790500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a790970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a790de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a791250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a7916c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a791b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a791fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a792410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a792880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a792cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a793160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a7935d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a793a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a793eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a794320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a794790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a794c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a795070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a7954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a795950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a795dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a796230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a7966a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a796b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a796f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a7973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a797860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a797cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a798140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a7985b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a798a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a798e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a799300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a799770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a799be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a79a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a79a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a79a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a79ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a79b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a79b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a79baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a79bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a79c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a79c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a79ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a79d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a79d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a79da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a79de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a79e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a79e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a79ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a79f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a79f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a79f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a7a0380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a7a0aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a7a11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a7a18e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a7a1ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a7a2390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a7a2650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a7a2c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1091044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109104950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109104dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109105230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1091056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109105b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109105f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1091063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109106860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109106cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109107140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109107860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109108380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109108b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109109340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109109a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10910a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10910a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10910afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10910b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10910be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10910c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10910cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10910d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10910da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10910dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10910e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10910e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10910e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10910ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10910f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10910f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10910fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10910fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1091102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109110710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109110b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109110ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109111460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1091118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109111d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1091121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109112620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109112a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109112f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109113370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1091137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109113c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1091140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109114530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1091149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109114e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109115280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1091156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109115b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109115fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109116540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109116a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109117320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109117790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109117c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109118070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1091184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109118950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109118dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109119230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1091196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109119b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109119f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10911a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10911a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10911acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10911b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10911b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10911ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10911be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10911c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10911c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10911cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10911d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10911d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10911d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10911dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10911e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10911e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10911eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10911ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10911f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10911f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10911fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109120120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109120590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109120a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109120e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1091212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109121750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109121bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109122030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1091224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109122910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109122d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1091231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109123a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109123d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1091241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109124620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109124a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x109124f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109125370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1091257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109125c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1091260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109126530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1091269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x109126e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109127280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1091276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109127b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109127fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109128440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1091288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109128d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109129190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109129600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109129a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109129ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10912a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10912a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10912ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10912b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10912b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10912b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10912bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10912c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10912c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10912cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10912cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10912d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10912d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10912dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10912e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10912e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10912ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10912eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10912f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10912f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10912fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109130080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1091304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109130960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109130dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109131240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1091316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109131b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109131f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109132400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109132870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109132ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109133150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1091335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109133a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109133ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109134310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109134780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109134bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109135060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1091354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109135940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109135db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109136220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109136690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109136b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109136f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1091373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109137850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109137cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109138130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1091385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109138a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109138e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1091392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109139760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109139bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10913a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10913a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10913a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10913ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10913b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10913b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10913bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10913bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10913c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10913c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10913cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10913d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10913d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10913d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10913de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10913e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10913e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10913ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10913f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10913f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10913f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10913fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1091401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109140650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109140ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109140f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109141ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109141d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109142030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1091424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109142910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109142d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1091431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109143660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109143ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109143f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1091443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109144820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109144c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109145100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109145570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1091459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109145e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1091462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109146730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109146ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109147010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109147480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1091478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109147d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1091481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109148640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109148ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109148f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109149390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109149800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109149c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10914a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10914a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10914a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10914ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10914b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10914b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10914bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10914bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10914c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10914c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10914cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10914d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10914d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10914da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10914df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10914e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10914e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10914ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10914f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10914f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10914f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10914fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109150280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1091506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109150b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109150fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109151440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1091518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109151d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109152190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109152600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109152a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109152ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109153350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1091537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109153c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1091540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109154510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109154980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109154df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109155260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1091556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109156140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109156860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109156f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1091576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109157960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109157dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1091583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1091589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.908s
user	0m0.242s
sys	0m0.135s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.12 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.15 user         0.04 sys
```
