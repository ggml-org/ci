Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.552s
user	0m0.893s
sys	0m1.234s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-perplexity
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.028s
user	0m6.061s
sys	0m9.778s

main: quantize time =  3392.03 ms
main:    total time =  3392.03 ms

main: quantize time =  1954.70 ms
main:    total time =  1954.70 ms

main: quantize time =  3249.91 ms
main:    total time =  3249.91 ms

main: quantize time =  1720.42 ms
main:    total time =  1720.42 ms

main: quantize time =  2384.39 ms
main:    total time =  2384.39 ms

main: quantize time =  5220.80 ms
main:    total time =  5220.80 ms

main: quantize time =  5605.97 ms
main:    total time =  5605.97 ms

main: quantize time =  7251.62 ms
main:    total time =  7251.62 ms

main: quantize time =  5967.26 ms
main:    total time =  5967.26 ms

main: quantize time =  4507.74 ms
main:    total time =  4507.74 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.278 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.036.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.051.794 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.051.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.051.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.051.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.051.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.051.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.051.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.051.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.051.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.051.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.051.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.051.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.051.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.051.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.051.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.051.831 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.051.831 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.069.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.069.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.069.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.069.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.069.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.069.759 I llama_model_loader: - type  f32:  194 tensors
0.00.069.759 I llama_model_loader: - type  f16:   98 tensors
0.00.069.761 I print_info: file format = GGUF V3 (latest)
0.00.069.762 I print_info: file type   = all F32 (guessed)
0.00.069.764 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.462 I load: special tokens cache size = 25
0.00.103.315 I load: token to piece cache size = 0.2984 MB
0.00.103.319 I print_info: arch             = gptneox
0.00.103.319 I print_info: vocab_only       = 0
0.00.103.319 I print_info: n_ctx_train      = 2048
0.00.103.319 I print_info: n_embd           = 2048
0.00.103.319 I print_info: n_layer          = 24
0.00.103.322 I print_info: n_head           = 16
0.00.103.323 I print_info: n_head_kv        = 16
0.00.103.323 I print_info: n_rot            = 32
0.00.103.324 I print_info: n_swa            = 0
0.00.103.324 I print_info: n_embd_head_k    = 128
0.00.103.324 I print_info: n_embd_head_v    = 128
0.00.103.325 I print_info: n_gqa            = 1
0.00.103.325 I print_info: n_embd_k_gqa     = 2048
0.00.103.326 I print_info: n_embd_v_gqa     = 2048
0.00.103.326 I print_info: f_norm_eps       = 1.0e-05
0.00.103.327 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.327 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.327 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.327 I print_info: f_logit_scale    = 0.0e+00
0.00.103.328 I print_info: n_ff             = 8192
0.00.103.328 I print_info: n_expert         = 0
0.00.103.328 I print_info: n_expert_used    = 0
0.00.103.328 I print_info: causal attn      = 1
0.00.103.328 I print_info: pooling type     = 0
0.00.103.329 I print_info: rope type        = 2
0.00.103.330 I print_info: rope scaling     = linear
0.00.103.330 I print_info: freq_base_train  = 10000.0
0.00.103.330 I print_info: freq_scale_train = 1
0.00.103.330 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.331 I print_info: rope_finetuned   = unknown
0.00.103.331 I print_info: ssm_d_conv       = 0
0.00.103.331 I print_info: ssm_d_inner      = 0
0.00.103.331 I print_info: ssm_d_state      = 0
0.00.103.331 I print_info: ssm_dt_rank      = 0
0.00.103.331 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.332 I print_info: model type       = 1.4B
0.00.103.332 I print_info: model params     = 1.41 B
0.00.103.332 I print_info: general.name     = 1.4B
0.00.103.333 I print_info: vocab type       = BPE
0.00.103.333 I print_info: n_vocab          = 50304
0.00.103.333 I print_info: n_merges         = 50009
0.00.103.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.336 I print_info: LF token         = 128 'Ä'
0.00.103.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.336 I print_info: max token length = 1024
0.00.105.787 I load_tensors: offloading 24 repeating layers to GPU
0.00.105.787 I load_tensors: offloading output layer to GPU
0.00.105.787 I load_tensors: offloaded 25/25 layers to GPU
0.00.105.805 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.806 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.106.073 I llama_init_from_model: n_seq_max     = 1
0.00.106.074 I llama_init_from_model: n_ctx         = 2048
0.00.106.074 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.106.075 I llama_init_from_model: n_batch       = 2048
0.00.106.075 I llama_init_from_model: n_ubatch      = 512
0.00.106.075 I llama_init_from_model: flash_attn    = 0
0.00.106.075 I llama_init_from_model: freq_base     = 10000.0
0.00.106.076 I llama_init_from_model: freq_scale    = 1
0.00.106.076 I ggml_metal_init: allocating
0.00.106.079 I ggml_metal_init: found device: Apple M4
0.00.106.081 I ggml_metal_init: picking default device: Apple M4
0.00.106.730 I ggml_metal_init: using embedded metal library
0.00.116.867 I ggml_metal_init: GPU name:   Apple M4
0.00.116.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.870 I ggml_metal_init: simdgroup reduction   = true
0.00.116.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.870 I ggml_metal_init: has bfloat            = true
0.00.116.870 I ggml_metal_init: use bfloat            = true
0.00.116.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.000 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.160.277 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.283 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.161.315 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.161.317 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.161.317 I llama_init_from_model: graph nodes  = 967
0.00.161.318 I llama_init_from_model: graph splits = 2
0.00.161.321 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.240.841 I main: llama threadpool init, n_threads = 4
0.00.240.887 I 
0.00.240.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.240.930 I 
0.00.240.994 I sampler seed: 1234
0.00.240.999 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.241.024 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.241.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.241.026 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.080.921 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.02.080.922 I llama_perf_context_print:        load time =     203.84 ms
0.02.080.922 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.21 tokens per second)
0.02.080.924 I llama_perf_context_print:        eval time =    1793.44 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.080.925 I llama_perf_context_print:       total time =    1840.08 ms /    70 tokens
0.02.081.129 I ggml_metal_free: deallocating

real	0m2.412s
user	0m0.142s
sys	0m0.101s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.002 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.231 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.234 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.234 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.234 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.235 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.771 I llama_model_loader: - type  f32:  194 tensors
0.00.031.772 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.773 I print_info: file format = GGUF V3 (latest)
0.00.031.773 I print_info: file type   = Q8_0
0.00.031.775 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.684 I load: special tokens cache size = 25
0.00.057.655 I load: token to piece cache size = 0.2984 MB
0.00.057.660 I print_info: arch             = gptneox
0.00.057.660 I print_info: vocab_only       = 0
0.00.057.660 I print_info: n_ctx_train      = 2048
0.00.057.660 I print_info: n_embd           = 2048
0.00.057.660 I print_info: n_layer          = 24
0.00.057.667 I print_info: n_head           = 16
0.00.057.668 I print_info: n_head_kv        = 16
0.00.057.668 I print_info: n_rot            = 32
0.00.057.668 I print_info: n_swa            = 0
0.00.057.668 I print_info: n_embd_head_k    = 128
0.00.057.668 I print_info: n_embd_head_v    = 128
0.00.057.669 I print_info: n_gqa            = 1
0.00.057.670 I print_info: n_embd_k_gqa     = 2048
0.00.057.670 I print_info: n_embd_v_gqa     = 2048
0.00.057.671 I print_info: f_norm_eps       = 1.0e-05
0.00.057.672 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.672 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.672 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.673 I print_info: f_logit_scale    = 0.0e+00
0.00.057.674 I print_info: n_ff             = 8192
0.00.057.674 I print_info: n_expert         = 0
0.00.057.674 I print_info: n_expert_used    = 0
0.00.057.674 I print_info: causal attn      = 1
0.00.057.674 I print_info: pooling type     = 0
0.00.057.674 I print_info: rope type        = 2
0.00.057.676 I print_info: rope scaling     = linear
0.00.057.676 I print_info: freq_base_train  = 10000.0
0.00.057.677 I print_info: freq_scale_train = 1
0.00.057.677 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.677 I print_info: rope_finetuned   = unknown
0.00.057.677 I print_info: ssm_d_conv       = 0
0.00.057.677 I print_info: ssm_d_inner      = 0
0.00.057.677 I print_info: ssm_d_state      = 0
0.00.057.680 I print_info: ssm_dt_rank      = 0
0.00.057.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.681 I print_info: model type       = 1.4B
0.00.057.681 I print_info: model params     = 1.41 B
0.00.057.681 I print_info: general.name     = 1.4B
0.00.057.682 I print_info: vocab type       = BPE
0.00.057.682 I print_info: n_vocab          = 50304
0.00.057.682 I print_info: n_merges         = 50009
0.00.057.683 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.683 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.687 I print_info: LF token         = 128 'Ä'
0.00.057.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.688 I print_info: max token length = 1024
0.00.060.072 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.072 I load_tensors: offloading output layer to GPU
0.00.060.072 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.084 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.085 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.060.421 I llama_init_from_model: n_seq_max     = 1
0.00.060.422 I llama_init_from_model: n_ctx         = 2048
0.00.060.422 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.422 I llama_init_from_model: n_batch       = 2048
0.00.060.422 I llama_init_from_model: n_ubatch      = 512
0.00.060.422 I llama_init_from_model: flash_attn    = 0
0.00.060.423 I llama_init_from_model: freq_base     = 10000.0
0.00.060.423 I llama_init_from_model: freq_scale    = 1
0.00.060.424 I ggml_metal_init: allocating
0.00.060.427 I ggml_metal_init: found device: Apple M4
0.00.060.429 I ggml_metal_init: picking default device: Apple M4
0.00.061.188 I ggml_metal_init: using embedded metal library
0.00.063.793 I ggml_metal_init: GPU name:   Apple M4
0.00.063.795 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.795 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.796 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.796 I ggml_metal_init: simdgroup reduction   = true
0.00.063.796 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.796 I ggml_metal_init: has bfloat            = true
0.00.063.797 I ggml_metal_init: use bfloat            = true
0.00.063.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.511 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.963 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.992 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.226 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.101.229 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.101.229 I llama_init_from_model: graph nodes  = 967
0.00.101.229 I llama_init_from_model: graph splits = 2
0.00.101.234 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.362 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.133.565 I main: llama threadpool init, n_threads = 4
0.01.133.603 I 
0.01.133.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.133.635 I 
0.01.133.822 I sampler seed: 1234
0.01.133.827 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.133.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.133.838 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.133.838 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.221.578 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.02.221.578 I llama_perf_context_print:        load time =    1123.56 ms
0.02.221.579 I llama_perf_context_print: prompt eval time =      42.80 ms /     7 tokens (    6.11 ms per token,   163.55 tokens per second)
0.02.221.580 I llama_perf_context_print:        eval time =    1042.07 ms /    63 runs   (   16.54 ms per token,    60.46 tokens per second)
0.02.221.580 I llama_perf_context_print:       total time =    1088.02 ms /    70 tokens
0.02.221.783 I ggml_metal_free: deallocating

real	0m2.240s
user	0m0.112s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.017.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.196 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.204 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.204 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.208 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.880 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.882 I llama_model_loader: - type  f32:  194 tensors
0.00.039.882 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.882 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.883 I print_info: file format = GGUF V3 (latest)
0.00.039.883 I print_info: file type   = Q4_0
0.00.039.884 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.450 I load: special tokens cache size = 25
0.00.069.742 I load: token to piece cache size = 0.2984 MB
0.00.069.745 I print_info: arch             = gptneox
0.00.069.745 I print_info: vocab_only       = 0
0.00.069.745 I print_info: n_ctx_train      = 2048
0.00.069.746 I print_info: n_embd           = 2048
0.00.069.746 I print_info: n_layer          = 24
0.00.069.751 I print_info: n_head           = 16
0.00.069.751 I print_info: n_head_kv        = 16
0.00.069.751 I print_info: n_rot            = 32
0.00.069.752 I print_info: n_swa            = 0
0.00.069.752 I print_info: n_embd_head_k    = 128
0.00.069.753 I print_info: n_embd_head_v    = 128
0.00.069.755 I print_info: n_gqa            = 1
0.00.069.756 I print_info: n_embd_k_gqa     = 2048
0.00.069.756 I print_info: n_embd_v_gqa     = 2048
0.00.069.757 I print_info: f_norm_eps       = 1.0e-05
0.00.069.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.759 I print_info: f_logit_scale    = 0.0e+00
0.00.069.760 I print_info: n_ff             = 8192
0.00.069.760 I print_info: n_expert         = 0
0.00.069.760 I print_info: n_expert_used    = 0
0.00.069.760 I print_info: causal attn      = 1
0.00.069.760 I print_info: pooling type     = 0
0.00.069.760 I print_info: rope type        = 2
0.00.069.760 I print_info: rope scaling     = linear
0.00.069.761 I print_info: freq_base_train  = 10000.0
0.00.069.761 I print_info: freq_scale_train = 1
0.00.069.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.761 I print_info: rope_finetuned   = unknown
0.00.069.762 I print_info: ssm_d_conv       = 0
0.00.069.762 I print_info: ssm_d_inner      = 0
0.00.069.762 I print_info: ssm_d_state      = 0
0.00.069.762 I print_info: ssm_dt_rank      = 0
0.00.069.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.762 I print_info: model type       = 1.4B
0.00.069.762 I print_info: model params     = 1.41 B
0.00.069.766 I print_info: general.name     = 1.4B
0.00.069.767 I print_info: vocab type       = BPE
0.00.069.767 I print_info: n_vocab          = 50304
0.00.069.767 I print_info: n_merges         = 50009
0.00.069.768 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.768 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.768 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.769 I print_info: LF token         = 128 'Ä'
0.00.069.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.769 I print_info: max token length = 1024
0.00.072.139 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.139 I load_tensors: offloading output layer to GPU
0.00.072.139 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.151 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.152 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.503 I llama_init_from_model: n_seq_max     = 1
0.00.072.504 I llama_init_from_model: n_ctx         = 2048
0.00.072.504 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.504 I llama_init_from_model: n_batch       = 2048
0.00.072.505 I llama_init_from_model: n_ubatch      = 512
0.00.072.505 I llama_init_from_model: flash_attn    = 0
0.00.072.505 I llama_init_from_model: freq_base     = 10000.0
0.00.072.505 I llama_init_from_model: freq_scale    = 1
0.00.072.506 I ggml_metal_init: allocating
0.00.072.509 I ggml_metal_init: found device: Apple M4
0.00.072.511 I ggml_metal_init: picking default device: Apple M4
0.00.073.333 I ggml_metal_init: using embedded metal library
0.00.076.294 I ggml_metal_init: GPU name:   Apple M4
0.00.076.296 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.297 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.297 I ggml_metal_init: simdgroup reduction   = true
0.00.076.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.297 I ggml_metal_init: has bfloat            = true
0.00.076.298 I ggml_metal_init: use bfloat            = true
0.00.076.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.409 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.455 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.463 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.119.748 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.119.750 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.119.751 I llama_init_from_model: graph nodes  = 967
0.00.119.751 I llama_init_from_model: graph splits = 2
0.00.119.755 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.125 I main: llama threadpool init, n_threads = 4
0.00.795.165 I 
0.00.795.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.205 I 
0.00.795.428 I sampler seed: 1234
0.00.795.434 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.445 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.446 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.446 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.472.085 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.472.086 I llama_perf_context_print:        load time =     778.11 ms
0.01.472.086 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.01 tokens per second)
0.01.472.087 I llama_perf_context_print:        eval time =     629.97 ms /    63 runs   (   10.00 ms per token,   100.00 tokens per second)
0.01.472.087 I llama_perf_context_print:       total time =     676.96 ms /    70 tokens
0.01.472.334 I ggml_metal_free: deallocating

real	0m1.492s
user	0m0.119s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.016.558 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.219 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.221 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.799 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.800 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.801 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.802 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.043.802 I llama_model_loader: - type  f32:  194 tensors
0.00.043.803 I llama_model_loader: - type q4_1:   97 tensors
0.00.043.803 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.803 I print_info: file format = GGUF V3 (latest)
0.00.043.804 I print_info: file type   = Q4_1
0.00.043.805 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.074.438 I load: special tokens cache size = 25
0.00.085.467 I load: token to piece cache size = 0.2984 MB
0.00.085.471 I print_info: arch             = gptneox
0.00.085.472 I print_info: vocab_only       = 0
0.00.085.472 I print_info: n_ctx_train      = 2048
0.00.085.472 I print_info: n_embd           = 2048
0.00.085.472 I print_info: n_layer          = 24
0.00.085.476 I print_info: n_head           = 16
0.00.085.477 I print_info: n_head_kv        = 16
0.00.085.480 I print_info: n_rot            = 32
0.00.085.480 I print_info: n_swa            = 0
0.00.085.480 I print_info: n_embd_head_k    = 128
0.00.085.480 I print_info: n_embd_head_v    = 128
0.00.085.481 I print_info: n_gqa            = 1
0.00.085.482 I print_info: n_embd_k_gqa     = 2048
0.00.085.483 I print_info: n_embd_v_gqa     = 2048
0.00.085.484 I print_info: f_norm_eps       = 1.0e-05
0.00.085.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.485 I print_info: f_logit_scale    = 0.0e+00
0.00.085.486 I print_info: n_ff             = 8192
0.00.085.486 I print_info: n_expert         = 0
0.00.085.486 I print_info: n_expert_used    = 0
0.00.085.487 I print_info: causal attn      = 1
0.00.085.487 I print_info: pooling type     = 0
0.00.085.487 I print_info: rope type        = 2
0.00.085.487 I print_info: rope scaling     = linear
0.00.085.488 I print_info: freq_base_train  = 10000.0
0.00.085.488 I print_info: freq_scale_train = 1
0.00.085.489 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.489 I print_info: rope_finetuned   = unknown
0.00.085.489 I print_info: ssm_d_conv       = 0
0.00.085.489 I print_info: ssm_d_inner      = 0
0.00.085.490 I print_info: ssm_d_state      = 0
0.00.085.491 I print_info: ssm_dt_rank      = 0
0.00.085.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.491 I print_info: model type       = 1.4B
0.00.085.492 I print_info: model params     = 1.41 B
0.00.085.492 I print_info: general.name     = 1.4B
0.00.085.493 I print_info: vocab type       = BPE
0.00.085.493 I print_info: n_vocab          = 50304
0.00.085.493 I print_info: n_merges         = 50009
0.00.085.494 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.494 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.494 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.494 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.495 I print_info: LF token         = 128 'Ä'
0.00.085.495 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.495 I print_info: max token length = 1024
0.00.087.730 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.731 I load_tensors: offloading output layer to GPU
0.00.087.731 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.742 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.087.743 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.088.145 I llama_init_from_model: n_seq_max     = 1
0.00.088.146 I llama_init_from_model: n_ctx         = 2048
0.00.088.146 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.088.146 I llama_init_from_model: n_batch       = 2048
0.00.088.147 I llama_init_from_model: n_ubatch      = 512
0.00.088.147 I llama_init_from_model: flash_attn    = 0
0.00.088.148 I llama_init_from_model: freq_base     = 10000.0
0.00.088.148 I llama_init_from_model: freq_scale    = 1
0.00.088.148 I ggml_metal_init: allocating
0.00.088.152 I ggml_metal_init: found device: Apple M4
0.00.088.155 I ggml_metal_init: picking default device: Apple M4
0.00.088.935 I ggml_metal_init: using embedded metal library
0.00.092.532 I ggml_metal_init: GPU name:   Apple M4
0.00.092.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.536 I ggml_metal_init: simdgroup reduction   = true
0.00.092.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.536 I ggml_metal_init: has bfloat            = true
0.00.092.536 I ggml_metal_init: use bfloat            = true
0.00.092.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.585 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.125.661 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.667 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.686 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.126.684 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.126.685 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.126.686 I llama_init_from_model: graph nodes  = 967
0.00.126.686 I llama_init_from_model: graph splits = 2
0.00.126.689 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.049.277 I main: llama threadpool init, n_threads = 4
0.01.049.325 I 
0.01.049.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.049.367 I 
0.01.049.610 I sampler seed: 1234
0.01.049.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.049.668 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.049.673 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.049.673 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.774.933 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65985.13 tokens per second)
0.01.774.934 I llama_perf_context_print:        load time =    1032.71 ms
0.01.774.935 I llama_perf_context_print: prompt eval time =      45.99 ms /     7 tokens (    6.57 ms per token,   152.20 tokens per second)
0.01.774.935 I llama_perf_context_print:        eval time =     676.45 ms /    63 runs   (   10.74 ms per token,    93.13 tokens per second)
0.01.774.936 I llama_perf_context_print:       total time =     725.66 ms /    70 tokens
0.01.775.178 I ggml_metal_free: deallocating

real	0m1.800s
user	0m0.135s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.018.617 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.047.970 I llama_model_loader: - type  f32:  194 tensors
0.00.047.970 I llama_model_loader: - type q5_0:   97 tensors
0.00.047.970 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.971 I print_info: file format = GGUF V3 (latest)
0.00.047.971 I print_info: file type   = Q5_0
0.00.047.974 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.082.191 I load: special tokens cache size = 25
0.00.092.891 I load: token to piece cache size = 0.2984 MB
0.00.092.895 I print_info: arch             = gptneox
0.00.092.896 I print_info: vocab_only       = 0
0.00.092.896 I print_info: n_ctx_train      = 2048
0.00.092.896 I print_info: n_embd           = 2048
0.00.092.896 I print_info: n_layer          = 24
0.00.092.900 I print_info: n_head           = 16
0.00.092.901 I print_info: n_head_kv        = 16
0.00.092.901 I print_info: n_rot            = 32
0.00.092.901 I print_info: n_swa            = 0
0.00.092.902 I print_info: n_embd_head_k    = 128
0.00.092.904 I print_info: n_embd_head_v    = 128
0.00.092.905 I print_info: n_gqa            = 1
0.00.092.906 I print_info: n_embd_k_gqa     = 2048
0.00.092.907 I print_info: n_embd_v_gqa     = 2048
0.00.092.907 I print_info: f_norm_eps       = 1.0e-05
0.00.092.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.910 I print_info: f_logit_scale    = 0.0e+00
0.00.092.910 I print_info: n_ff             = 8192
0.00.092.911 I print_info: n_expert         = 0
0.00.092.911 I print_info: n_expert_used    = 0
0.00.092.911 I print_info: causal attn      = 1
0.00.092.911 I print_info: pooling type     = 0
0.00.092.913 I print_info: rope type        = 2
0.00.092.914 I print_info: rope scaling     = linear
0.00.092.915 I print_info: freq_base_train  = 10000.0
0.00.092.915 I print_info: freq_scale_train = 1
0.00.092.915 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.916 I print_info: rope_finetuned   = unknown
0.00.092.916 I print_info: ssm_d_conv       = 0
0.00.092.916 I print_info: ssm_d_inner      = 0
0.00.092.916 I print_info: ssm_d_state      = 0
0.00.092.916 I print_info: ssm_dt_rank      = 0
0.00.092.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.917 I print_info: model type       = 1.4B
0.00.092.917 I print_info: model params     = 1.41 B
0.00.092.918 I print_info: general.name     = 1.4B
0.00.092.924 I print_info: vocab type       = BPE
0.00.092.924 I print_info: n_vocab          = 50304
0.00.092.924 I print_info: n_merges         = 50009
0.00.092.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.925 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.925 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.927 I print_info: LF token         = 128 'Ä'
0.00.092.927 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.928 I print_info: max token length = 1024
0.00.095.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.653 I load_tensors: offloading output layer to GPU
0.00.095.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.665 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.095.667 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.096.095 I llama_init_from_model: n_seq_max     = 1
0.00.096.096 I llama_init_from_model: n_ctx         = 2048
0.00.096.096 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.096.096 I llama_init_from_model: n_batch       = 2048
0.00.096.097 I llama_init_from_model: n_ubatch      = 512
0.00.096.097 I llama_init_from_model: flash_attn    = 0
0.00.096.098 I llama_init_from_model: freq_base     = 10000.0
0.00.096.098 I llama_init_from_model: freq_scale    = 1
0.00.096.099 I ggml_metal_init: allocating
0.00.096.103 I ggml_metal_init: found device: Apple M4
0.00.096.105 I ggml_metal_init: picking default device: Apple M4
0.00.096.963 I ggml_metal_init: using embedded metal library
0.00.100.726 I ggml_metal_init: GPU name:   Apple M4
0.00.100.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.729 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.730 I ggml_metal_init: simdgroup reduction   = true
0.00.100.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.730 I ggml_metal_init: has bfloat            = true
0.00.100.731 I ggml_metal_init: use bfloat            = true
0.00.100.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.379 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.138.987 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.138.997 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.139.017 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.140.012 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.140.014 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.140.014 I llama_init_from_model: graph nodes  = 967
0.00.140.014 I llama_init_from_model: graph splits = 2
0.00.140.018 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.140.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.850 I main: llama threadpool init, n_threads = 4
0.00.942.963 I 
0.00.943.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.943.044 I 
0.00.943.556 I sampler seed: 1234
0.00.943.564 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.943.649 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.943.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.943.664 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.754.613 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47364.91 tokens per second)
0.01.754.614 I llama_perf_context_print:        load time =     924.22 ms
0.01.754.615 I llama_perf_context_print: prompt eval time =      54.03 ms /     7 tokens (    7.72 ms per token,   129.57 tokens per second)
0.01.754.617 I llama_perf_context_print:        eval time =     754.23 ms /    63 runs   (   11.97 ms per token,    83.53 tokens per second)
0.01.754.617 I llama_perf_context_print:       total time =     811.77 ms /    70 tokens
0.01.754.858 I ggml_metal_free: deallocating

real	0m1.791s
user	0m0.149s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.073 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.107 I main: llama backend init
0.00.000.110 I main: load the model and apply lora adapter, if any
0.00.016.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.980 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.034.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.044.223 I llama_model_loader: - type  f32:  194 tensors
0.00.044.224 I llama_model_loader: - type q5_1:   97 tensors
0.00.044.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.225 I print_info: file format = GGUF V3 (latest)
0.00.044.226 I print_info: file type   = Q5_1
0.00.044.228 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.070.786 I load: special tokens cache size = 25
0.00.076.825 I load: token to piece cache size = 0.2984 MB
0.00.076.829 I print_info: arch             = gptneox
0.00.076.830 I print_info: vocab_only       = 0
0.00.076.830 I print_info: n_ctx_train      = 2048
0.00.076.830 I print_info: n_embd           = 2048
0.00.076.830 I print_info: n_layer          = 24
0.00.076.835 I print_info: n_head           = 16
0.00.076.835 I print_info: n_head_kv        = 16
0.00.076.837 I print_info: n_rot            = 32
0.00.076.837 I print_info: n_swa            = 0
0.00.076.838 I print_info: n_embd_head_k    = 128
0.00.076.838 I print_info: n_embd_head_v    = 128
0.00.076.839 I print_info: n_gqa            = 1
0.00.076.840 I print_info: n_embd_k_gqa     = 2048
0.00.076.841 I print_info: n_embd_v_gqa     = 2048
0.00.076.841 I print_info: f_norm_eps       = 1.0e-05
0.00.076.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.842 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.842 I print_info: f_logit_scale    = 0.0e+00
0.00.076.843 I print_info: n_ff             = 8192
0.00.076.843 I print_info: n_expert         = 0
0.00.076.843 I print_info: n_expert_used    = 0
0.00.076.843 I print_info: causal attn      = 1
0.00.076.843 I print_info: pooling type     = 0
0.00.076.843 I print_info: rope type        = 2
0.00.076.844 I print_info: rope scaling     = linear
0.00.076.844 I print_info: freq_base_train  = 10000.0
0.00.076.844 I print_info: freq_scale_train = 1
0.00.076.844 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.844 I print_info: rope_finetuned   = unknown
0.00.076.844 I print_info: ssm_d_conv       = 0
0.00.076.845 I print_info: ssm_d_inner      = 0
0.00.076.845 I print_info: ssm_d_state      = 0
0.00.076.845 I print_info: ssm_dt_rank      = 0
0.00.076.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.845 I print_info: model type       = 1.4B
0.00.076.846 I print_info: model params     = 1.41 B
0.00.076.846 I print_info: general.name     = 1.4B
0.00.076.846 I print_info: vocab type       = BPE
0.00.076.846 I print_info: n_vocab          = 50304
0.00.076.847 I print_info: n_merges         = 50009
0.00.076.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.849 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.849 I print_info: LF token         = 128 'Ä'
0.00.076.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.850 I print_info: max token length = 1024
0.00.078.831 I load_tensors: offloading 24 repeating layers to GPU
0.00.078.831 I load_tensors: offloading output layer to GPU
0.00.078.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.078.842 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.078.843 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.079.163 I llama_init_from_model: n_seq_max     = 1
0.00.079.164 I llama_init_from_model: n_ctx         = 2048
0.00.079.164 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.164 I llama_init_from_model: n_batch       = 2048
0.00.079.164 I llama_init_from_model: n_ubatch      = 512
0.00.079.165 I llama_init_from_model: flash_attn    = 0
0.00.079.165 I llama_init_from_model: freq_base     = 10000.0
0.00.079.165 I llama_init_from_model: freq_scale    = 1
0.00.079.166 I ggml_metal_init: allocating
0.00.079.169 I ggml_metal_init: found device: Apple M4
0.00.079.171 I ggml_metal_init: picking default device: Apple M4
0.00.079.793 I ggml_metal_init: using embedded metal library
0.00.082.228 I ggml_metal_init: GPU name:   Apple M4
0.00.082.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.231 I ggml_metal_init: simdgroup reduction   = true
0.00.082.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.231 I ggml_metal_init: has bfloat            = true
0.00.082.232 I ggml_metal_init: use bfloat            = true
0.00.082.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.588 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.986 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.926 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.927 I llama_init_from_model: graph nodes  = 967
0.00.114.927 I llama_init_from_model: graph splits = 2
0.00.114.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.248.238 I main: llama threadpool init, n_threads = 4
0.01.248.335 I 
0.01.248.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.248.404 I 
0.01.248.703 I sampler seed: 1234
0.01.248.715 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.248.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.248.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.248.754 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.097.201 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.097.202 I llama_perf_context_print:        load time =    1231.41 ms
0.02.097.203 I llama_perf_context_print: prompt eval time =      53.43 ms /     7 tokens (    7.63 ms per token,   131.01 tokens per second)
0.02.097.204 I llama_perf_context_print:        eval time =     792.03 ms /    63 runs   (   12.57 ms per token,    79.54 tokens per second)
0.02.097.204 I llama_perf_context_print:       total time =     848.97 ms /    70 tokens
0.02.097.394 I ggml_metal_free: deallocating

real	0m2.149s
user	0m0.130s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.489 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.675 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.458 I llama_model_loader: - type  f32:  194 tensors
0.00.030.459 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.459 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.460 I print_info: file format = GGUF V3 (latest)
0.00.030.460 I print_info: file type   = Q2_K - Medium
0.00.030.461 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.058.026 I load: special tokens cache size = 25
0.00.065.723 I load: token to piece cache size = 0.2984 MB
0.00.065.726 I print_info: arch             = gptneox
0.00.065.726 I print_info: vocab_only       = 0
0.00.065.727 I print_info: n_ctx_train      = 2048
0.00.065.727 I print_info: n_embd           = 2048
0.00.065.727 I print_info: n_layer          = 24
0.00.065.730 I print_info: n_head           = 16
0.00.065.731 I print_info: n_head_kv        = 16
0.00.065.731 I print_info: n_rot            = 32
0.00.065.731 I print_info: n_swa            = 0
0.00.065.731 I print_info: n_embd_head_k    = 128
0.00.065.731 I print_info: n_embd_head_v    = 128
0.00.065.732 I print_info: n_gqa            = 1
0.00.065.733 I print_info: n_embd_k_gqa     = 2048
0.00.065.737 I print_info: n_embd_v_gqa     = 2048
0.00.065.737 I print_info: f_norm_eps       = 1.0e-05
0.00.065.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.738 I print_info: f_logit_scale    = 0.0e+00
0.00.065.739 I print_info: n_ff             = 8192
0.00.065.739 I print_info: n_expert         = 0
0.00.065.739 I print_info: n_expert_used    = 0
0.00.065.739 I print_info: causal attn      = 1
0.00.065.740 I print_info: pooling type     = 0
0.00.065.740 I print_info: rope type        = 2
0.00.065.741 I print_info: rope scaling     = linear
0.00.065.742 I print_info: freq_base_train  = 10000.0
0.00.065.742 I print_info: freq_scale_train = 1
0.00.065.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.742 I print_info: rope_finetuned   = unknown
0.00.065.743 I print_info: ssm_d_conv       = 0
0.00.065.743 I print_info: ssm_d_inner      = 0
0.00.065.743 I print_info: ssm_d_state      = 0
0.00.065.743 I print_info: ssm_dt_rank      = 0
0.00.065.743 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.743 I print_info: model type       = 1.4B
0.00.065.744 I print_info: model params     = 1.41 B
0.00.065.744 I print_info: general.name     = 1.4B
0.00.065.746 I print_info: vocab type       = BPE
0.00.065.746 I print_info: n_vocab          = 50304
0.00.065.746 I print_info: n_merges         = 50009
0.00.065.746 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.752 I print_info: LF token         = 128 'Ä'
0.00.065.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.752 I print_info: max token length = 1024
0.00.068.049 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.049 I load_tensors: offloading output layer to GPU
0.00.068.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.061 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.068.063 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.068.505 I llama_init_from_model: n_seq_max     = 1
0.00.068.506 I llama_init_from_model: n_ctx         = 2048
0.00.068.506 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.068.507 I llama_init_from_model: n_batch       = 2048
0.00.068.507 I llama_init_from_model: n_ubatch      = 512
0.00.068.507 I llama_init_from_model: flash_attn    = 0
0.00.068.508 I llama_init_from_model: freq_base     = 10000.0
0.00.068.508 I llama_init_from_model: freq_scale    = 1
0.00.068.509 I ggml_metal_init: allocating
0.00.068.513 I ggml_metal_init: found device: Apple M4
0.00.068.516 I ggml_metal_init: picking default device: Apple M4
0.00.069.396 I ggml_metal_init: using embedded metal library
0.00.073.267 I ggml_metal_init: GPU name:   Apple M4
0.00.073.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.271 I ggml_metal_init: simdgroup reduction   = true
0.00.073.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.271 I ggml_metal_init: has bfloat            = true
0.00.073.272 I ggml_metal_init: use bfloat            = true
0.00.073.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.205 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.249 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.256 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.111.424 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.111.426 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.111.426 I llama_init_from_model: graph nodes  = 967
0.00.111.427 I llama_init_from_model: graph splits = 2
0.00.111.430 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.854 I main: llama threadpool init, n_threads = 4
0.00.605.894 I 
0.00.605.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.925 I 
0.00.606.152 I sampler seed: 1234
0.00.606.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.177 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.177 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.283.514 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.283.514 I llama_perf_context_print:        load time =     594.36 ms
0.01.283.515 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.49 tokens per second)
0.01.283.515 I llama_perf_context_print:        eval time =     638.57 ms /    63 runs   (   10.14 ms per token,    98.66 tokens per second)
0.01.283.516 I llama_perf_context_print:       total time =     677.66 ms /    70 tokens
0.01.283.737 I ggml_metal_free: deallocating

real	0m1.316s
user	0m0.129s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.633 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.634 I llama_model_loader: - type  f32:  194 tensors
0.00.026.635 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.635 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.635 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.636 I print_info: file format = GGUF V3 (latest)
0.00.026.636 I print_info: file type   = Q3_K - Medium
0.00.026.638 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.036 I load: special tokens cache size = 25
0.00.052.183 I load: token to piece cache size = 0.2984 MB
0.00.052.185 I print_info: arch             = gptneox
0.00.052.186 I print_info: vocab_only       = 0
0.00.052.186 I print_info: n_ctx_train      = 2048
0.00.052.186 I print_info: n_embd           = 2048
0.00.052.186 I print_info: n_layer          = 24
0.00.052.189 I print_info: n_head           = 16
0.00.052.190 I print_info: n_head_kv        = 16
0.00.052.190 I print_info: n_rot            = 32
0.00.052.190 I print_info: n_swa            = 0
0.00.052.190 I print_info: n_embd_head_k    = 128
0.00.052.190 I print_info: n_embd_head_v    = 128
0.00.052.191 I print_info: n_gqa            = 1
0.00.052.192 I print_info: n_embd_k_gqa     = 2048
0.00.052.193 I print_info: n_embd_v_gqa     = 2048
0.00.052.193 I print_info: f_norm_eps       = 1.0e-05
0.00.052.193 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.194 I print_info: f_logit_scale    = 0.0e+00
0.00.052.195 I print_info: n_ff             = 8192
0.00.052.195 I print_info: n_expert         = 0
0.00.052.195 I print_info: n_expert_used    = 0
0.00.052.197 I print_info: causal attn      = 1
0.00.052.198 I print_info: pooling type     = 0
0.00.052.198 I print_info: rope type        = 2
0.00.052.198 I print_info: rope scaling     = linear
0.00.052.199 I print_info: freq_base_train  = 10000.0
0.00.052.199 I print_info: freq_scale_train = 1
0.00.052.200 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.200 I print_info: rope_finetuned   = unknown
0.00.052.200 I print_info: ssm_d_conv       = 0
0.00.052.200 I print_info: ssm_d_inner      = 0
0.00.052.200 I print_info: ssm_d_state      = 0
0.00.052.202 I print_info: ssm_dt_rank      = 0
0.00.052.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.202 I print_info: model type       = 1.4B
0.00.052.203 I print_info: model params     = 1.41 B
0.00.052.203 I print_info: general.name     = 1.4B
0.00.052.204 I print_info: vocab type       = BPE
0.00.052.204 I print_info: n_vocab          = 50304
0.00.052.204 I print_info: n_merges         = 50009
0.00.052.204 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.205 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.205 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.205 I print_info: LF token         = 128 'Ä'
0.00.052.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.206 I print_info: max token length = 1024
0.00.054.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.147 I load_tensors: offloading output layer to GPU
0.00.054.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.158 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.159 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.446 I llama_init_from_model: n_seq_max     = 1
0.00.054.446 I llama_init_from_model: n_ctx         = 2048
0.00.054.447 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.447 I llama_init_from_model: n_batch       = 2048
0.00.054.447 I llama_init_from_model: n_ubatch      = 512
0.00.054.447 I llama_init_from_model: flash_attn    = 0
0.00.054.447 I llama_init_from_model: freq_base     = 10000.0
0.00.054.448 I llama_init_from_model: freq_scale    = 1
0.00.054.448 I ggml_metal_init: allocating
0.00.054.451 I ggml_metal_init: found device: Apple M4
0.00.054.453 I ggml_metal_init: picking default device: Apple M4
0.00.055.041 I ggml_metal_init: using embedded metal library
0.00.057.404 I ggml_metal_init: GPU name:   Apple M4
0.00.057.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.407 I ggml_metal_init: simdgroup reduction   = true
0.00.057.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.407 I ggml_metal_init: has bfloat            = true
0.00.057.407 I ggml_metal_init: use bfloat            = true
0.00.057.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.215 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.044 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.050 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.009 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.010 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.010 I llama_init_from_model: graph nodes  = 967
0.00.087.011 I llama_init_from_model: graph splits = 2
0.00.087.013 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.498 I main: llama threadpool init, n_threads = 4
0.00.545.547 I 
0.00.545.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.581 I 
0.00.545.811 I sampler seed: 1234
0.00.545.816 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.545.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.545.856 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.545.856 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.638 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.289.639 I llama_perf_context_print:        load time =     534.97 ms
0.01.289.640 I llama_perf_context_print: prompt eval time =      44.45 ms /     7 tokens (    6.35 ms per token,   157.46 tokens per second)
0.01.289.641 I llama_perf_context_print:        eval time =     696.16 ms /    63 runs   (   11.05 ms per token,    90.50 tokens per second)
0.01.289.641 I llama_perf_context_print:       total time =     744.14 ms /    70 tokens
0.01.289.887 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.110s
sys	0m0.129s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.555 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.003 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.007 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.547 I llama_model_loader: - type  f32:  194 tensors
0.00.026.548 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.548 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.548 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.549 I print_info: file format = GGUF V3 (latest)
0.00.026.549 I print_info: file type   = Q4_K - Medium
0.00.026.550 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.838 I load: special tokens cache size = 25
0.00.051.730 I load: token to piece cache size = 0.2984 MB
0.00.051.733 I print_info: arch             = gptneox
0.00.051.733 I print_info: vocab_only       = 0
0.00.051.734 I print_info: n_ctx_train      = 2048
0.00.051.734 I print_info: n_embd           = 2048
0.00.051.734 I print_info: n_layer          = 24
0.00.051.737 I print_info: n_head           = 16
0.00.051.737 I print_info: n_head_kv        = 16
0.00.051.738 I print_info: n_rot            = 32
0.00.051.738 I print_info: n_swa            = 0
0.00.051.739 I print_info: n_embd_head_k    = 128
0.00.051.740 I print_info: n_embd_head_v    = 128
0.00.051.740 I print_info: n_gqa            = 1
0.00.051.741 I print_info: n_embd_k_gqa     = 2048
0.00.051.742 I print_info: n_embd_v_gqa     = 2048
0.00.051.742 I print_info: f_norm_eps       = 1.0e-05
0.00.051.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.743 I print_info: f_logit_scale    = 0.0e+00
0.00.051.744 I print_info: n_ff             = 8192
0.00.051.744 I print_info: n_expert         = 0
0.00.051.744 I print_info: n_expert_used    = 0
0.00.051.744 I print_info: causal attn      = 1
0.00.051.745 I print_info: pooling type     = 0
0.00.051.745 I print_info: rope type        = 2
0.00.051.745 I print_info: rope scaling     = linear
0.00.051.745 I print_info: freq_base_train  = 10000.0
0.00.051.746 I print_info: freq_scale_train = 1
0.00.051.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.747 I print_info: rope_finetuned   = unknown
0.00.051.747 I print_info: ssm_d_conv       = 0
0.00.051.747 I print_info: ssm_d_inner      = 0
0.00.051.748 I print_info: ssm_d_state      = 0
0.00.051.748 I print_info: ssm_dt_rank      = 0
0.00.051.748 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.750 I print_info: model type       = 1.4B
0.00.051.750 I print_info: model params     = 1.41 B
0.00.051.750 I print_info: general.name     = 1.4B
0.00.051.751 I print_info: vocab type       = BPE
0.00.051.751 I print_info: n_vocab          = 50304
0.00.051.751 I print_info: n_merges         = 50009
0.00.051.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.752 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.752 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.752 I print_info: LF token         = 128 'Ä'
0.00.051.753 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.753 I print_info: max token length = 1024
0.00.053.284 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.285 I load_tensors: offloading output layer to GPU
0.00.053.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.295 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.296 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.576 I llama_init_from_model: n_seq_max     = 1
0.00.053.577 I llama_init_from_model: n_ctx         = 2048
0.00.053.577 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.577 I llama_init_from_model: n_batch       = 2048
0.00.053.577 I llama_init_from_model: n_ubatch      = 512
0.00.053.577 I llama_init_from_model: flash_attn    = 0
0.00.053.578 I llama_init_from_model: freq_base     = 10000.0
0.00.053.578 I llama_init_from_model: freq_scale    = 1
0.00.053.578 I ggml_metal_init: allocating
0.00.053.581 I ggml_metal_init: found device: Apple M4
0.00.053.583 I ggml_metal_init: picking default device: Apple M4
0.00.054.144 I ggml_metal_init: using embedded metal library
0.00.056.495 I ggml_metal_init: GPU name:   Apple M4
0.00.056.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.497 I ggml_metal_init: simdgroup reduction   = true
0.00.056.498 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.498 I ggml_metal_init: has bfloat            = true
0.00.056.498 I ggml_metal_init: use bfloat            = true
0.00.056.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.909 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.761 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.778 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.795 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.797 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.797 I llama_init_from_model: graph nodes  = 967
0.00.085.797 I llama_init_from_model: graph splits = 2
0.00.085.800 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.020 I main: llama threadpool init, n_threads = 4
0.00.619.080 I 
0.00.619.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.119 I 
0.00.619.348 I sampler seed: 1234
0.00.619.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.619.366 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.619.366 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.619.366 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.482 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.373.483 I llama_perf_context_print:        load time =     608.46 ms
0.01.373.483 I llama_perf_context_print: prompt eval time =      50.37 ms /     7 tokens (    7.20 ms per token,   138.97 tokens per second)
0.01.373.484 I llama_perf_context_print:        eval time =     700.48 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.373.484 I llama_perf_context_print:       total time =     754.47 ms /    70 tokens
0.01.373.721 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.899 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.736 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.736 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.737 I llama_model_loader: - type  f32:  194 tensors
0.00.025.738 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.738 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.738 I print_info: file format = GGUF V3 (latest)
0.00.025.739 I print_info: file type   = Q5_K - Medium
0.00.025.740 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.298 I load: special tokens cache size = 25
0.00.050.275 I load: token to piece cache size = 0.2984 MB
0.00.050.278 I print_info: arch             = gptneox
0.00.050.279 I print_info: vocab_only       = 0
0.00.050.279 I print_info: n_ctx_train      = 2048
0.00.050.279 I print_info: n_embd           = 2048
0.00.050.279 I print_info: n_layer          = 24
0.00.050.282 I print_info: n_head           = 16
0.00.050.283 I print_info: n_head_kv        = 16
0.00.050.283 I print_info: n_rot            = 32
0.00.050.283 I print_info: n_swa            = 0
0.00.050.284 I print_info: n_embd_head_k    = 128
0.00.050.284 I print_info: n_embd_head_v    = 128
0.00.050.284 I print_info: n_gqa            = 1
0.00.050.285 I print_info: n_embd_k_gqa     = 2048
0.00.050.286 I print_info: n_embd_v_gqa     = 2048
0.00.050.287 I print_info: f_norm_eps       = 1.0e-05
0.00.050.287 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.287 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.287 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.289 I print_info: f_logit_scale    = 0.0e+00
0.00.050.289 I print_info: n_ff             = 8192
0.00.050.289 I print_info: n_expert         = 0
0.00.050.290 I print_info: n_expert_used    = 0
0.00.050.290 I print_info: causal attn      = 1
0.00.050.290 I print_info: pooling type     = 0
0.00.050.291 I print_info: rope type        = 2
0.00.050.293 I print_info: rope scaling     = linear
0.00.050.294 I print_info: freq_base_train  = 10000.0
0.00.050.294 I print_info: freq_scale_train = 1
0.00.050.294 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.294 I print_info: rope_finetuned   = unknown
0.00.050.295 I print_info: ssm_d_conv       = 0
0.00.050.295 I print_info: ssm_d_inner      = 0
0.00.050.295 I print_info: ssm_d_state      = 0
0.00.050.295 I print_info: ssm_dt_rank      = 0
0.00.050.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.295 I print_info: model type       = 1.4B
0.00.050.296 I print_info: model params     = 1.41 B
0.00.050.296 I print_info: general.name     = 1.4B
0.00.050.301 I print_info: vocab type       = BPE
0.00.050.301 I print_info: n_vocab          = 50304
0.00.050.301 I print_info: n_merges         = 50009
0.00.050.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: LF token         = 128 'Ä'
0.00.050.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.304 I print_info: max token length = 1024
0.00.052.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.319 I load_tensors: offloading output layer to GPU
0.00.052.320 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.330 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.332 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.614 I llama_init_from_model: n_seq_max     = 1
0.00.052.614 I llama_init_from_model: n_ctx         = 2048
0.00.052.615 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.615 I llama_init_from_model: n_batch       = 2048
0.00.052.615 I llama_init_from_model: n_ubatch      = 512
0.00.052.615 I llama_init_from_model: flash_attn    = 0
0.00.052.615 I llama_init_from_model: freq_base     = 10000.0
0.00.052.616 I llama_init_from_model: freq_scale    = 1
0.00.052.616 I ggml_metal_init: allocating
0.00.052.619 I ggml_metal_init: found device: Apple M4
0.00.052.621 I ggml_metal_init: picking default device: Apple M4
0.00.053.213 I ggml_metal_init: using embedded metal library
0.00.055.537 I ggml_metal_init: GPU name:   Apple M4
0.00.055.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.539 I ggml_metal_init: simdgroup reduction   = true
0.00.055.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.540 I ggml_metal_init: has bfloat            = true
0.00.055.540 I ggml_metal_init: use bfloat            = true
0.00.055.540 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.151 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.417 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.422 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.442 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.440 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.441 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.442 I llama_init_from_model: graph nodes  = 967
0.00.086.442 I llama_init_from_model: graph splits = 2
0.00.086.445 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.282 I main: llama threadpool init, n_threads = 4
0.00.687.318 I 
0.00.687.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.348 I 
0.00.687.590 I sampler seed: 1234
0.00.687.595 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.643 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.644 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.644 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.532.726 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.01.532.727 I llama_perf_context_print:        load time =     677.38 ms
0.01.532.728 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.88 tokens per second)
0.01.532.729 I llama_perf_context_print:        eval time =     790.59 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.532.729 I llama_perf_context_print:       total time =     845.44 ms /    70 tokens
0.01.532.957 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.108s
sys	0m0.146s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.884 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.887 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.888 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.893 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.485 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.487 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.487 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.487 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.487 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.488 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.488 I llama_model_loader: - type  f32:  194 tensors
0.00.026.489 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.489 I print_info: file format = GGUF V3 (latest)
0.00.026.490 I print_info: file type   = Q6_K
0.00.026.490 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.778 I load: special tokens cache size = 25
0.00.051.800 I load: token to piece cache size = 0.2984 MB
0.00.051.803 I print_info: arch             = gptneox
0.00.051.804 I print_info: vocab_only       = 0
0.00.051.804 I print_info: n_ctx_train      = 2048
0.00.051.804 I print_info: n_embd           = 2048
0.00.051.804 I print_info: n_layer          = 24
0.00.051.807 I print_info: n_head           = 16
0.00.051.808 I print_info: n_head_kv        = 16
0.00.051.808 I print_info: n_rot            = 32
0.00.051.808 I print_info: n_swa            = 0
0.00.051.808 I print_info: n_embd_head_k    = 128
0.00.051.809 I print_info: n_embd_head_v    = 128
0.00.051.809 I print_info: n_gqa            = 1
0.00.051.810 I print_info: n_embd_k_gqa     = 2048
0.00.051.812 I print_info: n_embd_v_gqa     = 2048
0.00.051.813 I print_info: f_norm_eps       = 1.0e-05
0.00.051.813 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.813 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.814 I print_info: f_logit_scale    = 0.0e+00
0.00.051.814 I print_info: n_ff             = 8192
0.00.051.815 I print_info: n_expert         = 0
0.00.051.815 I print_info: n_expert_used    = 0
0.00.051.815 I print_info: causal attn      = 1
0.00.051.815 I print_info: pooling type     = 0
0.00.051.815 I print_info: rope type        = 2
0.00.051.815 I print_info: rope scaling     = linear
0.00.051.816 I print_info: freq_base_train  = 10000.0
0.00.051.816 I print_info: freq_scale_train = 1
0.00.051.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.816 I print_info: rope_finetuned   = unknown
0.00.051.817 I print_info: ssm_d_conv       = 0
0.00.051.817 I print_info: ssm_d_inner      = 0
0.00.051.817 I print_info: ssm_d_state      = 0
0.00.051.818 I print_info: ssm_dt_rank      = 0
0.00.051.818 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.819 I print_info: model type       = 1.4B
0.00.051.819 I print_info: model params     = 1.41 B
0.00.051.819 I print_info: general.name     = 1.4B
0.00.051.820 I print_info: vocab type       = BPE
0.00.051.820 I print_info: n_vocab          = 50304
0.00.051.820 I print_info: n_merges         = 50009
0.00.051.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.821 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.821 I print_info: LF token         = 128 'Ä'
0.00.051.821 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.822 I print_info: max token length = 1024
0.00.053.836 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.836 I load_tensors: offloading output layer to GPU
0.00.053.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.847 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.848 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.143 I llama_init_from_model: n_seq_max     = 1
0.00.054.144 I llama_init_from_model: n_ctx         = 2048
0.00.054.144 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.144 I llama_init_from_model: n_batch       = 2048
0.00.054.144 I llama_init_from_model: n_ubatch      = 512
0.00.054.145 I llama_init_from_model: flash_attn    = 0
0.00.054.145 I llama_init_from_model: freq_base     = 10000.0
0.00.054.145 I llama_init_from_model: freq_scale    = 1
0.00.054.146 I ggml_metal_init: allocating
0.00.054.149 I ggml_metal_init: found device: Apple M4
0.00.054.151 I ggml_metal_init: picking default device: Apple M4
0.00.054.734 I ggml_metal_init: using embedded metal library
0.00.057.130 I ggml_metal_init: GPU name:   Apple M4
0.00.057.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.132 I ggml_metal_init: simdgroup reduction   = true
0.00.057.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.132 I ggml_metal_init: has bfloat            = true
0.00.057.133 I ggml_metal_init: use bfloat            = true
0.00.057.133 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.295 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.953 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.963 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.132 I llama_init_from_model: graph nodes  = 967
0.00.088.132 I llama_init_from_model: graph splits = 2
0.00.088.135 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.096 I main: llama threadpool init, n_threads = 4
0.00.748.163 I 
0.00.748.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.193 I 
0.00.748.421 I sampler seed: 1234
0.00.748.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.469 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.470 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.470 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.629.935 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.629.936 I llama_perf_context_print:        load time =     738.15 ms
0.01.629.936 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.62 tokens per second)
0.01.629.937 I llama_perf_context_print:        eval time =     824.07 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.629.937 I llama_perf_context_print:       total time =     881.84 ms /    70 tokens
0.01.630.143 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.564 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.532 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.552 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.557 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.797 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.950 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.952 I llama_model_loader: - type  f32:  194 tensors
0.00.055.952 I llama_model_loader: - type  f16:   98 tensors
0.00.055.954 I print_info: file format = GGUF V3 (latest)
0.00.055.956 I print_info: file type   = all F32 (guessed)
0.00.055.959 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.086.003 I load: special tokens cache size = 25
0.00.092.988 I load: token to piece cache size = 0.2984 MB
0.00.092.991 I print_info: arch             = gptneox
0.00.092.992 I print_info: vocab_only       = 0
0.00.092.992 I print_info: n_ctx_train      = 2048
0.00.092.992 I print_info: n_embd           = 2048
0.00.092.992 I print_info: n_layer          = 24
0.00.092.996 I print_info: n_head           = 16
0.00.092.997 I print_info: n_head_kv        = 16
0.00.092.998 I print_info: n_rot            = 32
0.00.092.998 I print_info: n_swa            = 0
0.00.093.000 I print_info: n_embd_head_k    = 128
0.00.093.000 I print_info: n_embd_head_v    = 128
0.00.093.001 I print_info: n_gqa            = 1
0.00.093.001 I print_info: n_embd_k_gqa     = 2048
0.00.093.002 I print_info: n_embd_v_gqa     = 2048
0.00.093.003 I print_info: f_norm_eps       = 1.0e-05
0.00.093.003 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.093.003 I print_info: f_clamp_kqv      = 0.0e+00
0.00.093.003 I print_info: f_max_alibi_bias = 0.0e+00
0.00.093.003 I print_info: f_logit_scale    = 0.0e+00
0.00.093.004 I print_info: n_ff             = 8192
0.00.093.004 I print_info: n_expert         = 0
0.00.093.004 I print_info: n_expert_used    = 0
0.00.093.005 I print_info: causal attn      = 1
0.00.093.005 I print_info: pooling type     = 0
0.00.093.005 I print_info: rope type        = 2
0.00.093.006 I print_info: rope scaling     = linear
0.00.093.007 I print_info: freq_base_train  = 10000.0
0.00.093.007 I print_info: freq_scale_train = 1
0.00.093.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.093.007 I print_info: rope_finetuned   = unknown
0.00.093.007 I print_info: ssm_d_conv       = 0
0.00.093.007 I print_info: ssm_d_inner      = 0
0.00.093.007 I print_info: ssm_d_state      = 0
0.00.093.008 I print_info: ssm_dt_rank      = 0
0.00.093.008 I print_info: ssm_dt_b_c_rms   = 0
0.00.093.008 I print_info: model type       = 1.4B
0.00.093.008 I print_info: model params     = 1.41 B
0.00.093.008 I print_info: general.name     = 1.4B
0.00.093.009 I print_info: vocab type       = BPE
0.00.093.009 I print_info: n_vocab          = 50304
0.00.093.009 I print_info: n_merges         = 50009
0.00.093.014 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.093.014 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.093.014 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.093.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.093.015 I print_info: LF token         = 128 'Ä'
0.00.093.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.093.015 I print_info: max token length = 1024
0.00.095.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.654 I load_tensors: offloading output layer to GPU
0.00.095.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.665 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.666 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.095.970 I llama_init_from_model: n_seq_max     = 1
0.00.095.971 I llama_init_from_model: n_ctx         = 128
0.00.095.971 I llama_init_from_model: n_ctx_per_seq = 128
0.00.095.971 I llama_init_from_model: n_batch       = 128
0.00.095.971 I llama_init_from_model: n_ubatch      = 128
0.00.095.972 I llama_init_from_model: flash_attn    = 0
0.00.095.972 I llama_init_from_model: freq_base     = 10000.0
0.00.095.972 I llama_init_from_model: freq_scale    = 1
0.00.095.973 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.973 I ggml_metal_init: allocating
0.00.095.976 I ggml_metal_init: found device: Apple M4
0.00.095.978 I ggml_metal_init: picking default device: Apple M4
0.00.096.628 I ggml_metal_init: using embedded metal library
0.00.099.341 I ggml_metal_init: GPU name:   Apple M4
0.00.099.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.344 I ggml_metal_init: simdgroup reduction   = true
0.00.099.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.344 I ggml_metal_init: has bfloat            = true
0.00.099.344 I ggml_metal_init: use bfloat            = true
0.00.099.345 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.828 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.095 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.097 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.111 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.111.038 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.111.039 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.111.039 I llama_init_from_model: graph nodes  = 967
0.00.111.039 I llama_init_from_model: graph splits = 2
0.00.111.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.920.261 I 
0.00.920.335 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.920.345 I perplexity: tokenizing the input ..
0.00.932.211 I perplexity: tokenization took 11.864 ms
0.00.932.222 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.052.918 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.055.040 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.055.090 I llama_perf_context_print:        load time =     895.48 ms
0.01.055.095 I llama_perf_context_print: prompt eval time =     120.31 ms /   128 tokens (    0.94 ms per token,  1063.96 tokens per second)
0.01.055.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.055.097 I llama_perf_context_print:       total time =     134.84 ms /   129 tokens
0.01.055.690 I ggml_metal_free: deallocating

real	0m1.246s
user	0m0.124s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.535 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.593 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.609 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.616 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.617 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.762 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.765 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.768 I llama_model_loader: - type  f32:  194 tensors
0.00.035.769 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.770 I print_info: file format = GGUF V3 (latest)
0.00.035.770 I print_info: file type   = Q8_0
0.00.035.772 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.062.645 I load: special tokens cache size = 25
0.00.068.918 I load: token to piece cache size = 0.2984 MB
0.00.068.943 I print_info: arch             = gptneox
0.00.068.945 I print_info: vocab_only       = 0
0.00.068.945 I print_info: n_ctx_train      = 2048
0.00.068.945 I print_info: n_embd           = 2048
0.00.068.946 I print_info: n_layer          = 24
0.00.068.954 I print_info: n_head           = 16
0.00.068.955 I print_info: n_head_kv        = 16
0.00.068.955 I print_info: n_rot            = 32
0.00.068.956 I print_info: n_swa            = 0
0.00.068.956 I print_info: n_embd_head_k    = 128
0.00.068.956 I print_info: n_embd_head_v    = 128
0.00.068.959 I print_info: n_gqa            = 1
0.00.068.960 I print_info: n_embd_k_gqa     = 2048
0.00.068.961 I print_info: n_embd_v_gqa     = 2048
0.00.068.963 I print_info: f_norm_eps       = 1.0e-05
0.00.068.963 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.964 I print_info: f_logit_scale    = 0.0e+00
0.00.068.965 I print_info: n_ff             = 8192
0.00.068.965 I print_info: n_expert         = 0
0.00.068.965 I print_info: n_expert_used    = 0
0.00.068.966 I print_info: causal attn      = 1
0.00.068.966 I print_info: pooling type     = 0
0.00.068.966 I print_info: rope type        = 2
0.00.068.966 I print_info: rope scaling     = linear
0.00.068.967 I print_info: freq_base_train  = 10000.0
0.00.068.967 I print_info: freq_scale_train = 1
0.00.068.967 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.967 I print_info: rope_finetuned   = unknown
0.00.068.968 I print_info: ssm_d_conv       = 0
0.00.068.969 I print_info: ssm_d_inner      = 0
0.00.068.969 I print_info: ssm_d_state      = 0
0.00.068.969 I print_info: ssm_dt_rank      = 0
0.00.068.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.969 I print_info: model type       = 1.4B
0.00.068.970 I print_info: model params     = 1.41 B
0.00.068.970 I print_info: general.name     = 1.4B
0.00.068.970 I print_info: vocab type       = BPE
0.00.068.970 I print_info: n_vocab          = 50304
0.00.068.971 I print_info: n_merges         = 50009
0.00.068.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.971 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.971 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.972 I print_info: LF token         = 128 'Ä'
0.00.068.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.972 I print_info: max token length = 1024
0.00.071.290 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.290 I load_tensors: offloading output layer to GPU
0.00.071.290 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.301 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.302 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.071.705 I llama_init_from_model: n_seq_max     = 1
0.00.071.706 I llama_init_from_model: n_ctx         = 128
0.00.071.706 I llama_init_from_model: n_ctx_per_seq = 128
0.00.071.706 I llama_init_from_model: n_batch       = 128
0.00.071.706 I llama_init_from_model: n_ubatch      = 128
0.00.071.706 I llama_init_from_model: flash_attn    = 0
0.00.071.707 I llama_init_from_model: freq_base     = 10000.0
0.00.071.707 I llama_init_from_model: freq_scale    = 1
0.00.071.707 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.708 I ggml_metal_init: allocating
0.00.071.712 I ggml_metal_init: found device: Apple M4
0.00.071.714 I ggml_metal_init: picking default device: Apple M4
0.00.072.367 I ggml_metal_init: using embedded metal library
0.00.077.636 I ggml_metal_init: GPU name:   Apple M4
0.00.077.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.639 I ggml_metal_init: simdgroup reduction   = true
0.00.077.639 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.639 I ggml_metal_init: has bfloat            = true
0.00.077.639 I ggml_metal_init: use bfloat            = true
0.00.077.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.904 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.364 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.088.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.088.390 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.349 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.089.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.089.351 I llama_init_from_model: graph nodes  = 967
0.00.089.351 I llama_init_from_model: graph splits = 2
0.00.089.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.089.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.022.629 I 
0.01.022.688 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.022.700 I perplexity: tokenizing the input ..
0.01.036.330 I perplexity: tokenization took 13.627 ms
0.01.036.341 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.161.085 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.166.149 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.166.199 I llama_perf_context_print:        load time =    1010.09 ms
0.01.166.202 I llama_perf_context_print: prompt eval time =     124.48 ms /   128 tokens (    0.97 ms per token,  1028.24 tokens per second)
0.01.166.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.166.205 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.01.166.972 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.125s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.720 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.406 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.409 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.410 I llama_model_loader: - type  f32:  194 tensors
0.00.025.410 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.411 I print_info: file format = GGUF V3 (latest)
0.00.025.411 I print_info: file type   = Q4_0
0.00.025.412 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.548 I load: special tokens cache size = 25
0.00.051.408 I load: token to piece cache size = 0.2984 MB
0.00.051.413 I print_info: arch             = gptneox
0.00.051.413 I print_info: vocab_only       = 0
0.00.051.414 I print_info: n_ctx_train      = 2048
0.00.051.414 I print_info: n_embd           = 2048
0.00.051.414 I print_info: n_layer          = 24
0.00.051.418 I print_info: n_head           = 16
0.00.051.419 I print_info: n_head_kv        = 16
0.00.051.419 I print_info: n_rot            = 32
0.00.051.419 I print_info: n_swa            = 0
0.00.051.420 I print_info: n_embd_head_k    = 128
0.00.051.420 I print_info: n_embd_head_v    = 128
0.00.051.420 I print_info: n_gqa            = 1
0.00.051.424 I print_info: n_embd_k_gqa     = 2048
0.00.051.425 I print_info: n_embd_v_gqa     = 2048
0.00.051.426 I print_info: f_norm_eps       = 1.0e-05
0.00.051.426 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.427 I print_info: f_logit_scale    = 0.0e+00
0.00.051.428 I print_info: n_ff             = 8192
0.00.051.428 I print_info: n_expert         = 0
0.00.051.428 I print_info: n_expert_used    = 0
0.00.051.428 I print_info: causal attn      = 1
0.00.051.428 I print_info: pooling type     = 0
0.00.051.429 I print_info: rope type        = 2
0.00.051.429 I print_info: rope scaling     = linear
0.00.051.430 I print_info: freq_base_train  = 10000.0
0.00.051.430 I print_info: freq_scale_train = 1
0.00.051.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.431 I print_info: rope_finetuned   = unknown
0.00.051.431 I print_info: ssm_d_conv       = 0
0.00.051.431 I print_info: ssm_d_inner      = 0
0.00.051.431 I print_info: ssm_d_state      = 0
0.00.051.431 I print_info: ssm_dt_rank      = 0
0.00.051.431 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.432 I print_info: model type       = 1.4B
0.00.051.432 I print_info: model params     = 1.41 B
0.00.051.432 I print_info: general.name     = 1.4B
0.00.051.433 I print_info: vocab type       = BPE
0.00.051.433 I print_info: n_vocab          = 50304
0.00.051.433 I print_info: n_merges         = 50009
0.00.051.433 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.433 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.433 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.438 I print_info: LF token         = 128 'Ä'
0.00.051.438 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.438 I print_info: max token length = 1024
0.00.053.255 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.255 I load_tensors: offloading output layer to GPU
0.00.053.255 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.266 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.267 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.553 I llama_init_from_model: n_seq_max     = 1
0.00.053.554 I llama_init_from_model: n_ctx         = 128
0.00.053.554 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.554 I llama_init_from_model: n_batch       = 128
0.00.053.554 I llama_init_from_model: n_ubatch      = 128
0.00.053.555 I llama_init_from_model: flash_attn    = 0
0.00.053.555 I llama_init_from_model: freq_base     = 10000.0
0.00.053.555 I llama_init_from_model: freq_scale    = 1
0.00.053.556 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.556 I ggml_metal_init: allocating
0.00.053.559 I ggml_metal_init: found device: Apple M4
0.00.053.561 I ggml_metal_init: picking default device: Apple M4
0.00.054.143 I ggml_metal_init: using embedded metal library
0.00.056.463 I ggml_metal_init: GPU name:   Apple M4
0.00.056.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.466 I ggml_metal_init: simdgroup reduction   = true
0.00.056.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.466 I ggml_metal_init: has bfloat            = true
0.00.056.466 I ggml_metal_init: use bfloat            = true
0.00.056.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.003 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.279 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.285 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.300 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.200 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.202 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.202 I llama_init_from_model: graph nodes  = 967
0.00.069.202 I llama_init_from_model: graph splits = 2
0.00.069.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.410 I 
0.00.588.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.470 I perplexity: tokenizing the input ..
0.00.600.939 I perplexity: tokenization took 12.466 ms
0.00.600.948 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.460 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.640 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.675 I llama_perf_context_print:        load time =     578.68 ms
0.00.733.676 I llama_perf_context_print: prompt eval time =     131.10 ms /   128 tokens (    1.02 ms per token,   976.37 tokens per second)
0.00.733.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.677 I llama_perf_context_print:       total time =     145.27 ms /   129 tokens
0.00.734.208 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.086s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.899 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.900 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.900 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.900 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.901 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.578 I llama_model_loader: - type  f32:  194 tensors
0.00.024.578 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.579 I print_info: file format = GGUF V3 (latest)
0.00.024.580 I print_info: file type   = Q4_1
0.00.024.580 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.615 I load: special tokens cache size = 25
0.00.049.588 I load: token to piece cache size = 0.2984 MB
0.00.049.591 I print_info: arch             = gptneox
0.00.049.592 I print_info: vocab_only       = 0
0.00.049.592 I print_info: n_ctx_train      = 2048
0.00.049.592 I print_info: n_embd           = 2048
0.00.049.592 I print_info: n_layer          = 24
0.00.049.595 I print_info: n_head           = 16
0.00.049.596 I print_info: n_head_kv        = 16
0.00.049.596 I print_info: n_rot            = 32
0.00.049.596 I print_info: n_swa            = 0
0.00.049.596 I print_info: n_embd_head_k    = 128
0.00.049.598 I print_info: n_embd_head_v    = 128
0.00.049.599 I print_info: n_gqa            = 1
0.00.049.599 I print_info: n_embd_k_gqa     = 2048
0.00.049.600 I print_info: n_embd_v_gqa     = 2048
0.00.049.610 I print_info: f_norm_eps       = 1.0e-05
0.00.049.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.614 I print_info: f_logit_scale    = 0.0e+00
0.00.049.621 I print_info: n_ff             = 8192
0.00.049.622 I print_info: n_expert         = 0
0.00.049.622 I print_info: n_expert_used    = 0
0.00.049.622 I print_info: causal attn      = 1
0.00.049.623 I print_info: pooling type     = 0
0.00.049.623 I print_info: rope type        = 2
0.00.049.623 I print_info: rope scaling     = linear
0.00.049.625 I print_info: freq_base_train  = 10000.0
0.00.049.625 I print_info: freq_scale_train = 1
0.00.049.625 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.625 I print_info: rope_finetuned   = unknown
0.00.049.625 I print_info: ssm_d_conv       = 0
0.00.049.626 I print_info: ssm_d_inner      = 0
0.00.049.626 I print_info: ssm_d_state      = 0
0.00.049.626 I print_info: ssm_dt_rank      = 0
0.00.049.626 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.626 I print_info: model type       = 1.4B
0.00.049.627 I print_info: model params     = 1.41 B
0.00.049.627 I print_info: general.name     = 1.4B
0.00.049.627 I print_info: vocab type       = BPE
0.00.049.627 I print_info: n_vocab          = 50304
0.00.049.628 I print_info: n_merges         = 50009
0.00.049.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.630 I print_info: LF token         = 128 'Ä'
0.00.049.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: max token length = 1024
0.00.051.593 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.593 I load_tensors: offloading output layer to GPU
0.00.051.593 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.604 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.605 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.880 I llama_init_from_model: n_seq_max     = 1
0.00.051.881 I llama_init_from_model: n_ctx         = 128
0.00.051.881 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.881 I llama_init_from_model: n_batch       = 128
0.00.051.881 I llama_init_from_model: n_ubatch      = 128
0.00.051.882 I llama_init_from_model: flash_attn    = 0
0.00.051.882 I llama_init_from_model: freq_base     = 10000.0
0.00.051.882 I llama_init_from_model: freq_scale    = 1
0.00.051.883 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.883 I ggml_metal_init: allocating
0.00.051.886 I ggml_metal_init: found device: Apple M4
0.00.051.888 I ggml_metal_init: picking default device: Apple M4
0.00.052.475 I ggml_metal_init: using embedded metal library
0.00.054.854 I ggml_metal_init: GPU name:   Apple M4
0.00.054.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.857 I ggml_metal_init: simdgroup reduction   = true
0.00.054.857 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.857 I ggml_metal_init: has bfloat            = true
0.00.054.857 I ggml_metal_init: use bfloat            = true
0.00.054.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.345 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.566 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.570 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.532 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.533 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.533 I llama_init_from_model: graph nodes  = 967
0.00.066.533 I llama_init_from_model: graph splits = 2
0.00.066.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.078 I 
0.00.661.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.113 I perplexity: tokenizing the input ..
0.00.668.625 I perplexity: tokenization took 7.51 ms
0.00.668.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.594 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.793.737 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.793.765 I llama_perf_context_print:        load time =     652.25 ms
0.00.793.766 I llama_perf_context_print: prompt eval time =     123.74 ms /   128 tokens (    0.97 ms per token,  1034.44 tokens per second)
0.00.793.767 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.767 I llama_perf_context_print:       total time =     132.69 ms /   129 tokens
0.00.794.111 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.077s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.065 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.686 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.689 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.689 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.446 I llama_model_loader: - type  f32:  194 tensors
0.00.025.446 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.446 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.447 I print_info: file format = GGUF V3 (latest)
0.00.025.447 I print_info: file type   = Q5_0
0.00.025.448 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.721 I load: special tokens cache size = 25
0.00.049.608 I load: token to piece cache size = 0.2984 MB
0.00.049.611 I print_info: arch             = gptneox
0.00.049.611 I print_info: vocab_only       = 0
0.00.049.611 I print_info: n_ctx_train      = 2048
0.00.049.611 I print_info: n_embd           = 2048
0.00.049.611 I print_info: n_layer          = 24
0.00.049.614 I print_info: n_head           = 16
0.00.049.615 I print_info: n_head_kv        = 16
0.00.049.615 I print_info: n_rot            = 32
0.00.049.615 I print_info: n_swa            = 0
0.00.049.616 I print_info: n_embd_head_k    = 128
0.00.049.616 I print_info: n_embd_head_v    = 128
0.00.049.617 I print_info: n_gqa            = 1
0.00.049.617 I print_info: n_embd_k_gqa     = 2048
0.00.049.618 I print_info: n_embd_v_gqa     = 2048
0.00.049.619 I print_info: f_norm_eps       = 1.0e-05
0.00.049.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.619 I print_info: f_logit_scale    = 0.0e+00
0.00.049.620 I print_info: n_ff             = 8192
0.00.049.620 I print_info: n_expert         = 0
0.00.049.620 I print_info: n_expert_used    = 0
0.00.049.621 I print_info: causal attn      = 1
0.00.049.621 I print_info: pooling type     = 0
0.00.049.621 I print_info: rope type        = 2
0.00.049.621 I print_info: rope scaling     = linear
0.00.049.621 I print_info: freq_base_train  = 10000.0
0.00.049.622 I print_info: freq_scale_train = 1
0.00.049.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.622 I print_info: rope_finetuned   = unknown
0.00.049.622 I print_info: ssm_d_conv       = 0
0.00.049.623 I print_info: ssm_d_inner      = 0
0.00.049.623 I print_info: ssm_d_state      = 0
0.00.049.623 I print_info: ssm_dt_rank      = 0
0.00.049.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.623 I print_info: model type       = 1.4B
0.00.049.624 I print_info: model params     = 1.41 B
0.00.049.624 I print_info: general.name     = 1.4B
0.00.049.625 I print_info: vocab type       = BPE
0.00.049.627 I print_info: n_vocab          = 50304
0.00.049.628 I print_info: n_merges         = 50009
0.00.049.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: LF token         = 128 'Ä'
0.00.049.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.629 I print_info: max token length = 1024
0.00.051.586 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.586 I load_tensors: offloading output layer to GPU
0.00.051.586 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.597 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.598 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.877 I llama_init_from_model: n_seq_max     = 1
0.00.051.877 I llama_init_from_model: n_ctx         = 128
0.00.051.877 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.878 I llama_init_from_model: n_batch       = 128
0.00.051.878 I llama_init_from_model: n_ubatch      = 128
0.00.051.878 I llama_init_from_model: flash_attn    = 0
0.00.051.878 I llama_init_from_model: freq_base     = 10000.0
0.00.051.879 I llama_init_from_model: freq_scale    = 1
0.00.051.879 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.879 I ggml_metal_init: allocating
0.00.051.882 I ggml_metal_init: found device: Apple M4
0.00.051.884 I ggml_metal_init: picking default device: Apple M4
0.00.052.471 I ggml_metal_init: using embedded metal library
0.00.054.872 I ggml_metal_init: GPU name:   Apple M4
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.874 I ggml_metal_init: simdgroup reduction   = true
0.00.054.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.874 I ggml_metal_init: has bfloat            = true
0.00.054.875 I ggml_metal_init: use bfloat            = true
0.00.054.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.301 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.307 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.326 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.211 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.212 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.213 I llama_init_from_model: graph nodes  = 967
0.00.066.213 I llama_init_from_model: graph splits = 2
0.00.066.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.943 I 
0.00.681.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.991 I perplexity: tokenizing the input ..
0.00.690.215 I perplexity: tokenization took 8.222 ms
0.00.690.219 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.361 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.826.528 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.826.560 I llama_perf_context_print:        load time =     671.87 ms
0.00.826.563 I llama_perf_context_print: prompt eval time =     134.91 ms /   128 tokens (    1.05 ms per token,   948.75 tokens per second)
0.00.826.564 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.566 I llama_perf_context_print:       total time =     144.62 ms /   129 tokens
0.00.827.064 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.076s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.751 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.183 I llama_model_loader: - type  f32:  194 tensors
0.00.024.184 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.184 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.185 I print_info: file format = GGUF V3 (latest)
0.00.024.185 I print_info: file type   = Q5_1
0.00.024.186 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.469 I load: special tokens cache size = 25
0.00.048.383 I load: token to piece cache size = 0.2984 MB
0.00.048.386 I print_info: arch             = gptneox
0.00.048.386 I print_info: vocab_only       = 0
0.00.048.387 I print_info: n_ctx_train      = 2048
0.00.048.387 I print_info: n_embd           = 2048
0.00.048.387 I print_info: n_layer          = 24
0.00.048.390 I print_info: n_head           = 16
0.00.048.390 I print_info: n_head_kv        = 16
0.00.048.390 I print_info: n_rot            = 32
0.00.048.391 I print_info: n_swa            = 0
0.00.048.391 I print_info: n_embd_head_k    = 128
0.00.048.391 I print_info: n_embd_head_v    = 128
0.00.048.392 I print_info: n_gqa            = 1
0.00.048.393 I print_info: n_embd_k_gqa     = 2048
0.00.048.393 I print_info: n_embd_v_gqa     = 2048
0.00.048.394 I print_info: f_norm_eps       = 1.0e-05
0.00.048.394 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.395 I print_info: f_logit_scale    = 0.0e+00
0.00.048.395 I print_info: n_ff             = 8192
0.00.048.395 I print_info: n_expert         = 0
0.00.048.396 I print_info: n_expert_used    = 0
0.00.048.396 I print_info: causal attn      = 1
0.00.048.396 I print_info: pooling type     = 0
0.00.048.396 I print_info: rope type        = 2
0.00.048.396 I print_info: rope scaling     = linear
0.00.048.397 I print_info: freq_base_train  = 10000.0
0.00.048.397 I print_info: freq_scale_train = 1
0.00.048.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.397 I print_info: rope_finetuned   = unknown
0.00.048.397 I print_info: ssm_d_conv       = 0
0.00.048.400 I print_info: ssm_d_inner      = 0
0.00.048.400 I print_info: ssm_d_state      = 0
0.00.048.400 I print_info: ssm_dt_rank      = 0
0.00.048.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.400 I print_info: model type       = 1.4B
0.00.048.401 I print_info: model params     = 1.41 B
0.00.048.401 I print_info: general.name     = 1.4B
0.00.048.401 I print_info: vocab type       = BPE
0.00.048.402 I print_info: n_vocab          = 50304
0.00.048.402 I print_info: n_merges         = 50009
0.00.048.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.403 I print_info: LF token         = 128 'Ä'
0.00.048.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.404 I print_info: max token length = 1024
0.00.050.409 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.409 I load_tensors: offloading output layer to GPU
0.00.050.409 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.420 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.421 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.708 I llama_init_from_model: n_seq_max     = 1
0.00.050.709 I llama_init_from_model: n_ctx         = 128
0.00.050.709 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.709 I llama_init_from_model: n_batch       = 128
0.00.050.709 I llama_init_from_model: n_ubatch      = 128
0.00.050.709 I llama_init_from_model: flash_attn    = 0
0.00.050.710 I llama_init_from_model: freq_base     = 10000.0
0.00.050.710 I llama_init_from_model: freq_scale    = 1
0.00.050.710 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.711 I ggml_metal_init: allocating
0.00.050.714 I ggml_metal_init: found device: Apple M4
0.00.050.716 I ggml_metal_init: picking default device: Apple M4
0.00.051.293 I ggml_metal_init: using embedded metal library
0.00.053.611 I ggml_metal_init: GPU name:   Apple M4
0.00.053.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.613 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.614 I ggml_metal_init: simdgroup reduction   = true
0.00.053.614 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.614 I ggml_metal_init: has bfloat            = true
0.00.053.614 I ggml_metal_init: use bfloat            = true
0.00.053.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.019 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.300 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.303 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.196 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.197 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.197 I llama_init_from_model: graph nodes  = 967
0.00.065.197 I llama_init_from_model: graph splits = 2
0.00.065.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.898 I 
0.00.724.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.944 I perplexity: tokenizing the input ..
0.00.732.752 I perplexity: tokenization took 7.806 ms
0.00.732.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.184 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.868.343 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.868.367 I llama_perf_context_print:        load time =     716.05 ms
0.00.868.370 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.78 tokens per second)
0.00.868.371 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.372 I llama_perf_context_print:       total time =     143.47 ms /   129 tokens
0.00.868.753 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.076s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.416 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.655 I llama_model_loader: - type  f32:  194 tensors
0.00.026.655 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.655 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.656 I print_info: file format = GGUF V3 (latest)
0.00.026.656 I print_info: file type   = Q2_K - Medium
0.00.026.657 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.858 I load: special tokens cache size = 25
0.00.051.726 I load: token to piece cache size = 0.2984 MB
0.00.051.729 I print_info: arch             = gptneox
0.00.051.729 I print_info: vocab_only       = 0
0.00.051.730 I print_info: n_ctx_train      = 2048
0.00.051.730 I print_info: n_embd           = 2048
0.00.051.730 I print_info: n_layer          = 24
0.00.051.733 I print_info: n_head           = 16
0.00.051.734 I print_info: n_head_kv        = 16
0.00.051.734 I print_info: n_rot            = 32
0.00.051.734 I print_info: n_swa            = 0
0.00.051.734 I print_info: n_embd_head_k    = 128
0.00.051.735 I print_info: n_embd_head_v    = 128
0.00.051.735 I print_info: n_gqa            = 1
0.00.051.736 I print_info: n_embd_k_gqa     = 2048
0.00.051.737 I print_info: n_embd_v_gqa     = 2048
0.00.051.737 I print_info: f_norm_eps       = 1.0e-05
0.00.051.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.739 I print_info: f_logit_scale    = 0.0e+00
0.00.051.741 I print_info: n_ff             = 8192
0.00.051.741 I print_info: n_expert         = 0
0.00.051.741 I print_info: n_expert_used    = 0
0.00.051.742 I print_info: causal attn      = 1
0.00.051.742 I print_info: pooling type     = 0
0.00.051.742 I print_info: rope type        = 2
0.00.051.742 I print_info: rope scaling     = linear
0.00.051.742 I print_info: freq_base_train  = 10000.0
0.00.051.743 I print_info: freq_scale_train = 1
0.00.051.743 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.743 I print_info: rope_finetuned   = unknown
0.00.051.743 I print_info: ssm_d_conv       = 0
0.00.051.743 I print_info: ssm_d_inner      = 0
0.00.051.743 I print_info: ssm_d_state      = 0
0.00.051.744 I print_info: ssm_dt_rank      = 0
0.00.051.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.744 I print_info: model type       = 1.4B
0.00.051.744 I print_info: model params     = 1.41 B
0.00.051.744 I print_info: general.name     = 1.4B
0.00.051.745 I print_info: vocab type       = BPE
0.00.051.745 I print_info: n_vocab          = 50304
0.00.051.745 I print_info: n_merges         = 50009
0.00.051.746 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.746 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.746 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.748 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.748 I print_info: LF token         = 128 'Ä'
0.00.051.749 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.749 I print_info: max token length = 1024
0.00.053.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.455 I load_tensors: offloading output layer to GPU
0.00.053.456 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.461 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.462 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.728 I llama_init_from_model: n_seq_max     = 1
0.00.053.729 I llama_init_from_model: n_ctx         = 128
0.00.053.729 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.729 I llama_init_from_model: n_batch       = 128
0.00.053.729 I llama_init_from_model: n_ubatch      = 128
0.00.053.730 I llama_init_from_model: flash_attn    = 0
0.00.053.730 I llama_init_from_model: freq_base     = 10000.0
0.00.053.730 I llama_init_from_model: freq_scale    = 1
0.00.053.730 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.731 I ggml_metal_init: allocating
0.00.053.734 I ggml_metal_init: found device: Apple M4
0.00.053.736 I ggml_metal_init: picking default device: Apple M4
0.00.054.309 I ggml_metal_init: using embedded metal library
0.00.056.638 I ggml_metal_init: GPU name:   Apple M4
0.00.056.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.639 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.640 I ggml_metal_init: simdgroup reduction   = true
0.00.056.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.640 I ggml_metal_init: has bfloat            = true
0.00.056.641 I ggml_metal_init: use bfloat            = true
0.00.056.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.954 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.270 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.273 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.287 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.163 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.164 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.164 I llama_init_from_model: graph nodes  = 967
0.00.068.164 I llama_init_from_model: graph splits = 2
0.00.068.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.453 I 
0.00.423.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.493 I perplexity: tokenizing the input ..
0.00.431.678 I perplexity: tokenization took 8.184 ms
0.00.431.682 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.195 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.406 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.433 I llama_perf_context_print:        load time =     412.03 ms
0.00.565.434 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.59 tokens per second)
0.00.565.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.436 I llama_perf_context_print:       total time =     141.98 ms /   129 tokens
0.00.565.898 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.077s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.004 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.105 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.113 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.117 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.117 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.865 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.576 I llama_model_loader: - type  f32:  194 tensors
0.00.026.576 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.577 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.577 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.578 I print_info: file format = GGUF V3 (latest)
0.00.026.578 I print_info: file type   = Q3_K - Medium
0.00.026.579 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.723 I load: special tokens cache size = 25
0.00.051.651 I load: token to piece cache size = 0.2984 MB
0.00.051.654 I print_info: arch             = gptneox
0.00.051.654 I print_info: vocab_only       = 0
0.00.051.654 I print_info: n_ctx_train      = 2048
0.00.051.655 I print_info: n_embd           = 2048
0.00.051.655 I print_info: n_layer          = 24
0.00.051.657 I print_info: n_head           = 16
0.00.051.658 I print_info: n_head_kv        = 16
0.00.051.658 I print_info: n_rot            = 32
0.00.051.661 I print_info: n_swa            = 0
0.00.051.661 I print_info: n_embd_head_k    = 128
0.00.051.661 I print_info: n_embd_head_v    = 128
0.00.051.662 I print_info: n_gqa            = 1
0.00.051.662 I print_info: n_embd_k_gqa     = 2048
0.00.051.663 I print_info: n_embd_v_gqa     = 2048
0.00.051.664 I print_info: f_norm_eps       = 1.0e-05
0.00.051.664 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.664 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.664 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.664 I print_info: f_logit_scale    = 0.0e+00
0.00.051.665 I print_info: n_ff             = 8192
0.00.051.665 I print_info: n_expert         = 0
0.00.051.665 I print_info: n_expert_used    = 0
0.00.051.666 I print_info: causal attn      = 1
0.00.051.670 I print_info: pooling type     = 0
0.00.051.670 I print_info: rope type        = 2
0.00.051.670 I print_info: rope scaling     = linear
0.00.051.671 I print_info: freq_base_train  = 10000.0
0.00.051.671 I print_info: freq_scale_train = 1
0.00.051.671 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.672 I print_info: rope_finetuned   = unknown
0.00.051.672 I print_info: ssm_d_conv       = 0
0.00.051.672 I print_info: ssm_d_inner      = 0
0.00.051.672 I print_info: ssm_d_state      = 0
0.00.051.672 I print_info: ssm_dt_rank      = 0
0.00.051.672 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.675 I print_info: model type       = 1.4B
0.00.051.676 I print_info: model params     = 1.41 B
0.00.051.676 I print_info: general.name     = 1.4B
0.00.051.676 I print_info: vocab type       = BPE
0.00.051.677 I print_info: n_vocab          = 50304
0.00.051.677 I print_info: n_merges         = 50009
0.00.051.677 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.677 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.677 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.677 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.678 I print_info: LF token         = 128 'Ä'
0.00.051.679 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.679 I print_info: max token length = 1024
0.00.053.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.594 I load_tensors: offloading output layer to GPU
0.00.053.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.607 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.608 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.900 I llama_init_from_model: n_seq_max     = 1
0.00.053.901 I llama_init_from_model: n_ctx         = 128
0.00.053.901 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.901 I llama_init_from_model: n_batch       = 128
0.00.053.901 I llama_init_from_model: n_ubatch      = 128
0.00.053.901 I llama_init_from_model: flash_attn    = 0
0.00.053.902 I llama_init_from_model: freq_base     = 10000.0
0.00.053.902 I llama_init_from_model: freq_scale    = 1
0.00.053.902 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.903 I ggml_metal_init: allocating
0.00.053.906 I ggml_metal_init: found device: Apple M4
0.00.053.908 I ggml_metal_init: picking default device: Apple M4
0.00.054.464 I ggml_metal_init: using embedded metal library
0.00.056.760 I ggml_metal_init: GPU name:   Apple M4
0.00.056.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.762 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.762 I ggml_metal_init: simdgroup reduction   = true
0.00.056.763 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.763 I ggml_metal_init: has bfloat            = true
0.00.056.763 I ggml_metal_init: use bfloat            = true
0.00.056.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.178 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.588 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.592 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.460 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.461 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.461 I llama_init_from_model: graph nodes  = 967
0.00.068.461 I llama_init_from_model: graph splits = 2
0.00.068.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.048 I 
0.00.470.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.103 I perplexity: tokenizing the input ..
0.00.478.078 I perplexity: tokenization took 7.973 ms
0.00.478.081 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.610.101 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.611.262 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.611.287 I llama_perf_context_print:        load time =     461.04 ms
0.00.611.288 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.23 tokens per second)
0.00.611.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.290 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.611.756 I ggml_metal_free: deallocating

real	0m0.626s
user	0m0.077s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.028 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.709 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.478 I llama_model_loader: - type  f32:  194 tensors
0.00.025.478 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.478 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.478 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.479 I print_info: file format = GGUF V3 (latest)
0.00.025.479 I print_info: file type   = Q4_K - Medium
0.00.025.480 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.582 I load: special tokens cache size = 25
0.00.050.443 I load: token to piece cache size = 0.2984 MB
0.00.050.446 I print_info: arch             = gptneox
0.00.050.446 I print_info: vocab_only       = 0
0.00.050.446 I print_info: n_ctx_train      = 2048
0.00.050.447 I print_info: n_embd           = 2048
0.00.050.447 I print_info: n_layer          = 24
0.00.050.450 I print_info: n_head           = 16
0.00.050.450 I print_info: n_head_kv        = 16
0.00.050.451 I print_info: n_rot            = 32
0.00.050.451 I print_info: n_swa            = 0
0.00.050.451 I print_info: n_embd_head_k    = 128
0.00.050.451 I print_info: n_embd_head_v    = 128
0.00.050.452 I print_info: n_gqa            = 1
0.00.050.453 I print_info: n_embd_k_gqa     = 2048
0.00.050.453 I print_info: n_embd_v_gqa     = 2048
0.00.050.454 I print_info: f_norm_eps       = 1.0e-05
0.00.050.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.454 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.457 I print_info: f_logit_scale    = 0.0e+00
0.00.050.458 I print_info: n_ff             = 8192
0.00.050.458 I print_info: n_expert         = 0
0.00.050.464 I print_info: n_expert_used    = 0
0.00.050.465 I print_info: causal attn      = 1
0.00.050.465 I print_info: pooling type     = 0
0.00.050.466 I print_info: rope type        = 2
0.00.050.466 I print_info: rope scaling     = linear
0.00.050.466 I print_info: freq_base_train  = 10000.0
0.00.050.467 I print_info: freq_scale_train = 1
0.00.050.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.467 I print_info: rope_finetuned   = unknown
0.00.050.467 I print_info: ssm_d_conv       = 0
0.00.050.467 I print_info: ssm_d_inner      = 0
0.00.050.468 I print_info: ssm_d_state      = 0
0.00.050.468 I print_info: ssm_dt_rank      = 0
0.00.050.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.468 I print_info: model type       = 1.4B
0.00.050.469 I print_info: model params     = 1.41 B
0.00.050.469 I print_info: general.name     = 1.4B
0.00.050.470 I print_info: vocab type       = BPE
0.00.050.470 I print_info: n_vocab          = 50304
0.00.050.470 I print_info: n_merges         = 50009
0.00.050.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: LF token         = 128 'Ä'
0.00.050.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: max token length = 1024
0.00.052.056 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.056 I load_tensors: offloading output layer to GPU
0.00.052.057 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.066 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.067 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.342 I llama_init_from_model: n_seq_max     = 1
0.00.052.343 I llama_init_from_model: n_ctx         = 128
0.00.052.343 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.343 I llama_init_from_model: n_batch       = 128
0.00.052.343 I llama_init_from_model: n_ubatch      = 128
0.00.052.344 I llama_init_from_model: flash_attn    = 0
0.00.052.344 I llama_init_from_model: freq_base     = 10000.0
0.00.052.344 I llama_init_from_model: freq_scale    = 1
0.00.052.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.345 I ggml_metal_init: allocating
0.00.052.348 I ggml_metal_init: found device: Apple M4
0.00.052.350 I ggml_metal_init: picking default device: Apple M4
0.00.052.908 I ggml_metal_init: using embedded metal library
0.00.055.266 I ggml_metal_init: GPU name:   Apple M4
0.00.055.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.268 I ggml_metal_init: simdgroup reduction   = true
0.00.055.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.268 I ggml_metal_init: has bfloat            = true
0.00.055.268 I ggml_metal_init: use bfloat            = true
0.00.055.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.682 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.936 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.951 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.843 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.844 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.844 I llama_init_from_model: graph nodes  = 967
0.00.066.845 I llama_init_from_model: graph splits = 2
0.00.066.846 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.807 I 
0.00.565.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.848 I perplexity: tokenizing the input ..
0.00.573.570 I perplexity: tokenization took 7.72 ms
0.00.573.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.878 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.709.046 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.709.070 I llama_perf_context_print:        load time =     555.78 ms
0.00.709.071 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.64 tokens per second)
0.00.709.072 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.709.072 I llama_perf_context_print:       total time =     143.26 ms /   129 tokens
0.00.709.542 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.591 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.597 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.094 I llama_model_loader: - type  f32:  194 tensors
0.00.024.094 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.094 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.095 I print_info: file format = GGUF V3 (latest)
0.00.024.095 I print_info: file type   = Q5_K - Medium
0.00.024.096 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.224 I load: special tokens cache size = 25
0.00.049.298 I load: token to piece cache size = 0.2984 MB
0.00.049.301 I print_info: arch             = gptneox
0.00.049.302 I print_info: vocab_only       = 0
0.00.049.302 I print_info: n_ctx_train      = 2048
0.00.049.302 I print_info: n_embd           = 2048
0.00.049.302 I print_info: n_layer          = 24
0.00.049.305 I print_info: n_head           = 16
0.00.049.306 I print_info: n_head_kv        = 16
0.00.049.306 I print_info: n_rot            = 32
0.00.049.306 I print_info: n_swa            = 0
0.00.049.306 I print_info: n_embd_head_k    = 128
0.00.049.306 I print_info: n_embd_head_v    = 128
0.00.049.307 I print_info: n_gqa            = 1
0.00.049.308 I print_info: n_embd_k_gqa     = 2048
0.00.049.309 I print_info: n_embd_v_gqa     = 2048
0.00.049.309 I print_info: f_norm_eps       = 1.0e-05
0.00.049.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.310 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.312 I print_info: f_logit_scale    = 0.0e+00
0.00.049.313 I print_info: n_ff             = 8192
0.00.049.313 I print_info: n_expert         = 0
0.00.049.313 I print_info: n_expert_used    = 0
0.00.049.315 I print_info: causal attn      = 1
0.00.049.315 I print_info: pooling type     = 0
0.00.049.315 I print_info: rope type        = 2
0.00.049.316 I print_info: rope scaling     = linear
0.00.049.316 I print_info: freq_base_train  = 10000.0
0.00.049.316 I print_info: freq_scale_train = 1
0.00.049.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.317 I print_info: rope_finetuned   = unknown
0.00.049.317 I print_info: ssm_d_conv       = 0
0.00.049.317 I print_info: ssm_d_inner      = 0
0.00.049.317 I print_info: ssm_d_state      = 0
0.00.049.317 I print_info: ssm_dt_rank      = 0
0.00.049.321 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.322 I print_info: model type       = 1.4B
0.00.049.322 I print_info: model params     = 1.41 B
0.00.049.322 I print_info: general.name     = 1.4B
0.00.049.323 I print_info: vocab type       = BPE
0.00.049.323 I print_info: n_vocab          = 50304
0.00.049.323 I print_info: n_merges         = 50009
0.00.049.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.324 I print_info: LF token         = 128 'Ä'
0.00.049.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.324 I print_info: max token length = 1024
0.00.051.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.252 I load_tensors: offloading output layer to GPU
0.00.051.252 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.263 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.264 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.559 I llama_init_from_model: n_seq_max     = 1
0.00.051.560 I llama_init_from_model: n_ctx         = 128
0.00.051.560 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.560 I llama_init_from_model: n_batch       = 128
0.00.051.560 I llama_init_from_model: n_ubatch      = 128
0.00.051.560 I llama_init_from_model: flash_attn    = 0
0.00.051.561 I llama_init_from_model: freq_base     = 10000.0
0.00.051.561 I llama_init_from_model: freq_scale    = 1
0.00.051.562 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.562 I ggml_metal_init: allocating
0.00.051.565 I ggml_metal_init: found device: Apple M4
0.00.051.567 I ggml_metal_init: picking default device: Apple M4
0.00.052.143 I ggml_metal_init: using embedded metal library
0.00.054.448 I ggml_metal_init: GPU name:   Apple M4
0.00.054.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.451 I ggml_metal_init: simdgroup reduction   = true
0.00.054.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.451 I ggml_metal_init: has bfloat            = true
0.00.054.451 I ggml_metal_init: use bfloat            = true
0.00.054.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.088 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.311 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.313 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.328 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.292 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.293 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.293 I llama_init_from_model: graph nodes  = 967
0.00.066.294 I llama_init_from_model: graph splits = 2
0.00.066.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.413 I 
0.00.613.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.459 I perplexity: tokenizing the input ..
0.00.621.366 I perplexity: tokenization took 7.905 ms
0.00.621.369 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.036 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.763.183 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.763.221 I llama_perf_context_print:        load time =     604.82 ms
0.00.763.222 I llama_perf_context_print: prompt eval time =     140.44 ms /   128 tokens (    1.10 ms per token,   911.43 tokens per second)
0.00.763.223 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.223 I llama_perf_context_print:       total time =     149.81 ms /   129 tokens
0.00.763.812 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.077s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.462 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.463 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.847 I llama_model_loader: - type  f32:  194 tensors
0.00.024.847 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.848 I print_info: file format = GGUF V3 (latest)
0.00.024.848 I print_info: file type   = Q6_K
0.00.024.849 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.234 I load: special tokens cache size = 25
0.00.049.086 I load: token to piece cache size = 0.2984 MB
0.00.049.089 I print_info: arch             = gptneox
0.00.049.090 I print_info: vocab_only       = 0
0.00.049.090 I print_info: n_ctx_train      = 2048
0.00.049.090 I print_info: n_embd           = 2048
0.00.049.090 I print_info: n_layer          = 24
0.00.049.093 I print_info: n_head           = 16
0.00.049.094 I print_info: n_head_kv        = 16
0.00.049.094 I print_info: n_rot            = 32
0.00.049.095 I print_info: n_swa            = 0
0.00.049.095 I print_info: n_embd_head_k    = 128
0.00.049.095 I print_info: n_embd_head_v    = 128
0.00.049.096 I print_info: n_gqa            = 1
0.00.049.097 I print_info: n_embd_k_gqa     = 2048
0.00.049.097 I print_info: n_embd_v_gqa     = 2048
0.00.049.098 I print_info: f_norm_eps       = 1.0e-05
0.00.049.099 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.099 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.099 I print_info: f_logit_scale    = 0.0e+00
0.00.049.100 I print_info: n_ff             = 8192
0.00.049.100 I print_info: n_expert         = 0
0.00.049.100 I print_info: n_expert_used    = 0
0.00.049.101 I print_info: causal attn      = 1
0.00.049.101 I print_info: pooling type     = 0
0.00.049.101 I print_info: rope type        = 2
0.00.049.101 I print_info: rope scaling     = linear
0.00.049.103 I print_info: freq_base_train  = 10000.0
0.00.049.103 I print_info: freq_scale_train = 1
0.00.049.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.104 I print_info: rope_finetuned   = unknown
0.00.049.104 I print_info: ssm_d_conv       = 0
0.00.049.104 I print_info: ssm_d_inner      = 0
0.00.049.104 I print_info: ssm_d_state      = 0
0.00.049.104 I print_info: ssm_dt_rank      = 0
0.00.049.105 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.105 I print_info: model type       = 1.4B
0.00.049.105 I print_info: model params     = 1.41 B
0.00.049.105 I print_info: general.name     = 1.4B
0.00.049.106 I print_info: vocab type       = BPE
0.00.049.106 I print_info: n_vocab          = 50304
0.00.049.106 I print_info: n_merges         = 50009
0.00.049.106 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.107 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.107 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.107 I print_info: LF token         = 128 'Ä'
0.00.049.107 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.108 I print_info: max token length = 1024
0.00.050.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.677 I load_tensors: offloading output layer to GPU
0.00.050.677 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.687 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.688 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.960 I llama_init_from_model: n_seq_max     = 1
0.00.050.961 I llama_init_from_model: n_ctx         = 128
0.00.050.961 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.961 I llama_init_from_model: n_batch       = 128
0.00.050.962 I llama_init_from_model: n_ubatch      = 128
0.00.050.962 I llama_init_from_model: flash_attn    = 0
0.00.050.962 I llama_init_from_model: freq_base     = 10000.0
0.00.050.962 I llama_init_from_model: freq_scale    = 1
0.00.050.963 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.963 I ggml_metal_init: allocating
0.00.050.965 I ggml_metal_init: found device: Apple M4
0.00.050.968 I ggml_metal_init: picking default device: Apple M4
0.00.051.564 I ggml_metal_init: using embedded metal library
0.00.053.926 I ggml_metal_init: GPU name:   Apple M4
0.00.053.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.928 I ggml_metal_init: simdgroup reduction   = true
0.00.053.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.928 I ggml_metal_init: has bfloat            = true
0.00.053.928 I ggml_metal_init: use bfloat            = true
0.00.053.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.662 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.664 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.679 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.523 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.524 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.524 I llama_init_from_model: graph nodes  = 967
0.00.064.524 I llama_init_from_model: graph splits = 2
0.00.064.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.170.545 I 
0.00.170.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.170.604 I perplexity: tokenizing the input ..
0.00.178.142 I perplexity: tokenization took 7.537 ms
0.00.178.147 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.317.592 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.318.876 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.318.910 I llama_perf_context_print:        load time =     160.85 ms
0.00.318.911 I llama_perf_context_print: prompt eval time =     139.20 ms /   128 tokens (    1.09 ms per token,   919.56 tokens per second)
0.00.318.912 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.318.912 I llama_perf_context_print:       total time =     148.37 ms /   129 tokens
0.00.319.332 I ggml_metal_free: deallocating

real	0m0.334s
user	0m0.075s
sys	0m0.041s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.248 I build: 4518 (aea8ddd5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.762 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.874 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.903 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.865 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.867 I llama_model_loader: - type  f32:  194 tensors
0.00.053.867 I llama_model_loader: - type  f16:   98 tensors
0.00.053.868 I print_info: file format = GGUF V3 (latest)
0.00.053.869 I print_info: file type   = all F32 (guessed)
0.00.053.871 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.994 I load: special tokens cache size = 25
0.00.088.824 I load: token to piece cache size = 0.2984 MB
0.00.088.827 I print_info: arch             = gptneox
0.00.088.828 I print_info: vocab_only       = 0
0.00.088.828 I print_info: n_ctx_train      = 2048
0.00.088.828 I print_info: n_embd           = 2048
0.00.088.828 I print_info: n_layer          = 24
0.00.088.831 I print_info: n_head           = 16
0.00.088.832 I print_info: n_head_kv        = 16
0.00.088.832 I print_info: n_rot            = 32
0.00.088.832 I print_info: n_swa            = 0
0.00.088.832 I print_info: n_embd_head_k    = 128
0.00.088.833 I print_info: n_embd_head_v    = 128
0.00.088.833 I print_info: n_gqa            = 1
0.00.088.834 I print_info: n_embd_k_gqa     = 2048
0.00.088.835 I print_info: n_embd_v_gqa     = 2048
0.00.088.835 I print_info: f_norm_eps       = 1.0e-05
0.00.088.836 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.836 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.836 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.836 I print_info: f_logit_scale    = 0.0e+00
0.00.088.839 I print_info: n_ff             = 8192
0.00.088.839 I print_info: n_expert         = 0
0.00.088.839 I print_info: n_expert_used    = 0
0.00.088.839 I print_info: causal attn      = 1
0.00.088.839 I print_info: pooling type     = 0
0.00.088.841 I print_info: rope type        = 2
0.00.088.841 I print_info: rope scaling     = linear
0.00.088.841 I print_info: freq_base_train  = 10000.0
0.00.088.841 I print_info: freq_scale_train = 1
0.00.088.842 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.842 I print_info: rope_finetuned   = unknown
0.00.088.842 I print_info: ssm_d_conv       = 0
0.00.088.842 I print_info: ssm_d_inner      = 0
0.00.088.842 I print_info: ssm_d_state      = 0
0.00.088.843 I print_info: ssm_dt_rank      = 0
0.00.088.843 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.843 I print_info: model type       = 1.4B
0.00.088.843 I print_info: model params     = 1.41 B
0.00.088.843 I print_info: general.name     = 1.4B
0.00.088.844 I print_info: vocab type       = BPE
0.00.088.844 I print_info: n_vocab          = 50304
0.00.088.844 I print_info: n_merges         = 50009
0.00.088.844 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.844 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.845 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.845 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.845 I print_info: LF token         = 128 'Ä'
0.00.088.845 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.846 I print_info: max token length = 1024
0.00.091.378 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.378 I load_tensors: offloading output layer to GPU
0.00.091.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.389 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.390 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.661 I llama_init_from_model: n_seq_max     = 1
0.00.091.662 I llama_init_from_model: n_ctx         = 128
0.00.091.662 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.662 I llama_init_from_model: n_batch       = 128
0.00.091.662 I llama_init_from_model: n_ubatch      = 128
0.00.091.662 I llama_init_from_model: flash_attn    = 0
0.00.091.663 I llama_init_from_model: freq_base     = 10000.0
0.00.091.663 I llama_init_from_model: freq_scale    = 1
0.00.091.663 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.664 I ggml_metal_init: allocating
0.00.091.667 I ggml_metal_init: found device: Apple M4
0.00.091.669 I ggml_metal_init: picking default device: Apple M4
0.00.092.317 I ggml_metal_init: using embedded metal library
0.00.094.937 I ggml_metal_init: GPU name:   Apple M4
0.00.094.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.940 I ggml_metal_init: simdgroup reduction   = true
0.00.094.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.940 I ggml_metal_init: has bfloat            = true
0.00.094.940 I ggml_metal_init: use bfloat            = true
0.00.094.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.421 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.733 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.735 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.749 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.599 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.106.600 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.106.600 I llama_init_from_model: graph nodes  = 967
0.00.106.600 I llama_init_from_model: graph splits = 2
0.00.106.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.602 I 
0.00.106.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.639 I compute_imatrix: tokenizing the input ..
0.00.113.122 I compute_imatrix: tokenization took 6.482 ms
0.00.113.124 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.613.784 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.616.265 I llama_perf_context_print:        load time =    1592.02 ms
0.01.616.266 I llama_perf_context_print: prompt eval time =    1500.02 ms /   128 tokens (   11.72 ms per token,    85.33 tokens per second)
0.01.616.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.616.267 I llama_perf_context_print:       total time =    1594.49 ms /   129 tokens
0.01.616.862 I ggml_metal_free: deallocating

real	0m1.800s
user	0m0.172s
sys	0m0.231s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4518 (aea8ddd5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e70d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e70d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e70dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e70e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e70ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e70f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e70fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e710310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e711150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e711870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e712040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e712e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e7135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e713e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e714820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e715aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e7162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e716a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e717290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e7177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e717a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e7183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e718d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e7191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e719f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e71a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e71a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e71f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e71f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e71fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e720540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e7209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e720e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e721320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e7217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e721c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e722100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e7225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e722ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e723380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e723cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e724210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e725200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e725750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e725ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e7261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e726740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e726c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e7271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e727c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e728c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e7291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e729710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e72a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e72a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e72ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e72b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e72b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e72bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e71b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e72c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e72c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e72cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e72d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e72d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e72dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e72e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e72e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e72ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e72f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e72f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e72fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e7302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e730820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e730d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e731210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e7316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e731ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e732490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e732930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e732dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e733270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e733710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e733bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e734050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e7344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e734990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e734e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e7352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e735770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e735c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e7360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e736550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e7369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e737330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e7377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e7385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e738a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e738ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e739390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e739830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e739cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e73a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e73a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e73aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e73af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e73b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e73b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e73bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e73c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e73c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e73cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e73cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e73d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e73d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e73dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e73e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e73e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e73eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e73f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e73f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e73f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e73fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e740290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e740bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e741070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e741510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e7419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e7422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e742790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e742c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e7430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e743570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e743a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e743eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e744350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e7447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e745130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e7455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e745a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e745f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e7463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e746850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e746cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e747190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e747630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e747ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e747f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e7484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e748f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e7494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e749770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e749d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e74a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e74b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e74b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e74b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e74bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e74c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e74cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e74d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e74dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e74e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e74e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e74ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e74f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e74f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e74fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e750270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e7507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e750d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e751260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e7517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e751d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e752250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e7527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e752cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e753240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e753ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e754230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e754780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e754cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e755220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e755770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e755cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e756210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e756760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e756cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e757200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e757750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e7581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e758740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e758c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e7591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e759730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e759c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e75a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e75a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e75ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e75b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e75b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e75bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e75c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e75c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e75cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e75d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e75d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e75dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e75e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e75e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e75ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e75f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e75f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e75fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e760170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e7606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e760c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e7610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e761550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e7619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e761e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e762330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e7627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e763110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e7635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e763a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e763ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e764390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e764830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e764cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e765170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e7656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e765de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e766500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e766c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e767340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e7680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e7686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.139.905 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.139.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e704d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e7051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e705630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e705aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e705f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e706380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e7067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e706c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e7070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e707540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e7079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e7080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e708bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e709370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e709b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e70a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e70a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e70b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e70b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e70bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e70c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e70cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e70dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e70e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e70e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e70e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e70ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e70f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e70f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e70fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e70ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e7103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e710670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e710ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e710f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e7113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e711830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e711ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e712110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e7129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e712e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e7132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e713740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e713bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e714020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e714490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e714900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e714d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e715650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f004230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f0046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f004b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f005060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f005560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f005a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f005ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f006340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f0067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f006c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f007090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f007500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f007970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f007de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f008250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f0086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f008b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f008fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f009410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f009880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f009cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f00a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f00a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f00aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f00aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f00b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f00b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f00bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f00c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f00c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f00c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f00cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f00d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f00d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f00db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f00df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f00e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f00e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f00ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f00f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f00f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f00fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f00fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f010300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f010770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f010be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f011050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f0114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f011930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f011da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f012210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f012680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f012af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f012f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f0133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f013840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f013cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f014120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f014590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f014a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f014e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f0152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f015750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f016030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f0164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f016910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f016d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f0171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f017660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f017ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f017f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f0183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f018820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f018c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f019100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f019570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f0199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f019e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f01a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f01a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f01aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f01b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f01b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f01b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f01bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f01c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f01c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f01cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f01cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f01d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f01d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f01dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f01e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f01e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f01e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f01ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f01f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f01f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f01fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f01fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f020460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f0208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f020d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f0211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f021620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f021a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f021f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f022370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f0227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f022c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f0230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f023530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f0239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f0245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f024b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f024fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f025430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f0258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f025d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f026180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f0265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f026a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f026ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f027340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f0277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f027c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f028090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f028500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f028970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f028de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f029250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f0296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f029b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f029fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f02a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f02a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f02acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f02b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f02b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f02ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f02beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f02c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f02c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f02cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f02d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f02d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f02d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f02ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f02e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f02e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f02eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f02f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f02f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f02f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f02ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f030420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f030f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f031250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f031810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f031dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f032390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f032950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f032f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f0334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f034050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f034610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f034bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f035190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f035750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f035d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f0362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f036890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f036e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f037410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f0379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f037f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f038550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f038b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f0390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f039690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f039c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f03a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f03a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f03ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f03b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f03b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f03bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f03c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f03ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f03d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f03db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f03e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f03e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f03ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f03f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f03f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f03fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f0403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f040990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f040f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f041510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f041ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f042090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f042650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f042c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f0431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f043790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e715910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e715d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e7161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e716660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e716ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e717010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e717480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e7178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e717d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e7181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e718640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e718ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e718f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e719390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e719800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e719c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e71a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e71a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e71a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e71ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e71b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e71c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e71c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e71cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e71d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e71d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e71d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e71dd40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e6044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e6056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e6063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e6077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e6082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e608a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e609290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e6099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e60c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e60cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e60d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e60da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e60dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e60e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e60e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e60e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e60ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e60f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e60f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e60fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e60fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e610290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e610b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e611450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e6118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e611d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e6121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e612610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e612a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e612ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e613360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e6137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e613c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e6140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e614520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e614990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e615270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e6156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e615b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e616a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e616ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e6184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e61a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e61a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e61acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e61b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e61b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e61ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e61be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e61c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e61c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e61cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e61d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e61d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e61d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e61dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e61e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e61e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e61eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e61ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e61f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e61f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e61fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e6209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e6212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e622020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e622900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e6231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e623a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e6241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e74bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e74a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e749a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e74a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e71d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e71d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e71f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e74c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e714ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e71bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e71c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e71afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e71a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e71dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e71cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e70e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e71fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e72c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e7678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e716cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e716f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e74c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e74ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e7150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e7153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e715670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e768b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e768de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e7690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e769620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e7698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e769ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e769e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e76a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e76a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e76a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e76a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e76ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e76aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e76b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e76b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e76b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e76b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e76bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e76bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e76c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e76c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e76c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e76ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e76cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e76cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e76d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e76d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e76d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e76dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e76dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e76e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e76e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e76e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e76e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e76eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e76ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e76f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e76f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e76f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e76f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e76fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e76fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e770160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e770420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e7706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e7709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e770c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e770f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e7711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e7714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e771760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e771a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e771ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e771fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e772260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e772520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e7727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e772aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e772d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e773020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e7732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e7735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e773860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e773b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e773de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e7740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e774360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e774620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e7748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e774ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e774e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e775120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e7756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e7759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e775c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e775f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e7761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e7764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e776770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e776a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e776cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e776fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e777270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e777530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e7777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e777ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e777d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e778030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e7782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e7785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e778870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e778b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e778df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e7790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e779370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e779630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e7798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e779bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e779e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e77a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e77a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e77a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e77a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e77ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e77aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e77b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e77b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e77b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e77b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e77bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e77bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e77c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e77c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e77c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e77ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e77cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e77cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e77d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e77d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e77d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e77daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e77ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e77e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e77e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e77e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e77e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e77eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e77ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e77f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e77f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e77f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e77f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e77fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e77feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e780170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e780430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e7806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e7809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e780c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e780f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e7811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e7814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e781770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e781a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e781cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e781fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e782270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e782530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e7827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e782ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e782d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e783030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e7832f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.756s
user	0m0.298s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4518 (aea8ddd5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e607410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e607b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e6080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e608680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e608c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e6091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e609790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e609d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e60c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e60ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e60d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e60ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e610680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e611640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e611900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e611f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e612b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e6130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e613820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e614370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e6148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e614b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e615010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e6154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e615df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e617510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e617de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e6183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e619320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e619930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e619f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e61a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e61ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e61b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e61b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e61be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e61c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e61c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e61cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e61d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e61d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e61dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e61df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e61e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e61e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e61ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e61f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e61f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e61fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e61ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e620460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e620da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e6212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e621840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e621d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e6222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e622830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e622d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e6232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e6242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e624810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e6252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e625800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e6262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e6267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e6277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e628280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e6287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e629190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e629940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e629e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e62a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e62a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e62ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e62b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e62b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e62be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e62c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e62c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e62ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e62d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e62d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e62de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e62e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e62e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e62ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e62f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e62f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e62fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e62feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e630350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e6307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e630c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e631130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e6315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e631a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e631f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e6323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e632850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e632cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e633190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e633630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e633ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e633f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e634410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e6348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e634d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e6351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e635690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e635b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e635fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e636470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e636910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e636db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e637250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e6376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e637b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e638030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e6384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e638970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e638e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e6392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e639750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e639bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e63a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e63a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e63a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e63ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e63b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e63b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e63bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e63c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e63c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e63ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e63ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e63d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e63d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e63dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e63e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e63e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e63ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e63ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e63f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e63f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e63fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e6401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e640650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e640af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e640f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e641430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e6418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e641d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e642210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e6426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e642b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e642ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e643490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e643dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e644270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e644710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e644bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e645050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e6455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e646e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e647470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e647a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e648270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e6489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e648fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e6495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e649de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e64a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e64a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e64abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e64b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e64b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e64be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e64c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e64c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e64ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e64d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e64d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e64ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e64e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e64e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e64ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e64f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e64f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e64fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e650320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e650870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e650dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e651310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e651860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e651db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e652300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e652850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e652da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e6532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e653d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e6542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e654830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e654d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e6552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e655820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e655d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e6562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e656810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e656d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e6572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e657800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e657d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e6582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e6587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e659290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e6597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e659d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e65a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e65a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e65ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e65b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e65b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e65bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e65c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e65c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e65cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e65d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e65d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e65dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e65e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e65e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e65ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e65ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e65f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e65f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e65fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e6601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e660690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e660b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e660fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e661470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e661910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e661db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e662250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e6627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e662ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e6635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e663d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e664420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e6646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e664ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e665190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e6657a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.089.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e665450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e647120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e646b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e647730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e61a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e61a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e61c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e6492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e611bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e6186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e618fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e6195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e610bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e61ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e629450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e6649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e614060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e6498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e647d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e6121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e612490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e612750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e665c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e666180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e666440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e666700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e6669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e666c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e666f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e667200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e6674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e667780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e667a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e667d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e667fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e668280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e668540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e668800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e668ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e668d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e669040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e669300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e6695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e669880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e669b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e669e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e66a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e66a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e66a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e66a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e66abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e66ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e66b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e66b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e66b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e66b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e66bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e66bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e66c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e66c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e66c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e66ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e66ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e66cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e66d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e66d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e66d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e66da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e66dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e66e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e66e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e66e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e66e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e66eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e66edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e66f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e66f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e66f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e66f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e66fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e66fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e670100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e6703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e670680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e670940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e670c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e670ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e671180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e671440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e671700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e6719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e671c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e671f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e672200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e6724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e672780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e672a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e672d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e672fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e673280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e673540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e673800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e673ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e673d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e674040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e674300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e6745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e674880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e674b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e674e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e6750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e675380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e675640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e675900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e675bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e675e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e676140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e676400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e6766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e676980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e676c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e676f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e6771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e677480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e677740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e677a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e677cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e677f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e678240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e678500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e6787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e678a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e678d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e679000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e6792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e679580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e679840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e679b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e679dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e67a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e67a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e67a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e67a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e67ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e67ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e67b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e67b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e67b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e67b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e67bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e67bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e67c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e67c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e67c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e67c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e67cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e67cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e67d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e67d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e67d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e67da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e67dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e67dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e67e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e67e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e67e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e67eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e67ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e67f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e67f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e67f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e67f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e67fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e67fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e6800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e680380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e680640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e680900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e680bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e680e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e681140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e681400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e6816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e681980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e681c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e681f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e6821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e682480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e682740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e682a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e682cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e682f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e683240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e683500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e6837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e683a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e683d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e684000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e6842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e684580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e684840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e684b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e684dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e685080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e685340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e685600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e685db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e686070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e686330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e6867a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e686c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e687080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e6874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e687960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e687dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e688240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e6886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e688b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e688f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e689400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e689870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e689ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e68a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e68a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e68aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e68aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e68b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e68b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e68bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e68c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e68c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e68c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e68cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e68d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e68d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e68db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e68df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e68e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e68e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e68ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e68f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e68f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e68fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e68fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e6902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e690760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e690bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e691040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e6914b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e691920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e691d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e692200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e692670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e692ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e692f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e6933c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e693830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e693ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e694110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e694580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e6949f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e694e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e6952d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e695740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e695bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e696020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e696490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e696900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e696d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e6971e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e697650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e697ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e697f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e6983a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e698810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e698c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e6990f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e699560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e6999d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e69a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e69ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e69b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e69b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e69bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e69c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e69c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e69cd20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e704820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e704c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e705100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e705570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e7059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e705e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e7062c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e706730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e706ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e707010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e707480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e707ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e7086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e708e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e709680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e709da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e70a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e70abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e70b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e70ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e70c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e70c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e70cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e70d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e70ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e70e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e70e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e70e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e70ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e70f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e70f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e70fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e70feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e710170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e7105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e710a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e710ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e711330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e7117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e711c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e712080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e712960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e712dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e713240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e7136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e713b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e713f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e714400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e714870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e715150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e7155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e715a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e715ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e716310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e716880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e7171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e717ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e717f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e7183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e718820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e718c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e719100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e719570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e7199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e719e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e71a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e71a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e71aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e71b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e71b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e71b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e71bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e71c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e71c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e71cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e71cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e71d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e71d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e71dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e71e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e71e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e71e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e71ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e71f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e71f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e71fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e71fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e720460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e7208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e720d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e7211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e721620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e721a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e721f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e722370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e7227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e722c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e7230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e723530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e723dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e7244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e724960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e724dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e725240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e7256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e725b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e725f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e726400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e726870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e726ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e727150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e7275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e727a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e727ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e728310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e728780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e729060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e7294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e729db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e72a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e72a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e72ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e72af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e72b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e72b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e72bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e72c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e72c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e72ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e72ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e72d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e72d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e72dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e72e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e72e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e72e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e72ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e72f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e72f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e72fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e72ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e7303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e730830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e730ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e731110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e731580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e7319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e731e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e7322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e732740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e732bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e733020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e733490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e733d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e7341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e734ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e734f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e7353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e735810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e735c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e7360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e736560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e7369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e736e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e7372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e737720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e738000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e738470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e7388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e738d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e7391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e739630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e739aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e739f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e73a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e73a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e73ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e73b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e73b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e73b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e73be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e73c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e73c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e73cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e73cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e73d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e73d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e73dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e73e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e73e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e73ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e73eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e73f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e73f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e73fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e7400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e740520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e740990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e740e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e741270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e741df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e7420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e742370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e7427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e742c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e7430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e7439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e743e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e744280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e7446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e744b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e744fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e7458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e745d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e746190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e746600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e746a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e746ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e747350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e7477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e747c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e7480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e748510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e748980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e748df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e749260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e7496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e749fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e74a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e74a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e74ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e74b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e74b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e74ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e74bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e74c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e74c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e74cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e74d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e74d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e74d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e74ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e74e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e74e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e74eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e74ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e74f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e74f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e74fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e7505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e750a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e750ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e751310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e751780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e752060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e7524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e752940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e753220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e753690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e753b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e753f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e7543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e754850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e754cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e755130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e7555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e755a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e756480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e756ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e7572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e7579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e758110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e758710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e758d20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.921s
user	0m0.246s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
