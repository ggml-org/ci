### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.70 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.26 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.92 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.17 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  189.05 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.21 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 250.59 sec*proc (29 tests)

Total Test time (real) = 250.61 sec

real	4m10.673s
user	8m19.847s
sys	0m6.990s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.82 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.70 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.09 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.74 sec*proc (29 tests)

Total Test time (real) =  54.75 sec

real	0m54.764s
user	1m16.569s
sys	0m6.314s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.136 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.278 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.790 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.799 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.800 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.801 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.802 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.803 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.804 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.805 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.805 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.806 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.809 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.810 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.812 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.813 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.813 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.814 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.817 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.257 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.259 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.259 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.260 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.260 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.260 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.028.261 I llama_model_loader: - type  f32:  124 tensors
0.00.028.261 I llama_model_loader: - type  f16:   73 tensors
0.00.028.262 I print_info: file format = GGUF V3 (latest)
0.00.028.263 I print_info: file type   = F16
0.00.028.264 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.032.267 I load: special tokens cache size = 5
0.00.034.320 I load: token to piece cache size = 0.2032 MB
0.00.034.324 I print_info: arch             = bert
0.00.034.324 I print_info: vocab_only       = 0
0.00.034.324 I print_info: n_ctx_train      = 512
0.00.034.325 I print_info: n_embd           = 384
0.00.034.325 I print_info: n_layer          = 12
0.00.034.328 I print_info: n_head           = 12
0.00.034.329 I print_info: n_head_kv        = 12
0.00.034.329 I print_info: n_rot            = 32
0.00.034.329 I print_info: n_swa            = 0
0.00.034.331 I print_info: n_embd_head_k    = 32
0.00.034.331 I print_info: n_embd_head_v    = 32
0.00.034.332 I print_info: n_gqa            = 1
0.00.034.333 I print_info: n_embd_k_gqa     = 384
0.00.034.334 I print_info: n_embd_v_gqa     = 384
0.00.034.335 I print_info: f_norm_eps       = 1.0e-12
0.00.034.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.336 I print_info: f_logit_scale    = 0.0e+00
0.00.034.337 I print_info: n_ff             = 1536
0.00.034.338 I print_info: n_expert         = 0
0.00.034.338 I print_info: n_expert_used    = 0
0.00.034.338 I print_info: causal attn      = 0
0.00.034.338 I print_info: pooling type     = 2
0.00.034.338 I print_info: rope type        = 2
0.00.034.339 I print_info: rope scaling     = linear
0.00.034.339 I print_info: freq_base_train  = 10000.0
0.00.034.340 I print_info: freq_scale_train = 1
0.00.034.340 I print_info: n_ctx_orig_yarn  = 512
0.00.034.340 I print_info: rope_finetuned   = unknown
0.00.034.340 I print_info: ssm_d_conv       = 0
0.00.034.341 I print_info: ssm_d_inner      = 0
0.00.034.341 I print_info: ssm_d_state      = 0
0.00.034.341 I print_info: ssm_dt_rank      = 0
0.00.034.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.341 I print_info: model type       = 33M
0.00.034.342 I print_info: model params     = 33.21 M
0.00.034.343 I print_info: general.name     = Bge Small
0.00.034.344 I print_info: vocab type       = WPM
0.00.034.344 I print_info: n_vocab          = 30522
0.00.034.345 I print_info: n_merges         = 0
0.00.034.345 I print_info: BOS token        = 101 '[CLS]'
0.00.034.345 I print_info: UNK token        = 100 '[UNK]'
0.00.034.345 I print_info: SEP token        = 102 '[SEP]'
0.00.034.346 I print_info: PAD token        = 0 '[PAD]'
0.00.034.347 I print_info: MASK token       = 103 '[MASK]'
0.00.034.347 I print_info: LF token         = 0 '[PAD]'
0.00.034.348 I print_info: max token length = 21
0.00.034.348 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.426 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.427 I load_tensors: offloading output layer to GPU
0.00.037.428 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.451 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.453 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.737 I llama_init_from_model: n_seq_max     = 1
0.00.037.739 I llama_init_from_model: n_ctx         = 512
0.00.037.739 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.739 I llama_init_from_model: n_batch       = 2048
0.00.037.739 I llama_init_from_model: n_ubatch      = 2048
0.00.037.740 I llama_init_from_model: flash_attn    = 0
0.00.037.740 I llama_init_from_model: freq_base     = 10000.0
0.00.037.741 I llama_init_from_model: freq_scale    = 1
0.00.037.741 I ggml_metal_init: allocating
0.00.037.753 I ggml_metal_init: found device: Apple M4
0.00.037.761 I ggml_metal_init: picking default device: Apple M4
0.00.038.538 I ggml_metal_init: using embedded metal library
0.00.042.603 I ggml_metal_init: GPU name:   Apple M4
0.00.042.606 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.607 I ggml_metal_init: simdgroup reduction   = true
0.00.042.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.607 I ggml_metal_init: has residency sets    = true
0.00.042.607 I ggml_metal_init: has bfloat            = true
0.00.042.608 I ggml_metal_init: use bfloat            = true
0.00.042.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.779 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.534 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.536 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.558 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.871 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.872 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.873 I llama_init_from_model: graph nodes  = 429
0.00.056.873 I llama_init_from_model: graph splits = 2
0.00.056.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.567 I 
0.00.062.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.324 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.407 I llama_perf_context_print:        load time =      45.27 ms
0.00.068.408 I llama_perf_context_print: prompt eval time =       4.94 ms /     9 tokens (    0.55 ms per token,  1820.76 tokens per second)
0.00.068.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.409 I llama_perf_context_print:       total time =       5.84 ms /    10 tokens
0.00.068.553 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.050s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.530 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.013.390 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.396 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.397 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.398 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.398 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.399 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.399 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.400 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.400 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.400 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.403 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.404 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.013.404 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.013.404 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.404 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.013.405 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.000 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.678 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.680 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.680 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.681 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.681 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.681 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.681 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.682 I llama_model_loader: - type  f32:  124 tensors
0.00.016.682 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.683 I print_info: file format = GGUF V3 (latest)
0.00.016.683 I print_info: file type   = Q8_0
0.00.016.684 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.019.287 I load: special tokens cache size = 5
0.00.020.574 I load: token to piece cache size = 0.2032 MB
0.00.020.578 I print_info: arch             = bert
0.00.020.581 I print_info: vocab_only       = 0
0.00.020.581 I print_info: n_ctx_train      = 512
0.00.020.581 I print_info: n_embd           = 384
0.00.020.581 I print_info: n_layer          = 12
0.00.020.584 I print_info: n_head           = 12
0.00.020.585 I print_info: n_head_kv        = 12
0.00.020.585 I print_info: n_rot            = 32
0.00.020.585 I print_info: n_swa            = 0
0.00.020.585 I print_info: n_embd_head_k    = 32
0.00.020.586 I print_info: n_embd_head_v    = 32
0.00.020.586 I print_info: n_gqa            = 1
0.00.020.587 I print_info: n_embd_k_gqa     = 384
0.00.020.587 I print_info: n_embd_v_gqa     = 384
0.00.020.588 I print_info: f_norm_eps       = 1.0e-12
0.00.020.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.589 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.589 I print_info: f_logit_scale    = 0.0e+00
0.00.020.589 I print_info: n_ff             = 1536
0.00.020.590 I print_info: n_expert         = 0
0.00.020.590 I print_info: n_expert_used    = 0
0.00.020.590 I print_info: causal attn      = 0
0.00.020.590 I print_info: pooling type     = 2
0.00.020.590 I print_info: rope type        = 2
0.00.020.590 I print_info: rope scaling     = linear
0.00.020.591 I print_info: freq_base_train  = 10000.0
0.00.020.592 I print_info: freq_scale_train = 1
0.00.020.592 I print_info: n_ctx_orig_yarn  = 512
0.00.020.593 I print_info: rope_finetuned   = unknown
0.00.020.593 I print_info: ssm_d_conv       = 0
0.00.020.593 I print_info: ssm_d_inner      = 0
0.00.020.593 I print_info: ssm_d_state      = 0
0.00.020.593 I print_info: ssm_dt_rank      = 0
0.00.020.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.593 I print_info: model type       = 33M
0.00.020.594 I print_info: model params     = 33.21 M
0.00.020.594 I print_info: general.name     = Bge Small
0.00.020.595 I print_info: vocab type       = WPM
0.00.020.595 I print_info: n_vocab          = 30522
0.00.020.595 I print_info: n_merges         = 0
0.00.020.595 I print_info: BOS token        = 101 '[CLS]'
0.00.020.595 I print_info: UNK token        = 100 '[UNK]'
0.00.020.595 I print_info: SEP token        = 102 '[SEP]'
0.00.020.596 I print_info: PAD token        = 0 '[PAD]'
0.00.020.596 I print_info: MASK token       = 103 '[MASK]'
0.00.020.596 I print_info: LF token         = 0 '[PAD]'
0.00.020.596 I print_info: max token length = 21
0.00.020.597 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.514 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.515 I load_tensors: offloading output layer to GPU
0.00.022.515 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.522 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.522 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.758 I llama_init_from_model: n_seq_max     = 1
0.00.022.759 I llama_init_from_model: n_ctx         = 512
0.00.022.759 I llama_init_from_model: n_ctx_per_seq = 512
0.00.022.760 I llama_init_from_model: n_batch       = 2048
0.00.022.760 I llama_init_from_model: n_ubatch      = 2048
0.00.022.760 I llama_init_from_model: flash_attn    = 0
0.00.022.760 I llama_init_from_model: freq_base     = 10000.0
0.00.022.761 I llama_init_from_model: freq_scale    = 1
0.00.022.761 I ggml_metal_init: allocating
0.00.022.770 I ggml_metal_init: found device: Apple M4
0.00.022.774 I ggml_metal_init: picking default device: Apple M4
0.00.023.394 I ggml_metal_init: using embedded metal library
0.00.026.059 I ggml_metal_init: GPU name:   Apple M4
0.00.026.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.062 I ggml_metal_init: simdgroup reduction   = true
0.00.026.062 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.062 I ggml_metal_init: has residency sets    = true
0.00.026.062 I ggml_metal_init: has bfloat            = true
0.00.026.062 I ggml_metal_init: use bfloat            = true
0.00.026.063 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.353 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.027 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.029 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.043 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.220 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.221 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.222 I llama_init_from_model: graph nodes  = 429
0.00.038.222 I llama_init_from_model: graph splits = 2
0.00.038.224 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.282 I 
0.00.042.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.864 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.440 I llama_perf_context_print:        load time =      31.74 ms
0.00.047.441 I llama_perf_context_print: prompt eval time =       4.43 ms /     9 tokens (    0.49 ms per token,  2031.14 tokens per second)
0.00.047.442 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.442 I llama_perf_context_print:       total time =       5.16 ms /    10 tokens
0.00.047.655 I ggml_metal_free: deallocating

real	0m0.060s
user	0m0.032s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.278 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.602 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.610 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.612 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.612 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.613 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.614 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.615 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.616 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.616 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.617 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.620 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.621 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.622 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.228 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.648 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.651 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.652 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.652 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.652 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.653 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.653 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.653 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.654 I llama_model_loader: - type  f32:   40 tensors
0.00.048.654 I llama_model_loader: - type  f16:   30 tensors
0.00.048.655 I print_info: file format = GGUF V3 (latest)
0.00.048.656 I print_info: file type   = F16
0.00.048.657 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.914 W load: empty token at index 5
0.00.058.101 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.714 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.751 I load: special tokens cache size = 5
0.00.322.262 I load: token to piece cache size = 1.5060 MB
0.00.322.268 I print_info: arch             = jina-bert-v2
0.00.322.268 I print_info: vocab_only       = 0
0.00.322.269 I print_info: n_ctx_train      = 8192
0.00.322.272 I print_info: n_embd           = 384
0.00.322.272 I print_info: n_layer          = 4
0.00.322.279 I print_info: n_head           = 12
0.00.322.280 I print_info: n_head_kv        = 12
0.00.322.280 I print_info: n_rot            = 32
0.00.322.280 I print_info: n_swa            = 0
0.00.322.280 I print_info: n_embd_head_k    = 32
0.00.322.280 I print_info: n_embd_head_v    = 32
0.00.322.281 I print_info: n_gqa            = 1
0.00.322.282 I print_info: n_embd_k_gqa     = 384
0.00.322.282 I print_info: n_embd_v_gqa     = 384
0.00.322.283 I print_info: f_norm_eps       = 1.0e-12
0.00.322.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.284 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.284 I print_info: f_logit_scale    = 0.0e+00
0.00.322.285 I print_info: n_ff             = 1536
0.00.322.285 I print_info: n_expert         = 0
0.00.322.285 I print_info: n_expert_used    = 0
0.00.322.286 I print_info: causal attn      = 0
0.00.322.286 I print_info: pooling type     = -1
0.00.322.286 I print_info: rope type        = -1
0.00.322.286 I print_info: rope scaling     = linear
0.00.322.287 I print_info: freq_base_train  = 10000.0
0.00.322.287 I print_info: freq_scale_train = 1
0.00.322.288 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.289 I print_info: rope_finetuned   = unknown
0.00.322.289 I print_info: ssm_d_conv       = 0
0.00.322.289 I print_info: ssm_d_inner      = 0
0.00.322.289 I print_info: ssm_d_state      = 0
0.00.322.289 I print_info: ssm_dt_rank      = 0
0.00.322.289 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.290 I print_info: model type       = 33M
0.00.322.290 I print_info: model params     = 32.90 M
0.00.322.290 I print_info: general.name     = Jina Bert Implementation
0.00.322.293 I print_info: vocab type       = BPE
0.00.322.293 I print_info: n_vocab          = 61056
0.00.322.293 I print_info: n_merges         = 39382
0.00.322.293 I print_info: BOS token        = 0 '<s>'
0.00.322.293 I print_info: EOS token        = 2 '</s>'
0.00.322.294 I print_info: UNK token        = 3 '<unk>'
0.00.322.294 I print_info: SEP token        = 2 '</s>'
0.00.322.294 I print_info: PAD token        = 1 '<pad>'
0.00.322.294 I print_info: MASK token       = 4 '<mask>'
0.00.322.294 I print_info: EOG token        = 2 '</s>'
0.00.322.295 I print_info: max token length = 45
0.00.322.295 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.369 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.370 I load_tensors: offloading output layer to GPU
0.00.324.370 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.393 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.395 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.676 I llama_init_from_model: n_seq_max     = 1
0.00.324.677 I llama_init_from_model: n_ctx         = 8192
0.00.324.677 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.677 I llama_init_from_model: n_batch       = 2048
0.00.324.677 I llama_init_from_model: n_ubatch      = 2048
0.00.324.677 I llama_init_from_model: flash_attn    = 0
0.00.324.678 I llama_init_from_model: freq_base     = 10000.0
0.00.324.678 I llama_init_from_model: freq_scale    = 1
0.00.324.679 I ggml_metal_init: allocating
0.00.324.682 I ggml_metal_init: found device: Apple M4
0.00.324.685 I ggml_metal_init: picking default device: Apple M4
0.00.325.575 I ggml_metal_init: using embedded metal library
0.00.328.320 I ggml_metal_init: GPU name:   Apple M4
0.00.328.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.323 I ggml_metal_init: simdgroup reduction   = true
0.00.328.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.323 I ggml_metal_init: has residency sets    = true
0.00.328.323 I ggml_metal_init: has bfloat            = true
0.00.328.323 I ggml_metal_init: use bfloat            = true
0.00.328.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.964 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.991 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.993 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.013 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.101 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.102 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.103 I llama_init_from_model: graph nodes  = 154
0.00.347.103 I llama_init_from_model: graph splits = 2
0.00.347.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.478 I 
0.00.354.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.592 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.354.593 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.354.596 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.354.596 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.354.601 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.354.601 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.355.169 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.358.702 I llama_perf_context_print:        load time =     332.30 ms
0.00.358.703 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17593.64 tokens per second)
0.00.358.704 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.705 I llama_perf_context_print:       total time =       4.22 ms /    63 tokens
0.00.358.939 I ggml_metal_free: deallocating

real	0m1.079s
user	0m0.330s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.127 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.272 I main: llama backend init
0.00.000.278 I main: load the model and apply lora adapter, if any
0.00.256.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.269.274 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.269.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.269.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.269.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.269.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.269.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.269.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.269.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.269.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.269.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.269.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.269.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.269.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.269.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.269.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.269.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.269.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.276.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.278.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.284.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.284.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.284.839 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.284.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.284.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.284.840 I llama_model_loader: - type  f32:  194 tensors
0.00.284.841 I llama_model_loader: - type  f16:   98 tensors
0.00.284.841 I print_info: file format = GGUF V3 (latest)
0.00.284.842 I print_info: file type   = all F32 (guessed)
0.00.284.848 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.292.905 I load: special tokens cache size = 25
0.00.299.170 I load: token to piece cache size = 0.2984 MB
0.00.299.174 I print_info: arch             = gptneox
0.00.299.174 I print_info: vocab_only       = 0
0.00.299.174 I print_info: n_ctx_train      = 2048
0.00.299.175 I print_info: n_embd           = 2048
0.00.299.175 I print_info: n_layer          = 24
0.00.299.179 I print_info: n_head           = 16
0.00.299.180 I print_info: n_head_kv        = 16
0.00.299.181 I print_info: n_rot            = 32
0.00.299.181 I print_info: n_swa            = 0
0.00.299.181 I print_info: n_embd_head_k    = 128
0.00.299.181 I print_info: n_embd_head_v    = 128
0.00.299.182 I print_info: n_gqa            = 1
0.00.299.183 I print_info: n_embd_k_gqa     = 2048
0.00.299.183 I print_info: n_embd_v_gqa     = 2048
0.00.299.184 I print_info: f_norm_eps       = 1.0e-05
0.00.299.184 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.299.187 I print_info: f_clamp_kqv      = 0.0e+00
0.00.299.187 I print_info: f_max_alibi_bias = 0.0e+00
0.00.299.187 I print_info: f_logit_scale    = 0.0e+00
0.00.299.188 I print_info: n_ff             = 8192
0.00.299.188 I print_info: n_expert         = 0
0.00.299.188 I print_info: n_expert_used    = 0
0.00.299.188 I print_info: causal attn      = 1
0.00.299.188 I print_info: pooling type     = 0
0.00.299.188 I print_info: rope type        = 2
0.00.299.189 I print_info: rope scaling     = linear
0.00.299.189 I print_info: freq_base_train  = 10000.0
0.00.299.190 I print_info: freq_scale_train = 1
0.00.299.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.299.190 I print_info: rope_finetuned   = unknown
0.00.299.190 I print_info: ssm_d_conv       = 0
0.00.299.190 I print_info: ssm_d_inner      = 0
0.00.299.191 I print_info: ssm_d_state      = 0
0.00.299.192 I print_info: ssm_dt_rank      = 0
0.00.299.192 I print_info: ssm_dt_b_c_rms   = 0
0.00.299.192 I print_info: model type       = 1.4B
0.00.299.192 I print_info: model params     = 1.41 B
0.00.299.193 I print_info: general.name     = 1.4B
0.00.299.193 I print_info: vocab type       = BPE
0.00.299.193 I print_info: n_vocab          = 50304
0.00.299.193 I print_info: n_merges         = 50009
0.00.299.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.299.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.299.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.299.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.299.194 I print_info: LF token         = 187 ''
0.00.299.195 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.299.195 I print_info: max token length = 1024
0.00.299.195 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.042 I load_tensors: offloading output layer to GPU
0.00.338.042 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.069 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.338.071 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.338.451 I llama_init_from_model: n_seq_max     = 1
0.00.338.452 I llama_init_from_model: n_ctx         = 2048
0.00.338.452 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.452 I llama_init_from_model: n_batch       = 2048
0.00.338.452 I llama_init_from_model: n_ubatch      = 512
0.00.338.452 I llama_init_from_model: flash_attn    = 0
0.00.338.453 I llama_init_from_model: freq_base     = 10000.0
0.00.338.453 I llama_init_from_model: freq_scale    = 1
0.00.338.454 I ggml_metal_init: allocating
0.00.338.484 I ggml_metal_init: found device: Apple M4
0.00.338.490 I ggml_metal_init: picking default device: Apple M4
0.00.339.115 I ggml_metal_init: using embedded metal library
0.00.388.278 I ggml_metal_init: GPU name:   Apple M4
0.00.388.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.388.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.388.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.388.284 I ggml_metal_init: simdgroup reduction   = true
0.00.388.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.388.284 I ggml_metal_init: has residency sets    = true
0.00.388.284 I ggml_metal_init: has bfloat            = true
0.00.388.285 I ggml_metal_init: use bfloat            = true
0.00.388.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.388.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.420.938 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.451.986 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.451.992 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.452.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.455.510 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.455.512 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.455.512 I llama_init_from_model: graph nodes  = 967
0.00.455.512 I llama_init_from_model: graph splits = 2
0.00.455.515 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.455.644 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.455.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.708 I main: llama threadpool init, n_threads = 4
0.00.521.751 I 
0.00.521.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.766 I 
0.00.521.807 I sampler seed: 1234
0.00.521.812 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.836 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.839 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.839 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.354.767 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.02.354.768 I llama_perf_context_print:        load time =     263.87 ms
0.02.354.768 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.93 tokens per second)
0.02.354.769 I llama_perf_context_print:        eval time =    1786.22 ms /    63 runs   (   28.35 ms per token,    35.27 tokens per second)
0.02.354.770 I llama_perf_context_print:       total time =    1833.90 ms /    70 tokens
0.02.355.043 I ggml_metal_free: deallocating

real	0m2.829s
user	0m0.121s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.812 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.457 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.743 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.754 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.755 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.829 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.830 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.830 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.831 I llama_model_loader: - type  f32:  194 tensors
0.00.056.831 I llama_model_loader: - type  f16:   98 tensors
0.00.056.832 I print_info: file format = GGUF V3 (latest)
0.00.056.833 I print_info: file type   = all F32 (guessed)
0.00.056.834 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.709 I load: special tokens cache size = 25
0.00.076.724 I load: token to piece cache size = 0.2984 MB
0.00.076.727 I print_info: arch             = gptneox
0.00.076.727 I print_info: vocab_only       = 0
0.00.076.727 I print_info: n_ctx_train      = 2048
0.00.076.728 I print_info: n_embd           = 2048
0.00.076.728 I print_info: n_layer          = 24
0.00.076.730 I print_info: n_head           = 16
0.00.076.731 I print_info: n_head_kv        = 16
0.00.076.731 I print_info: n_rot            = 32
0.00.076.731 I print_info: n_swa            = 0
0.00.076.731 I print_info: n_embd_head_k    = 128
0.00.076.731 I print_info: n_embd_head_v    = 128
0.00.076.734 I print_info: n_gqa            = 1
0.00.076.735 I print_info: n_embd_k_gqa     = 2048
0.00.076.735 I print_info: n_embd_v_gqa     = 2048
0.00.076.736 I print_info: f_norm_eps       = 1.0e-05
0.00.076.736 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.737 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.737 I print_info: f_logit_scale    = 0.0e+00
0.00.076.738 I print_info: n_ff             = 8192
0.00.076.738 I print_info: n_expert         = 0
0.00.076.739 I print_info: n_expert_used    = 0
0.00.076.739 I print_info: causal attn      = 1
0.00.076.739 I print_info: pooling type     = 0
0.00.076.740 I print_info: rope type        = 2
0.00.076.740 I print_info: rope scaling     = linear
0.00.076.740 I print_info: freq_base_train  = 10000.0
0.00.076.740 I print_info: freq_scale_train = 1
0.00.076.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.741 I print_info: rope_finetuned   = unknown
0.00.076.741 I print_info: ssm_d_conv       = 0
0.00.076.741 I print_info: ssm_d_inner      = 0
0.00.076.741 I print_info: ssm_d_state      = 0
0.00.076.741 I print_info: ssm_dt_rank      = 0
0.00.076.741 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.742 I print_info: model type       = 1.4B
0.00.076.742 I print_info: model params     = 1.41 B
0.00.076.742 I print_info: general.name     = 1.4B
0.00.076.743 I print_info: vocab type       = BPE
0.00.076.743 I print_info: n_vocab          = 50304
0.00.076.743 I print_info: n_merges         = 50009
0.00.076.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.744 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.744 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.744 I print_info: LF token         = 187 ''
0.00.076.744 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.745 I print_info: max token length = 1024
0.00.076.745 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.076.142 I load_tensors: offloading 24 repeating layers to GPU
0.01.076.152 I load_tensors: offloading output layer to GPU
0.01.076.153 I load_tensors: offloaded 25/25 layers to GPU
0.01.076.187 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.076.199 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.076.968 I llama_init_from_model: n_seq_max     = 1
0.01.076.969 I llama_init_from_model: n_ctx         = 128
0.01.076.969 I llama_init_from_model: n_ctx_per_seq = 128
0.01.076.970 I llama_init_from_model: n_batch       = 128
0.01.076.970 I llama_init_from_model: n_ubatch      = 128
0.01.076.971 I llama_init_from_model: flash_attn    = 0
0.01.076.971 I llama_init_from_model: freq_base     = 10000.0
0.01.076.972 I llama_init_from_model: freq_scale    = 1
0.01.076.972 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.076.973 I ggml_metal_init: allocating
0.01.077.017 I ggml_metal_init: found device: Apple M4
0.01.077.024 I ggml_metal_init: picking default device: Apple M4
0.01.078.011 I ggml_metal_init: using embedded metal library
0.01.081.497 I ggml_metal_init: GPU name:   Apple M4
0.01.081.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.081.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.081.500 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.081.500 I ggml_metal_init: simdgroup reduction   = true
0.01.081.500 I ggml_metal_init: simdgroup matrix mul. = true
0.01.081.500 I ggml_metal_init: has residency sets    = true
0.01.081.500 I ggml_metal_init: has bfloat            = true
0.01.081.501 I ggml_metal_init: use bfloat            = true
0.01.081.501 I ggml_metal_init: hasUnifiedMemory      = true
0.01.081.502 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.091.052 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.093.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.093.047 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.093.079 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.094.650 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.094.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.094.651 I llama_init_from_model: graph nodes  = 967
0.01.094.651 I llama_init_from_model: graph splits = 2
0.01.094.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.094.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.127.477 I 
0.01.127.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.127.499 I perplexity: tokenizing the input ..
0.01.131.853 I perplexity: tokenization took 4.352 ms
0.01.131.857 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.250.078 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.251.365 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.251.404 I llama_perf_context_print:        load time =    1103.00 ms
0.01.251.405 I llama_perf_context_print: prompt eval time =     117.96 ms /   128 tokens (    0.92 ms per token,  1085.10 tokens per second)
0.01.251.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.251.406 I llama_perf_context_print:       total time =     123.93 ms /   129 tokens
0.01.251.766 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.100s
sys	0m0.291s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.460 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.235 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.176 I llama_model_loader: - type  f32:  194 tensors
0.00.035.176 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.177 I print_info: file format = GGUF V3 (latest)
0.00.035.177 I print_info: file type   = Q8_0
0.00.035.180 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.204 I load: special tokens cache size = 25
0.00.051.173 I load: token to piece cache size = 0.2984 MB
0.00.051.176 I print_info: arch             = gptneox
0.00.051.177 I print_info: vocab_only       = 0
0.00.051.177 I print_info: n_ctx_train      = 2048
0.00.051.177 I print_info: n_embd           = 2048
0.00.051.177 I print_info: n_layer          = 24
0.00.051.182 I print_info: n_head           = 16
0.00.051.186 I print_info: n_head_kv        = 16
0.00.051.186 I print_info: n_rot            = 32
0.00.051.187 I print_info: n_swa            = 0
0.00.051.187 I print_info: n_embd_head_k    = 128
0.00.051.187 I print_info: n_embd_head_v    = 128
0.00.051.188 I print_info: n_gqa            = 1
0.00.051.188 I print_info: n_embd_k_gqa     = 2048
0.00.051.189 I print_info: n_embd_v_gqa     = 2048
0.00.051.195 I print_info: f_norm_eps       = 1.0e-05
0.00.051.195 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.195 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.198 I print_info: f_logit_scale    = 0.0e+00
0.00.051.199 I print_info: n_ff             = 8192
0.00.051.199 I print_info: n_expert         = 0
0.00.051.199 I print_info: n_expert_used    = 0
0.00.051.199 I print_info: causal attn      = 1
0.00.051.199 I print_info: pooling type     = 0
0.00.051.200 I print_info: rope type        = 2
0.00.051.200 I print_info: rope scaling     = linear
0.00.051.201 I print_info: freq_base_train  = 10000.0
0.00.051.201 I print_info: freq_scale_train = 1
0.00.051.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.202 I print_info: rope_finetuned   = unknown
0.00.051.202 I print_info: ssm_d_conv       = 0
0.00.051.202 I print_info: ssm_d_inner      = 0
0.00.051.202 I print_info: ssm_d_state      = 0
0.00.051.202 I print_info: ssm_dt_rank      = 0
0.00.051.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.203 I print_info: model type       = 1.4B
0.00.051.203 I print_info: model params     = 1.41 B
0.00.051.203 I print_info: general.name     = 1.4B
0.00.051.204 I print_info: vocab type       = BPE
0.00.051.204 I print_info: n_vocab          = 50304
0.00.051.204 I print_info: n_merges         = 50009
0.00.051.205 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: LF token         = 187 ''
0.00.051.206 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.206 I print_info: max token length = 1024
0.00.051.207 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.169.492 I load_tensors: offloading 24 repeating layers to GPU
0.01.169.496 I load_tensors: offloading output layer to GPU
0.01.169.497 I load_tensors: offloaded 25/25 layers to GPU
0.01.169.518 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.169.519 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.170.534 I llama_init_from_model: n_seq_max     = 1
0.01.170.535 I llama_init_from_model: n_ctx         = 2048
0.01.170.536 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.170.536 I llama_init_from_model: n_batch       = 2048
0.01.170.537 I llama_init_from_model: n_ubatch      = 512
0.01.170.537 I llama_init_from_model: flash_attn    = 0
0.01.170.538 I llama_init_from_model: freq_base     = 10000.0
0.01.170.538 I llama_init_from_model: freq_scale    = 1
0.01.170.539 I ggml_metal_init: allocating
0.01.170.551 I ggml_metal_init: found device: Apple M4
0.01.170.559 I ggml_metal_init: picking default device: Apple M4
0.01.171.724 I ggml_metal_init: using embedded metal library
0.01.177.023 I ggml_metal_init: GPU name:   Apple M4
0.01.177.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.177.027 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.177.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.177.028 I ggml_metal_init: simdgroup reduction   = true
0.01.177.028 I ggml_metal_init: simdgroup matrix mul. = true
0.01.177.029 I ggml_metal_init: has residency sets    = true
0.01.177.029 I ggml_metal_init: has bfloat            = true
0.01.177.029 I ggml_metal_init: use bfloat            = true
0.01.177.030 I ggml_metal_init: hasUnifiedMemory      = true
0.01.177.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.196.062 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.256.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.256.172 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.256.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.260.859 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.260.861 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.260.861 I llama_init_from_model: graph nodes  = 967
0.01.260.862 I llama_init_from_model: graph splits = 2
0.01.260.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.260.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.260.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.310.206 I main: llama threadpool init, n_threads = 4
0.01.310.249 I 
0.01.310.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.310.269 I 
0.01.310.386 I sampler seed: 1234
0.01.310.391 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.310.400 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.310.401 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.310.401 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.403.053 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.02.403.053 I llama_perf_context_print:        load time =    1300.03 ms
0.02.403.055 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.34 tokens per second)
0.02.403.056 I llama_perf_context_print:        eval time =    1040.79 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.403.056 I llama_perf_context_print:       total time =    1093.56 ms /    70 tokens
0.02.403.293 I ggml_metal_free: deallocating

real	0m2.429s
user	0m0.110s
sys	0m0.321s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.356 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.782 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.482 I llama_model_loader: - type  f32:  194 tensors
0.00.026.483 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.483 I print_info: file format = GGUF V3 (latest)
0.00.026.484 I print_info: file type   = Q8_0
0.00.026.485 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.793 I load: special tokens cache size = 25
0.00.040.826 I load: token to piece cache size = 0.2984 MB
0.00.040.832 I print_info: arch             = gptneox
0.00.040.832 I print_info: vocab_only       = 0
0.00.040.832 I print_info: n_ctx_train      = 2048
0.00.040.833 I print_info: n_embd           = 2048
0.00.040.833 I print_info: n_layer          = 24
0.00.040.837 I print_info: n_head           = 16
0.00.040.840 I print_info: n_head_kv        = 16
0.00.040.840 I print_info: n_rot            = 32
0.00.040.840 I print_info: n_swa            = 0
0.00.040.840 I print_info: n_embd_head_k    = 128
0.00.040.840 I print_info: n_embd_head_v    = 128
0.00.040.841 I print_info: n_gqa            = 1
0.00.040.842 I print_info: n_embd_k_gqa     = 2048
0.00.040.843 I print_info: n_embd_v_gqa     = 2048
0.00.040.843 I print_info: f_norm_eps       = 1.0e-05
0.00.040.844 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.844 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.844 I print_info: f_logit_scale    = 0.0e+00
0.00.040.845 I print_info: n_ff             = 8192
0.00.040.845 I print_info: n_expert         = 0
0.00.040.845 I print_info: n_expert_used    = 0
0.00.040.845 I print_info: causal attn      = 1
0.00.040.845 I print_info: pooling type     = 0
0.00.040.845 I print_info: rope type        = 2
0.00.040.846 I print_info: rope scaling     = linear
0.00.040.846 I print_info: freq_base_train  = 10000.0
0.00.040.846 I print_info: freq_scale_train = 1
0.00.040.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.846 I print_info: rope_finetuned   = unknown
0.00.040.849 I print_info: ssm_d_conv       = 0
0.00.040.849 I print_info: ssm_d_inner      = 0
0.00.040.849 I print_info: ssm_d_state      = 0
0.00.040.849 I print_info: ssm_dt_rank      = 0
0.00.040.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.849 I print_info: model type       = 1.4B
0.00.040.850 I print_info: model params     = 1.41 B
0.00.040.850 I print_info: general.name     = 1.4B
0.00.040.850 I print_info: vocab type       = BPE
0.00.040.852 I print_info: n_vocab          = 50304
0.00.040.852 I print_info: n_merges         = 50009
0.00.040.852 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: LF token         = 187 ''
0.00.040.853 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: max token length = 1024
0.00.040.853 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.935.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.935.945 I load_tensors: offloading output layer to GPU
0.00.935.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.935.973 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.935.975 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.936.954 I llama_init_from_model: n_seq_max     = 1
0.00.936.955 I llama_init_from_model: n_ctx         = 128
0.00.936.956 I llama_init_from_model: n_ctx_per_seq = 128
0.00.936.956 I llama_init_from_model: n_batch       = 128
0.00.936.957 I llama_init_from_model: n_ubatch      = 128
0.00.936.957 I llama_init_from_model: flash_attn    = 0
0.00.936.958 I llama_init_from_model: freq_base     = 10000.0
0.00.936.958 I llama_init_from_model: freq_scale    = 1
0.00.936.959 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.936.959 I ggml_metal_init: allocating
0.00.937.003 I ggml_metal_init: found device: Apple M4
0.00.937.012 I ggml_metal_init: picking default device: Apple M4
0.00.938.212 I ggml_metal_init: using embedded metal library
0.00.942.726 I ggml_metal_init: GPU name:   Apple M4
0.00.942.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.942.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.942.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.942.731 I ggml_metal_init: simdgroup reduction   = true
0.00.942.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.942.731 I ggml_metal_init: has residency sets    = true
0.00.942.731 I ggml_metal_init: has bfloat            = true
0.00.942.732 I ggml_metal_init: use bfloat            = true
0.00.942.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.942.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.955.353 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.957.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.957.298 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.957.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.959.136 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.959.137 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.959.138 I llama_init_from_model: graph nodes  = 967
0.00.959.138 I llama_init_from_model: graph splits = 2
0.00.959.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.959.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.981.510 I 
0.00.981.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.981.539 I perplexity: tokenizing the input ..
0.00.986.704 I perplexity: tokenization took 5.163 ms
0.00.986.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.123.670 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.124.919 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.124.950 I llama_perf_context_print:        load time =     971.14 ms
0.01.124.952 I llama_perf_context_print: prompt eval time =     136.74 ms /   128 tokens (    1.07 ms per token,   936.10 tokens per second)
0.01.124.953 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.124.953 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.01.125.345 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.071s
sys	0m0.222s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.477 I llama_model_loader: - type  f32:  194 tensors
0.00.027.477 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.478 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.478 I print_info: file format = GGUF V3 (latest)
0.00.027.479 I print_info: file type   = Q4_0
0.00.027.480 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.952 I load: special tokens cache size = 25
0.00.041.795 I load: token to piece cache size = 0.2984 MB
0.00.041.799 I print_info: arch             = gptneox
0.00.041.799 I print_info: vocab_only       = 0
0.00.041.799 I print_info: n_ctx_train      = 2048
0.00.041.799 I print_info: n_embd           = 2048
0.00.041.800 I print_info: n_layer          = 24
0.00.041.804 I print_info: n_head           = 16
0.00.041.805 I print_info: n_head_kv        = 16
0.00.041.805 I print_info: n_rot            = 32
0.00.041.805 I print_info: n_swa            = 0
0.00.041.805 I print_info: n_embd_head_k    = 128
0.00.041.806 I print_info: n_embd_head_v    = 128
0.00.041.807 I print_info: n_gqa            = 1
0.00.041.807 I print_info: n_embd_k_gqa     = 2048
0.00.041.808 I print_info: n_embd_v_gqa     = 2048
0.00.041.809 I print_info: f_norm_eps       = 1.0e-05
0.00.041.809 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.810 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.810 I print_info: f_logit_scale    = 0.0e+00
0.00.041.811 I print_info: n_ff             = 8192
0.00.041.811 I print_info: n_expert         = 0
0.00.041.811 I print_info: n_expert_used    = 0
0.00.041.811 I print_info: causal attn      = 1
0.00.041.811 I print_info: pooling type     = 0
0.00.041.812 I print_info: rope type        = 2
0.00.041.812 I print_info: rope scaling     = linear
0.00.041.812 I print_info: freq_base_train  = 10000.0
0.00.041.813 I print_info: freq_scale_train = 1
0.00.041.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.813 I print_info: rope_finetuned   = unknown
0.00.041.813 I print_info: ssm_d_conv       = 0
0.00.041.813 I print_info: ssm_d_inner      = 0
0.00.041.814 I print_info: ssm_d_state      = 0
0.00.041.814 I print_info: ssm_dt_rank      = 0
0.00.041.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.814 I print_info: model type       = 1.4B
0.00.041.815 I print_info: model params     = 1.41 B
0.00.041.815 I print_info: general.name     = 1.4B
0.00.041.816 I print_info: vocab type       = BPE
0.00.041.816 I print_info: n_vocab          = 50304
0.00.041.816 I print_info: n_merges         = 50009
0.00.041.816 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: LF token         = 187 ''
0.00.041.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.818 I print_info: max token length = 1024
0.00.041.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.603 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.606 I load_tensors: offloading output layer to GPU
0.00.647.607 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.628 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.647.630 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.648.784 I llama_init_from_model: n_seq_max     = 1
0.00.648.786 I llama_init_from_model: n_ctx         = 2048
0.00.648.787 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.787 I llama_init_from_model: n_batch       = 2048
0.00.648.788 I llama_init_from_model: n_ubatch      = 512
0.00.648.789 I llama_init_from_model: flash_attn    = 0
0.00.648.790 I llama_init_from_model: freq_base     = 10000.0
0.00.648.790 I llama_init_from_model: freq_scale    = 1
0.00.648.792 I ggml_metal_init: allocating
0.00.648.811 I ggml_metal_init: found device: Apple M4
0.00.648.821 I ggml_metal_init: picking default device: Apple M4
0.00.650.260 I ggml_metal_init: using embedded metal library
0.00.655.997 I ggml_metal_init: GPU name:   Apple M4
0.00.656.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.003 I ggml_metal_init: simdgroup reduction   = true
0.00.656.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.003 I ggml_metal_init: has residency sets    = true
0.00.656.004 I ggml_metal_init: has bfloat            = true
0.00.656.004 I ggml_metal_init: use bfloat            = true
0.00.656.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.468 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.406 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.726.416 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.726.451 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.555 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.557 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.557 I llama_init_from_model: graph nodes  = 967
0.00.731.558 I llama_init_from_model: graph splits = 2
0.00.731.562 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.966 I main: llama threadpool init, n_threads = 4
0.00.778.014 I 
0.00.778.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.041 I 
0.00.778.157 I sampler seed: 1234
0.00.778.162 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.198 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.202 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.202 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.456.359 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.456.360 I llama_perf_context_print:        load time =     766.17 ms
0.01.456.360 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.83 tokens per second)
0.01.456.361 I llama_perf_context_print:        eval time =     631.72 ms /    63 runs   (   10.03 ms per token,    99.73 tokens per second)
0.01.456.361 I llama_perf_context_print:       total time =     679.14 ms /    70 tokens
0.01.456.661 I ggml_metal_free: deallocating

real	0m1.474s
user	0m0.108s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.474 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.294 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.125 I llama_model_loader: - type  f32:  194 tensors
0.00.026.125 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.125 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.126 I print_info: file format = GGUF V3 (latest)
0.00.026.127 I print_info: file type   = Q4_0
0.00.026.128 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.373 I load: special tokens cache size = 25
0.00.040.620 I load: token to piece cache size = 0.2984 MB
0.00.040.625 I print_info: arch             = gptneox
0.00.040.625 I print_info: vocab_only       = 0
0.00.040.625 I print_info: n_ctx_train      = 2048
0.00.040.625 I print_info: n_embd           = 2048
0.00.040.625 I print_info: n_layer          = 24
0.00.040.630 I print_info: n_head           = 16
0.00.040.631 I print_info: n_head_kv        = 16
0.00.040.631 I print_info: n_rot            = 32
0.00.040.631 I print_info: n_swa            = 0
0.00.040.635 I print_info: n_embd_head_k    = 128
0.00.040.635 I print_info: n_embd_head_v    = 128
0.00.040.636 I print_info: n_gqa            = 1
0.00.040.637 I print_info: n_embd_k_gqa     = 2048
0.00.040.638 I print_info: n_embd_v_gqa     = 2048
0.00.040.638 I print_info: f_norm_eps       = 1.0e-05
0.00.040.638 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.639 I print_info: f_logit_scale    = 0.0e+00
0.00.040.640 I print_info: n_ff             = 8192
0.00.040.640 I print_info: n_expert         = 0
0.00.040.640 I print_info: n_expert_used    = 0
0.00.040.640 I print_info: causal attn      = 1
0.00.040.640 I print_info: pooling type     = 0
0.00.040.641 I print_info: rope type        = 2
0.00.040.641 I print_info: rope scaling     = linear
0.00.040.641 I print_info: freq_base_train  = 10000.0
0.00.040.642 I print_info: freq_scale_train = 1
0.00.040.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.642 I print_info: rope_finetuned   = unknown
0.00.040.642 I print_info: ssm_d_conv       = 0
0.00.040.642 I print_info: ssm_d_inner      = 0
0.00.040.643 I print_info: ssm_d_state      = 0
0.00.040.643 I print_info: ssm_dt_rank      = 0
0.00.040.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.643 I print_info: model type       = 1.4B
0.00.040.643 I print_info: model params     = 1.41 B
0.00.040.644 I print_info: general.name     = 1.4B
0.00.040.644 I print_info: vocab type       = BPE
0.00.040.644 I print_info: n_vocab          = 50304
0.00.040.644 I print_info: n_merges         = 50009
0.00.040.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.646 I print_info: LF token         = 187 ''
0.00.040.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.646 I print_info: max token length = 1024
0.00.040.646 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.644 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.651 I load_tensors: offloading output layer to GPU
0.00.641.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.679 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.641.681 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.642.880 I llama_init_from_model: n_seq_max     = 1
0.00.642.883 I llama_init_from_model: n_ctx         = 128
0.00.642.884 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.884 I llama_init_from_model: n_batch       = 128
0.00.642.885 I llama_init_from_model: n_ubatch      = 128
0.00.642.885 I llama_init_from_model: flash_attn    = 0
0.00.642.886 I llama_init_from_model: freq_base     = 10000.0
0.00.642.887 I llama_init_from_model: freq_scale    = 1
0.00.642.888 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.889 I ggml_metal_init: allocating
0.00.642.940 I ggml_metal_init: found device: Apple M4
0.00.642.953 I ggml_metal_init: picking default device: Apple M4
0.00.644.437 I ggml_metal_init: using embedded metal library
0.00.650.434 I ggml_metal_init: GPU name:   Apple M4
0.00.650.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.441 I ggml_metal_init: simdgroup reduction   = true
0.00.650.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.441 I ggml_metal_init: has residency sets    = true
0.00.650.441 I ggml_metal_init: has bfloat            = true
0.00.650.442 I ggml_metal_init: use bfloat            = true
0.00.650.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.637 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.036 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.671.040 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.671.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.674.222 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.674.224 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.674.224 I llama_init_from_model: graph nodes  = 967
0.00.674.224 I llama_init_from_model: graph splits = 2
0.00.674.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.674.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.684 I 
0.00.700.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.738 I perplexity: tokenizing the input ..
0.00.706.139 I perplexity: tokenization took 5.4 ms
0.00.706.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.404 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.831.662 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.831.693 I llama_perf_context_print:        load time =     690.20 ms
0.00.831.695 I llama_perf_context_print: prompt eval time =     124.04 ms /   128 tokens (    0.97 ms per token,  1031.96 tokens per second)
0.00.831.695 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.696 I llama_perf_context_print:       total time =     131.01 ms /   129 tokens
0.00.832.093 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.077s
sys	0m0.168s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.115 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.868 I llama_model_loader: - type  f32:  194 tensors
0.00.026.868 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.869 I print_info: file format = GGUF V3 (latest)
0.00.026.870 I print_info: file type   = Q4_1
0.00.026.870 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.006 I load: special tokens cache size = 25
0.00.041.038 I load: token to piece cache size = 0.2984 MB
0.00.041.041 I print_info: arch             = gptneox
0.00.041.041 I print_info: vocab_only       = 0
0.00.041.042 I print_info: n_ctx_train      = 2048
0.00.041.042 I print_info: n_embd           = 2048
0.00.041.042 I print_info: n_layer          = 24
0.00.041.045 I print_info: n_head           = 16
0.00.041.045 I print_info: n_head_kv        = 16
0.00.041.046 I print_info: n_rot            = 32
0.00.041.047 I print_info: n_swa            = 0
0.00.041.047 I print_info: n_embd_head_k    = 128
0.00.041.048 I print_info: n_embd_head_v    = 128
0.00.041.048 I print_info: n_gqa            = 1
0.00.041.049 I print_info: n_embd_k_gqa     = 2048
0.00.041.050 I print_info: n_embd_v_gqa     = 2048
0.00.041.052 I print_info: f_norm_eps       = 1.0e-05
0.00.041.052 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.053 I print_info: f_logit_scale    = 0.0e+00
0.00.041.054 I print_info: n_ff             = 8192
0.00.041.054 I print_info: n_expert         = 0
0.00.041.054 I print_info: n_expert_used    = 0
0.00.041.054 I print_info: causal attn      = 1
0.00.041.060 I print_info: pooling type     = 0
0.00.041.060 I print_info: rope type        = 2
0.00.041.060 I print_info: rope scaling     = linear
0.00.041.061 I print_info: freq_base_train  = 10000.0
0.00.041.061 I print_info: freq_scale_train = 1
0.00.041.061 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.062 I print_info: rope_finetuned   = unknown
0.00.041.062 I print_info: ssm_d_conv       = 0
0.00.041.063 I print_info: ssm_d_inner      = 0
0.00.041.063 I print_info: ssm_d_state      = 0
0.00.041.063 I print_info: ssm_dt_rank      = 0
0.00.041.063 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.063 I print_info: model type       = 1.4B
0.00.041.064 I print_info: model params     = 1.41 B
0.00.041.064 I print_info: general.name     = 1.4B
0.00.041.064 I print_info: vocab type       = BPE
0.00.041.064 I print_info: n_vocab          = 50304
0.00.041.064 I print_info: n_merges         = 50009
0.00.041.065 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.065 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.065 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.065 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.065 I print_info: LF token         = 187 ''
0.00.041.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: max token length = 1024
0.00.041.066 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.171 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.183 I load_tensors: offloading output layer to GPU
0.00.696.184 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.212 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.696.217 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.697.477 I llama_init_from_model: n_seq_max     = 1
0.00.697.481 I llama_init_from_model: n_ctx         = 2048
0.00.697.481 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.697.482 I llama_init_from_model: n_batch       = 2048
0.00.697.483 I llama_init_from_model: n_ubatch      = 512
0.00.697.483 I llama_init_from_model: flash_attn    = 0
0.00.697.484 I llama_init_from_model: freq_base     = 10000.0
0.00.697.485 I llama_init_from_model: freq_scale    = 1
0.00.697.487 I ggml_metal_init: allocating
0.00.697.615 I ggml_metal_init: found device: Apple M4
0.00.697.638 I ggml_metal_init: picking default device: Apple M4
0.00.699.002 I ggml_metal_init: using embedded metal library
0.00.702.934 I ggml_metal_init: GPU name:   Apple M4
0.00.702.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.939 I ggml_metal_init: simdgroup reduction   = true
0.00.702.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.940 I ggml_metal_init: has residency sets    = true
0.00.702.940 I ggml_metal_init: has bfloat            = true
0.00.702.940 I ggml_metal_init: use bfloat            = true
0.00.702.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.953 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.748.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.748.571 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.748.629 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.269 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.754.272 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.754.272 I llama_init_from_model: graph nodes  = 967
0.00.754.272 I llama_init_from_model: graph splits = 2
0.00.754.277 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.042 I main: llama threadpool init, n_threads = 4
0.00.802.085 I 
0.00.802.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.103 I 
0.00.802.230 I sampler seed: 1234
0.00.802.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.245 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.245 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.532.088 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.532.088 I llama_perf_context_print:        load time =     791.22 ms
0.01.532.089 I llama_perf_context_print: prompt eval time =      48.72 ms /     7 tokens (    6.96 ms per token,   143.67 tokens per second)
0.01.532.090 I llama_perf_context_print:        eval time =     678.29 ms /    63 runs   (   10.77 ms per token,    92.88 tokens per second)
0.01.532.095 I llama_perf_context_print:       total time =     730.75 ms /    70 tokens
0.01.532.362 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.102s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.390 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.962 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.899 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.900 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.900 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.901 I llama_model_loader: - type  f32:  194 tensors
0.00.025.901 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.901 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.902 I print_info: file format = GGUF V3 (latest)
0.00.025.903 I print_info: file type   = Q4_1
0.00.025.904 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.648 I load: special tokens cache size = 25
0.00.040.881 I load: token to piece cache size = 0.2984 MB
0.00.040.886 I print_info: arch             = gptneox
0.00.040.886 I print_info: vocab_only       = 0
0.00.040.886 I print_info: n_ctx_train      = 2048
0.00.040.886 I print_info: n_embd           = 2048
0.00.040.886 I print_info: n_layer          = 24
0.00.040.891 I print_info: n_head           = 16
0.00.040.892 I print_info: n_head_kv        = 16
0.00.040.892 I print_info: n_rot            = 32
0.00.040.892 I print_info: n_swa            = 0
0.00.040.892 I print_info: n_embd_head_k    = 128
0.00.040.892 I print_info: n_embd_head_v    = 128
0.00.040.893 I print_info: n_gqa            = 1
0.00.040.894 I print_info: n_embd_k_gqa     = 2048
0.00.040.894 I print_info: n_embd_v_gqa     = 2048
0.00.040.895 I print_info: f_norm_eps       = 1.0e-05
0.00.040.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.896 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.896 I print_info: f_logit_scale    = 0.0e+00
0.00.040.897 I print_info: n_ff             = 8192
0.00.040.897 I print_info: n_expert         = 0
0.00.040.897 I print_info: n_expert_used    = 0
0.00.040.897 I print_info: causal attn      = 1
0.00.040.897 I print_info: pooling type     = 0
0.00.040.897 I print_info: rope type        = 2
0.00.040.897 I print_info: rope scaling     = linear
0.00.040.898 I print_info: freq_base_train  = 10000.0
0.00.040.898 I print_info: freq_scale_train = 1
0.00.040.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.899 I print_info: rope_finetuned   = unknown
0.00.040.899 I print_info: ssm_d_conv       = 0
0.00.040.899 I print_info: ssm_d_inner      = 0
0.00.040.899 I print_info: ssm_d_state      = 0
0.00.040.899 I print_info: ssm_dt_rank      = 0
0.00.040.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.899 I print_info: model type       = 1.4B
0.00.040.900 I print_info: model params     = 1.41 B
0.00.040.900 I print_info: general.name     = 1.4B
0.00.040.900 I print_info: vocab type       = BPE
0.00.040.900 I print_info: n_vocab          = 50304
0.00.040.900 I print_info: n_merges         = 50009
0.00.040.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.902 I print_info: LF token         = 187 ''
0.00.040.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.902 I print_info: max token length = 1024
0.00.040.902 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.689.028 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.036 I load_tensors: offloading output layer to GPU
0.00.689.037 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.064 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.689.067 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.690.190 I llama_init_from_model: n_seq_max     = 1
0.00.690.193 I llama_init_from_model: n_ctx         = 128
0.00.690.194 I llama_init_from_model: n_ctx_per_seq = 128
0.00.690.194 I llama_init_from_model: n_batch       = 128
0.00.690.195 I llama_init_from_model: n_ubatch      = 128
0.00.690.195 I llama_init_from_model: flash_attn    = 0
0.00.690.197 I llama_init_from_model: freq_base     = 10000.0
0.00.690.197 I llama_init_from_model: freq_scale    = 1
0.00.690.198 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.690.202 I ggml_metal_init: allocating
0.00.690.261 I ggml_metal_init: found device: Apple M4
0.00.690.274 I ggml_metal_init: picking default device: Apple M4
0.00.691.767 I ggml_metal_init: using embedded metal library
0.00.697.835 I ggml_metal_init: GPU name:   Apple M4
0.00.697.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.841 I ggml_metal_init: simdgroup reduction   = true
0.00.697.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.842 I ggml_metal_init: has residency sets    = true
0.00.697.842 I ggml_metal_init: has bfloat            = true
0.00.697.842 I ggml_metal_init: use bfloat            = true
0.00.697.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.715.318 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.650 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.718.655 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.718.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.800 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.802 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.802 I llama_init_from_model: graph nodes  = 967
0.00.721.802 I llama_init_from_model: graph splits = 2
0.00.721.805 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.771 I 
0.00.749.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.834 I perplexity: tokenizing the input ..
0.00.756.753 I perplexity: tokenization took 6.916 ms
0.00.756.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.575 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.887.842 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.887.875 I llama_perf_context_print:        load time =     740.37 ms
0.00.887.876 I llama_perf_context_print: prompt eval time =     128.97 ms /   128 tokens (    1.01 ms per token,   992.48 tokens per second)
0.00.887.877 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.877 I llama_perf_context_print:       total time =     138.11 ms /   129 tokens
0.00.888.228 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.080s
sys	0m0.175s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.989 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.994 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.130 I llama_model_loader: - type  f32:  194 tensors
0.00.026.130 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.131 I print_info: file format = GGUF V3 (latest)
0.00.026.132 I print_info: file type   = Q5_0
0.00.026.132 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.203 I load: special tokens cache size = 25
0.00.040.312 I load: token to piece cache size = 0.2984 MB
0.00.040.315 I print_info: arch             = gptneox
0.00.040.315 I print_info: vocab_only       = 0
0.00.040.315 I print_info: n_ctx_train      = 2048
0.00.040.315 I print_info: n_embd           = 2048
0.00.040.315 I print_info: n_layer          = 24
0.00.040.318 I print_info: n_head           = 16
0.00.040.319 I print_info: n_head_kv        = 16
0.00.040.319 I print_info: n_rot            = 32
0.00.040.319 I print_info: n_swa            = 0
0.00.040.320 I print_info: n_embd_head_k    = 128
0.00.040.320 I print_info: n_embd_head_v    = 128
0.00.040.321 I print_info: n_gqa            = 1
0.00.040.321 I print_info: n_embd_k_gqa     = 2048
0.00.040.322 I print_info: n_embd_v_gqa     = 2048
0.00.040.323 I print_info: f_norm_eps       = 1.0e-05
0.00.040.323 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.323 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.323 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.323 I print_info: f_logit_scale    = 0.0e+00
0.00.040.324 I print_info: n_ff             = 8192
0.00.040.324 I print_info: n_expert         = 0
0.00.040.325 I print_info: n_expert_used    = 0
0.00.040.325 I print_info: causal attn      = 1
0.00.040.325 I print_info: pooling type     = 0
0.00.040.325 I print_info: rope type        = 2
0.00.040.325 I print_info: rope scaling     = linear
0.00.040.326 I print_info: freq_base_train  = 10000.0
0.00.040.326 I print_info: freq_scale_train = 1
0.00.040.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.327 I print_info: rope_finetuned   = unknown
0.00.040.327 I print_info: ssm_d_conv       = 0
0.00.040.327 I print_info: ssm_d_inner      = 0
0.00.040.327 I print_info: ssm_d_state      = 0
0.00.040.327 I print_info: ssm_dt_rank      = 0
0.00.040.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.328 I print_info: model type       = 1.4B
0.00.040.328 I print_info: model params     = 1.41 B
0.00.040.328 I print_info: general.name     = 1.4B
0.00.040.329 I print_info: vocab type       = BPE
0.00.040.329 I print_info: n_vocab          = 50304
0.00.040.329 I print_info: n_merges         = 50009
0.00.040.329 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: LF token         = 187 ''
0.00.040.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.331 I print_info: max token length = 1024
0.00.040.331 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.287 I load_tensors: offloading output layer to GPU
0.00.659.289 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.312 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.659.313 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.660.076 I llama_init_from_model: n_seq_max     = 1
0.00.660.077 I llama_init_from_model: n_ctx         = 2048
0.00.660.078 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.079 I llama_init_from_model: n_batch       = 2048
0.00.660.079 I llama_init_from_model: n_ubatch      = 512
0.00.660.080 I llama_init_from_model: flash_attn    = 0
0.00.660.080 I llama_init_from_model: freq_base     = 10000.0
0.00.660.081 I llama_init_from_model: freq_scale    = 1
0.00.660.082 I ggml_metal_init: allocating
0.00.660.094 I ggml_metal_init: found device: Apple M4
0.00.660.102 I ggml_metal_init: picking default device: Apple M4
0.00.661.385 I ggml_metal_init: using embedded metal library
0.00.666.981 I ggml_metal_init: GPU name:   Apple M4
0.00.666.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.986 I ggml_metal_init: simdgroup reduction   = true
0.00.666.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.987 I ggml_metal_init: has residency sets    = true
0.00.666.987 I ggml_metal_init: has bfloat            = true
0.00.666.987 I ggml_metal_init: use bfloat            = true
0.00.666.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.499 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.867 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.874 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.817 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.819 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.819 I llama_init_from_model: graph nodes  = 967
0.00.728.820 I llama_init_from_model: graph splits = 2
0.00.728.823 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.201 I main: llama threadpool init, n_threads = 4
0.00.777.255 I 
0.00.777.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.274 I 
0.00.777.387 I sampler seed: 1234
0.00.777.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.434 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.434 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.561.187 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.561.188 I llama_perf_context_print:        load time =     767.51 ms
0.01.561.189 I llama_perf_context_print: prompt eval time =      43.33 ms /     7 tokens (    6.19 ms per token,   161.55 tokens per second)
0.01.561.189 I llama_perf_context_print:        eval time =     737.48 ms /    63 runs   (   11.71 ms per token,    85.43 tokens per second)
0.01.561.190 I llama_perf_context_print:       total time =     784.68 ms /    70 tokens
0.01.561.450 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.106s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.083 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.813 I llama_model_loader: - type  f32:  194 tensors
0.00.026.813 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.813 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.814 I print_info: file format = GGUF V3 (latest)
0.00.026.815 I print_info: file type   = Q5_0
0.00.026.816 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.347 I load: special tokens cache size = 25
0.00.041.417 I load: token to piece cache size = 0.2984 MB
0.00.041.421 I print_info: arch             = gptneox
0.00.041.422 I print_info: vocab_only       = 0
0.00.041.422 I print_info: n_ctx_train      = 2048
0.00.041.422 I print_info: n_embd           = 2048
0.00.041.422 I print_info: n_layer          = 24
0.00.041.427 I print_info: n_head           = 16
0.00.041.428 I print_info: n_head_kv        = 16
0.00.041.428 I print_info: n_rot            = 32
0.00.041.428 I print_info: n_swa            = 0
0.00.041.428 I print_info: n_embd_head_k    = 128
0.00.041.428 I print_info: n_embd_head_v    = 128
0.00.041.429 I print_info: n_gqa            = 1
0.00.041.430 I print_info: n_embd_k_gqa     = 2048
0.00.041.430 I print_info: n_embd_v_gqa     = 2048
0.00.041.433 I print_info: f_norm_eps       = 1.0e-05
0.00.041.434 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.434 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.434 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.434 I print_info: f_logit_scale    = 0.0e+00
0.00.041.435 I print_info: n_ff             = 8192
0.00.041.435 I print_info: n_expert         = 0
0.00.041.435 I print_info: n_expert_used    = 0
0.00.041.435 I print_info: causal attn      = 1
0.00.041.435 I print_info: pooling type     = 0
0.00.041.435 I print_info: rope type        = 2
0.00.041.435 I print_info: rope scaling     = linear
0.00.041.436 I print_info: freq_base_train  = 10000.0
0.00.041.436 I print_info: freq_scale_train = 1
0.00.041.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.437 I print_info: rope_finetuned   = unknown
0.00.041.437 I print_info: ssm_d_conv       = 0
0.00.041.437 I print_info: ssm_d_inner      = 0
0.00.041.437 I print_info: ssm_d_state      = 0
0.00.041.437 I print_info: ssm_dt_rank      = 0
0.00.041.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.437 I print_info: model type       = 1.4B
0.00.041.438 I print_info: model params     = 1.41 B
0.00.041.438 I print_info: general.name     = 1.4B
0.00.041.438 I print_info: vocab type       = BPE
0.00.041.439 I print_info: n_vocab          = 50304
0.00.041.443 I print_info: n_merges         = 50009
0.00.041.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.444 I print_info: LF token         = 187 ''
0.00.041.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.444 I print_info: max token length = 1024
0.00.041.445 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.665.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.899 I load_tensors: offloading output layer to GPU
0.00.665.900 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.921 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.665.923 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.667.142 I llama_init_from_model: n_seq_max     = 1
0.00.667.145 I llama_init_from_model: n_ctx         = 128
0.00.667.145 I llama_init_from_model: n_ctx_per_seq = 128
0.00.667.146 I llama_init_from_model: n_batch       = 128
0.00.667.146 I llama_init_from_model: n_ubatch      = 128
0.00.667.147 I llama_init_from_model: flash_attn    = 0
0.00.667.148 I llama_init_from_model: freq_base     = 10000.0
0.00.667.148 I llama_init_from_model: freq_scale    = 1
0.00.667.149 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.667.151 I ggml_metal_init: allocating
0.00.667.166 I ggml_metal_init: found device: Apple M4
0.00.667.187 I ggml_metal_init: picking default device: Apple M4
0.00.668.502 I ggml_metal_init: using embedded metal library
0.00.674.525 I ggml_metal_init: GPU name:   Apple M4
0.00.674.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.531 I ggml_metal_init: simdgroup reduction   = true
0.00.674.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.532 I ggml_metal_init: has residency sets    = true
0.00.674.532 I ggml_metal_init: has bfloat            = true
0.00.674.532 I ggml_metal_init: use bfloat            = true
0.00.674.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.039 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.042 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.081 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.234 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.236 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.236 I llama_init_from_model: graph nodes  = 967
0.00.698.237 I llama_init_from_model: graph splits = 2
0.00.698.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.657 I 
0.00.723.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.724 I perplexity: tokenizing the input ..
0.00.730.008 I perplexity: tokenization took 6.281 ms
0.00.730.016 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.171 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.439 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.469 I llama_perf_context_print:        load time =     712.80 ms
0.00.866.470 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.85 tokens per second)
0.00.866.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.471 I llama_perf_context_print:       total time =     142.82 ms /   129 tokens
0.00.866.908 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.078s
sys	0m0.185s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.631 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.393 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.144 I llama_model_loader: - type  f32:  194 tensors
0.00.026.144 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.145 I print_info: file format = GGUF V3 (latest)
0.00.026.146 I print_info: file type   = Q5_1
0.00.026.147 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.050 I load: special tokens cache size = 25
0.00.040.014 I load: token to piece cache size = 0.2984 MB
0.00.040.017 I print_info: arch             = gptneox
0.00.040.017 I print_info: vocab_only       = 0
0.00.040.017 I print_info: n_ctx_train      = 2048
0.00.040.017 I print_info: n_embd           = 2048
0.00.040.018 I print_info: n_layer          = 24
0.00.040.020 I print_info: n_head           = 16
0.00.040.020 I print_info: n_head_kv        = 16
0.00.040.021 I print_info: n_rot            = 32
0.00.040.022 I print_info: n_swa            = 0
0.00.040.022 I print_info: n_embd_head_k    = 128
0.00.040.023 I print_info: n_embd_head_v    = 128
0.00.040.023 I print_info: n_gqa            = 1
0.00.040.024 I print_info: n_embd_k_gqa     = 2048
0.00.040.025 I print_info: n_embd_v_gqa     = 2048
0.00.040.025 I print_info: f_norm_eps       = 1.0e-05
0.00.040.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.026 I print_info: f_logit_scale    = 0.0e+00
0.00.040.027 I print_info: n_ff             = 8192
0.00.040.027 I print_info: n_expert         = 0
0.00.040.027 I print_info: n_expert_used    = 0
0.00.040.027 I print_info: causal attn      = 1
0.00.040.027 I print_info: pooling type     = 0
0.00.040.029 I print_info: rope type        = 2
0.00.040.030 I print_info: rope scaling     = linear
0.00.040.030 I print_info: freq_base_train  = 10000.0
0.00.040.030 I print_info: freq_scale_train = 1
0.00.040.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.031 I print_info: rope_finetuned   = unknown
0.00.040.031 I print_info: ssm_d_conv       = 0
0.00.040.031 I print_info: ssm_d_inner      = 0
0.00.040.031 I print_info: ssm_d_state      = 0
0.00.040.031 I print_info: ssm_dt_rank      = 0
0.00.040.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.033 I print_info: model type       = 1.4B
0.00.040.034 I print_info: model params     = 1.41 B
0.00.040.034 I print_info: general.name     = 1.4B
0.00.040.034 I print_info: vocab type       = BPE
0.00.040.034 I print_info: n_vocab          = 50304
0.00.040.035 I print_info: n_merges         = 50009
0.00.040.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.036 I print_info: LF token         = 187 ''
0.00.040.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.040 I print_info: max token length = 1024
0.00.040.040 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.791.248 I load_tensors: offloading 24 repeating layers to GPU
0.00.791.252 I load_tensors: offloading output layer to GPU
0.00.791.253 I load_tensors: offloaded 25/25 layers to GPU
0.00.791.275 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.791.276 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.792.530 I llama_init_from_model: n_seq_max     = 1
0.00.792.532 I llama_init_from_model: n_ctx         = 2048
0.00.792.533 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.792.533 I llama_init_from_model: n_batch       = 2048
0.00.792.533 I llama_init_from_model: n_ubatch      = 512
0.00.792.534 I llama_init_from_model: flash_attn    = 0
0.00.792.535 I llama_init_from_model: freq_base     = 10000.0
0.00.792.536 I llama_init_from_model: freq_scale    = 1
0.00.792.537 I ggml_metal_init: allocating
0.00.792.560 I ggml_metal_init: found device: Apple M4
0.00.792.572 I ggml_metal_init: picking default device: Apple M4
0.00.793.929 I ggml_metal_init: using embedded metal library
0.00.799.713 I ggml_metal_init: GPU name:   Apple M4
0.00.799.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.799.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.799.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.799.719 I ggml_metal_init: simdgroup reduction   = true
0.00.799.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.799.719 I ggml_metal_init: has residency sets    = true
0.00.799.720 I ggml_metal_init: has bfloat            = true
0.00.799.720 I ggml_metal_init: use bfloat            = true
0.00.799.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.799.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.815.949 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.870.278 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.870.288 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.870.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.875.621 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.875.624 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.875.624 I llama_init_from_model: graph nodes  = 967
0.00.875.624 I llama_init_from_model: graph splits = 2
0.00.875.629 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.875.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.875.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.704 I main: llama threadpool init, n_threads = 4
0.00.926.752 I 
0.00.926.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.926.768 I 
0.00.926.893 I sampler seed: 1234
0.00.926.898 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.926.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.926.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.926.940 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.771.302 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.771.305 I llama_perf_context_print:        load time =     916.37 ms
0.01.771.306 I llama_perf_context_print: prompt eval time =      52.77 ms /     7 tokens (    7.54 ms per token,   132.66 tokens per second)
0.01.771.307 I llama_perf_context_print:        eval time =     788.60 ms /    63 runs   (   12.52 ms per token,    79.89 tokens per second)
0.01.771.307 I llama_perf_context_print:       total time =     845.30 ms /    70 tokens
0.01.771.529 I ggml_metal_free: deallocating

real	0m1.788s
user	0m0.107s
sys	0m0.273s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.824 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.828 I llama_model_loader: - type  f32:  194 tensors
0.00.025.828 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.829 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.830 I print_info: file type   = Q5_1
0.00.025.831 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.217 I load: special tokens cache size = 25
0.00.040.372 I load: token to piece cache size = 0.2984 MB
0.00.040.377 I print_info: arch             = gptneox
0.00.040.377 I print_info: vocab_only       = 0
0.00.040.377 I print_info: n_ctx_train      = 2048
0.00.040.378 I print_info: n_embd           = 2048
0.00.040.378 I print_info: n_layer          = 24
0.00.040.382 I print_info: n_head           = 16
0.00.040.383 I print_info: n_head_kv        = 16
0.00.040.383 I print_info: n_rot            = 32
0.00.040.385 I print_info: n_swa            = 0
0.00.040.385 I print_info: n_embd_head_k    = 128
0.00.040.385 I print_info: n_embd_head_v    = 128
0.00.040.388 I print_info: n_gqa            = 1
0.00.040.388 I print_info: n_embd_k_gqa     = 2048
0.00.040.389 I print_info: n_embd_v_gqa     = 2048
0.00.040.389 I print_info: f_norm_eps       = 1.0e-05
0.00.040.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.390 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.391 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.391 I print_info: f_logit_scale    = 0.0e+00
0.00.040.392 I print_info: n_ff             = 8192
0.00.040.392 I print_info: n_expert         = 0
0.00.040.392 I print_info: n_expert_used    = 0
0.00.040.393 I print_info: causal attn      = 1
0.00.040.393 I print_info: pooling type     = 0
0.00.040.393 I print_info: rope type        = 2
0.00.040.393 I print_info: rope scaling     = linear
0.00.040.393 I print_info: freq_base_train  = 10000.0
0.00.040.394 I print_info: freq_scale_train = 1
0.00.040.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.394 I print_info: rope_finetuned   = unknown
0.00.040.394 I print_info: ssm_d_conv       = 0
0.00.040.394 I print_info: ssm_d_inner      = 0
0.00.040.394 I print_info: ssm_d_state      = 0
0.00.040.395 I print_info: ssm_dt_rank      = 0
0.00.040.395 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.395 I print_info: model type       = 1.4B
0.00.040.395 I print_info: model params     = 1.41 B
0.00.040.395 I print_info: general.name     = 1.4B
0.00.040.396 I print_info: vocab type       = BPE
0.00.040.396 I print_info: n_vocab          = 50304
0.00.040.396 I print_info: n_merges         = 50009
0.00.040.396 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.397 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.397 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.397 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.397 I print_info: LF token         = 187 ''
0.00.040.397 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.398 I print_info: max token length = 1024
0.00.040.398 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.789.020 I load_tensors: offloading 24 repeating layers to GPU
0.00.789.025 I load_tensors: offloading output layer to GPU
0.00.789.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.789.050 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.789.052 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.790.170 I llama_init_from_model: n_seq_max     = 1
0.00.790.172 I llama_init_from_model: n_ctx         = 128
0.00.790.172 I llama_init_from_model: n_ctx_per_seq = 128
0.00.790.173 I llama_init_from_model: n_batch       = 128
0.00.790.173 I llama_init_from_model: n_ubatch      = 128
0.00.790.173 I llama_init_from_model: flash_attn    = 0
0.00.790.174 I llama_init_from_model: freq_base     = 10000.0
0.00.790.175 I llama_init_from_model: freq_scale    = 1
0.00.790.175 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.790.177 I ggml_metal_init: allocating
0.00.790.191 I ggml_metal_init: found device: Apple M4
0.00.790.199 I ggml_metal_init: picking default device: Apple M4
0.00.791.404 I ggml_metal_init: using embedded metal library
0.00.796.909 I ggml_metal_init: GPU name:   Apple M4
0.00.796.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.796.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.796.914 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.796.915 I ggml_metal_init: simdgroup reduction   = true
0.00.796.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.796.916 I ggml_metal_init: has residency sets    = true
0.00.796.916 I ggml_metal_init: has bfloat            = true
0.00.796.916 I ggml_metal_init: use bfloat            = true
0.00.796.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.796.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.812.732 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.111 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.816.114 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.816.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.819.447 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.819.449 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.819.449 I llama_init_from_model: graph nodes  = 967
0.00.819.449 I llama_init_from_model: graph splits = 2
0.00.819.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.819.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.850.703 I 
0.00.850.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.850.772 I perplexity: tokenizing the input ..
0.00.858.148 I perplexity: tokenization took 7.372 ms
0.00.858.157 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.007.636 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.01.008.924 I Final estimate: PPL = 10.1971 +/- 3.18866

0.01.008.953 I llama_perf_context_print:        load time =     840.88 ms
0.01.008.954 I llama_perf_context_print: prompt eval time =     148.64 ms /   128 tokens (    1.16 ms per token,   861.13 tokens per second)
0.01.008.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.008.955 I llama_perf_context_print:       total time =     158.25 ms /   129 tokens
0.01.009.338 I ggml_metal_free: deallocating

real	0m1.023s
user	0m0.078s
sys	0m0.204s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.389 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.389 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.925 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.926 I llama_model_loader: - type  f32:  194 tensors
0.00.023.926 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.926 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.927 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.927 I print_info: file format = GGUF V3 (latest)
0.00.023.928 I print_info: file type   = Q2_K - Medium
0.00.023.928 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.873 I load: special tokens cache size = 25
0.00.037.930 I load: token to piece cache size = 0.2984 MB
0.00.037.935 I print_info: arch             = gptneox
0.00.037.936 I print_info: vocab_only       = 0
0.00.037.936 I print_info: n_ctx_train      = 2048
0.00.037.936 I print_info: n_embd           = 2048
0.00.037.936 I print_info: n_layer          = 24
0.00.037.939 I print_info: n_head           = 16
0.00.037.939 I print_info: n_head_kv        = 16
0.00.037.940 I print_info: n_rot            = 32
0.00.037.940 I print_info: n_swa            = 0
0.00.037.940 I print_info: n_embd_head_k    = 128
0.00.037.940 I print_info: n_embd_head_v    = 128
0.00.037.941 I print_info: n_gqa            = 1
0.00.037.942 I print_info: n_embd_k_gqa     = 2048
0.00.037.942 I print_info: n_embd_v_gqa     = 2048
0.00.037.943 I print_info: f_norm_eps       = 1.0e-05
0.00.037.943 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.943 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.943 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.944 I print_info: f_logit_scale    = 0.0e+00
0.00.037.946 I print_info: n_ff             = 8192
0.00.037.947 I print_info: n_expert         = 0
0.00.037.947 I print_info: n_expert_used    = 0
0.00.037.947 I print_info: causal attn      = 1
0.00.037.947 I print_info: pooling type     = 0
0.00.037.949 I print_info: rope type        = 2
0.00.037.949 I print_info: rope scaling     = linear
0.00.037.950 I print_info: freq_base_train  = 10000.0
0.00.037.950 I print_info: freq_scale_train = 1
0.00.037.950 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.950 I print_info: rope_finetuned   = unknown
0.00.037.950 I print_info: ssm_d_conv       = 0
0.00.037.951 I print_info: ssm_d_inner      = 0
0.00.037.952 I print_info: ssm_d_state      = 0
0.00.037.952 I print_info: ssm_dt_rank      = 0
0.00.037.952 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.952 I print_info: model type       = 1.4B
0.00.037.953 I print_info: model params     = 1.41 B
0.00.037.953 I print_info: general.name     = 1.4B
0.00.037.953 I print_info: vocab type       = BPE
0.00.037.954 I print_info: n_vocab          = 50304
0.00.037.954 I print_info: n_merges         = 50009
0.00.037.954 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.955 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.959 I print_info: LF token         = 187 ''
0.00.037.959 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.959 I print_info: max token length = 1024
0.00.037.960 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.400.238 I load_tensors: offloading 24 repeating layers to GPU
0.00.400.248 I load_tensors: offloading output layer to GPU
0.00.400.248 I load_tensors: offloaded 25/25 layers to GPU
0.00.400.278 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.400.280 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.401.685 I llama_init_from_model: n_seq_max     = 1
0.00.401.688 I llama_init_from_model: n_ctx         = 2048
0.00.401.689 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.401.690 I llama_init_from_model: n_batch       = 2048
0.00.401.690 I llama_init_from_model: n_ubatch      = 512
0.00.401.690 I llama_init_from_model: flash_attn    = 0
0.00.401.692 I llama_init_from_model: freq_base     = 10000.0
0.00.401.693 I llama_init_from_model: freq_scale    = 1
0.00.401.700 I ggml_metal_init: allocating
0.00.401.756 I ggml_metal_init: found device: Apple M4
0.00.401.769 I ggml_metal_init: picking default device: Apple M4
0.00.403.700 I ggml_metal_init: using embedded metal library
0.00.410.593 I ggml_metal_init: GPU name:   Apple M4
0.00.410.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.410.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.410.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.410.599 I ggml_metal_init: simdgroup reduction   = true
0.00.410.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.410.600 I ggml_metal_init: has residency sets    = true
0.00.410.600 I ggml_metal_init: has bfloat            = true
0.00.410.600 I ggml_metal_init: use bfloat            = true
0.00.410.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.410.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.428.801 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.713 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.483.720 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.483.763 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.015 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.489.018 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.489.019 I llama_init_from_model: graph nodes  = 967
0.00.489.019 I llama_init_from_model: graph splits = 2
0.00.489.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.489.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.489.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.682 I main: llama threadpool init, n_threads = 4
0.00.540.725 I 
0.00.540.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.742 I 
0.00.540.890 I sampler seed: 1234
0.00.540.894 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.926 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.928 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.928 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.234.680 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.234.681 I llama_perf_context_print:        load time =     530.87 ms
0.01.234.682 I llama_perf_context_print: prompt eval time =      44.37 ms /     7 tokens (    6.34 ms per token,   157.75 tokens per second)
0.01.234.682 I llama_perf_context_print:        eval time =     646.47 ms /    63 runs   (   10.26 ms per token,    97.45 tokens per second)
0.01.234.683 I llama_perf_context_print:       total time =     694.98 ms /    70 tokens
0.01.234.964 I ggml_metal_free: deallocating

real	0m1.251s
user	0m0.109s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.320 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.200 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.201 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.201 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.205 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.205 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.208 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.208 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.209 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.067 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.925 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.926 I llama_model_loader: - type  f32:  194 tensors
0.00.026.926 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.926 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.927 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.928 I print_info: file format = GGUF V3 (latest)
0.00.026.928 I print_info: file type   = Q2_K - Medium
0.00.026.934 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.904 I load: special tokens cache size = 25
0.00.040.662 I load: token to piece cache size = 0.2984 MB
0.00.040.665 I print_info: arch             = gptneox
0.00.040.666 I print_info: vocab_only       = 0
0.00.040.666 I print_info: n_ctx_train      = 2048
0.00.040.666 I print_info: n_embd           = 2048
0.00.040.666 I print_info: n_layer          = 24
0.00.040.670 I print_info: n_head           = 16
0.00.040.670 I print_info: n_head_kv        = 16
0.00.040.671 I print_info: n_rot            = 32
0.00.040.671 I print_info: n_swa            = 0
0.00.040.671 I print_info: n_embd_head_k    = 128
0.00.040.671 I print_info: n_embd_head_v    = 128
0.00.040.672 I print_info: n_gqa            = 1
0.00.040.673 I print_info: n_embd_k_gqa     = 2048
0.00.040.673 I print_info: n_embd_v_gqa     = 2048
0.00.040.674 I print_info: f_norm_eps       = 1.0e-05
0.00.040.674 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.674 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.675 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.675 I print_info: f_logit_scale    = 0.0e+00
0.00.040.675 I print_info: n_ff             = 8192
0.00.040.676 I print_info: n_expert         = 0
0.00.040.677 I print_info: n_expert_used    = 0
0.00.040.678 I print_info: causal attn      = 1
0.00.040.678 I print_info: pooling type     = 0
0.00.040.678 I print_info: rope type        = 2
0.00.040.678 I print_info: rope scaling     = linear
0.00.040.678 I print_info: freq_base_train  = 10000.0
0.00.040.679 I print_info: freq_scale_train = 1
0.00.040.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.679 I print_info: rope_finetuned   = unknown
0.00.040.679 I print_info: ssm_d_conv       = 0
0.00.040.680 I print_info: ssm_d_inner      = 0
0.00.040.680 I print_info: ssm_d_state      = 0
0.00.040.680 I print_info: ssm_dt_rank      = 0
0.00.040.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.680 I print_info: model type       = 1.4B
0.00.040.680 I print_info: model params     = 1.41 B
0.00.040.681 I print_info: general.name     = 1.4B
0.00.040.681 I print_info: vocab type       = BPE
0.00.040.681 I print_info: n_vocab          = 50304
0.00.040.682 I print_info: n_merges         = 50009
0.00.040.682 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.682 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.682 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: LF token         = 187 ''
0.00.040.683 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: max token length = 1024
0.00.040.684 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.406.151 I load_tensors: offloading 24 repeating layers to GPU
0.00.406.163 I load_tensors: offloading output layer to GPU
0.00.406.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.406.192 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.406.194 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.407.677 I llama_init_from_model: n_seq_max     = 1
0.00.407.680 I llama_init_from_model: n_ctx         = 128
0.00.407.682 I llama_init_from_model: n_ctx_per_seq = 128
0.00.407.682 I llama_init_from_model: n_batch       = 128
0.00.407.682 I llama_init_from_model: n_ubatch      = 128
0.00.407.683 I llama_init_from_model: flash_attn    = 0
0.00.407.685 I llama_init_from_model: freq_base     = 10000.0
0.00.407.686 I llama_init_from_model: freq_scale    = 1
0.00.407.697 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.407.699 I ggml_metal_init: allocating
0.00.407.763 I ggml_metal_init: found device: Apple M4
0.00.407.779 I ggml_metal_init: picking default device: Apple M4
0.00.409.543 I ggml_metal_init: using embedded metal library
0.00.415.855 I ggml_metal_init: GPU name:   Apple M4
0.00.415.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.415.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.415.862 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.415.863 I ggml_metal_init: simdgroup reduction   = true
0.00.415.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.415.863 I ggml_metal_init: has residency sets    = true
0.00.415.864 I ggml_metal_init: has bfloat            = true
0.00.415.864 I ggml_metal_init: use bfloat            = true
0.00.415.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.415.867 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.434.846 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.382 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.438.385 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.438.425 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.503 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.441.504 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.441.505 I llama_init_from_model: graph nodes  = 967
0.00.441.505 I llama_init_from_model: graph splits = 2
0.00.441.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.441.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.308 I 
0.00.474.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.385 I perplexity: tokenizing the input ..
0.00.480.957 I perplexity: tokenization took 6.568 ms
0.00.480.964 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.958 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.628.234 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.628.263 I llama_perf_context_print:        load time =     462.98 ms
0.00.628.264 I llama_perf_context_print: prompt eval time =     145.09 ms /   128 tokens (    1.13 ms per token,   882.19 tokens per second)
0.00.628.265 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.265 I llama_perf_context_print:       total time =     153.96 ms /   129 tokens
0.00.628.654 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.080s
sys	0m0.123s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.895 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.906 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.585 I llama_model_loader: - type  f32:  194 tensors
0.00.025.586 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.586 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.586 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.586 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.587 I print_info: file format = GGUF V3 (latest)
0.00.025.587 I print_info: file type   = Q3_K - Medium
0.00.025.588 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.705 I load: special tokens cache size = 25
0.00.039.707 I load: token to piece cache size = 0.2984 MB
0.00.039.710 I print_info: arch             = gptneox
0.00.039.710 I print_info: vocab_only       = 0
0.00.039.711 I print_info: n_ctx_train      = 2048
0.00.039.711 I print_info: n_embd           = 2048
0.00.039.711 I print_info: n_layer          = 24
0.00.039.713 I print_info: n_head           = 16
0.00.039.714 I print_info: n_head_kv        = 16
0.00.039.714 I print_info: n_rot            = 32
0.00.039.716 I print_info: n_swa            = 0
0.00.039.716 I print_info: n_embd_head_k    = 128
0.00.039.716 I print_info: n_embd_head_v    = 128
0.00.039.717 I print_info: n_gqa            = 1
0.00.039.718 I print_info: n_embd_k_gqa     = 2048
0.00.039.718 I print_info: n_embd_v_gqa     = 2048
0.00.039.719 I print_info: f_norm_eps       = 1.0e-05
0.00.039.719 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.719 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.720 I print_info: f_logit_scale    = 0.0e+00
0.00.039.720 I print_info: n_ff             = 8192
0.00.039.721 I print_info: n_expert         = 0
0.00.039.721 I print_info: n_expert_used    = 0
0.00.039.722 I print_info: causal attn      = 1
0.00.039.723 I print_info: pooling type     = 0
0.00.039.723 I print_info: rope type        = 2
0.00.039.723 I print_info: rope scaling     = linear
0.00.039.724 I print_info: freq_base_train  = 10000.0
0.00.039.724 I print_info: freq_scale_train = 1
0.00.039.724 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.725 I print_info: rope_finetuned   = unknown
0.00.039.725 I print_info: ssm_d_conv       = 0
0.00.039.725 I print_info: ssm_d_inner      = 0
0.00.039.725 I print_info: ssm_d_state      = 0
0.00.039.725 I print_info: ssm_dt_rank      = 0
0.00.039.725 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.726 I print_info: model type       = 1.4B
0.00.039.726 I print_info: model params     = 1.41 B
0.00.039.728 I print_info: general.name     = 1.4B
0.00.039.728 I print_info: vocab type       = BPE
0.00.039.729 I print_info: n_vocab          = 50304
0.00.039.729 I print_info: n_merges         = 50009
0.00.039.729 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.730 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.730 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.730 I print_info: LF token         = 187 ''
0.00.039.730 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.730 I print_info: max token length = 1024
0.00.039.731 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.486.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.486.275 I load_tensors: offloading output layer to GPU
0.00.486.276 I load_tensors: offloaded 25/25 layers to GPU
0.00.486.306 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.486.316 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.487.666 I llama_init_from_model: n_seq_max     = 1
0.00.487.668 I llama_init_from_model: n_ctx         = 2048
0.00.487.669 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.487.670 I llama_init_from_model: n_batch       = 2048
0.00.487.670 I llama_init_from_model: n_ubatch      = 512
0.00.487.671 I llama_init_from_model: flash_attn    = 0
0.00.487.672 I llama_init_from_model: freq_base     = 10000.0
0.00.487.673 I llama_init_from_model: freq_scale    = 1
0.00.487.675 I ggml_metal_init: allocating
0.00.487.735 I ggml_metal_init: found device: Apple M4
0.00.487.751 I ggml_metal_init: picking default device: Apple M4
0.00.489.651 I ggml_metal_init: using embedded metal library
0.00.496.200 I ggml_metal_init: GPU name:   Apple M4
0.00.496.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.496.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.496.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.496.206 I ggml_metal_init: simdgroup reduction   = true
0.00.496.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.496.207 I ggml_metal_init: has residency sets    = true
0.00.496.207 I ggml_metal_init: has bfloat            = true
0.00.496.207 I ggml_metal_init: use bfloat            = true
0.00.496.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.496.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.514.499 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.568.408 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.568.414 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.568.449 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.573.863 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.573.865 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.573.865 I llama_init_from_model: graph nodes  = 967
0.00.573.866 I llama_init_from_model: graph splits = 2
0.00.573.871 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.573.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.573.996 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.079 I main: llama threadpool init, n_threads = 4
0.00.622.122 I 
0.00.622.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.139 I 
0.00.622.267 I sampler seed: 1234
0.00.622.271 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.282 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.282 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.372.465 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.372.466 I llama_perf_context_print:        load time =     612.64 ms
0.01.372.466 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.37 tokens per second)
0.01.372.468 I llama_perf_context_print:        eval time =     698.01 ms /    63 runs   (   11.08 ms per token,    90.26 tokens per second)
0.01.372.469 I llama_perf_context_print:       total time =     751.07 ms /    70 tokens
0.01.372.666 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.903 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.919 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.920 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.503 I llama_model_loader: - type  f32:  194 tensors
0.00.025.503 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.504 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.504 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.504 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.505 I print_info: file format = GGUF V3 (latest)
0.00.025.505 I print_info: file type   = Q3_K - Medium
0.00.025.506 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.606 I load: special tokens cache size = 25
0.00.039.750 I load: token to piece cache size = 0.2984 MB
0.00.039.753 I print_info: arch             = gptneox
0.00.039.753 I print_info: vocab_only       = 0
0.00.039.753 I print_info: n_ctx_train      = 2048
0.00.039.753 I print_info: n_embd           = 2048
0.00.039.754 I print_info: n_layer          = 24
0.00.039.757 I print_info: n_head           = 16
0.00.039.758 I print_info: n_head_kv        = 16
0.00.039.758 I print_info: n_rot            = 32
0.00.039.758 I print_info: n_swa            = 0
0.00.039.758 I print_info: n_embd_head_k    = 128
0.00.039.758 I print_info: n_embd_head_v    = 128
0.00.039.759 I print_info: n_gqa            = 1
0.00.039.760 I print_info: n_embd_k_gqa     = 2048
0.00.039.761 I print_info: n_embd_v_gqa     = 2048
0.00.039.761 I print_info: f_norm_eps       = 1.0e-05
0.00.039.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.764 I print_info: f_logit_scale    = 0.0e+00
0.00.039.765 I print_info: n_ff             = 8192
0.00.039.765 I print_info: n_expert         = 0
0.00.039.765 I print_info: n_expert_used    = 0
0.00.039.765 I print_info: causal attn      = 1
0.00.039.767 I print_info: pooling type     = 0
0.00.039.768 I print_info: rope type        = 2
0.00.039.769 I print_info: rope scaling     = linear
0.00.039.769 I print_info: freq_base_train  = 10000.0
0.00.039.769 I print_info: freq_scale_train = 1
0.00.039.769 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.771 I print_info: rope_finetuned   = unknown
0.00.039.771 I print_info: ssm_d_conv       = 0
0.00.039.771 I print_info: ssm_d_inner      = 0
0.00.039.771 I print_info: ssm_d_state      = 0
0.00.039.771 I print_info: ssm_dt_rank      = 0
0.00.039.772 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.772 I print_info: model type       = 1.4B
0.00.039.772 I print_info: model params     = 1.41 B
0.00.039.773 I print_info: general.name     = 1.4B
0.00.039.774 I print_info: vocab type       = BPE
0.00.039.774 I print_info: n_vocab          = 50304
0.00.039.774 I print_info: n_merges         = 50009
0.00.039.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.775 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.775 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.775 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.775 I print_info: LF token         = 187 ''
0.00.039.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.776 I print_info: max token length = 1024
0.00.039.776 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.484.752 I load_tensors: offloading 24 repeating layers to GPU
0.00.484.767 I load_tensors: offloading output layer to GPU
0.00.484.768 I load_tensors: offloaded 25/25 layers to GPU
0.00.484.795 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.484.796 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.486.195 I llama_init_from_model: n_seq_max     = 1
0.00.486.197 I llama_init_from_model: n_ctx         = 128
0.00.486.197 I llama_init_from_model: n_ctx_per_seq = 128
0.00.486.198 I llama_init_from_model: n_batch       = 128
0.00.486.199 I llama_init_from_model: n_ubatch      = 128
0.00.486.199 I llama_init_from_model: flash_attn    = 0
0.00.486.200 I llama_init_from_model: freq_base     = 10000.0
0.00.486.201 I llama_init_from_model: freq_scale    = 1
0.00.486.201 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.486.203 I ggml_metal_init: allocating
0.00.486.270 I ggml_metal_init: found device: Apple M4
0.00.486.283 I ggml_metal_init: picking default device: Apple M4
0.00.488.159 I ggml_metal_init: using embedded metal library
0.00.494.738 I ggml_metal_init: GPU name:   Apple M4
0.00.494.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.494.743 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.494.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.494.745 I ggml_metal_init: simdgroup reduction   = true
0.00.494.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.494.745 I ggml_metal_init: has residency sets    = true
0.00.494.745 I ggml_metal_init: has bfloat            = true
0.00.494.746 I ggml_metal_init: use bfloat            = true
0.00.494.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.494.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.513.218 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.516.678 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.516.681 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.516.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.520.021 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.520.023 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.520.023 I llama_init_from_model: graph nodes  = 967
0.00.520.024 I llama_init_from_model: graph splits = 2
0.00.520.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.520.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.375 I 
0.00.548.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.436 I perplexity: tokenizing the input ..
0.00.554.706 I perplexity: tokenization took 6.268 ms
0.00.554.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.700.934 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.702.221 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.702.253 I llama_perf_context_print:        load time =     538.65 ms
0.00.702.255 I llama_perf_context_print: prompt eval time =     145.68 ms /   128 tokens (    1.14 ms per token,   878.67 tokens per second)
0.00.702.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.260 I llama_perf_context_print:       total time =     153.88 ms /   129 tokens
0.00.702.629 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.080s
sys	0m0.143s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.687 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.384 I llama_model_loader: - type  f32:  194 tensors
0.00.026.384 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.384 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.384 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.385 I print_info: file format = GGUF V3 (latest)
0.00.026.386 I print_info: file type   = Q4_K - Medium
0.00.026.386 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.665 I load: special tokens cache size = 25
0.00.040.610 I load: token to piece cache size = 0.2984 MB
0.00.040.615 I print_info: arch             = gptneox
0.00.040.615 I print_info: vocab_only       = 0
0.00.040.616 I print_info: n_ctx_train      = 2048
0.00.040.616 I print_info: n_embd           = 2048
0.00.040.616 I print_info: n_layer          = 24
0.00.040.620 I print_info: n_head           = 16
0.00.040.621 I print_info: n_head_kv        = 16
0.00.040.621 I print_info: n_rot            = 32
0.00.040.621 I print_info: n_swa            = 0
0.00.040.621 I print_info: n_embd_head_k    = 128
0.00.040.621 I print_info: n_embd_head_v    = 128
0.00.040.624 I print_info: n_gqa            = 1
0.00.040.625 I print_info: n_embd_k_gqa     = 2048
0.00.040.625 I print_info: n_embd_v_gqa     = 2048
0.00.040.626 I print_info: f_norm_eps       = 1.0e-05
0.00.040.626 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.626 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.626 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.627 I print_info: f_logit_scale    = 0.0e+00
0.00.040.627 I print_info: n_ff             = 8192
0.00.040.627 I print_info: n_expert         = 0
0.00.040.628 I print_info: n_expert_used    = 0
0.00.040.628 I print_info: causal attn      = 1
0.00.040.628 I print_info: pooling type     = 0
0.00.040.628 I print_info: rope type        = 2
0.00.040.629 I print_info: rope scaling     = linear
0.00.040.630 I print_info: freq_base_train  = 10000.0
0.00.040.630 I print_info: freq_scale_train = 1
0.00.040.630 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.630 I print_info: rope_finetuned   = unknown
0.00.040.631 I print_info: ssm_d_conv       = 0
0.00.040.631 I print_info: ssm_d_inner      = 0
0.00.040.631 I print_info: ssm_d_state      = 0
0.00.040.631 I print_info: ssm_dt_rank      = 0
0.00.040.631 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.631 I print_info: model type       = 1.4B
0.00.040.632 I print_info: model params     = 1.41 B
0.00.040.636 I print_info: general.name     = 1.4B
0.00.040.636 I print_info: vocab type       = BPE
0.00.040.636 I print_info: n_vocab          = 50304
0.00.040.637 I print_info: n_merges         = 50009
0.00.040.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.637 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.637 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.637 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.638 I print_info: LF token         = 187 ''
0.00.040.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.638 I print_info: max token length = 1024
0.00.040.638 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.006 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.011 I load_tensors: offloading output layer to GPU
0.00.586.012 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.034 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.586.035 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.587.105 I llama_init_from_model: n_seq_max     = 1
0.00.587.107 I llama_init_from_model: n_ctx         = 2048
0.00.587.108 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.587.108 I llama_init_from_model: n_batch       = 2048
0.00.587.109 I llama_init_from_model: n_ubatch      = 512
0.00.587.109 I llama_init_from_model: flash_attn    = 0
0.00.587.110 I llama_init_from_model: freq_base     = 10000.0
0.00.587.111 I llama_init_from_model: freq_scale    = 1
0.00.587.112 I ggml_metal_init: allocating
0.00.587.121 I ggml_metal_init: found device: Apple M4
0.00.587.129 I ggml_metal_init: picking default device: Apple M4
0.00.588.592 I ggml_metal_init: using embedded metal library
0.00.594.708 I ggml_metal_init: GPU name:   Apple M4
0.00.594.711 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.713 I ggml_metal_init: simdgroup reduction   = true
0.00.594.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.713 I ggml_metal_init: has residency sets    = true
0.00.594.713 I ggml_metal_init: has bfloat            = true
0.00.594.714 I ggml_metal_init: use bfloat            = true
0.00.594.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.447 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.664.452 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.664.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.712 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.669.714 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.669.715 I llama_init_from_model: graph nodes  = 967
0.00.669.715 I llama_init_from_model: graph splits = 2
0.00.669.720 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.669.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.669.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.369 I main: llama threadpool init, n_threads = 4
0.00.717.413 I 
0.00.717.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.430 I 
0.00.717.545 I sampler seed: 1234
0.00.717.549 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.560 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.560 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.472.733 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.472.734 I llama_perf_context_print:        load time =     706.88 ms
0.01.472.734 I llama_perf_context_print: prompt eval time =      46.72 ms /     7 tokens (    6.67 ms per token,   149.82 tokens per second)
0.01.472.737 I llama_perf_context_print:        eval time =     705.39 ms /    63 runs   (   11.20 ms per token,    89.31 tokens per second)
0.01.472.737 I llama_perf_context_print:       total time =     756.06 ms /    70 tokens
0.01.473.000 I ggml_metal_free: deallocating

real	0m1.490s
user	0m0.108s
sys	0m0.239s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.087 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.745 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.747 I llama_model_loader: - type  f32:  194 tensors
0.00.026.747 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.747 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.748 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.748 I print_info: file format = GGUF V3 (latest)
0.00.026.749 I print_info: file type   = Q4_K - Medium
0.00.026.750 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.116 I load: special tokens cache size = 25
0.00.041.031 I load: token to piece cache size = 0.2984 MB
0.00.041.034 I print_info: arch             = gptneox
0.00.041.034 I print_info: vocab_only       = 0
0.00.041.034 I print_info: n_ctx_train      = 2048
0.00.041.034 I print_info: n_embd           = 2048
0.00.041.034 I print_info: n_layer          = 24
0.00.041.038 I print_info: n_head           = 16
0.00.041.039 I print_info: n_head_kv        = 16
0.00.041.039 I print_info: n_rot            = 32
0.00.041.039 I print_info: n_swa            = 0
0.00.041.040 I print_info: n_embd_head_k    = 128
0.00.041.040 I print_info: n_embd_head_v    = 128
0.00.041.041 I print_info: n_gqa            = 1
0.00.041.041 I print_info: n_embd_k_gqa     = 2048
0.00.041.042 I print_info: n_embd_v_gqa     = 2048
0.00.041.042 I print_info: f_norm_eps       = 1.0e-05
0.00.041.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.045 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.045 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.045 I print_info: f_logit_scale    = 0.0e+00
0.00.041.046 I print_info: n_ff             = 8192
0.00.041.046 I print_info: n_expert         = 0
0.00.041.046 I print_info: n_expert_used    = 0
0.00.041.046 I print_info: causal attn      = 1
0.00.041.047 I print_info: pooling type     = 0
0.00.041.047 I print_info: rope type        = 2
0.00.041.047 I print_info: rope scaling     = linear
0.00.041.047 I print_info: freq_base_train  = 10000.0
0.00.041.048 I print_info: freq_scale_train = 1
0.00.041.048 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.048 I print_info: rope_finetuned   = unknown
0.00.041.048 I print_info: ssm_d_conv       = 0
0.00.041.048 I print_info: ssm_d_inner      = 0
0.00.041.049 I print_info: ssm_d_state      = 0
0.00.041.049 I print_info: ssm_dt_rank      = 0
0.00.041.049 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.049 I print_info: model type       = 1.4B
0.00.041.049 I print_info: model params     = 1.41 B
0.00.041.050 I print_info: general.name     = 1.4B
0.00.041.050 I print_info: vocab type       = BPE
0.00.041.050 I print_info: n_vocab          = 50304
0.00.041.052 I print_info: n_merges         = 50009
0.00.041.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.052 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.053 I print_info: LF token         = 187 ''
0.00.041.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.053 I print_info: max token length = 1024
0.00.041.054 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.520 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.529 I load_tensors: offloading output layer to GPU
0.00.589.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.556 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.589.557 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.590.895 I llama_init_from_model: n_seq_max     = 1
0.00.590.897 I llama_init_from_model: n_ctx         = 128
0.00.590.897 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.898 I llama_init_from_model: n_batch       = 128
0.00.590.899 I llama_init_from_model: n_ubatch      = 128
0.00.590.899 I llama_init_from_model: flash_attn    = 0
0.00.590.901 I llama_init_from_model: freq_base     = 10000.0
0.00.590.902 I llama_init_from_model: freq_scale    = 1
0.00.590.908 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.910 I ggml_metal_init: allocating
0.00.590.979 I ggml_metal_init: found device: Apple M4
0.00.590.990 I ggml_metal_init: picking default device: Apple M4
0.00.592.989 I ggml_metal_init: using embedded metal library
0.00.599.616 I ggml_metal_init: GPU name:   Apple M4
0.00.599.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.623 I ggml_metal_init: simdgroup reduction   = true
0.00.599.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.623 I ggml_metal_init: has residency sets    = true
0.00.599.624 I ggml_metal_init: has bfloat            = true
0.00.599.624 I ggml_metal_init: use bfloat            = true
0.00.599.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.851 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.298 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.303 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.357 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.464 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.466 I llama_init_from_model: graph nodes  = 967
0.00.623.467 I llama_init_from_model: graph splits = 2
0.00.623.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.585 I 
0.00.649.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.626 I perplexity: tokenizing the input ..
0.00.655.992 I perplexity: tokenization took 6.364 ms
0.00.655.998 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.998 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.801.256 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.801.288 I llama_perf_context_print:        load time =     638.25 ms
0.00.801.289 I llama_perf_context_print: prompt eval time =     143.46 ms /   128 tokens (    1.12 ms per token,   892.22 tokens per second)
0.00.801.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.290 I llama_perf_context_print:       total time =     151.70 ms /   129 tokens
0.00.801.672 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.079s
sys	0m0.179s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.516 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.522 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.527 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.529 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.529 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.418 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.214 I llama_model_loader: - type  f32:  194 tensors
0.00.025.214 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.214 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.215 I print_info: file format = GGUF V3 (latest)
0.00.025.215 I print_info: file type   = Q5_K - Medium
0.00.025.216 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.484 I load: special tokens cache size = 25
0.00.039.549 I load: token to piece cache size = 0.2984 MB
0.00.039.552 I print_info: arch             = gptneox
0.00.039.552 I print_info: vocab_only       = 0
0.00.039.552 I print_info: n_ctx_train      = 2048
0.00.039.552 I print_info: n_embd           = 2048
0.00.039.553 I print_info: n_layer          = 24
0.00.039.555 I print_info: n_head           = 16
0.00.039.556 I print_info: n_head_kv        = 16
0.00.039.556 I print_info: n_rot            = 32
0.00.039.557 I print_info: n_swa            = 0
0.00.039.557 I print_info: n_embd_head_k    = 128
0.00.039.557 I print_info: n_embd_head_v    = 128
0.00.039.558 I print_info: n_gqa            = 1
0.00.039.558 I print_info: n_embd_k_gqa     = 2048
0.00.039.559 I print_info: n_embd_v_gqa     = 2048
0.00.039.560 I print_info: f_norm_eps       = 1.0e-05
0.00.039.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.561 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.563 I print_info: f_logit_scale    = 0.0e+00
0.00.039.564 I print_info: n_ff             = 8192
0.00.039.564 I print_info: n_expert         = 0
0.00.039.564 I print_info: n_expert_used    = 0
0.00.039.565 I print_info: causal attn      = 1
0.00.039.565 I print_info: pooling type     = 0
0.00.039.565 I print_info: rope type        = 2
0.00.039.566 I print_info: rope scaling     = linear
0.00.039.566 I print_info: freq_base_train  = 10000.0
0.00.039.566 I print_info: freq_scale_train = 1
0.00.039.567 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.567 I print_info: rope_finetuned   = unknown
0.00.039.567 I print_info: ssm_d_conv       = 0
0.00.039.567 I print_info: ssm_d_inner      = 0
0.00.039.567 I print_info: ssm_d_state      = 0
0.00.039.567 I print_info: ssm_dt_rank      = 0
0.00.039.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.576 I print_info: model type       = 1.4B
0.00.039.578 I print_info: model params     = 1.41 B
0.00.039.578 I print_info: general.name     = 1.4B
0.00.039.579 I print_info: vocab type       = BPE
0.00.039.579 I print_info: n_vocab          = 50304
0.00.039.579 I print_info: n_merges         = 50009
0.00.039.580 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: LF token         = 187 ''
0.00.039.581 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: max token length = 1024
0.00.039.582 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.871 I load_tensors: offloading output layer to GPU
0.00.672.872 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.895 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.672.897 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.673.931 I llama_init_from_model: n_seq_max     = 1
0.00.673.934 I llama_init_from_model: n_ctx         = 2048
0.00.673.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.673.934 I llama_init_from_model: n_batch       = 2048
0.00.673.935 I llama_init_from_model: n_ubatch      = 512
0.00.673.935 I llama_init_from_model: flash_attn    = 0
0.00.673.936 I llama_init_from_model: freq_base     = 10000.0
0.00.673.937 I llama_init_from_model: freq_scale    = 1
0.00.673.938 I ggml_metal_init: allocating
0.00.673.961 I ggml_metal_init: found device: Apple M4
0.00.673.969 I ggml_metal_init: picking default device: Apple M4
0.00.675.315 I ggml_metal_init: using embedded metal library
0.00.681.397 I ggml_metal_init: GPU name:   Apple M4
0.00.681.401 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.402 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.403 I ggml_metal_init: simdgroup reduction   = true
0.00.681.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.403 I ggml_metal_init: has residency sets    = true
0.00.681.404 I ggml_metal_init: has bfloat            = true
0.00.681.404 I ggml_metal_init: use bfloat            = true
0.00.681.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.916 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.755.126 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.755.133 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.755.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.759.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.759.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.759.948 I llama_init_from_model: graph nodes  = 967
0.00.759.948 I llama_init_from_model: graph splits = 2
0.00.759.952 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.760.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.760.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.923 I main: llama threadpool init, n_threads = 4
0.00.814.965 I 
0.00.814.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.983 I 
0.00.815.117 I sampler seed: 1234
0.00.815.121 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.157 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.161 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.161 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.658.146 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.658.147 I llama_perf_context_print:        load time =     805.44 ms
0.01.658.147 I llama_perf_context_print: prompt eval time =      51.74 ms /     7 tokens (    7.39 ms per token,   135.28 tokens per second)
0.01.658.148 I llama_perf_context_print:        eval time =     788.24 ms /    63 runs   (   12.51 ms per token,    79.92 tokens per second)
0.01.658.152 I llama_perf_context_print:       total time =     843.94 ms /    70 tokens
0.01.658.386 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.108s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.559 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.335 I llama_model_loader: - type  f32:  194 tensors
0.00.025.335 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.335 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.336 I print_info: file format = GGUF V3 (latest)
0.00.025.337 I print_info: file type   = Q5_K - Medium
0.00.025.338 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.417 I load: special tokens cache size = 25
0.00.039.558 I load: token to piece cache size = 0.2984 MB
0.00.039.563 I print_info: arch             = gptneox
0.00.039.563 I print_info: vocab_only       = 0
0.00.039.563 I print_info: n_ctx_train      = 2048
0.00.039.563 I print_info: n_embd           = 2048
0.00.039.564 I print_info: n_layer          = 24
0.00.039.568 I print_info: n_head           = 16
0.00.039.569 I print_info: n_head_kv        = 16
0.00.039.569 I print_info: n_rot            = 32
0.00.039.569 I print_info: n_swa            = 0
0.00.039.569 I print_info: n_embd_head_k    = 128
0.00.039.570 I print_info: n_embd_head_v    = 128
0.00.039.570 I print_info: n_gqa            = 1
0.00.039.571 I print_info: n_embd_k_gqa     = 2048
0.00.039.574 I print_info: n_embd_v_gqa     = 2048
0.00.039.575 I print_info: f_norm_eps       = 1.0e-05
0.00.039.575 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.575 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.576 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.576 I print_info: f_logit_scale    = 0.0e+00
0.00.039.576 I print_info: n_ff             = 8192
0.00.039.577 I print_info: n_expert         = 0
0.00.039.577 I print_info: n_expert_used    = 0
0.00.039.577 I print_info: causal attn      = 1
0.00.039.577 I print_info: pooling type     = 0
0.00.039.577 I print_info: rope type        = 2
0.00.039.577 I print_info: rope scaling     = linear
0.00.039.578 I print_info: freq_base_train  = 10000.0
0.00.039.578 I print_info: freq_scale_train = 1
0.00.039.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.578 I print_info: rope_finetuned   = unknown
0.00.039.578 I print_info: ssm_d_conv       = 0
0.00.039.579 I print_info: ssm_d_inner      = 0
0.00.039.579 I print_info: ssm_d_state      = 0
0.00.039.579 I print_info: ssm_dt_rank      = 0
0.00.039.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.579 I print_info: model type       = 1.4B
0.00.039.581 I print_info: model params     = 1.41 B
0.00.039.581 I print_info: general.name     = 1.4B
0.00.039.581 I print_info: vocab type       = BPE
0.00.039.582 I print_info: n_vocab          = 50304
0.00.039.582 I print_info: n_merges         = 50009
0.00.039.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.582 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.582 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.582 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: LF token         = 187 ''
0.00.039.583 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: max token length = 1024
0.00.039.583 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.433 I load_tensors: offloading output layer to GPU
0.00.669.435 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.456 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.669.458 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.670.578 I llama_init_from_model: n_seq_max     = 1
0.00.670.580 I llama_init_from_model: n_ctx         = 128
0.00.670.581 I llama_init_from_model: n_ctx_per_seq = 128
0.00.670.581 I llama_init_from_model: n_batch       = 128
0.00.670.582 I llama_init_from_model: n_ubatch      = 128
0.00.670.582 I llama_init_from_model: flash_attn    = 0
0.00.670.583 I llama_init_from_model: freq_base     = 10000.0
0.00.670.583 I llama_init_from_model: freq_scale    = 1
0.00.670.584 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.670.585 I ggml_metal_init: allocating
0.00.670.604 I ggml_metal_init: found device: Apple M4
0.00.670.611 I ggml_metal_init: picking default device: Apple M4
0.00.671.864 I ggml_metal_init: using embedded metal library
0.00.677.178 I ggml_metal_init: GPU name:   Apple M4
0.00.677.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.183 I ggml_metal_init: simdgroup reduction   = true
0.00.677.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.184 I ggml_metal_init: has residency sets    = true
0.00.677.184 I ggml_metal_init: has bfloat            = true
0.00.677.184 I ggml_metal_init: use bfloat            = true
0.00.677.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.856 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.696.164 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.696.201 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.148 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.699.149 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.699.150 I llama_init_from_model: graph nodes  = 967
0.00.699.150 I llama_init_from_model: graph splits = 2
0.00.699.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.330 I 
0.00.733.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.380 I perplexity: tokenizing the input ..
0.00.738.515 I perplexity: tokenization took 5.133 ms
0.00.738.518 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.378 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.882.674 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.882.707 I llama_perf_context_print:        load time =     723.62 ms
0.00.882.708 I llama_perf_context_print: prompt eval time =     142.57 ms /   128 tokens (    1.11 ms per token,   897.80 tokens per second)
0.00.882.709 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.709 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.883.131 I ggml_metal_free: deallocating

real	0m0.897s
user	0m0.074s
sys	0m0.194s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.481 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.046 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.046 I llama_model_loader: - type  f32:  194 tensors
0.00.026.047 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.047 I print_info: file format = GGUF V3 (latest)
0.00.026.048 I print_info: file type   = Q6_K
0.00.026.048 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.908 I load: special tokens cache size = 25
0.00.039.999 I load: token to piece cache size = 0.2984 MB
0.00.040.002 I print_info: arch             = gptneox
0.00.040.002 I print_info: vocab_only       = 0
0.00.040.003 I print_info: n_ctx_train      = 2048
0.00.040.003 I print_info: n_embd           = 2048
0.00.040.003 I print_info: n_layer          = 24
0.00.040.005 I print_info: n_head           = 16
0.00.040.006 I print_info: n_head_kv        = 16
0.00.040.006 I print_info: n_rot            = 32
0.00.040.006 I print_info: n_swa            = 0
0.00.040.006 I print_info: n_embd_head_k    = 128
0.00.040.007 I print_info: n_embd_head_v    = 128
0.00.040.007 I print_info: n_gqa            = 1
0.00.040.008 I print_info: n_embd_k_gqa     = 2048
0.00.040.009 I print_info: n_embd_v_gqa     = 2048
0.00.040.009 I print_info: f_norm_eps       = 1.0e-05
0.00.040.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.010 I print_info: f_logit_scale    = 0.0e+00
0.00.040.011 I print_info: n_ff             = 8192
0.00.040.011 I print_info: n_expert         = 0
0.00.040.011 I print_info: n_expert_used    = 0
0.00.040.012 I print_info: causal attn      = 1
0.00.040.012 I print_info: pooling type     = 0
0.00.040.012 I print_info: rope type        = 2
0.00.040.012 I print_info: rope scaling     = linear
0.00.040.013 I print_info: freq_base_train  = 10000.0
0.00.040.013 I print_info: freq_scale_train = 1
0.00.040.013 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.014 I print_info: rope_finetuned   = unknown
0.00.040.014 I print_info: ssm_d_conv       = 0
0.00.040.014 I print_info: ssm_d_inner      = 0
0.00.040.014 I print_info: ssm_d_state      = 0
0.00.040.014 I print_info: ssm_dt_rank      = 0
0.00.040.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.015 I print_info: model type       = 1.4B
0.00.040.015 I print_info: model params     = 1.41 B
0.00.040.015 I print_info: general.name     = 1.4B
0.00.040.016 I print_info: vocab type       = BPE
0.00.040.016 I print_info: n_vocab          = 50304
0.00.040.016 I print_info: n_merges         = 50009
0.00.040.016 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.017 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.017 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.017 I print_info: LF token         = 187 ''
0.00.040.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.018 I print_info: max token length = 1024
0.00.040.018 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.747.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.747.653 I load_tensors: offloading output layer to GPU
0.00.747.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.747.676 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.747.677 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.748.717 I llama_init_from_model: n_seq_max     = 1
0.00.748.719 I llama_init_from_model: n_ctx         = 2048
0.00.748.719 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.748.719 I llama_init_from_model: n_batch       = 2048
0.00.748.720 I llama_init_from_model: n_ubatch      = 512
0.00.748.720 I llama_init_from_model: flash_attn    = 0
0.00.748.721 I llama_init_from_model: freq_base     = 10000.0
0.00.748.721 I llama_init_from_model: freq_scale    = 1
0.00.748.722 I ggml_metal_init: allocating
0.00.748.734 I ggml_metal_init: found device: Apple M4
0.00.748.740 I ggml_metal_init: picking default device: Apple M4
0.00.750.024 I ggml_metal_init: using embedded metal library
0.00.754.941 I ggml_metal_init: GPU name:   Apple M4
0.00.754.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.945 I ggml_metal_init: simdgroup reduction   = true
0.00.754.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.946 I ggml_metal_init: has residency sets    = true
0.00.754.946 I ggml_metal_init: has bfloat            = true
0.00.754.946 I ggml_metal_init: use bfloat            = true
0.00.754.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.187 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.820.809 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.820.815 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.820.892 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.827.390 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.827.392 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.827.393 I llama_init_from_model: graph nodes  = 967
0.00.827.393 I llama_init_from_model: graph splits = 2
0.00.827.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.827.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.827.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.888.119 I main: llama threadpool init, n_threads = 4
0.00.888.169 I 
0.00.888.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.888.186 I 
0.00.888.318 I sampler seed: 1234
0.00.888.323 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.888.357 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.888.358 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.888.358 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.755.772 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.755.773 I llama_perf_context_print:        load time =     877.37 ms
0.01.755.774 I llama_perf_context_print: prompt eval time =      54.02 ms /     7 tokens (    7.72 ms per token,   129.58 tokens per second)
0.01.755.774 I llama_perf_context_print:        eval time =     810.44 ms /    63 runs   (   12.86 ms per token,    77.74 tokens per second)
0.01.755.775 I llama_perf_context_print:       total time =     868.63 ms /    70 tokens
0.01.756.011 I ggml_metal_free: deallocating

real	0m1.774s
user	0m0.105s
sys	0m0.284s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4717 (94b87f87) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.551 I llama_model_loader: - type  f32:  194 tensors
0.00.027.551 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.552 I print_info: file format = GGUF V3 (latest)
0.00.027.552 I print_info: file type   = Q6_K
0.00.027.557 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.020 I load: special tokens cache size = 25
0.00.042.003 I load: token to piece cache size = 0.2984 MB
0.00.042.006 I print_info: arch             = gptneox
0.00.042.006 I print_info: vocab_only       = 0
0.00.042.007 I print_info: n_ctx_train      = 2048
0.00.042.007 I print_info: n_embd           = 2048
0.00.042.007 I print_info: n_layer          = 24
0.00.042.011 I print_info: n_head           = 16
0.00.042.012 I print_info: n_head_kv        = 16
0.00.042.012 I print_info: n_rot            = 32
0.00.042.012 I print_info: n_swa            = 0
0.00.042.012 I print_info: n_embd_head_k    = 128
0.00.042.012 I print_info: n_embd_head_v    = 128
0.00.042.013 I print_info: n_gqa            = 1
0.00.042.014 I print_info: n_embd_k_gqa     = 2048
0.00.042.014 I print_info: n_embd_v_gqa     = 2048
0.00.042.015 I print_info: f_norm_eps       = 1.0e-05
0.00.042.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.016 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.016 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.016 I print_info: f_logit_scale    = 0.0e+00
0.00.042.017 I print_info: n_ff             = 8192
0.00.042.017 I print_info: n_expert         = 0
0.00.042.017 I print_info: n_expert_used    = 0
0.00.042.017 I print_info: causal attn      = 1
0.00.042.017 I print_info: pooling type     = 0
0.00.042.017 I print_info: rope type        = 2
0.00.042.017 I print_info: rope scaling     = linear
0.00.042.018 I print_info: freq_base_train  = 10000.0
0.00.042.018 I print_info: freq_scale_train = 1
0.00.042.018 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.019 I print_info: rope_finetuned   = unknown
0.00.042.019 I print_info: ssm_d_conv       = 0
0.00.042.019 I print_info: ssm_d_inner      = 0
0.00.042.019 I print_info: ssm_d_state      = 0
0.00.042.019 I print_info: ssm_dt_rank      = 0
0.00.042.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.019 I print_info: model type       = 1.4B
0.00.042.020 I print_info: model params     = 1.41 B
0.00.042.020 I print_info: general.name     = 1.4B
0.00.042.021 I print_info: vocab type       = BPE
0.00.042.022 I print_info: n_vocab          = 50304
0.00.042.022 I print_info: n_merges         = 50009
0.00.042.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.023 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.023 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.023 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.024 I print_info: LF token         = 187 ''
0.00.042.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.024 I print_info: max token length = 1024
0.00.042.024 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.280.885 I load_tensors: offloading 24 repeating layers to GPU
0.00.280.889 I load_tensors: offloading output layer to GPU
0.00.280.889 I load_tensors: offloaded 25/25 layers to GPU
0.00.280.913 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.280.916 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.281.884 I llama_init_from_model: n_seq_max     = 1
0.00.281.885 I llama_init_from_model: n_ctx         = 128
0.00.281.885 I llama_init_from_model: n_ctx_per_seq = 128
0.00.281.886 I llama_init_from_model: n_batch       = 128
0.00.281.886 I llama_init_from_model: n_ubatch      = 128
0.00.281.886 I llama_init_from_model: flash_attn    = 0
0.00.281.887 I llama_init_from_model: freq_base     = 10000.0
0.00.281.888 I llama_init_from_model: freq_scale    = 1
0.00.281.888 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.281.891 I ggml_metal_init: allocating
0.00.281.951 I ggml_metal_init: found device: Apple M4
0.00.281.960 I ggml_metal_init: picking default device: Apple M4
0.00.283.196 I ggml_metal_init: using embedded metal library
0.00.288.421 I ggml_metal_init: GPU name:   Apple M4
0.00.288.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.288.425 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.288.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.288.427 I ggml_metal_init: simdgroup reduction   = true
0.00.288.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.288.427 I ggml_metal_init: has residency sets    = true
0.00.288.428 I ggml_metal_init: has bfloat            = true
0.00.288.428 I ggml_metal_init: use bfloat            = true
0.00.288.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.288.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.302.581 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.305.927 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.305.931 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.305.971 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.308.745 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.308.747 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.308.748 I llama_init_from_model: graph nodes  = 967
0.00.308.748 I llama_init_from_model: graph splits = 2
0.00.308.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.308.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.293 I 
0.00.339.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.339.331 I perplexity: tokenizing the input ..
0.00.344.804 I perplexity: tokenization took 5.472 ms
0.00.344.808 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.483.719 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.485.014 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.485.047 I llama_perf_context_print:        load time =     327.54 ms
0.00.485.048 I llama_perf_context_print: prompt eval time =     138.69 ms /   128 tokens (    1.08 ms per token,   922.92 tokens per second)
0.00.485.049 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.485.049 I llama_perf_context_print:       total time =     145.76 ms /   129 tokens
0.00.485.464 I ggml_metal_free: deallocating

real	0m0.502s
user	0m0.073s
sys	0m0.109s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4717 (94b87f87)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146a086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146a08dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146a09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146a09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146a09ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146a0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146a0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146a0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146a0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146a0bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146a0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146a0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146a0d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146a0df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146a0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146a0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146a0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146a0fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146a103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146a10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146a11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146a11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146a121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146a128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146a12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146a131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146a13e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146a14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146a14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146a14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146a14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146a15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146a15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146a15e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146a162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146a16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146a16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146a170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146a17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146a179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146a17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146a18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146a187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146a18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146a19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146a196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146a19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146a1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146a1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146a1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146a1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146a1be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146a1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146a1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146a1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146a1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146a1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146a1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146a1e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146a1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146a1ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146a1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146a1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146a1fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146a1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146a20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146a20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146a20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146a21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146a21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146a21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146a22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146a225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146a22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146a23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146a23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146a23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146a24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146a24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146a24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146a25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146a25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146a25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146a26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146a26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146a26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146a27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146a27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146a27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146a27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146a28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146a28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146a29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146a29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146a29fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146a19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146a2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146a2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146a2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146a2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146a2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146a2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146a2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146a2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146a2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146a2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146a2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146a2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146a2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146a2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146a2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146a2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146a2fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146a2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146a30380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146a30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146a30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146a31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146a31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146a31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146a31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146a323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146a32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146a32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146a331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146a33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146a33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146a33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146a34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146a348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146a34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146a35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146a356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146a35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146a36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146a364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146a36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146a36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146a37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146a37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146a37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146a38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146a38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146a389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146a38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146a392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146a39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146a39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146a3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146a3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146a3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146a3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146a3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146a3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146a3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146a3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146a3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146a3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146a3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146a3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146a3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146a3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146a3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146a3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146a3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146a3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146a3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146a3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146a401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146a40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146a40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146a40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146a41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146a41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146a41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146a42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146a426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146a42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146a43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146a434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146a43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146a43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146a442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146a44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146a44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146a45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146a45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146a459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146a45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146a46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146a46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146a46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146a472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146a47840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146a47b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146a48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146a48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146a48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146a49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146a499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146a49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146a4a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146a4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146a4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146a4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146a4b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146a4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146a4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146a4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146a4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146a4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146a4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146a4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146a4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146a4eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146a4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146a4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146a4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146a50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146a505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146a51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146a515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146a51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146a52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146a525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146a52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146a53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146a535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146a53b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146a54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146a545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146a54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146a55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146a55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146a55ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146a56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146a56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146a56ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146a57020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146a57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146a57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146a58010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146a58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146a58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146a59000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146a59550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146a59aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146a59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146a5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146a5aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146a5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146a5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146a5ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146a5bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146a5c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146a5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146a5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146a5d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146a5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146a5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146a5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146a5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146a5efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146a5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146a5f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146a5fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146a60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146a606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146a60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146a61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146a614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146a61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146a61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146a62280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146a62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146a62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146a63060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146a63500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146a63a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146a64170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146a64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146a64fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146a656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146a65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146a66180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146a66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146a66a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.743.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146a66700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146a483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146a47dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146a489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146a1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146a1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146a1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146a4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146a12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146a19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146a1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146a1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146a18d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146a1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146a11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146a1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146a2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146a65c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146a15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146a15310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146a4ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146a48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146a13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146a13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146a13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146a66eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146a67170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146a67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146a676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146a679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146a67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146a67f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146a681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146a684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146a68770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146a68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146a68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146a68fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146a69270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146a69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146a697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146a69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146a69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146a6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146a6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146a6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146a6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146a6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146a6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146a6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146a6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146a6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146a6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146a6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146a6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146a6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146a6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146a6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146a6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146a6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146a6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146a6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146a6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146a6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146a6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146a6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146a6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146a6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146a6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146a6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146a6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146a6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146a6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146a6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146a6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146a6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146a6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146a6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146a70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146a70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146a705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146a708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146a70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146a70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146a710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146a713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146a71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146a71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146a71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146a71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146a72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146a72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146a726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146a729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146a72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146a72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146a731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146a734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146a73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146a73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146a73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146a73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146a74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146a74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146a747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146a74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146a74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146a75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146a752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146a755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146a75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146a75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146a75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146a760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146a76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146a76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146a768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146a76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146a76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146a77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146a773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146a776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146a77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146a77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146a77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146a781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146a78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146a78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146a789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146a78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146a78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146a79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146a794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146a797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146a79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146a79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146a79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146a7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146a7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146a7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146a7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146a7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146a7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146a7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146a7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146a7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146a7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146a7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146a7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146a7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146a7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146a7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146a7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146a7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146a7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146a7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146a7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146a7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146a7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146a7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146a7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146a7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146a7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146a7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146a7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146a7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146a7f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146a7f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146a7f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146a7fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146a7fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146a80030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146a802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146a805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146a80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146a80b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146a80df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146a810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146a81370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146a81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146a818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146a81bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146a81e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146a82130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146a823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146a826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146a82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146a82c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146a82ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146a831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146a83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146a83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146a839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146a83cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146a83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146a84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146a844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146a847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146a84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146a84d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146a84ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146a852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146a85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146a85830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146a85d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146a862b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146a86570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146a86970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146a86e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146a872b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146a87a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146a87d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146a87fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146a88450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146a888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146a88d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146a891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146a89610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146a89a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146a89ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146a8a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146a8a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146a8ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146a8b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146a8b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146a8b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146a8be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146a8c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146a8c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146a8cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146a8cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146a8d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146a8d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146a8dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146a8e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146a8e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146a8ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146a8eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146a8f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146a8f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146a8fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146a90090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146a90500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146a90970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146a90de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146a91250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146a916c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146a91b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146a91fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146a92410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146a92880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146a92cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146a93160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146a935d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146a93a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146a93eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146a94320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146a94790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146a94c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146a95070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146a954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146a95950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146a95dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146a96230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146a966a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146a96b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146a96f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146a973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146a97860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146a97cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146a98140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146a985b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146a98a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146a98e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146a99300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146a99770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146a99be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146a9a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146a9a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146a9a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146a9ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146a9b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146a9b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146a9c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146a9c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146a9cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146a9d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146a9d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146a9e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146a9e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146a9e9d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1277044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1277056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1277063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1277078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1277083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12770a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12770a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12770b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12770b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12770bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12770c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12770cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12770d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12770db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12770de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12770e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12770e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12770e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12770ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12770f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12770f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12770fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12770ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1277107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1277110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1277119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1277138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1277141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1277157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1277160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1277185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12771a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12771a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12771a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12771adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12771b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12771b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12771bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12771bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12771c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12771c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12771ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12771d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12771d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12771da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12771de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12771e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12771e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12771ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12771f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12771f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12771f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12771fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1277213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1277229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1277232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1277258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1277261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1277277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1277280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1277296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12772a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12772a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12772ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12772b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12772b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12772ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12772bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12772c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12772c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12772cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12772d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12772d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12772d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12772dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12772e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12772e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12772eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12772efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12772f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12772f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12772fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1277305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1277324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1277336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1277343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1277355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1277374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1277393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12773a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12773a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12773aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12773ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12773b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12773b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12773bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12773c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12773c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12773c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12773cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12773d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12773d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12773dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12773df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12773e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12773e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12773ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12773f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12773f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12773f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12773fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1277402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1277429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1277432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1277451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1277463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1277470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1277479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1277482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1277498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12774a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12774a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12774aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12774af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12774b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12774b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12774bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12774c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12774c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12774c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12774ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12774d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12774d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12774db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12774dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12774e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12774e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12774ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12774f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12774f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12774fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12774fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1277507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1277510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1277526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1277538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1277545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1277557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1277584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.790s
user	0m0.281s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4717 (94b87f87)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13970df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13970e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13970ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13970f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13970f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13970fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1397102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139710870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139710e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139711320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139711820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139712840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139712ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139713800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139713f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139714640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139714d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139715c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139716370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139716a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1397171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139717a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139718170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139718a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1397196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139719eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13971a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13971a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13971aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13971b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13971b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13971bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13971bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13971c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13971c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13971cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13971d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13971d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13971dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13971e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13971e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13971e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13971ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13971f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13971fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139720460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139720a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139721080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139721690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139722930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139722dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139723090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1397236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139723e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139724150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1397245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139724a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139724f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1397253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139725870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139725d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1397261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139726650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139726af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139726f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139727430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1397278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139727e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1397288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139728e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139729360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1397298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139729e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13972a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13972a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13972adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13972b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13972b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13972bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13972c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13972c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13972cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13972d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13972d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13972ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13972e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13972e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13972edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13972f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13972f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13971f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13972fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139730470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1397309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139730f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139731460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1397319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139731f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139732450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1397329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139732ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139733440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139733990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139733ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139734430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139734980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139734e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1397352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139735760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139735c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1397360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139736540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1397369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139736e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139737320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1397377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139737c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139738100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1397385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139738ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139739380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139739820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139739cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13973a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13973a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13973aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13973af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13973b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13973b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13973bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13973c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13973c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13973cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13973cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13973d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13973d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13973dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13973e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13973e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13973eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13973f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13973f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13973f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13973fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139740280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139740720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139740bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139741060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139741500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1397419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139741e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1397422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139742780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139742c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1397430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139743560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139743a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139743ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139744340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1397447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139744c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139745120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1397455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139745a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139745f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1397463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139746840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139746ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139747180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139747620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139747ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139747f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139748400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1397488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139748d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1397491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139749680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139749b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139749fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13974a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13974a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13974ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13974b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13974b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13974bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13974c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13974c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13974cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13974d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13974d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13974d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13974dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13974e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13974eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13974f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13974f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13974fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139750120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139750910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139750db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139751250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1397516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139751ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1397523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139752940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139752e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1397533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139753930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139753e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1397543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139754920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139754e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1397553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139755910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139755e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1397563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139756900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139756e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1397573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1397578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139757e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139758390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1397588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139758e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139759380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1397598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139759e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13975a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13975a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13975ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13975b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13975b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13975be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13975c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13975c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13975cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13975d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13975d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13975dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13975e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13975e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13975edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13975f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13975f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13975fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139760310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139760860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139760db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139761300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139761850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139761da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1397622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139762840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139762d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1397632e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139763830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139763d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1397642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139764820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139764cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139765160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139765600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139765aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139765f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1397663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139766880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139766d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1397671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139767660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139767b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139767fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139768440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1397688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139768d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1397692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1397699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13976a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13976a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13976af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13976b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13976ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13976bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13976c2d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a83a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a83a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a83ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a83b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a83ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a83bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a83c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a83cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a83d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a83d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a83d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a83ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a83e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a83e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a83eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a83ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a83f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a83f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a83fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a8402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a8425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a8439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a85a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a85a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a85ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a85bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a85c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a85cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a85d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a85d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a85dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a85e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a85e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de1ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de26f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de29290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de2ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de30180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de4ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de4e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de58ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.906s
user	0m0.231s
sys	0m0.143s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.90 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.34 sec*proc (2 tests)

Total Test time (real) =   2.35 sec
        2.37 real         0.51 user         0.37 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
