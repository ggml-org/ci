Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.549s
user	0m0.874s
sys	0m1.206s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Built target llava_shared
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Built target test-grammar-parser
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Built target test-log
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-chat-template
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-embedding
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Built target llama-gritlm
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-bench
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-lookup-merge
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Built target llama-cli
[ 86%] Built target llama-parallel
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 89%] Built target llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-speculative-simple
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.941s
user	0m6.076s
sys	0m9.425s

main: quantize time =  3022.06 ms
main:    total time =  3022.06 ms

main: quantize time =  1336.92 ms
main:    total time =  1336.92 ms

main: quantize time =  1293.21 ms
main:    total time =  1293.21 ms

main: quantize time =  2158.30 ms
main:    total time =  2158.30 ms

main: quantize time =  2313.99 ms
main:    total time =  2313.99 ms

main: quantize time =  4959.27 ms
main:    total time =  4959.27 ms

main: quantize time =  5660.08 ms
main:    total time =  5660.08 ms

main: quantize time =  7259.36 ms
main:    total time =  7259.36 ms

main: quantize time =  5893.34 ms
main:    total time =  5893.34 ms

main: quantize time =  4621.06 ms
main:    total time =  4621.06 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.195 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.306 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.055.203 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.067.924 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.983 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.086.661 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.661 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.662 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.662 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.664 I llama_model_loader: - type  f32:  194 tensors
0.00.086.664 I llama_model_loader: - type  f16:   98 tensors
0.00.086.669 I print_info: file format = GGUF V3 (latest)
0.00.086.670 I print_info: file type   = all F32 (guessed)
0.00.086.672 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.119.915 I load: special tokens cache size = 25
0.00.127.503 I load: token to piece cache size = 0.2984 MB
0.00.127.507 I print_info: arch             = gptneox
0.00.127.507 I print_info: vocab_only       = 0
0.00.127.507 I print_info: n_ctx_train      = 2048
0.00.127.508 I print_info: n_embd           = 2048
0.00.127.508 I print_info: n_layer          = 24
0.00.127.511 I print_info: n_head           = 16
0.00.127.514 I print_info: n_head_kv        = 16
0.00.127.514 I print_info: n_rot            = 32
0.00.127.514 I print_info: n_swa            = 0
0.00.127.514 I print_info: n_embd_head_k    = 128
0.00.127.514 I print_info: n_embd_head_v    = 128
0.00.127.515 I print_info: n_gqa            = 1
0.00.127.516 I print_info: n_embd_k_gqa     = 2048
0.00.127.517 I print_info: n_embd_v_gqa     = 2048
0.00.127.517 I print_info: f_norm_eps       = 1.0e-05
0.00.127.518 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.127.518 I print_info: f_clamp_kqv      = 0.0e+00
0.00.127.518 I print_info: f_max_alibi_bias = 0.0e+00
0.00.127.518 I print_info: f_logit_scale    = 0.0e+00
0.00.127.519 I print_info: n_ff             = 8192
0.00.127.520 I print_info: n_expert         = 0
0.00.127.520 I print_info: n_expert_used    = 0
0.00.127.520 I print_info: causal attn      = 1
0.00.127.522 I print_info: pooling type     = 0
0.00.127.522 I print_info: rope type        = 2
0.00.127.523 I print_info: rope scaling     = linear
0.00.127.523 I print_info: freq_base_train  = 10000.0
0.00.127.523 I print_info: freq_scale_train = 1
0.00.127.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.127.524 I print_info: rope_finetuned   = unknown
0.00.127.524 I print_info: ssm_d_conv       = 0
0.00.127.524 I print_info: ssm_d_inner      = 0
0.00.127.524 I print_info: ssm_d_state      = 0
0.00.127.524 I print_info: ssm_dt_rank      = 0
0.00.127.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.127.525 I print_info: model type       = 1.4B
0.00.127.525 I print_info: model params     = 1.41 B
0.00.127.525 I print_info: general.name     = 1.4B
0.00.127.529 I print_info: vocab type       = BPE
0.00.127.530 I print_info: n_vocab          = 50304
0.00.127.530 I print_info: n_merges         = 50009
0.00.127.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.127.530 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.127.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.127.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.127.533 I print_info: LF token         = 128 'Ä'
0.00.127.534 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.127.534 I print_info: max token length = 1024
0.00.130.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.130.236 I load_tensors: offloading output layer to GPU
0.00.130.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.130.255 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.130.256 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.130.578 I llama_init_from_model: n_seq_max     = 1
0.00.130.579 I llama_init_from_model: n_ctx         = 2048
0.00.130.579 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.130.580 I llama_init_from_model: n_batch       = 2048
0.00.130.580 I llama_init_from_model: n_ubatch      = 512
0.00.130.580 I llama_init_from_model: flash_attn    = 0
0.00.130.580 I llama_init_from_model: freq_base     = 10000.0
0.00.130.581 I llama_init_from_model: freq_scale    = 1
0.00.130.581 I ggml_metal_init: allocating
0.00.130.584 I ggml_metal_init: found device: Apple M4
0.00.130.586 I ggml_metal_init: picking default device: Apple M4
0.00.132.353 I ggml_metal_init: using embedded metal library
0.00.165.470 I ggml_metal_init: GPU name:   Apple M4
0.00.165.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.165.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.165.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.165.473 I ggml_metal_init: simdgroup reduction   = true
0.00.165.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.165.474 I ggml_metal_init: has bfloat            = true
0.00.165.474 I ggml_metal_init: use bfloat            = true
0.00.165.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.165.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.304.747 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.329.198 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.329.206 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.329.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.330.157 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.330.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.330.159 I llama_init_from_model: graph nodes  = 967
0.00.330.159 I llama_init_from_model: graph splits = 2
0.00.330.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.330.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.330.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.408.979 I main: llama threadpool init, n_threads = 4
0.00.409.021 I 
0.00.409.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.051 I 
0.00.409.129 I sampler seed: 1234
0.00.409.135 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.409.167 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.409.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.409.169 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.266.091 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.02.266.092 I llama_perf_context_print:        load time =     353.76 ms
0.02.266.093 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.67 tokens per second)
0.02.266.094 I llama_perf_context_print:        eval time =    1810.15 ms /    63 runs   (   28.73 ms per token,    34.80 tokens per second)
0.02.266.095 I llama_perf_context_print:       total time =    1857.11 ms /    70 tokens
0.02.266.337 I ggml_metal_free: deallocating

real	0m2.578s
user	0m0.160s
sys	0m0.108s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.224 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.310 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.453 I llama_model_loader: - type  f32:  194 tensors
0.00.039.453 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.454 I print_info: file format = GGUF V3 (latest)
0.00.039.454 I print_info: file type   = Q8_0
0.00.039.456 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.062.248 I load: special tokens cache size = 25
0.00.068.729 I load: token to piece cache size = 0.2984 MB
0.00.068.734 I print_info: arch             = gptneox
0.00.068.734 I print_info: vocab_only       = 0
0.00.068.734 I print_info: n_ctx_train      = 2048
0.00.068.734 I print_info: n_embd           = 2048
0.00.068.735 I print_info: n_layer          = 24
0.00.068.741 I print_info: n_head           = 16
0.00.068.742 I print_info: n_head_kv        = 16
0.00.068.743 I print_info: n_rot            = 32
0.00.068.743 I print_info: n_swa            = 0
0.00.068.743 I print_info: n_embd_head_k    = 128
0.00.068.743 I print_info: n_embd_head_v    = 128
0.00.068.744 I print_info: n_gqa            = 1
0.00.068.744 I print_info: n_embd_k_gqa     = 2048
0.00.068.745 I print_info: n_embd_v_gqa     = 2048
0.00.068.746 I print_info: f_norm_eps       = 1.0e-05
0.00.068.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.746 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.746 I print_info: f_logit_scale    = 0.0e+00
0.00.068.747 I print_info: n_ff             = 8192
0.00.068.748 I print_info: n_expert         = 0
0.00.068.748 I print_info: n_expert_used    = 0
0.00.068.748 I print_info: causal attn      = 1
0.00.068.748 I print_info: pooling type     = 0
0.00.068.748 I print_info: rope type        = 2
0.00.068.748 I print_info: rope scaling     = linear
0.00.068.749 I print_info: freq_base_train  = 10000.0
0.00.068.749 I print_info: freq_scale_train = 1
0.00.068.749 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.749 I print_info: rope_finetuned   = unknown
0.00.068.750 I print_info: ssm_d_conv       = 0
0.00.068.752 I print_info: ssm_d_inner      = 0
0.00.068.752 I print_info: ssm_d_state      = 0
0.00.068.752 I print_info: ssm_dt_rank      = 0
0.00.068.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.752 I print_info: model type       = 1.4B
0.00.068.753 I print_info: model params     = 1.41 B
0.00.068.753 I print_info: general.name     = 1.4B
0.00.068.753 I print_info: vocab type       = BPE
0.00.068.754 I print_info: n_vocab          = 50304
0.00.068.754 I print_info: n_merges         = 50009
0.00.068.754 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.754 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.754 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.754 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.755 I print_info: LF token         = 128 'Ä'
0.00.068.755 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.755 I print_info: max token length = 1024
0.00.070.753 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.753 I load_tensors: offloading output layer to GPU
0.00.070.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.764 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.765 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.071.182 I llama_init_from_model: n_seq_max     = 1
0.00.071.182 I llama_init_from_model: n_ctx         = 2048
0.00.071.183 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.071.183 I llama_init_from_model: n_batch       = 2048
0.00.071.183 I llama_init_from_model: n_ubatch      = 512
0.00.071.183 I llama_init_from_model: flash_attn    = 0
0.00.071.184 I llama_init_from_model: freq_base     = 10000.0
0.00.071.184 I llama_init_from_model: freq_scale    = 1
0.00.071.185 I ggml_metal_init: allocating
0.00.071.187 I ggml_metal_init: found device: Apple M4
0.00.071.189 I ggml_metal_init: picking default device: Apple M4
0.00.071.995 I ggml_metal_init: using embedded metal library
0.00.074.897 I ggml_metal_init: GPU name:   Apple M4
0.00.074.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.899 I ggml_metal_init: simdgroup reduction   = true
0.00.074.899 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.900 I ggml_metal_init: has bfloat            = true
0.00.074.900 I ggml_metal_init: use bfloat            = true
0.00.074.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.901 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.355 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.206 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.218 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.261 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.481 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.483 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.483 I llama_init_from_model: graph nodes  = 967
0.00.114.484 I llama_init_from_model: graph splits = 2
0.00.114.488 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.623 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.326.903 I main: llama threadpool init, n_threads = 4
0.01.326.946 I 
0.01.326.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.326.969 I 
0.01.327.134 I sampler seed: 1234
0.01.327.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.327.182 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.327.184 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.327.184 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.469.532 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.469.533 I llama_perf_context_print:        load time =    1316.67 ms
0.02.469.534 I llama_perf_context_print: prompt eval time =      40.98 ms /     7 tokens (    5.85 ms per token,   170.82 tokens per second)
0.02.469.534 I llama_perf_context_print:        eval time =    1098.31 ms /    63 runs   (   17.43 ms per token,    57.36 tokens per second)
0.02.469.535 I llama_perf_context_print:       total time =    1142.63 ms /    70 tokens
0.02.469.741 I ggml_metal_free: deallocating

real	0m2.486s
user	0m0.118s
sys	0m0.246s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.015.270 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.129 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.127 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.426 I llama_model_loader: - type  f32:  194 tensors
0.00.033.426 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.427 I print_info: file format = GGUF V3 (latest)
0.00.033.427 I print_info: file type   = Q4_0
0.00.033.429 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.429 I load: special tokens cache size = 25
0.00.060.312 I load: token to piece cache size = 0.2984 MB
0.00.060.316 I print_info: arch             = gptneox
0.00.060.316 I print_info: vocab_only       = 0
0.00.060.316 I print_info: n_ctx_train      = 2048
0.00.060.316 I print_info: n_embd           = 2048
0.00.060.316 I print_info: n_layer          = 24
0.00.060.321 I print_info: n_head           = 16
0.00.060.322 I print_info: n_head_kv        = 16
0.00.060.322 I print_info: n_rot            = 32
0.00.060.322 I print_info: n_swa            = 0
0.00.060.322 I print_info: n_embd_head_k    = 128
0.00.060.322 I print_info: n_embd_head_v    = 128
0.00.060.323 I print_info: n_gqa            = 1
0.00.060.324 I print_info: n_embd_k_gqa     = 2048
0.00.060.324 I print_info: n_embd_v_gqa     = 2048
0.00.060.325 I print_info: f_norm_eps       = 1.0e-05
0.00.060.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.327 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.327 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.328 I print_info: f_logit_scale    = 0.0e+00
0.00.060.329 I print_info: n_ff             = 8192
0.00.060.329 I print_info: n_expert         = 0
0.00.060.329 I print_info: n_expert_used    = 0
0.00.060.329 I print_info: causal attn      = 1
0.00.060.329 I print_info: pooling type     = 0
0.00.060.332 I print_info: rope type        = 2
0.00.060.332 I print_info: rope scaling     = linear
0.00.060.332 I print_info: freq_base_train  = 10000.0
0.00.060.333 I print_info: freq_scale_train = 1
0.00.060.333 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.333 I print_info: rope_finetuned   = unknown
0.00.060.333 I print_info: ssm_d_conv       = 0
0.00.060.333 I print_info: ssm_d_inner      = 0
0.00.060.333 I print_info: ssm_d_state      = 0
0.00.060.334 I print_info: ssm_dt_rank      = 0
0.00.060.334 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.334 I print_info: model type       = 1.4B
0.00.060.334 I print_info: model params     = 1.41 B
0.00.060.335 I print_info: general.name     = 1.4B
0.00.060.335 I print_info: vocab type       = BPE
0.00.060.339 I print_info: n_vocab          = 50304
0.00.060.340 I print_info: n_merges         = 50009
0.00.060.340 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.340 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.340 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.340 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.341 I print_info: LF token         = 128 'Ä'
0.00.060.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.341 I print_info: max token length = 1024
0.00.062.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.170 I load_tensors: offloading output layer to GPU
0.00.062.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.181 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.062.183 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.062.512 I llama_init_from_model: n_seq_max     = 1
0.00.062.513 I llama_init_from_model: n_ctx         = 2048
0.00.062.513 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.513 I llama_init_from_model: n_batch       = 2048
0.00.062.513 I llama_init_from_model: n_ubatch      = 512
0.00.062.513 I llama_init_from_model: flash_attn    = 0
0.00.062.514 I llama_init_from_model: freq_base     = 10000.0
0.00.062.514 I llama_init_from_model: freq_scale    = 1
0.00.062.514 I ggml_metal_init: allocating
0.00.062.517 I ggml_metal_init: found device: Apple M4
0.00.062.518 I ggml_metal_init: picking default device: Apple M4
0.00.063.260 I ggml_metal_init: using embedded metal library
0.00.065.751 I ggml_metal_init: GPU name:   Apple M4
0.00.065.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.753 I ggml_metal_init: simdgroup reduction   = true
0.00.065.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.754 I ggml_metal_init: has bfloat            = true
0.00.065.754 I ggml_metal_init: use bfloat            = true
0.00.065.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.967 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.228 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.238 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.536 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.538 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.538 I llama_init_from_model: graph nodes  = 967
0.00.102.539 I llama_init_from_model: graph splits = 2
0.00.102.543 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.970 I main: llama threadpool init, n_threads = 4
0.00.749.016 I 
0.00.749.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.038 I 
0.00.749.203 I sampler seed: 1234
0.00.749.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.221 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.221 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.460.336 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.460.337 I llama_perf_context_print:        load time =     733.69 ms
0.01.460.341 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.72 tokens per second)
0.01.460.343 I llama_perf_context_print:        eval time =     668.35 ms /    63 runs   (   10.61 ms per token,    94.26 tokens per second)
0.01.460.343 I llama_perf_context_print:       total time =     711.37 ms /    70 tokens
0.01.460.649 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.112s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.014.826 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.030.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.728 I llama_model_loader: - type  f32:  194 tensors
0.00.039.728 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.729 I print_info: file format = GGUF V3 (latest)
0.00.039.730 I print_info: file type   = Q4_1
0.00.039.731 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.063.837 I load: special tokens cache size = 25
0.00.072.732 I load: token to piece cache size = 0.2984 MB
0.00.072.736 I print_info: arch             = gptneox
0.00.072.736 I print_info: vocab_only       = 0
0.00.072.737 I print_info: n_ctx_train      = 2048
0.00.072.737 I print_info: n_embd           = 2048
0.00.072.737 I print_info: n_layer          = 24
0.00.072.740 I print_info: n_head           = 16
0.00.072.741 I print_info: n_head_kv        = 16
0.00.072.741 I print_info: n_rot            = 32
0.00.072.742 I print_info: n_swa            = 0
0.00.072.742 I print_info: n_embd_head_k    = 128
0.00.072.742 I print_info: n_embd_head_v    = 128
0.00.072.743 I print_info: n_gqa            = 1
0.00.072.746 I print_info: n_embd_k_gqa     = 2048
0.00.072.747 I print_info: n_embd_v_gqa     = 2048
0.00.072.748 I print_info: f_norm_eps       = 1.0e-05
0.00.072.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.749 I print_info: f_logit_scale    = 0.0e+00
0.00.072.750 I print_info: n_ff             = 8192
0.00.072.750 I print_info: n_expert         = 0
0.00.072.750 I print_info: n_expert_used    = 0
0.00.072.750 I print_info: causal attn      = 1
0.00.072.750 I print_info: pooling type     = 0
0.00.072.750 I print_info: rope type        = 2
0.00.072.751 I print_info: rope scaling     = linear
0.00.072.751 I print_info: freq_base_train  = 10000.0
0.00.072.752 I print_info: freq_scale_train = 1
0.00.072.752 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.752 I print_info: rope_finetuned   = unknown
0.00.072.753 I print_info: ssm_d_conv       = 0
0.00.072.754 I print_info: ssm_d_inner      = 0
0.00.072.754 I print_info: ssm_d_state      = 0
0.00.072.754 I print_info: ssm_dt_rank      = 0
0.00.072.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.754 I print_info: model type       = 1.4B
0.00.072.755 I print_info: model params     = 1.41 B
0.00.072.755 I print_info: general.name     = 1.4B
0.00.072.756 I print_info: vocab type       = BPE
0.00.072.756 I print_info: n_vocab          = 50304
0.00.072.756 I print_info: n_merges         = 50009
0.00.072.757 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.757 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.757 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.757 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.758 I print_info: LF token         = 128 'Ä'
0.00.072.758 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.758 I print_info: max token length = 1024
0.00.074.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.074.929 I load_tensors: offloading output layer to GPU
0.00.074.929 I load_tensors: offloaded 25/25 layers to GPU
0.00.074.939 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.074.941 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.075.409 I llama_init_from_model: n_seq_max     = 1
0.00.075.410 I llama_init_from_model: n_ctx         = 2048
0.00.075.410 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.075.411 I llama_init_from_model: n_batch       = 2048
0.00.075.411 I llama_init_from_model: n_ubatch      = 512
0.00.075.411 I llama_init_from_model: flash_attn    = 0
0.00.075.412 I llama_init_from_model: freq_base     = 10000.0
0.00.075.412 I llama_init_from_model: freq_scale    = 1
0.00.075.413 I ggml_metal_init: allocating
0.00.075.416 I ggml_metal_init: found device: Apple M4
0.00.075.419 I ggml_metal_init: picking default device: Apple M4
0.00.076.210 I ggml_metal_init: using embedded metal library
0.00.079.850 I ggml_metal_init: GPU name:   Apple M4
0.00.079.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.854 I ggml_metal_init: simdgroup reduction   = true
0.00.079.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.854 I ggml_metal_init: has bfloat            = true
0.00.079.854 I ggml_metal_init: use bfloat            = true
0.00.079.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.853 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.618 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.645 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.118.717 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.118.718 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.118.718 I llama_init_from_model: graph nodes  = 967
0.00.118.719 I llama_init_from_model: graph splits = 2
0.00.118.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.856 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.775 I main: llama threadpool init, n_threads = 4
0.00.789.833 I 
0.00.789.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.861 I 
0.00.790.174 I sampler seed: 1234
0.00.790.179 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.237 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.237 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.515.112 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.515.113 I llama_perf_context_print:        load time =     774.94 ms
0.01.515.113 I llama_perf_context_print: prompt eval time =      46.16 ms /     7 tokens (    6.59 ms per token,   151.64 tokens per second)
0.01.515.114 I llama_perf_context_print:        eval time =     675.68 ms /    63 runs   (   10.73 ms per token,    93.24 tokens per second)
0.01.515.114 I llama_perf_context_print:       total time =     725.34 ms /    70 tokens
0.01.515.333 I ggml_metal_free: deallocating

real	0m1.537s
user	0m0.127s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.713 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.374 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.380 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.792 I llama_model_loader: - type  f32:  194 tensors
0.00.024.792 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.792 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.798 I print_info: file format = GGUF V3 (latest)
0.00.024.799 I print_info: file type   = Q5_0
0.00.024.800 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.332 I load: special tokens cache size = 25
0.00.049.352 I load: token to piece cache size = 0.2984 MB
0.00.049.355 I print_info: arch             = gptneox
0.00.049.355 I print_info: vocab_only       = 0
0.00.049.356 I print_info: n_ctx_train      = 2048
0.00.049.356 I print_info: n_embd           = 2048
0.00.049.356 I print_info: n_layer          = 24
0.00.049.359 I print_info: n_head           = 16
0.00.049.359 I print_info: n_head_kv        = 16
0.00.049.359 I print_info: n_rot            = 32
0.00.049.360 I print_info: n_swa            = 0
0.00.049.362 I print_info: n_embd_head_k    = 128
0.00.049.362 I print_info: n_embd_head_v    = 128
0.00.049.363 I print_info: n_gqa            = 1
0.00.049.363 I print_info: n_embd_k_gqa     = 2048
0.00.049.364 I print_info: n_embd_v_gqa     = 2048
0.00.049.365 I print_info: f_norm_eps       = 1.0e-05
0.00.049.370 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.373 I print_info: f_logit_scale    = 0.0e+00
0.00.049.375 I print_info: n_ff             = 8192
0.00.049.375 I print_info: n_expert         = 0
0.00.049.375 I print_info: n_expert_used    = 0
0.00.049.375 I print_info: causal attn      = 1
0.00.049.376 I print_info: pooling type     = 0
0.00.049.376 I print_info: rope type        = 2
0.00.049.376 I print_info: rope scaling     = linear
0.00.049.377 I print_info: freq_base_train  = 10000.0
0.00.049.377 I print_info: freq_scale_train = 1
0.00.049.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.377 I print_info: rope_finetuned   = unknown
0.00.049.378 I print_info: ssm_d_conv       = 0
0.00.049.378 I print_info: ssm_d_inner      = 0
0.00.049.378 I print_info: ssm_d_state      = 0
0.00.049.378 I print_info: ssm_dt_rank      = 0
0.00.049.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.380 I print_info: model type       = 1.4B
0.00.049.380 I print_info: model params     = 1.41 B
0.00.049.380 I print_info: general.name     = 1.4B
0.00.049.381 I print_info: vocab type       = BPE
0.00.049.381 I print_info: n_vocab          = 50304
0.00.049.381 I print_info: n_merges         = 50009
0.00.049.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.382 I print_info: LF token         = 128 'Ä'
0.00.049.383 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.383 I print_info: max token length = 1024
0.00.051.146 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.146 I load_tensors: offloading output layer to GPU
0.00.051.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.152 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.152 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.425 I llama_init_from_model: n_seq_max     = 1
0.00.051.425 I llama_init_from_model: n_ctx         = 2048
0.00.051.426 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.426 I llama_init_from_model: n_batch       = 2048
0.00.051.426 I llama_init_from_model: n_ubatch      = 512
0.00.051.426 I llama_init_from_model: flash_attn    = 0
0.00.051.426 I llama_init_from_model: freq_base     = 10000.0
0.00.051.427 I llama_init_from_model: freq_scale    = 1
0.00.051.427 I ggml_metal_init: allocating
0.00.051.430 I ggml_metal_init: found device: Apple M4
0.00.051.432 I ggml_metal_init: picking default device: Apple M4
0.00.052.022 I ggml_metal_init: using embedded metal library
0.00.054.327 I ggml_metal_init: GPU name:   Apple M4
0.00.054.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.330 I ggml_metal_init: simdgroup reduction   = true
0.00.054.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.330 I ggml_metal_init: has bfloat            = true
0.00.054.330 I ggml_metal_init: use bfloat            = true
0.00.054.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.753 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.071 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.084 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.115 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.107 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.108 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.108 I llama_init_from_model: graph nodes  = 967
0.00.084.109 I llama_init_from_model: graph splits = 2
0.00.084.111 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.232 I main: llama threadpool init, n_threads = 4
0.00.784.287 I 
0.00.784.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.310 I 
0.00.784.598 I sampler seed: 1234
0.00.784.608 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.640 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.642 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.642 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.569.937 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.569.938 I llama_perf_context_print:        load time =     775.51 ms
0.01.569.939 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.53 tokens per second)
0.01.569.944 I llama_perf_context_print:        eval time =     739.70 ms /    63 runs   (   11.74 ms per token,    85.17 tokens per second)
0.01.569.945 I llama_perf_context_print:       total time =     785.71 ms /    70 tokens
0.01.570.209 I ggml_metal_free: deallocating

real	0m1.586s
user	0m0.107s
sys	0m0.152s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.764 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.481 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.186 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.187 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.188 I llama_model_loader: - type  f32:  194 tensors
0.00.028.188 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.189 I print_info: file format = GGUF V3 (latest)
0.00.028.190 I print_info: file type   = Q5_1
0.00.028.191 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.048.114 I load: special tokens cache size = 25
0.00.054.128 I load: token to piece cache size = 0.2984 MB
0.00.054.134 I print_info: arch             = gptneox
0.00.054.134 I print_info: vocab_only       = 0
0.00.054.135 I print_info: n_ctx_train      = 2048
0.00.054.135 I print_info: n_embd           = 2048
0.00.054.135 I print_info: n_layer          = 24
0.00.054.140 I print_info: n_head           = 16
0.00.054.140 I print_info: n_head_kv        = 16
0.00.054.141 I print_info: n_rot            = 32
0.00.054.143 I print_info: n_swa            = 0
0.00.054.143 I print_info: n_embd_head_k    = 128
0.00.054.143 I print_info: n_embd_head_v    = 128
0.00.054.144 I print_info: n_gqa            = 1
0.00.054.144 I print_info: n_embd_k_gqa     = 2048
0.00.054.145 I print_info: n_embd_v_gqa     = 2048
0.00.054.146 I print_info: f_norm_eps       = 1.0e-05
0.00.054.146 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.148 I print_info: f_logit_scale    = 0.0e+00
0.00.054.149 I print_info: n_ff             = 8192
0.00.054.149 I print_info: n_expert         = 0
0.00.054.149 I print_info: n_expert_used    = 0
0.00.054.149 I print_info: causal attn      = 1
0.00.054.153 I print_info: pooling type     = 0
0.00.054.153 I print_info: rope type        = 2
0.00.054.154 I print_info: rope scaling     = linear
0.00.054.154 I print_info: freq_base_train  = 10000.0
0.00.054.154 I print_info: freq_scale_train = 1
0.00.054.154 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.154 I print_info: rope_finetuned   = unknown
0.00.054.155 I print_info: ssm_d_conv       = 0
0.00.054.155 I print_info: ssm_d_inner      = 0
0.00.054.155 I print_info: ssm_d_state      = 0
0.00.054.155 I print_info: ssm_dt_rank      = 0
0.00.054.155 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.155 I print_info: model type       = 1.4B
0.00.054.156 I print_info: model params     = 1.41 B
0.00.054.156 I print_info: general.name     = 1.4B
0.00.054.156 I print_info: vocab type       = BPE
0.00.054.156 I print_info: n_vocab          = 50304
0.00.054.157 I print_info: n_merges         = 50009
0.00.054.157 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.158 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.158 I print_info: LF token         = 128 'Ä'
0.00.054.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.159 I print_info: max token length = 1024
0.00.056.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.178 I load_tensors: offloading output layer to GPU
0.00.056.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.189 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.191 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.056.504 I llama_init_from_model: n_seq_max     = 1
0.00.056.505 I llama_init_from_model: n_ctx         = 2048
0.00.056.505 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.505 I llama_init_from_model: n_batch       = 2048
0.00.056.505 I llama_init_from_model: n_ubatch      = 512
0.00.056.506 I llama_init_from_model: flash_attn    = 0
0.00.056.506 I llama_init_from_model: freq_base     = 10000.0
0.00.056.506 I llama_init_from_model: freq_scale    = 1
0.00.056.507 I ggml_metal_init: allocating
0.00.056.510 I ggml_metal_init: found device: Apple M4
0.00.056.512 I ggml_metal_init: picking default device: Apple M4
0.00.057.163 I ggml_metal_init: using embedded metal library
0.00.059.496 I ggml_metal_init: GPU name:   Apple M4
0.00.059.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.498 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.499 I ggml_metal_init: simdgroup reduction   = true
0.00.059.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.499 I ggml_metal_init: has bfloat            = true
0.00.059.500 I ggml_metal_init: use bfloat            = true
0.00.059.500 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.947 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.598 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.660 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.661 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.661 I llama_init_from_model: graph nodes  = 967
0.00.090.662 I llama_init_from_model: graph splits = 2
0.00.090.665 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.811 I main: llama threadpool init, n_threads = 4
0.00.705.852 I 
0.00.705.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.874 I 
0.00.706.084 I sampler seed: 1234
0.00.706.088 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.111 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.113 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.545.951 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.545.952 I llama_perf_context_print:        load time =     694.04 ms
0.01.545.953 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.49 tokens per second)
0.01.545.953 I llama_perf_context_print:        eval time =     794.49 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.545.955 I llama_perf_context_print:       total time =     840.14 ms /    70 tokens
0.01.546.166 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.796 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.452 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.455 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.456 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.459 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.841 I llama_model_loader: - type  f32:  194 tensors
0.00.023.842 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.842 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.842 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.843 I print_info: file format = GGUF V3 (latest)
0.00.023.843 I print_info: file type   = Q2_K - Medium
0.00.023.844 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.299 I load: special tokens cache size = 25
0.00.049.259 I load: token to piece cache size = 0.2984 MB
0.00.049.262 I print_info: arch             = gptneox
0.00.049.262 I print_info: vocab_only       = 0
0.00.049.263 I print_info: n_ctx_train      = 2048
0.00.049.263 I print_info: n_embd           = 2048
0.00.049.263 I print_info: n_layer          = 24
0.00.049.266 I print_info: n_head           = 16
0.00.049.267 I print_info: n_head_kv        = 16
0.00.049.267 I print_info: n_rot            = 32
0.00.049.267 I print_info: n_swa            = 0
0.00.049.267 I print_info: n_embd_head_k    = 128
0.00.049.268 I print_info: n_embd_head_v    = 128
0.00.049.269 I print_info: n_gqa            = 1
0.00.049.269 I print_info: n_embd_k_gqa     = 2048
0.00.049.270 I print_info: n_embd_v_gqa     = 2048
0.00.049.270 I print_info: f_norm_eps       = 1.0e-05
0.00.049.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.271 I print_info: f_logit_scale    = 0.0e+00
0.00.049.272 I print_info: n_ff             = 8192
0.00.049.272 I print_info: n_expert         = 0
0.00.049.272 I print_info: n_expert_used    = 0
0.00.049.273 I print_info: causal attn      = 1
0.00.049.273 I print_info: pooling type     = 0
0.00.049.273 I print_info: rope type        = 2
0.00.049.273 I print_info: rope scaling     = linear
0.00.049.273 I print_info: freq_base_train  = 10000.0
0.00.049.274 I print_info: freq_scale_train = 1
0.00.049.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.274 I print_info: rope_finetuned   = unknown
0.00.049.276 I print_info: ssm_d_conv       = 0
0.00.049.277 I print_info: ssm_d_inner      = 0
0.00.049.277 I print_info: ssm_d_state      = 0
0.00.049.277 I print_info: ssm_dt_rank      = 0
0.00.049.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.277 I print_info: model type       = 1.4B
0.00.049.279 I print_info: model params     = 1.41 B
0.00.049.279 I print_info: general.name     = 1.4B
0.00.049.280 I print_info: vocab type       = BPE
0.00.049.280 I print_info: n_vocab          = 50304
0.00.049.280 I print_info: n_merges         = 50009
0.00.049.280 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.281 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.281 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.281 I print_info: LF token         = 128 'Ä'
0.00.049.282 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.282 I print_info: max token length = 1024
0.00.051.189 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.190 I load_tensors: offloading output layer to GPU
0.00.051.190 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.200 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.202 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.515 I llama_init_from_model: n_seq_max     = 1
0.00.051.516 I llama_init_from_model: n_ctx         = 2048
0.00.051.516 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.516 I llama_init_from_model: n_batch       = 2048
0.00.051.516 I llama_init_from_model: n_ubatch      = 512
0.00.051.517 I llama_init_from_model: flash_attn    = 0
0.00.051.517 I llama_init_from_model: freq_base     = 10000.0
0.00.051.517 I llama_init_from_model: freq_scale    = 1
0.00.051.518 I ggml_metal_init: allocating
0.00.051.521 I ggml_metal_init: found device: Apple M4
0.00.051.522 I ggml_metal_init: picking default device: Apple M4
0.00.052.105 I ggml_metal_init: using embedded metal library
0.00.054.494 I ggml_metal_init: GPU name:   Apple M4
0.00.054.495 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.496 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.496 I ggml_metal_init: simdgroup reduction   = true
0.00.054.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.497 I ggml_metal_init: has bfloat            = true
0.00.054.497 I ggml_metal_init: use bfloat            = true
0.00.054.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.227 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.039 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.048 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.081 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.091 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.092 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.093 I llama_init_from_model: graph nodes  = 967
0.00.084.093 I llama_init_from_model: graph splits = 2
0.00.084.096 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.225 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.202 I main: llama threadpool init, n_threads = 4
0.00.433.242 I 
0.00.433.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.264 I 
0.00.433.504 I sampler seed: 1234
0.00.433.509 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.433.520 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.433.520 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.433.520 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.819 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.116.819 I llama_perf_context_print:        load time =     424.40 ms
0.01.116.820 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.63 tokens per second)
0.01.116.821 I llama_perf_context_print:        eval time =     640.80 ms /    63 runs   (   10.17 ms per token,    98.31 tokens per second)
0.01.116.821 I llama_perf_context_print:       total time =     683.62 ms /    70 tokens
0.01.117.053 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.109s
sys	0m0.105s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.240 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.516 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.969 I llama_model_loader: - type  f32:  194 tensors
0.00.024.969 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.969 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.969 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.970 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.970 I print_info: file format = GGUF V3 (latest)
0.00.024.971 I print_info: file type   = Q3_K - Medium
0.00.024.971 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.603 I load: special tokens cache size = 25
0.00.049.540 I load: token to piece cache size = 0.2984 MB
0.00.049.543 I print_info: arch             = gptneox
0.00.049.543 I print_info: vocab_only       = 0
0.00.049.544 I print_info: n_ctx_train      = 2048
0.00.049.544 I print_info: n_embd           = 2048
0.00.049.544 I print_info: n_layer          = 24
0.00.049.547 I print_info: n_head           = 16
0.00.049.547 I print_info: n_head_kv        = 16
0.00.049.548 I print_info: n_rot            = 32
0.00.049.549 I print_info: n_swa            = 0
0.00.049.549 I print_info: n_embd_head_k    = 128
0.00.049.549 I print_info: n_embd_head_v    = 128
0.00.049.550 I print_info: n_gqa            = 1
0.00.049.551 I print_info: n_embd_k_gqa     = 2048
0.00.049.552 I print_info: n_embd_v_gqa     = 2048
0.00.049.553 I print_info: f_norm_eps       = 1.0e-05
0.00.049.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.554 I print_info: f_logit_scale    = 0.0e+00
0.00.049.554 I print_info: n_ff             = 8192
0.00.049.555 I print_info: n_expert         = 0
0.00.049.555 I print_info: n_expert_used    = 0
0.00.049.555 I print_info: causal attn      = 1
0.00.049.555 I print_info: pooling type     = 0
0.00.049.555 I print_info: rope type        = 2
0.00.049.557 I print_info: rope scaling     = linear
0.00.049.558 I print_info: freq_base_train  = 10000.0
0.00.049.558 I print_info: freq_scale_train = 1
0.00.049.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.559 I print_info: rope_finetuned   = unknown
0.00.049.559 I print_info: ssm_d_conv       = 0
0.00.049.559 I print_info: ssm_d_inner      = 0
0.00.049.559 I print_info: ssm_d_state      = 0
0.00.049.559 I print_info: ssm_dt_rank      = 0
0.00.049.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.560 I print_info: model type       = 1.4B
0.00.049.560 I print_info: model params     = 1.41 B
0.00.049.562 I print_info: general.name     = 1.4B
0.00.049.562 I print_info: vocab type       = BPE
0.00.049.563 I print_info: n_vocab          = 50304
0.00.049.563 I print_info: n_merges         = 50009
0.00.049.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.564 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.565 I print_info: LF token         = 128 'Ä'
0.00.049.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.565 I print_info: max token length = 1024
0.00.051.439 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.440 I load_tensors: offloading output layer to GPU
0.00.051.440 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.450 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.451 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.794 I llama_init_from_model: n_seq_max     = 1
0.00.051.795 I llama_init_from_model: n_ctx         = 2048
0.00.051.795 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.795 I llama_init_from_model: n_batch       = 2048
0.00.051.796 I llama_init_from_model: n_ubatch      = 512
0.00.051.796 I llama_init_from_model: flash_attn    = 0
0.00.051.796 I llama_init_from_model: freq_base     = 10000.0
0.00.051.796 I llama_init_from_model: freq_scale    = 1
0.00.051.797 I ggml_metal_init: allocating
0.00.051.799 I ggml_metal_init: found device: Apple M4
0.00.051.801 I ggml_metal_init: picking default device: Apple M4
0.00.052.375 I ggml_metal_init: using embedded metal library
0.00.054.688 I ggml_metal_init: GPU name:   Apple M4
0.00.054.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.691 I ggml_metal_init: simdgroup reduction   = true
0.00.054.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.691 I ggml_metal_init: has bfloat            = true
0.00.054.691 I ggml_metal_init: use bfloat            = true
0.00.054.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.367 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.209 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.219 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.241 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.180 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.181 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.181 I llama_init_from_model: graph nodes  = 967
0.00.084.181 I llama_init_from_model: graph splits = 2
0.00.084.184 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.886 I main: llama threadpool init, n_threads = 4
0.00.518.934 I 
0.00.518.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.959 I 
0.00.519.177 I sampler seed: 1234
0.00.519.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.205 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.207 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.269.136 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.269.136 I llama_perf_context_print:        load time =     509.64 ms
0.01.269.137 I llama_perf_context_print: prompt eval time =      44.49 ms /     7 tokens (    6.36 ms per token,   157.33 tokens per second)
0.01.269.138 I llama_perf_context_print:        eval time =     702.44 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.269.138 I llama_perf_context_print:       total time =     750.26 ms /    70 tokens
0.01.269.348 I ggml_metal_free: deallocating

real	0m1.288s
user	0m0.108s
sys	0m0.120s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.344 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.090 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.786 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.788 I llama_model_loader: - type  f32:  194 tensors
0.00.024.788 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.788 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.788 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.789 I print_info: file format = GGUF V3 (latest)
0.00.024.790 I print_info: file type   = Q4_K - Medium
0.00.024.790 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.379 I load: special tokens cache size = 25
0.00.049.315 I load: token to piece cache size = 0.2984 MB
0.00.049.318 I print_info: arch             = gptneox
0.00.049.318 I print_info: vocab_only       = 0
0.00.049.318 I print_info: n_ctx_train      = 2048
0.00.049.319 I print_info: n_embd           = 2048
0.00.049.319 I print_info: n_layer          = 24
0.00.049.322 I print_info: n_head           = 16
0.00.049.323 I print_info: n_head_kv        = 16
0.00.049.323 I print_info: n_rot            = 32
0.00.049.323 I print_info: n_swa            = 0
0.00.049.325 I print_info: n_embd_head_k    = 128
0.00.049.325 I print_info: n_embd_head_v    = 128
0.00.049.326 I print_info: n_gqa            = 1
0.00.049.326 I print_info: n_embd_k_gqa     = 2048
0.00.049.331 I print_info: n_embd_v_gqa     = 2048
0.00.049.332 I print_info: f_norm_eps       = 1.0e-05
0.00.049.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.334 I print_info: f_logit_scale    = 0.0e+00
0.00.049.334 I print_info: n_ff             = 8192
0.00.049.335 I print_info: n_expert         = 0
0.00.049.338 I print_info: n_expert_used    = 0
0.00.049.338 I print_info: causal attn      = 1
0.00.049.340 I print_info: pooling type     = 0
0.00.049.341 I print_info: rope type        = 2
0.00.049.341 I print_info: rope scaling     = linear
0.00.049.342 I print_info: freq_base_train  = 10000.0
0.00.049.342 I print_info: freq_scale_train = 1
0.00.049.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.343 I print_info: rope_finetuned   = unknown
0.00.049.344 I print_info: ssm_d_conv       = 0
0.00.049.344 I print_info: ssm_d_inner      = 0
0.00.049.344 I print_info: ssm_d_state      = 0
0.00.049.344 I print_info: ssm_dt_rank      = 0
0.00.049.344 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.344 I print_info: model type       = 1.4B
0.00.049.347 I print_info: model params     = 1.41 B
0.00.049.347 I print_info: general.name     = 1.4B
0.00.049.348 I print_info: vocab type       = BPE
0.00.049.348 I print_info: n_vocab          = 50304
0.00.049.348 I print_info: n_merges         = 50009
0.00.049.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.349 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.349 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.350 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.350 I print_info: LF token         = 128 'Ä'
0.00.049.350 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.350 I print_info: max token length = 1024
0.00.051.313 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.313 I load_tensors: offloading output layer to GPU
0.00.051.313 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.323 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.325 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.603 I llama_init_from_model: n_seq_max     = 1
0.00.051.604 I llama_init_from_model: n_ctx         = 2048
0.00.051.604 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.604 I llama_init_from_model: n_batch       = 2048
0.00.051.604 I llama_init_from_model: n_ubatch      = 512
0.00.051.604 I llama_init_from_model: flash_attn    = 0
0.00.051.605 I llama_init_from_model: freq_base     = 10000.0
0.00.051.605 I llama_init_from_model: freq_scale    = 1
0.00.051.605 I ggml_metal_init: allocating
0.00.051.608 I ggml_metal_init: found device: Apple M4
0.00.051.610 I ggml_metal_init: picking default device: Apple M4
0.00.052.200 I ggml_metal_init: using embedded metal library
0.00.054.516 I ggml_metal_init: GPU name:   Apple M4
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.519 I ggml_metal_init: simdgroup reduction   = true
0.00.054.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.519 I ggml_metal_init: has bfloat            = true
0.00.054.519 I ggml_metal_init: use bfloat            = true
0.00.054.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.116 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.549 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.558 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.580 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.524 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.526 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.526 I llama_init_from_model: graph nodes  = 967
0.00.083.526 I llama_init_from_model: graph splits = 2
0.00.083.529 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.939 I main: llama threadpool init, n_threads = 4
0.00.599.981 I 
0.00.600.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.018 I 
0.00.600.251 I sampler seed: 1234
0.00.600.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.600.277 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.600.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.600.278 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.361.003 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.361.004 I llama_perf_context_print:        load time =     591.21 ms
0.01.361.005 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.45 tokens per second)
0.01.361.006 I llama_perf_context_print:        eval time =     710.37 ms /    63 runs   (   11.28 ms per token,    88.69 tokens per second)
0.01.361.006 I llama_perf_context_print:       total time =     761.07 ms /    70 tokens
0.01.361.202 I ggml_metal_free: deallocating

real	0m1.376s
user	0m0.107s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.372 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.518 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.221 I llama_model_loader: - type  f32:  194 tensors
0.00.027.221 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.221 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.222 I print_info: file format = GGUF V3 (latest)
0.00.027.222 I print_info: file type   = Q5_K - Medium
0.00.027.223 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.843 I load: special tokens cache size = 25
0.00.051.913 I load: token to piece cache size = 0.2984 MB
0.00.051.916 I print_info: arch             = gptneox
0.00.051.916 I print_info: vocab_only       = 0
0.00.051.916 I print_info: n_ctx_train      = 2048
0.00.051.916 I print_info: n_embd           = 2048
0.00.051.917 I print_info: n_layer          = 24
0.00.051.920 I print_info: n_head           = 16
0.00.051.921 I print_info: n_head_kv        = 16
0.00.051.921 I print_info: n_rot            = 32
0.00.051.921 I print_info: n_swa            = 0
0.00.051.921 I print_info: n_embd_head_k    = 128
0.00.051.921 I print_info: n_embd_head_v    = 128
0.00.051.923 I print_info: n_gqa            = 1
0.00.051.924 I print_info: n_embd_k_gqa     = 2048
0.00.051.925 I print_info: n_embd_v_gqa     = 2048
0.00.051.927 I print_info: f_norm_eps       = 1.0e-05
0.00.051.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.927 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.927 I print_info: f_logit_scale    = 0.0e+00
0.00.051.928 I print_info: n_ff             = 8192
0.00.051.928 I print_info: n_expert         = 0
0.00.051.928 I print_info: n_expert_used    = 0
0.00.051.928 I print_info: causal attn      = 1
0.00.051.929 I print_info: pooling type     = 0
0.00.051.929 I print_info: rope type        = 2
0.00.051.929 I print_info: rope scaling     = linear
0.00.051.929 I print_info: freq_base_train  = 10000.0
0.00.051.930 I print_info: freq_scale_train = 1
0.00.051.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.930 I print_info: rope_finetuned   = unknown
0.00.051.931 I print_info: ssm_d_conv       = 0
0.00.051.931 I print_info: ssm_d_inner      = 0
0.00.051.931 I print_info: ssm_d_state      = 0
0.00.051.931 I print_info: ssm_dt_rank      = 0
0.00.051.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.931 I print_info: model type       = 1.4B
0.00.051.932 I print_info: model params     = 1.41 B
0.00.051.932 I print_info: general.name     = 1.4B
0.00.051.932 I print_info: vocab type       = BPE
0.00.051.933 I print_info: n_vocab          = 50304
0.00.051.933 I print_info: n_merges         = 50009
0.00.051.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.934 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.935 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.935 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.935 I print_info: LF token         = 128 'Ä'
0.00.051.935 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.935 I print_info: max token length = 1024
0.00.053.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.919 I load_tensors: offloading output layer to GPU
0.00.053.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.930 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.931 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.263 I llama_init_from_model: n_seq_max     = 1
0.00.054.263 I llama_init_from_model: n_ctx         = 2048
0.00.054.263 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.263 I llama_init_from_model: n_batch       = 2048
0.00.054.264 I llama_init_from_model: n_ubatch      = 512
0.00.054.264 I llama_init_from_model: flash_attn    = 0
0.00.054.264 I llama_init_from_model: freq_base     = 10000.0
0.00.054.264 I llama_init_from_model: freq_scale    = 1
0.00.054.265 I ggml_metal_init: allocating
0.00.054.268 I ggml_metal_init: found device: Apple M4
0.00.054.270 I ggml_metal_init: picking default device: Apple M4
0.00.054.835 I ggml_metal_init: using embedded metal library
0.00.057.140 I ggml_metal_init: GPU name:   Apple M4
0.00.057.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.143 I ggml_metal_init: simdgroup reduction   = true
0.00.057.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.143 I ggml_metal_init: has bfloat            = true
0.00.057.143 I ggml_metal_init: use bfloat            = true
0.00.057.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.730 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.776 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.784 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.806 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.880 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.881 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.881 I llama_init_from_model: graph nodes  = 967
0.00.086.882 I llama_init_from_model: graph splits = 2
0.00.086.885 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.396 I main: llama threadpool init, n_threads = 4
0.00.691.440 I 
0.00.691.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.466 I 
0.00.691.708 I sampler seed: 1234
0.00.691.712 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.723 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.724 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.724 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.535.999 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.535.999 I llama_perf_context_print:        load time =     680.02 ms
0.01.536.000 I llama_perf_context_print: prompt eval time =      51.65 ms /     7 tokens (    7.38 ms per token,   135.52 tokens per second)
0.01.536.001 I llama_perf_context_print:        eval time =     789.66 ms /    63 runs   (   12.53 ms per token,    79.78 tokens per second)
0.01.536.001 I llama_perf_context_print:       total time =     844.61 ms /    70 tokens
0.01.536.204 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.734 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.645 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.646 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.647 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.647 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.649 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.650 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.414 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.100 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.100 I llama_model_loader: - type  f32:  194 tensors
0.00.025.100 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.101 I print_info: file format = GGUF V3 (latest)
0.00.025.101 I print_info: file type   = Q6_K
0.00.025.102 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.720 I load: special tokens cache size = 25
0.00.049.490 I load: token to piece cache size = 0.2984 MB
0.00.049.493 I print_info: arch             = gptneox
0.00.049.494 I print_info: vocab_only       = 0
0.00.049.494 I print_info: n_ctx_train      = 2048
0.00.049.494 I print_info: n_embd           = 2048
0.00.049.494 I print_info: n_layer          = 24
0.00.049.497 I print_info: n_head           = 16
0.00.049.498 I print_info: n_head_kv        = 16
0.00.049.498 I print_info: n_rot            = 32
0.00.049.498 I print_info: n_swa            = 0
0.00.049.499 I print_info: n_embd_head_k    = 128
0.00.049.499 I print_info: n_embd_head_v    = 128
0.00.049.500 I print_info: n_gqa            = 1
0.00.049.500 I print_info: n_embd_k_gqa     = 2048
0.00.049.501 I print_info: n_embd_v_gqa     = 2048
0.00.049.501 I print_info: f_norm_eps       = 1.0e-05
0.00.049.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.502 I print_info: f_logit_scale    = 0.0e+00
0.00.049.503 I print_info: n_ff             = 8192
0.00.049.503 I print_info: n_expert         = 0
0.00.049.503 I print_info: n_expert_used    = 0
0.00.049.504 I print_info: causal attn      = 1
0.00.049.504 I print_info: pooling type     = 0
0.00.049.504 I print_info: rope type        = 2
0.00.049.504 I print_info: rope scaling     = linear
0.00.049.506 I print_info: freq_base_train  = 10000.0
0.00.049.507 I print_info: freq_scale_train = 1
0.00.049.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.507 I print_info: rope_finetuned   = unknown
0.00.049.507 I print_info: ssm_d_conv       = 0
0.00.049.507 I print_info: ssm_d_inner      = 0
0.00.049.507 I print_info: ssm_d_state      = 0
0.00.049.508 I print_info: ssm_dt_rank      = 0
0.00.049.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.508 I print_info: model type       = 1.4B
0.00.049.510 I print_info: model params     = 1.41 B
0.00.049.510 I print_info: general.name     = 1.4B
0.00.049.510 I print_info: vocab type       = BPE
0.00.049.510 I print_info: n_vocab          = 50304
0.00.049.511 I print_info: n_merges         = 50009
0.00.049.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.512 I print_info: LF token         = 128 'Ä'
0.00.049.512 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.512 I print_info: max token length = 1024
0.00.051.485 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.485 I load_tensors: offloading output layer to GPU
0.00.051.486 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.496 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.497 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.782 I llama_init_from_model: n_seq_max     = 1
0.00.051.783 I llama_init_from_model: n_ctx         = 2048
0.00.051.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.783 I llama_init_from_model: n_batch       = 2048
0.00.051.783 I llama_init_from_model: n_ubatch      = 512
0.00.051.784 I llama_init_from_model: flash_attn    = 0
0.00.051.784 I llama_init_from_model: freq_base     = 10000.0
0.00.051.784 I llama_init_from_model: freq_scale    = 1
0.00.051.785 I ggml_metal_init: allocating
0.00.051.787 I ggml_metal_init: found device: Apple M4
0.00.051.789 I ggml_metal_init: picking default device: Apple M4
0.00.052.370 I ggml_metal_init: using embedded metal library
0.00.054.691 I ggml_metal_init: GPU name:   Apple M4
0.00.054.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.693 I ggml_metal_init: simdgroup reduction   = true
0.00.054.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.694 I ggml_metal_init: has bfloat            = true
0.00.054.694 I ggml_metal_init: use bfloat            = true
0.00.054.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.247 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.906 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.912 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.862 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.864 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.864 I llama_init_from_model: graph nodes  = 967
0.00.083.864 I llama_init_from_model: graph splits = 2
0.00.083.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.585 I main: llama threadpool init, n_threads = 4
0.00.741.638 I 
0.00.741.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.664 I 
0.00.741.957 I sampler seed: 1234
0.00.741.965 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.986 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.990 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.618.849 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.618.850 I llama_perf_context_print:        load time =     732.84 ms
0.01.618.851 I llama_perf_context_print: prompt eval time =      54.35 ms /     7 tokens (    7.76 ms per token,   128.79 tokens per second)
0.01.618.852 I llama_perf_context_print:        eval time =     819.72 ms /    63 runs   (   13.01 ms per token,    76.86 tokens per second)
0.01.618.852 I llama_perf_context_print:       total time =     877.27 ms /    70 tokens
0.01.619.129 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.107s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.900 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.555 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.888 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.894 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.897 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.898 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.902 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.903 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.903 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.904 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.658 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.121 I llama_model_loader: - type  f32:  194 tensors
0.00.054.122 I llama_model_loader: - type  f16:   98 tensors
0.00.054.122 I print_info: file format = GGUF V3 (latest)
0.00.054.123 I print_info: file type   = all F32 (guessed)
0.00.054.125 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.256 I load: special tokens cache size = 25
0.00.087.971 I load: token to piece cache size = 0.2984 MB
0.00.087.975 I print_info: arch             = gptneox
0.00.087.975 I print_info: vocab_only       = 0
0.00.087.975 I print_info: n_ctx_train      = 2048
0.00.087.975 I print_info: n_embd           = 2048
0.00.087.976 I print_info: n_layer          = 24
0.00.087.980 I print_info: n_head           = 16
0.00.087.981 I print_info: n_head_kv        = 16
0.00.087.981 I print_info: n_rot            = 32
0.00.087.981 I print_info: n_swa            = 0
0.00.087.981 I print_info: n_embd_head_k    = 128
0.00.087.981 I print_info: n_embd_head_v    = 128
0.00.087.982 I print_info: n_gqa            = 1
0.00.087.983 I print_info: n_embd_k_gqa     = 2048
0.00.087.983 I print_info: n_embd_v_gqa     = 2048
0.00.087.984 I print_info: f_norm_eps       = 1.0e-05
0.00.087.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.986 I print_info: f_logit_scale    = 0.0e+00
0.00.087.987 I print_info: n_ff             = 8192
0.00.087.987 I print_info: n_expert         = 0
0.00.087.987 I print_info: n_expert_used    = 0
0.00.087.987 I print_info: causal attn      = 1
0.00.087.987 I print_info: pooling type     = 0
0.00.087.990 I print_info: rope type        = 2
0.00.087.990 I print_info: rope scaling     = linear
0.00.087.990 I print_info: freq_base_train  = 10000.0
0.00.087.990 I print_info: freq_scale_train = 1
0.00.087.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.991 I print_info: rope_finetuned   = unknown
0.00.087.991 I print_info: ssm_d_conv       = 0
0.00.087.991 I print_info: ssm_d_inner      = 0
0.00.087.991 I print_info: ssm_d_state      = 0
0.00.087.991 I print_info: ssm_dt_rank      = 0
0.00.087.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.992 I print_info: model type       = 1.4B
0.00.087.992 I print_info: model params     = 1.41 B
0.00.087.992 I print_info: general.name     = 1.4B
0.00.087.993 I print_info: vocab type       = BPE
0.00.087.994 I print_info: n_vocab          = 50304
0.00.087.994 I print_info: n_merges         = 50009
0.00.087.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.995 I print_info: LF token         = 128 'Ä'
0.00.087.995 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.995 I print_info: max token length = 1024
0.00.090.679 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.679 I load_tensors: offloading output layer to GPU
0.00.090.679 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.690 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.691 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.098 I llama_init_from_model: n_seq_max     = 1
0.00.091.099 I llama_init_from_model: n_ctx         = 128
0.00.091.099 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.099 I llama_init_from_model: n_batch       = 128
0.00.091.100 I llama_init_from_model: n_ubatch      = 128
0.00.091.100 I llama_init_from_model: flash_attn    = 0
0.00.091.100 I llama_init_from_model: freq_base     = 10000.0
0.00.091.101 I llama_init_from_model: freq_scale    = 1
0.00.091.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.102 I ggml_metal_init: allocating
0.00.091.105 I ggml_metal_init: found device: Apple M4
0.00.091.107 I ggml_metal_init: picking default device: Apple M4
0.00.091.790 I ggml_metal_init: using embedded metal library
0.00.094.465 I ggml_metal_init: GPU name:   Apple M4
0.00.094.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.468 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.469 I ggml_metal_init: simdgroup reduction   = true
0.00.094.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.469 I ggml_metal_init: has bfloat            = true
0.00.094.469 I ggml_metal_init: use bfloat            = true
0.00.094.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.471 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.887 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.229 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.244 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.150 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.151 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.152 I llama_init_from_model: graph nodes  = 967
0.00.107.152 I llama_init_from_model: graph splits = 2
0.00.107.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.432.986 I 
0.01.433.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.433.056 I perplexity: tokenizing the input ..
0.01.446.681 I perplexity: tokenization took 13.622 ms
0.01.446.700 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.568.878 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.570.566 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.570.613 I llama_perf_context_print:        load time =    1411.42 ms
0.01.570.615 I llama_perf_context_print: prompt eval time =     121.29 ms /   128 tokens (    0.95 ms per token,  1055.30 tokens per second)
0.01.570.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.570.616 I llama_perf_context_print:       total time =     137.63 ms /   129 tokens
0.01.571.177 I ggml_metal_free: deallocating

real	0m1.768s
user	0m0.122s
sys	0m0.252s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.292 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.467 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.878 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.887 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.887 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.889 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.895 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.597 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.792 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.793 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.793 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.793 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.794 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.794 I llama_model_loader: - type  f32:  194 tensors
0.00.032.795 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.796 I print_info: file format = GGUF V3 (latest)
0.00.032.796 I print_info: file type   = Q8_0
0.00.032.798 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.306 I load: special tokens cache size = 25
0.00.061.416 I load: token to piece cache size = 0.2984 MB
0.00.061.420 I print_info: arch             = gptneox
0.00.061.420 I print_info: vocab_only       = 0
0.00.061.420 I print_info: n_ctx_train      = 2048
0.00.061.421 I print_info: n_embd           = 2048
0.00.061.421 I print_info: n_layer          = 24
0.00.061.425 I print_info: n_head           = 16
0.00.061.426 I print_info: n_head_kv        = 16
0.00.061.426 I print_info: n_rot            = 32
0.00.061.427 I print_info: n_swa            = 0
0.00.061.428 I print_info: n_embd_head_k    = 128
0.00.061.428 I print_info: n_embd_head_v    = 128
0.00.061.429 I print_info: n_gqa            = 1
0.00.061.430 I print_info: n_embd_k_gqa     = 2048
0.00.061.430 I print_info: n_embd_v_gqa     = 2048
0.00.061.431 I print_info: f_norm_eps       = 1.0e-05
0.00.061.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.441 I print_info: f_logit_scale    = 0.0e+00
0.00.061.443 I print_info: n_ff             = 8192
0.00.061.443 I print_info: n_expert         = 0
0.00.061.444 I print_info: n_expert_used    = 0
0.00.061.444 I print_info: causal attn      = 1
0.00.061.446 I print_info: pooling type     = 0
0.00.061.446 I print_info: rope type        = 2
0.00.061.446 I print_info: rope scaling     = linear
0.00.061.446 I print_info: freq_base_train  = 10000.0
0.00.061.447 I print_info: freq_scale_train = 1
0.00.061.447 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.447 I print_info: rope_finetuned   = unknown
0.00.061.447 I print_info: ssm_d_conv       = 0
0.00.061.447 I print_info: ssm_d_inner      = 0
0.00.061.447 I print_info: ssm_d_state      = 0
0.00.061.447 I print_info: ssm_dt_rank      = 0
0.00.061.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.448 I print_info: model type       = 1.4B
0.00.061.448 I print_info: model params     = 1.41 B
0.00.061.448 I print_info: general.name     = 1.4B
0.00.061.449 I print_info: vocab type       = BPE
0.00.061.449 I print_info: n_vocab          = 50304
0.00.061.449 I print_info: n_merges         = 50009
0.00.061.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.455 I print_info: LF token         = 128 'Ä'
0.00.061.457 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.457 I print_info: max token length = 1024
0.00.063.794 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.794 I load_tensors: offloading output layer to GPU
0.00.063.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.806 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.807 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.230 I llama_init_from_model: n_seq_max     = 1
0.00.064.231 I llama_init_from_model: n_ctx         = 128
0.00.064.231 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.231 I llama_init_from_model: n_batch       = 128
0.00.064.231 I llama_init_from_model: n_ubatch      = 128
0.00.064.232 I llama_init_from_model: flash_attn    = 0
0.00.064.232 I llama_init_from_model: freq_base     = 10000.0
0.00.064.232 I llama_init_from_model: freq_scale    = 1
0.00.064.233 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.233 I ggml_metal_init: allocating
0.00.064.237 I ggml_metal_init: found device: Apple M4
0.00.064.239 I ggml_metal_init: picking default device: Apple M4
0.00.064.897 I ggml_metal_init: using embedded metal library
0.00.067.576 I ggml_metal_init: GPU name:   Apple M4
0.00.067.578 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.580 I ggml_metal_init: simdgroup reduction   = true
0.00.067.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.580 I ggml_metal_init: has bfloat            = true
0.00.067.580 I ggml_metal_init: use bfloat            = true
0.00.067.580 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.581 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.410 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.426 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.303 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.304 I llama_init_from_model: graph nodes  = 967
0.00.080.304 I llama_init_from_model: graph splits = 2
0.00.080.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.930.374 I 
0.00.930.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.930.403 I perplexity: tokenizing the input ..
0.00.938.671 I perplexity: tokenization took 8.267 ms
0.00.938.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.063.252 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.064.506 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.064.529 I llama_perf_context_print:        load time =     918.90 ms
0.01.064.530 I llama_perf_context_print: prompt eval time =     124.35 ms /   128 tokens (    0.97 ms per token,  1029.36 tokens per second)
0.01.064.533 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.064.533 I llama_perf_context_print:       total time =     134.15 ms /   129 tokens
0.01.064.957 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.090s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.253 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.295 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.337 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.339 I print_info: file format = GGUF V3 (latest)
0.00.024.339 I print_info: file type   = Q4_0
0.00.024.340 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.880 I load: special tokens cache size = 25
0.00.048.785 I load: token to piece cache size = 0.2984 MB
0.00.048.788 I print_info: arch             = gptneox
0.00.048.788 I print_info: vocab_only       = 0
0.00.048.788 I print_info: n_ctx_train      = 2048
0.00.048.788 I print_info: n_embd           = 2048
0.00.048.788 I print_info: n_layer          = 24
0.00.048.791 I print_info: n_head           = 16
0.00.048.792 I print_info: n_head_kv        = 16
0.00.048.794 I print_info: n_rot            = 32
0.00.048.794 I print_info: n_swa            = 0
0.00.048.794 I print_info: n_embd_head_k    = 128
0.00.048.794 I print_info: n_embd_head_v    = 128
0.00.048.795 I print_info: n_gqa            = 1
0.00.048.796 I print_info: n_embd_k_gqa     = 2048
0.00.048.797 I print_info: n_embd_v_gqa     = 2048
0.00.048.797 I print_info: f_norm_eps       = 1.0e-05
0.00.048.798 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.798 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.798 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.798 I print_info: f_logit_scale    = 0.0e+00
0.00.048.799 I print_info: n_ff             = 8192
0.00.048.799 I print_info: n_expert         = 0
0.00.048.799 I print_info: n_expert_used    = 0
0.00.048.799 I print_info: causal attn      = 1
0.00.048.800 I print_info: pooling type     = 0
0.00.048.800 I print_info: rope type        = 2
0.00.048.800 I print_info: rope scaling     = linear
0.00.048.800 I print_info: freq_base_train  = 10000.0
0.00.048.801 I print_info: freq_scale_train = 1
0.00.048.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.801 I print_info: rope_finetuned   = unknown
0.00.048.801 I print_info: ssm_d_conv       = 0
0.00.048.802 I print_info: ssm_d_inner      = 0
0.00.048.802 I print_info: ssm_d_state      = 0
0.00.048.802 I print_info: ssm_dt_rank      = 0
0.00.048.803 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.803 I print_info: model type       = 1.4B
0.00.048.803 I print_info: model params     = 1.41 B
0.00.048.803 I print_info: general.name     = 1.4B
0.00.048.804 I print_info: vocab type       = BPE
0.00.048.804 I print_info: n_vocab          = 50304
0.00.048.805 I print_info: n_merges         = 50009
0.00.048.805 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.805 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.805 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.806 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.806 I print_info: LF token         = 128 'Ä'
0.00.048.806 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.806 I print_info: max token length = 1024
0.00.050.691 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.691 I load_tensors: offloading output layer to GPU
0.00.050.691 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.702 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.050.703 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.988 I llama_init_from_model: n_seq_max     = 1
0.00.050.988 I llama_init_from_model: n_ctx         = 128
0.00.050.989 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.989 I llama_init_from_model: n_batch       = 128
0.00.050.989 I llama_init_from_model: n_ubatch      = 128
0.00.050.989 I llama_init_from_model: flash_attn    = 0
0.00.050.989 I llama_init_from_model: freq_base     = 10000.0
0.00.050.990 I llama_init_from_model: freq_scale    = 1
0.00.050.990 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.991 I ggml_metal_init: allocating
0.00.050.993 I ggml_metal_init: found device: Apple M4
0.00.050.995 I ggml_metal_init: picking default device: Apple M4
0.00.051.571 I ggml_metal_init: using embedded metal library
0.00.053.930 I ggml_metal_init: GPU name:   Apple M4
0.00.053.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.933 I ggml_metal_init: simdgroup reduction   = true
0.00.053.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.933 I ggml_metal_init: has bfloat            = true
0.00.053.933 I ggml_metal_init: use bfloat            = true
0.00.053.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.697 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.065 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.067 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.080 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.064 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.065 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.066 I llama_init_from_model: graph nodes  = 967
0.00.066.066 I llama_init_from_model: graph splits = 2
0.00.066.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.951 I 
0.00.562.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.003 I perplexity: tokenizing the input ..
0.00.571.023 I perplexity: tokenization took 8.019 ms
0.00.571.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.894 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.695.146 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.695.179 I llama_perf_context_print:        load time =     553.65 ms
0.00.695.180 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.94 tokens per second)
0.00.695.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.182 I llama_perf_context_print:       total time =     132.23 ms /   129 tokens
0.00.695.662 I ggml_metal_free: deallocating

real	0m0.710s
user	0m0.076s
sys	0m0.082s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.946 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.626 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.111 I llama_model_loader: - type  f32:  194 tensors
0.00.024.111 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.111 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.112 I print_info: file format = GGUF V3 (latest)
0.00.024.112 I print_info: file type   = Q4_1
0.00.024.113 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.555 I load: special tokens cache size = 25
0.00.048.468 I load: token to piece cache size = 0.2984 MB
0.00.048.471 I print_info: arch             = gptneox
0.00.048.471 I print_info: vocab_only       = 0
0.00.048.471 I print_info: n_ctx_train      = 2048
0.00.048.471 I print_info: n_embd           = 2048
0.00.048.472 I print_info: n_layer          = 24
0.00.048.475 I print_info: n_head           = 16
0.00.048.476 I print_info: n_head_kv        = 16
0.00.048.476 I print_info: n_rot            = 32
0.00.048.476 I print_info: n_swa            = 0
0.00.048.476 I print_info: n_embd_head_k    = 128
0.00.048.476 I print_info: n_embd_head_v    = 128
0.00.048.477 I print_info: n_gqa            = 1
0.00.048.478 I print_info: n_embd_k_gqa     = 2048
0.00.048.479 I print_info: n_embd_v_gqa     = 2048
0.00.048.479 I print_info: f_norm_eps       = 1.0e-05
0.00.048.479 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.480 I print_info: f_logit_scale    = 0.0e+00
0.00.048.480 I print_info: n_ff             = 8192
0.00.048.481 I print_info: n_expert         = 0
0.00.048.481 I print_info: n_expert_used    = 0
0.00.048.481 I print_info: causal attn      = 1
0.00.048.481 I print_info: pooling type     = 0
0.00.048.481 I print_info: rope type        = 2
0.00.048.481 I print_info: rope scaling     = linear
0.00.048.482 I print_info: freq_base_train  = 10000.0
0.00.048.485 I print_info: freq_scale_train = 1
0.00.048.485 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.485 I print_info: rope_finetuned   = unknown
0.00.048.486 I print_info: ssm_d_conv       = 0
0.00.048.486 I print_info: ssm_d_inner      = 0
0.00.048.486 I print_info: ssm_d_state      = 0
0.00.048.486 I print_info: ssm_dt_rank      = 0
0.00.048.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.486 I print_info: model type       = 1.4B
0.00.048.487 I print_info: model params     = 1.41 B
0.00.048.487 I print_info: general.name     = 1.4B
0.00.048.487 I print_info: vocab type       = BPE
0.00.048.488 I print_info: n_vocab          = 50304
0.00.048.488 I print_info: n_merges         = 50009
0.00.048.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.489 I print_info: LF token         = 128 'Ä'
0.00.048.493 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.493 I print_info: max token length = 1024
0.00.050.221 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.221 I load_tensors: offloading output layer to GPU
0.00.050.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.227 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.227 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.050.495 I llama_init_from_model: n_seq_max     = 1
0.00.050.496 I llama_init_from_model: n_ctx         = 128
0.00.050.496 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.496 I llama_init_from_model: n_batch       = 128
0.00.050.496 I llama_init_from_model: n_ubatch      = 128
0.00.050.497 I llama_init_from_model: flash_attn    = 0
0.00.050.497 I llama_init_from_model: freq_base     = 10000.0
0.00.050.497 I llama_init_from_model: freq_scale    = 1
0.00.050.497 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.498 I ggml_metal_init: allocating
0.00.050.500 I ggml_metal_init: found device: Apple M4
0.00.050.502 I ggml_metal_init: picking default device: Apple M4
0.00.051.073 I ggml_metal_init: using embedded metal library
0.00.053.369 I ggml_metal_init: GPU name:   Apple M4
0.00.053.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.371 I ggml_metal_init: simdgroup reduction   = true
0.00.053.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.372 I ggml_metal_init: has bfloat            = true
0.00.053.372 I ggml_metal_init: use bfloat            = true
0.00.053.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.755 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.017 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.019 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.041 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.884 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.885 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.886 I llama_init_from_model: graph nodes  = 967
0.00.064.886 I llama_init_from_model: graph splits = 2
0.00.064.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.703 I 
0.00.722.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.768 I perplexity: tokenizing the input ..
0.00.730.474 I perplexity: tokenization took 7.703 ms
0.00.730.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.318 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.854.470 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.854.492 I llama_perf_context_print:        load time =     713.75 ms
0.00.854.493 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.93 tokens per second)
0.00.854.495 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.495 I llama_perf_context_print:       total time =     131.80 ms /   129 tokens
0.00.854.835 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.076s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.854 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.734 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.734 I llama_model_loader: - type  f32:  194 tensors
0.00.024.735 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.735 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.736 I print_info: file format = GGUF V3 (latest)
0.00.024.736 I print_info: file type   = Q5_0
0.00.024.737 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.147 I load: special tokens cache size = 25
0.00.049.123 I load: token to piece cache size = 0.2984 MB
0.00.049.126 I print_info: arch             = gptneox
0.00.049.126 I print_info: vocab_only       = 0
0.00.049.126 I print_info: n_ctx_train      = 2048
0.00.049.126 I print_info: n_embd           = 2048
0.00.049.127 I print_info: n_layer          = 24
0.00.049.129 I print_info: n_head           = 16
0.00.049.130 I print_info: n_head_kv        = 16
0.00.049.130 I print_info: n_rot            = 32
0.00.049.131 I print_info: n_swa            = 0
0.00.049.131 I print_info: n_embd_head_k    = 128
0.00.049.131 I print_info: n_embd_head_v    = 128
0.00.049.132 I print_info: n_gqa            = 1
0.00.049.132 I print_info: n_embd_k_gqa     = 2048
0.00.049.133 I print_info: n_embd_v_gqa     = 2048
0.00.049.134 I print_info: f_norm_eps       = 1.0e-05
0.00.049.134 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.134 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.135 I print_info: f_logit_scale    = 0.0e+00
0.00.049.135 I print_info: n_ff             = 8192
0.00.049.136 I print_info: n_expert         = 0
0.00.049.136 I print_info: n_expert_used    = 0
0.00.049.136 I print_info: causal attn      = 1
0.00.049.136 I print_info: pooling type     = 0
0.00.049.136 I print_info: rope type        = 2
0.00.049.136 I print_info: rope scaling     = linear
0.00.049.137 I print_info: freq_base_train  = 10000.0
0.00.049.137 I print_info: freq_scale_train = 1
0.00.049.137 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.138 I print_info: rope_finetuned   = unknown
0.00.049.138 I print_info: ssm_d_conv       = 0
0.00.049.138 I print_info: ssm_d_inner      = 0
0.00.049.138 I print_info: ssm_d_state      = 0
0.00.049.138 I print_info: ssm_dt_rank      = 0
0.00.049.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.141 I print_info: model type       = 1.4B
0.00.049.141 I print_info: model params     = 1.41 B
0.00.049.141 I print_info: general.name     = 1.4B
0.00.049.142 I print_info: vocab type       = BPE
0.00.049.142 I print_info: n_vocab          = 50304
0.00.049.142 I print_info: n_merges         = 50009
0.00.049.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.143 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.143 I print_info: LF token         = 128 'Ä'
0.00.049.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.144 I print_info: max token length = 1024
0.00.051.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.082 I load_tensors: offloading output layer to GPU
0.00.051.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.093 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.094 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.401 I llama_init_from_model: n_seq_max     = 1
0.00.051.402 I llama_init_from_model: n_ctx         = 128
0.00.051.402 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.402 I llama_init_from_model: n_batch       = 128
0.00.051.402 I llama_init_from_model: n_ubatch      = 128
0.00.051.402 I llama_init_from_model: flash_attn    = 0
0.00.051.403 I llama_init_from_model: freq_base     = 10000.0
0.00.051.403 I llama_init_from_model: freq_scale    = 1
0.00.051.403 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.403 I ggml_metal_init: allocating
0.00.051.406 I ggml_metal_init: found device: Apple M4
0.00.051.408 I ggml_metal_init: picking default device: Apple M4
0.00.051.960 I ggml_metal_init: using embedded metal library
0.00.054.281 I ggml_metal_init: GPU name:   Apple M4
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.283 I ggml_metal_init: simdgroup reduction   = true
0.00.054.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.283 I ggml_metal_init: has bfloat            = true
0.00.054.283 I ggml_metal_init: use bfloat            = true
0.00.054.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.992 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.420 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.422 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.454 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.315 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.316 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.317 I llama_init_from_model: graph nodes  = 967
0.00.065.317 I llama_init_from_model: graph splits = 2
0.00.065.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.561 I 
0.00.785.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.597 I perplexity: tokenizing the input ..
0.00.793.499 I perplexity: tokenization took 7.9 ms
0.00.793.503 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.928.383 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.929.560 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.929.582 I llama_perf_context_print:        load time =     775.70 ms
0.00.929.583 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.59 tokens per second)
0.00.929.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.929.584 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.00.929.963 I ggml_metal_free: deallocating

real	0m0.946s
user	0m0.076s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.839 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.661 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.663 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.130 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.131 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.131 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.132 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.133 I llama_model_loader: - type  f32:  194 tensors
0.00.024.133 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.133 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.134 I print_info: file format = GGUF V3 (latest)
0.00.024.134 I print_info: file type   = Q5_1
0.00.024.135 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.432 I load: special tokens cache size = 25
0.00.049.387 I load: token to piece cache size = 0.2984 MB
0.00.049.390 I print_info: arch             = gptneox
0.00.049.390 I print_info: vocab_only       = 0
0.00.049.390 I print_info: n_ctx_train      = 2048
0.00.049.391 I print_info: n_embd           = 2048
0.00.049.391 I print_info: n_layer          = 24
0.00.049.394 I print_info: n_head           = 16
0.00.049.394 I print_info: n_head_kv        = 16
0.00.049.395 I print_info: n_rot            = 32
0.00.049.396 I print_info: n_swa            = 0
0.00.049.396 I print_info: n_embd_head_k    = 128
0.00.049.396 I print_info: n_embd_head_v    = 128
0.00.049.397 I print_info: n_gqa            = 1
0.00.049.398 I print_info: n_embd_k_gqa     = 2048
0.00.049.398 I print_info: n_embd_v_gqa     = 2048
0.00.049.399 I print_info: f_norm_eps       = 1.0e-05
0.00.049.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.400 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.400 I print_info: f_logit_scale    = 0.0e+00
0.00.049.402 I print_info: n_ff             = 8192
0.00.049.403 I print_info: n_expert         = 0
0.00.049.403 I print_info: n_expert_used    = 0
0.00.049.403 I print_info: causal attn      = 1
0.00.049.403 I print_info: pooling type     = 0
0.00.049.403 I print_info: rope type        = 2
0.00.049.403 I print_info: rope scaling     = linear
0.00.049.413 I print_info: freq_base_train  = 10000.0
0.00.049.414 I print_info: freq_scale_train = 1
0.00.049.414 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.415 I print_info: rope_finetuned   = unknown
0.00.049.415 I print_info: ssm_d_conv       = 0
0.00.049.415 I print_info: ssm_d_inner      = 0
0.00.049.415 I print_info: ssm_d_state      = 0
0.00.049.415 I print_info: ssm_dt_rank      = 0
0.00.049.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.416 I print_info: model type       = 1.4B
0.00.049.416 I print_info: model params     = 1.41 B
0.00.049.416 I print_info: general.name     = 1.4B
0.00.049.417 I print_info: vocab type       = BPE
0.00.049.417 I print_info: n_vocab          = 50304
0.00.049.417 I print_info: n_merges         = 50009
0.00.049.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.417 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.418 I print_info: LF token         = 128 'Ä'
0.00.049.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.419 I print_info: max token length = 1024
0.00.051.434 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.434 I load_tensors: offloading output layer to GPU
0.00.051.434 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.445 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.446 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.729 I llama_init_from_model: n_seq_max     = 1
0.00.051.730 I llama_init_from_model: n_ctx         = 128
0.00.051.730 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.730 I llama_init_from_model: n_batch       = 128
0.00.051.730 I llama_init_from_model: n_ubatch      = 128
0.00.051.730 I llama_init_from_model: flash_attn    = 0
0.00.051.731 I llama_init_from_model: freq_base     = 10000.0
0.00.051.731 I llama_init_from_model: freq_scale    = 1
0.00.051.731 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.732 I ggml_metal_init: allocating
0.00.051.735 I ggml_metal_init: found device: Apple M4
0.00.051.736 I ggml_metal_init: picking default device: Apple M4
0.00.052.310 I ggml_metal_init: using embedded metal library
0.00.054.679 I ggml_metal_init: GPU name:   Apple M4
0.00.054.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.681 I ggml_metal_init: simdgroup reduction   = true
0.00.054.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.681 I ggml_metal_init: has bfloat            = true
0.00.054.681 I ggml_metal_init: use bfloat            = true
0.00.054.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.336 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.343 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.356 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.165 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.166 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.167 I llama_init_from_model: graph nodes  = 967
0.00.066.167 I llama_init_from_model: graph splits = 2
0.00.066.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.919 I 
0.00.642.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.955 I perplexity: tokenizing the input ..
0.00.651.105 I perplexity: tokenization took 8.149 ms
0.00.651.109 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.878 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.787.023 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.787.049 I llama_perf_context_print:        load time =     634.08 ms
0.00.787.050 I llama_perf_context_print: prompt eval time =     134.54 ms /   128 tokens (    1.05 ms per token,   951.36 tokens per second)
0.00.787.051 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.051 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.787.596 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.077s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.029 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.329 I llama_model_loader: - type  f32:  194 tensors
0.00.025.329 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.330 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.330 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.331 I print_info: file format = GGUF V3 (latest)
0.00.025.331 I print_info: file type   = Q2_K - Medium
0.00.025.332 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.647 I load: special tokens cache size = 25
0.00.050.729 I load: token to piece cache size = 0.2984 MB
0.00.050.732 I print_info: arch             = gptneox
0.00.050.732 I print_info: vocab_only       = 0
0.00.050.732 I print_info: n_ctx_train      = 2048
0.00.050.732 I print_info: n_embd           = 2048
0.00.050.733 I print_info: n_layer          = 24
0.00.050.735 I print_info: n_head           = 16
0.00.050.736 I print_info: n_head_kv        = 16
0.00.050.736 I print_info: n_rot            = 32
0.00.050.736 I print_info: n_swa            = 0
0.00.050.737 I print_info: n_embd_head_k    = 128
0.00.050.737 I print_info: n_embd_head_v    = 128
0.00.050.737 I print_info: n_gqa            = 1
0.00.050.739 I print_info: n_embd_k_gqa     = 2048
0.00.050.740 I print_info: n_embd_v_gqa     = 2048
0.00.050.740 I print_info: f_norm_eps       = 1.0e-05
0.00.050.741 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.741 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.741 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.741 I print_info: f_logit_scale    = 0.0e+00
0.00.050.742 I print_info: n_ff             = 8192
0.00.050.742 I print_info: n_expert         = 0
0.00.050.742 I print_info: n_expert_used    = 0
0.00.050.742 I print_info: causal attn      = 1
0.00.050.743 I print_info: pooling type     = 0
0.00.050.743 I print_info: rope type        = 2
0.00.050.743 I print_info: rope scaling     = linear
0.00.050.744 I print_info: freq_base_train  = 10000.0
0.00.050.744 I print_info: freq_scale_train = 1
0.00.050.744 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.744 I print_info: rope_finetuned   = unknown
0.00.050.746 I print_info: ssm_d_conv       = 0
0.00.050.746 I print_info: ssm_d_inner      = 0
0.00.050.746 I print_info: ssm_d_state      = 0
0.00.050.746 I print_info: ssm_dt_rank      = 0
0.00.050.746 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.747 I print_info: model type       = 1.4B
0.00.050.747 I print_info: model params     = 1.41 B
0.00.050.747 I print_info: general.name     = 1.4B
0.00.050.748 I print_info: vocab type       = BPE
0.00.050.748 I print_info: n_vocab          = 50304
0.00.050.748 I print_info: n_merges         = 50009
0.00.050.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.751 I print_info: LF token         = 128 'Ä'
0.00.050.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.752 I print_info: max token length = 1024
0.00.052.636 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.636 I load_tensors: offloading output layer to GPU
0.00.052.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.647 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.648 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.946 I llama_init_from_model: n_seq_max     = 1
0.00.052.947 I llama_init_from_model: n_ctx         = 128
0.00.052.947 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.947 I llama_init_from_model: n_batch       = 128
0.00.052.947 I llama_init_from_model: n_ubatch      = 128
0.00.052.947 I llama_init_from_model: flash_attn    = 0
0.00.052.948 I llama_init_from_model: freq_base     = 10000.0
0.00.052.948 I llama_init_from_model: freq_scale    = 1
0.00.052.949 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.949 I ggml_metal_init: allocating
0.00.052.952 I ggml_metal_init: found device: Apple M4
0.00.052.954 I ggml_metal_init: picking default device: Apple M4
0.00.053.533 I ggml_metal_init: using embedded metal library
0.00.055.861 I ggml_metal_init: GPU name:   Apple M4
0.00.055.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.862 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.863 I ggml_metal_init: simdgroup reduction   = true
0.00.055.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.863 I ggml_metal_init: has bfloat            = true
0.00.055.863 I ggml_metal_init: use bfloat            = true
0.00.055.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.864 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.949 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.952 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.966 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.838 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.839 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.839 I llama_init_from_model: graph nodes  = 967
0.00.067.839 I llama_init_from_model: graph splits = 2
0.00.067.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.392.882 I 
0.00.392.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.392.922 I perplexity: tokenizing the input ..
0.00.400.457 I perplexity: tokenization took 7.535 ms
0.00.400.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.532.885 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.534.060 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.534.088 I llama_perf_context_print:        load time =     382.85 ms
0.00.534.089 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.41 tokens per second)
0.00.534.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.534.090 I llama_perf_context_print:       total time =     141.21 ms /   129 tokens
0.00.534.590 I ggml_metal_free: deallocating

real	0m0.551s
user	0m0.078s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.757 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.231 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.232 I llama_model_loader: - type  f32:  194 tensors
0.00.024.233 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.233 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.233 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.233 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.234 I print_info: file format = GGUF V3 (latest)
0.00.024.234 I print_info: file type   = Q3_K - Medium
0.00.024.235 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.459 I load: special tokens cache size = 25
0.00.049.450 I load: token to piece cache size = 0.2984 MB
0.00.049.454 I print_info: arch             = gptneox
0.00.049.454 I print_info: vocab_only       = 0
0.00.049.454 I print_info: n_ctx_train      = 2048
0.00.049.454 I print_info: n_embd           = 2048
0.00.049.455 I print_info: n_layer          = 24
0.00.049.457 I print_info: n_head           = 16
0.00.049.458 I print_info: n_head_kv        = 16
0.00.049.458 I print_info: n_rot            = 32
0.00.049.458 I print_info: n_swa            = 0
0.00.049.458 I print_info: n_embd_head_k    = 128
0.00.049.459 I print_info: n_embd_head_v    = 128
0.00.049.459 I print_info: n_gqa            = 1
0.00.049.460 I print_info: n_embd_k_gqa     = 2048
0.00.049.461 I print_info: n_embd_v_gqa     = 2048
0.00.049.461 I print_info: f_norm_eps       = 1.0e-05
0.00.049.462 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.462 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.462 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.462 I print_info: f_logit_scale    = 0.0e+00
0.00.049.463 I print_info: n_ff             = 8192
0.00.049.464 I print_info: n_expert         = 0
0.00.049.464 I print_info: n_expert_used    = 0
0.00.049.464 I print_info: causal attn      = 1
0.00.049.464 I print_info: pooling type     = 0
0.00.049.465 I print_info: rope type        = 2
0.00.049.465 I print_info: rope scaling     = linear
0.00.049.465 I print_info: freq_base_train  = 10000.0
0.00.049.465 I print_info: freq_scale_train = 1
0.00.049.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.467 I print_info: rope_finetuned   = unknown
0.00.049.467 I print_info: ssm_d_conv       = 0
0.00.049.467 I print_info: ssm_d_inner      = 0
0.00.049.468 I print_info: ssm_d_state      = 0
0.00.049.468 I print_info: ssm_dt_rank      = 0
0.00.049.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.468 I print_info: model type       = 1.4B
0.00.049.468 I print_info: model params     = 1.41 B
0.00.049.469 I print_info: general.name     = 1.4B
0.00.049.469 I print_info: vocab type       = BPE
0.00.049.469 I print_info: n_vocab          = 50304
0.00.049.469 I print_info: n_merges         = 50009
0.00.049.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: LF token         = 128 'Ä'
0.00.049.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.471 I print_info: max token length = 1024
0.00.051.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.397 I load_tensors: offloading output layer to GPU
0.00.051.397 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.408 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.409 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.703 I llama_init_from_model: n_seq_max     = 1
0.00.051.703 I llama_init_from_model: n_ctx         = 128
0.00.051.704 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.704 I llama_init_from_model: n_batch       = 128
0.00.051.704 I llama_init_from_model: n_ubatch      = 128
0.00.051.704 I llama_init_from_model: flash_attn    = 0
0.00.051.704 I llama_init_from_model: freq_base     = 10000.0
0.00.051.705 I llama_init_from_model: freq_scale    = 1
0.00.051.705 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.705 I ggml_metal_init: allocating
0.00.051.708 I ggml_metal_init: found device: Apple M4
0.00.051.710 I ggml_metal_init: picking default device: Apple M4
0.00.052.288 I ggml_metal_init: using embedded metal library
0.00.054.615 I ggml_metal_init: GPU name:   Apple M4
0.00.054.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.617 I ggml_metal_init: simdgroup reduction   = true
0.00.054.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.618 I ggml_metal_init: has bfloat            = true
0.00.054.618 I ggml_metal_init: use bfloat            = true
0.00.054.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.480 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.494 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.356 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.357 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.358 I llama_init_from_model: graph nodes  = 967
0.00.066.358 I llama_init_from_model: graph splits = 2
0.00.066.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.222 I 
0.00.459.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.275 I perplexity: tokenizing the input ..
0.00.467.165 I perplexity: tokenization took 7.888 ms
0.00.467.168 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.261 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.600.427 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.600.459 I llama_perf_context_print:        load time =     450.42 ms
0.00.600.461 I llama_perf_context_print: prompt eval time =     131.87 ms /   128 tokens (    1.03 ms per token,   970.69 tokens per second)
0.00.600.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.462 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.600.948 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.077s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.684 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.673 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.681 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.360 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.936 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.936 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.937 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.937 I llama_model_loader: - type  f32:  194 tensors
0.00.023.938 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.938 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.938 I llama_model_loader: - type q6_K:   13 tensors
0.00.023.938 I print_info: file format = GGUF V3 (latest)
0.00.023.939 I print_info: file type   = Q4_K - Medium
0.00.023.939 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.356 I load: special tokens cache size = 25
0.00.048.214 I load: token to piece cache size = 0.2984 MB
0.00.048.217 I print_info: arch             = gptneox
0.00.048.217 I print_info: vocab_only       = 0
0.00.048.217 I print_info: n_ctx_train      = 2048
0.00.048.217 I print_info: n_embd           = 2048
0.00.048.218 I print_info: n_layer          = 24
0.00.048.220 I print_info: n_head           = 16
0.00.048.221 I print_info: n_head_kv        = 16
0.00.048.221 I print_info: n_rot            = 32
0.00.048.221 I print_info: n_swa            = 0
0.00.048.221 I print_info: n_embd_head_k    = 128
0.00.048.222 I print_info: n_embd_head_v    = 128
0.00.048.222 I print_info: n_gqa            = 1
0.00.048.223 I print_info: n_embd_k_gqa     = 2048
0.00.048.224 I print_info: n_embd_v_gqa     = 2048
0.00.048.224 I print_info: f_norm_eps       = 1.0e-05
0.00.048.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.225 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.225 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.225 I print_info: f_logit_scale    = 0.0e+00
0.00.048.226 I print_info: n_ff             = 8192
0.00.048.226 I print_info: n_expert         = 0
0.00.048.228 I print_info: n_expert_used    = 0
0.00.048.228 I print_info: causal attn      = 1
0.00.048.228 I print_info: pooling type     = 0
0.00.048.228 I print_info: rope type        = 2
0.00.048.228 I print_info: rope scaling     = linear
0.00.048.229 I print_info: freq_base_train  = 10000.0
0.00.048.229 I print_info: freq_scale_train = 1
0.00.048.235 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.237 I print_info: rope_finetuned   = unknown
0.00.048.238 I print_info: ssm_d_conv       = 0
0.00.048.238 I print_info: ssm_d_inner      = 0
0.00.048.239 I print_info: ssm_d_state      = 0
0.00.048.239 I print_info: ssm_dt_rank      = 0
0.00.048.239 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.240 I print_info: model type       = 1.4B
0.00.048.240 I print_info: model params     = 1.41 B
0.00.048.241 I print_info: general.name     = 1.4B
0.00.048.241 I print_info: vocab type       = BPE
0.00.048.241 I print_info: n_vocab          = 50304
0.00.048.241 I print_info: n_merges         = 50009
0.00.048.242 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.242 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.242 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.243 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.243 I print_info: LF token         = 128 'Ä'
0.00.048.243 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.243 I print_info: max token length = 1024
0.00.050.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.130 I load_tensors: offloading output layer to GPU
0.00.050.131 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.141 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.142 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.417 I llama_init_from_model: n_seq_max     = 1
0.00.050.417 I llama_init_from_model: n_ctx         = 128
0.00.050.418 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.418 I llama_init_from_model: n_batch       = 128
0.00.050.418 I llama_init_from_model: n_ubatch      = 128
0.00.050.418 I llama_init_from_model: flash_attn    = 0
0.00.050.418 I llama_init_from_model: freq_base     = 10000.0
0.00.050.419 I llama_init_from_model: freq_scale    = 1
0.00.050.419 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.419 I ggml_metal_init: allocating
0.00.050.422 I ggml_metal_init: found device: Apple M4
0.00.050.424 I ggml_metal_init: picking default device: Apple M4
0.00.051.002 I ggml_metal_init: using embedded metal library
0.00.053.360 I ggml_metal_init: GPU name:   Apple M4
0.00.053.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.362 I ggml_metal_init: simdgroup reduction   = true
0.00.053.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.362 I ggml_metal_init: has bfloat            = true
0.00.053.362 I ggml_metal_init: use bfloat            = true
0.00.053.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.232 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.617 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.619 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.632 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.471 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.472 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.472 I llama_init_from_model: graph nodes  = 967
0.00.064.472 I llama_init_from_model: graph splits = 2
0.00.064.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.472 I 
0.00.626.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.536 I perplexity: tokenizing the input ..
0.00.635.622 I perplexity: tokenization took 9.085 ms
0.00.635.626 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.893 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.770.314 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.770.335 I llama_perf_context_print:        load time =     617.78 ms
0.00.770.336 I llama_perf_context_print: prompt eval time =     133.04 ms /   128 tokens (    1.04 ms per token,   962.15 tokens per second)
0.00.770.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.337 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.770.717 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.076s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.436 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.746 I llama_model_loader: - type  f32:  194 tensors
0.00.024.746 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.746 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.747 I print_info: file format = GGUF V3 (latest)
0.00.024.747 I print_info: file type   = Q5_K - Medium
0.00.024.748 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.965 I load: special tokens cache size = 25
0.00.049.806 I load: token to piece cache size = 0.2984 MB
0.00.049.810 I print_info: arch             = gptneox
0.00.049.810 I print_info: vocab_only       = 0
0.00.049.811 I print_info: n_ctx_train      = 2048
0.00.049.811 I print_info: n_embd           = 2048
0.00.049.811 I print_info: n_layer          = 24
0.00.049.815 I print_info: n_head           = 16
0.00.049.816 I print_info: n_head_kv        = 16
0.00.049.816 I print_info: n_rot            = 32
0.00.049.816 I print_info: n_swa            = 0
0.00.049.817 I print_info: n_embd_head_k    = 128
0.00.049.817 I print_info: n_embd_head_v    = 128
0.00.049.817 I print_info: n_gqa            = 1
0.00.049.818 I print_info: n_embd_k_gqa     = 2048
0.00.049.819 I print_info: n_embd_v_gqa     = 2048
0.00.049.820 I print_info: f_norm_eps       = 1.0e-05
0.00.049.820 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.820 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.820 I print_info: f_logit_scale    = 0.0e+00
0.00.049.821 I print_info: n_ff             = 8192
0.00.049.825 I print_info: n_expert         = 0
0.00.049.825 I print_info: n_expert_used    = 0
0.00.049.825 I print_info: causal attn      = 1
0.00.049.825 I print_info: pooling type     = 0
0.00.049.825 I print_info: rope type        = 2
0.00.049.825 I print_info: rope scaling     = linear
0.00.049.826 I print_info: freq_base_train  = 10000.0
0.00.049.826 I print_info: freq_scale_train = 1
0.00.049.826 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.826 I print_info: rope_finetuned   = unknown
0.00.049.826 I print_info: ssm_d_conv       = 0
0.00.049.827 I print_info: ssm_d_inner      = 0
0.00.049.827 I print_info: ssm_d_state      = 0
0.00.049.827 I print_info: ssm_dt_rank      = 0
0.00.049.827 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.827 I print_info: model type       = 1.4B
0.00.049.828 I print_info: model params     = 1.41 B
0.00.049.828 I print_info: general.name     = 1.4B
0.00.049.828 I print_info: vocab type       = BPE
0.00.049.828 I print_info: n_vocab          = 50304
0.00.049.829 I print_info: n_merges         = 50009
0.00.049.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.830 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.831 I print_info: LF token         = 128 'Ä'
0.00.049.831 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.831 I print_info: max token length = 1024
0.00.051.853 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.853 I load_tensors: offloading output layer to GPU
0.00.051.853 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.864 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.865 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.159 I llama_init_from_model: n_seq_max     = 1
0.00.052.160 I llama_init_from_model: n_ctx         = 128
0.00.052.160 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.160 I llama_init_from_model: n_batch       = 128
0.00.052.160 I llama_init_from_model: n_ubatch      = 128
0.00.052.160 I llama_init_from_model: flash_attn    = 0
0.00.052.161 I llama_init_from_model: freq_base     = 10000.0
0.00.052.161 I llama_init_from_model: freq_scale    = 1
0.00.052.161 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.162 I ggml_metal_init: allocating
0.00.052.165 I ggml_metal_init: found device: Apple M4
0.00.052.167 I ggml_metal_init: picking default device: Apple M4
0.00.052.769 I ggml_metal_init: using embedded metal library
0.00.055.168 I ggml_metal_init: GPU name:   Apple M4
0.00.055.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.171 I ggml_metal_init: simdgroup reduction   = true
0.00.055.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.172 I ggml_metal_init: has bfloat            = true
0.00.055.172 I ggml_metal_init: use bfloat            = true
0.00.055.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.113 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.372 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.375 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.314 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.315 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.315 I llama_init_from_model: graph nodes  = 967
0.00.067.315 I llama_init_from_model: graph splits = 2
0.00.067.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.654 I 
0.00.750.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.815 I perplexity: tokenizing the input ..
0.00.769.786 I perplexity: tokenization took 18.965 ms
0.00.769.812 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.927.005 I perplexity: 0.16 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.928.227 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.928.251 I llama_perf_context_print:        load time =     741.21 ms
0.00.928.252 I llama_perf_context_print: prompt eval time =     156.24 ms /   128 tokens (    1.22 ms per token,   819.23 tokens per second)
0.00.928.253 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.928.253 I llama_perf_context_print:       total time =     177.60 ms /   129 tokens
0.00.928.562 I ggml_metal_free: deallocating

real	0m0.948s
user	0m0.089s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.204 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.234 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.818 I llama_model_loader: - type  f32:  194 tensors
0.00.025.819 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.819 I print_info: file format = GGUF V3 (latest)
0.00.025.820 I print_info: file type   = Q6_K
0.00.025.820 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.000 I load: special tokens cache size = 25
0.00.051.188 I load: token to piece cache size = 0.2984 MB
0.00.051.192 I print_info: arch             = gptneox
0.00.051.192 I print_info: vocab_only       = 0
0.00.051.192 I print_info: n_ctx_train      = 2048
0.00.051.192 I print_info: n_embd           = 2048
0.00.051.192 I print_info: n_layer          = 24
0.00.051.196 I print_info: n_head           = 16
0.00.051.197 I print_info: n_head_kv        = 16
0.00.051.197 I print_info: n_rot            = 32
0.00.051.197 I print_info: n_swa            = 0
0.00.051.199 I print_info: n_embd_head_k    = 128
0.00.051.199 I print_info: n_embd_head_v    = 128
0.00.051.200 I print_info: n_gqa            = 1
0.00.051.200 I print_info: n_embd_k_gqa     = 2048
0.00.051.201 I print_info: n_embd_v_gqa     = 2048
0.00.051.201 I print_info: f_norm_eps       = 1.0e-05
0.00.051.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.202 I print_info: f_logit_scale    = 0.0e+00
0.00.051.202 I print_info: n_ff             = 8192
0.00.051.203 I print_info: n_expert         = 0
0.00.051.203 I print_info: n_expert_used    = 0
0.00.051.203 I print_info: causal attn      = 1
0.00.051.203 I print_info: pooling type     = 0
0.00.051.203 I print_info: rope type        = 2
0.00.051.203 I print_info: rope scaling     = linear
0.00.051.204 I print_info: freq_base_train  = 10000.0
0.00.051.204 I print_info: freq_scale_train = 1
0.00.051.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.204 I print_info: rope_finetuned   = unknown
0.00.051.204 I print_info: ssm_d_conv       = 0
0.00.051.204 I print_info: ssm_d_inner      = 0
0.00.051.204 I print_info: ssm_d_state      = 0
0.00.051.204 I print_info: ssm_dt_rank      = 0
0.00.051.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.205 I print_info: model type       = 1.4B
0.00.051.205 I print_info: model params     = 1.41 B
0.00.051.205 I print_info: general.name     = 1.4B
0.00.051.206 I print_info: vocab type       = BPE
0.00.051.206 I print_info: n_vocab          = 50304
0.00.051.206 I print_info: n_merges         = 50009
0.00.051.207 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.209 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.209 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.209 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.209 I print_info: LF token         = 128 'Ä'
0.00.051.210 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.210 I print_info: max token length = 1024
0.00.053.829 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.830 I load_tensors: offloading output layer to GPU
0.00.053.830 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.841 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.842 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.186 I llama_init_from_model: n_seq_max     = 1
0.00.054.187 I llama_init_from_model: n_ctx         = 128
0.00.054.187 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.187 I llama_init_from_model: n_batch       = 128
0.00.054.187 I llama_init_from_model: n_ubatch      = 128
0.00.054.187 I llama_init_from_model: flash_attn    = 0
0.00.054.187 I llama_init_from_model: freq_base     = 10000.0
0.00.054.188 I llama_init_from_model: freq_scale    = 1
0.00.054.188 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.188 I ggml_metal_init: allocating
0.00.054.191 I ggml_metal_init: found device: Apple M4
0.00.054.193 I ggml_metal_init: picking default device: Apple M4
0.00.054.835 I ggml_metal_init: using embedded metal library
0.00.058.513 I ggml_metal_init: GPU name:   Apple M4
0.00.058.515 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.516 I ggml_metal_init: simdgroup reduction   = true
0.00.058.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.520 I ggml_metal_init: has bfloat            = true
0.00.058.520 I ggml_metal_init: use bfloat            = true
0.00.058.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.455 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.722 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.724 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.624 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.625 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.625 I llama_init_from_model: graph nodes  = 967
0.00.069.625 I llama_init_from_model: graph splits = 2
0.00.069.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.127.588 I 
0.01.127.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.127.625 I perplexity: tokenizing the input ..
0.01.134.922 I perplexity: tokenization took 7.295 ms
0.01.134.928 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.274.342 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.01.275.857 I Final estimate: PPL = 10.3179 +/- 3.28637

0.01.275.890 I llama_perf_context_print:        load time =    1118.38 ms
0.01.275.891 I llama_perf_context_print: prompt eval time =     139.18 ms /   128 tokens (    1.09 ms per token,   919.66 tokens per second)
0.01.275.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.275.892 I llama_perf_context_print:       total time =     148.30 ms /   129 tokens
0.01.276.363 I ggml_metal_free: deallocating

real	0m1.291s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.259 I build: 4491 (c67cc983) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.029.481 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.348 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.377 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.378 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.385 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.392 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.896 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.897 I llama_model_loader: - type  f32:  194 tensors
0.00.061.898 I llama_model_loader: - type  f16:   98 tensors
0.00.061.907 I print_info: file format = GGUF V3 (latest)
0.00.061.909 I print_info: file type   = all F32 (guessed)
0.00.061.912 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.453 I load: special tokens cache size = 25
0.00.105.010 I load: token to piece cache size = 0.2984 MB
0.00.105.013 I print_info: arch             = gptneox
0.00.105.013 I print_info: vocab_only       = 0
0.00.105.014 I print_info: n_ctx_train      = 2048
0.00.105.014 I print_info: n_embd           = 2048
0.00.105.014 I print_info: n_layer          = 24
0.00.105.018 I print_info: n_head           = 16
0.00.105.020 I print_info: n_head_kv        = 16
0.00.105.021 I print_info: n_rot            = 32
0.00.105.021 I print_info: n_swa            = 0
0.00.105.021 I print_info: n_embd_head_k    = 128
0.00.105.021 I print_info: n_embd_head_v    = 128
0.00.105.022 I print_info: n_gqa            = 1
0.00.105.023 I print_info: n_embd_k_gqa     = 2048
0.00.105.023 I print_info: n_embd_v_gqa     = 2048
0.00.105.024 I print_info: f_norm_eps       = 1.0e-05
0.00.105.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.025 I print_info: f_logit_scale    = 0.0e+00
0.00.105.026 I print_info: n_ff             = 8192
0.00.105.026 I print_info: n_expert         = 0
0.00.105.026 I print_info: n_expert_used    = 0
0.00.105.026 I print_info: causal attn      = 1
0.00.105.026 I print_info: pooling type     = 0
0.00.105.026 I print_info: rope type        = 2
0.00.105.027 I print_info: rope scaling     = linear
0.00.105.029 I print_info: freq_base_train  = 10000.0
0.00.105.030 I print_info: freq_scale_train = 1
0.00.105.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.030 I print_info: rope_finetuned   = unknown
0.00.105.030 I print_info: ssm_d_conv       = 0
0.00.105.030 I print_info: ssm_d_inner      = 0
0.00.105.030 I print_info: ssm_d_state      = 0
0.00.105.031 I print_info: ssm_dt_rank      = 0
0.00.105.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.031 I print_info: model type       = 1.4B
0.00.105.031 I print_info: model params     = 1.41 B
0.00.105.032 I print_info: general.name     = 1.4B
0.00.105.032 I print_info: vocab type       = BPE
0.00.105.033 I print_info: n_vocab          = 50304
0.00.105.033 I print_info: n_merges         = 50009
0.00.105.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.034 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.034 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.034 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.034 I print_info: LF token         = 128 'Ä'
0.00.105.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.035 I print_info: max token length = 1024
0.00.107.819 I load_tensors: offloading 24 repeating layers to GPU
0.00.107.819 I load_tensors: offloading output layer to GPU
0.00.107.820 I load_tensors: offloaded 25/25 layers to GPU
0.00.107.830 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.832 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.108.160 I llama_init_from_model: n_seq_max     = 1
0.00.108.161 I llama_init_from_model: n_ctx         = 128
0.00.108.161 I llama_init_from_model: n_ctx_per_seq = 128
0.00.108.162 I llama_init_from_model: n_batch       = 128
0.00.108.162 I llama_init_from_model: n_ubatch      = 128
0.00.108.162 I llama_init_from_model: flash_attn    = 0
0.00.108.162 I llama_init_from_model: freq_base     = 10000.0
0.00.108.163 I llama_init_from_model: freq_scale    = 1
0.00.108.163 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.108.163 I ggml_metal_init: allocating
0.00.108.167 I ggml_metal_init: found device: Apple M4
0.00.108.169 I ggml_metal_init: picking default device: Apple M4
0.00.108.842 I ggml_metal_init: using embedded metal library
0.00.111.686 I ggml_metal_init: GPU name:   Apple M4
0.00.111.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.111.688 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.111.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.111.689 I ggml_metal_init: simdgroup reduction   = true
0.00.111.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.111.689 I ggml_metal_init: has bfloat            = true
0.00.111.689 I ggml_metal_init: use bfloat            = true
0.00.111.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.111.690 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.273 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.122.665 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.122.679 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.123.586 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.123.587 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.123.588 I llama_init_from_model: graph nodes  = 967
0.00.123.588 I llama_init_from_model: graph splits = 2
0.00.123.589 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.123.590 I 
0.00.123.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.123.618 I compute_imatrix: tokenizing the input ..
0.00.130.698 I compute_imatrix: tokenization took 7.079 ms
0.00.130.699 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.605.260 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.608.886 I llama_perf_context_print:        load time =    1575.78 ms
0.01.608.888 I llama_perf_context_print: prompt eval time =    1473.94 ms /   128 tokens (   11.52 ms per token,    86.84 tokens per second)
0.01.608.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.608.889 I llama_perf_context_print:       total time =    1579.39 ms /   129 tokens
0.01.609.575 I ggml_metal_free: deallocating

real	0m1.803s
user	0m0.184s
sys	0m0.230s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4491 (c67cc983)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e0a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e0b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e0d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e0ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e0fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e14640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e19730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e1ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e1f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e20ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e22680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e2a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e2b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e31c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e32eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e34130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e3b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e3cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e3e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e40cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e48af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e4a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e4b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e4bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e5f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e60cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e61190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e61630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e61ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e61f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e62410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146e62d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146e631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146e63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146e63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146e63fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146e64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146e64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146e64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146e65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146e657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146e65ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146e665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146e66d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146e67420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146e676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146e67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146e68190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146e687a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.154.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.154.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1067056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1067075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106707ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106708800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106708fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1067097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106709ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10670a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10670ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10670b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10670bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10670c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10670c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10670d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10670d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10670df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10670e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10670e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10670e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10670ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10670f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10670f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10670fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10670fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1067102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106710720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106710b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106711000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106711470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1067118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106711d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1067121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106712630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106712aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106712f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106713380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1067137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106713c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1067140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106714540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1067149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106714e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106715290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106715700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106715b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106716450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1067169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106716ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106717330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1067177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106717c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106718080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1067184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106718960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106718dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106719240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1067196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106719b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106719f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10671a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10671a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10671ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10671b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10671b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10671ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10671bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10671c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10671c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10671cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10671d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10671d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10671d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10671ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10671e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10671e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10671eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10671ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10671f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10671f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10671fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106720130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1067205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106720a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106720e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1067212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106721760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106721bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106722040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1067224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106723200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106723670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106723f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1067241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106724630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106724aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106724f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106725380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1067257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106725c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1067260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106726540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1067269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106726e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106727290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106727700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106727b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106727fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106728450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1067288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106728d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1067291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106729610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106729a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106729ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10672a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10672a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10672ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10672b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10672b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10672b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10672be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10672c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10672c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10672cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10672cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10672d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10672d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10672dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10672e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10672e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10672ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10672eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10672f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10672f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10672fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106730090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106730970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106730de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106731250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1067316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106731b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106731fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106732410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106732880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106732cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106733160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1067335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106733a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106733eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106734320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106734790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106734c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106735070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1067354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106735950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106735dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106736230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1067366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106736b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106736f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1067373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106737860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106737cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106738140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1067385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106738a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106738e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106739300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106739770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106739be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10673a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10673a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10673a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10673ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10673b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10673b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10673baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10673bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10673c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10673c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10673ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10673d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10673d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10673da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10673de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10673e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10673e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10673ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10673f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10673f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10673f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10673fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1067401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106740660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106740ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106740f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1067413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106741f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1067421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1067424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106742920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106742d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106743670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106743ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106743f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1067443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106744830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106744ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106745580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1067459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106745e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1067462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106746740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106746bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106747020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106747490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106747900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106747d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1067481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106748650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106748ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106748f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1067493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106749810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106749c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10674a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10674a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10674a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10674ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10674b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10674b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10674bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10674c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10674c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10674c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10674cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10674d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10674d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10674daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10674df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10674e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10674e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10674ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10674f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10674f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10674f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10674fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106750290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106750700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106750b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106750fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106751450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1067518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106751d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1067521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106752610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106752a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106752ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106753360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1067537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106753c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1067540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106754520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106754990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106754e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106755270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1067556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106755b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1067565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106756ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106757400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106757b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106757de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106758e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.804s
user	0m0.298s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4491 (c67cc983)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12880c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12880cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12880d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12880d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12880de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12880e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12880e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12880ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12880f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12880fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12880ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128810400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128810f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1288116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128813b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128814330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128814a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128815170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128815890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128816130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128816850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128816b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128817120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128817d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1288182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128818590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128818a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128818cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128819580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128819ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128819d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12881a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12881a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12881ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12881b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12881b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12881b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12881bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12881c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12881c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12881c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12881cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12881d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12881df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12881e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12881eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12881f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12881f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12881fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128820380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128820b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128821010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1288214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128821770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128821d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128822570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128822830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128822cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128823610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128823ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128823f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1288243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128824890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128824d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1288251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128825670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128825b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128825fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128826500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128826a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1288274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128827a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128827f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1288284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128828a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128828f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1288294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128829a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128829f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12882a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12882aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12882af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12882b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12882ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12882bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12882c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12882c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12882cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12882d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12882d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12882df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12881dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12882e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12882eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12882f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12882f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12882fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128830090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1288305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128830b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128831080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1288315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128831b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128832070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1288325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128832b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128833060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128833500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1288339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1288342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128834780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128834c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1288350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128835560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128835a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128835ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128836340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1288367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128837120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1288375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128837a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128837f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1288383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128838840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128838ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128839180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128839620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128839ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128839f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12883a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12883a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12883ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12883b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12883b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12883bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12883bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12883c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12883c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12883cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12883d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12883d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12883db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12883e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12883e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12883e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12883ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12883f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12883f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12883fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128840080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128840520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1288409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128840e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128841300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1288417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128841c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1288420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128842580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128842a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128842ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128843360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128843800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128843ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128844140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1288445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128844a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128844f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1288453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128845860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128845d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1288461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128846640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128846ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128846f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128847420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1288478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128847d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128848200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1288486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128848b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128848fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128849480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128849920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128849dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12884a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12884a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12884ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12884b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12884b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12884ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12884c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12884c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12884cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12884d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12884d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12884dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12884e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12884e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12884eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12884f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12884f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12884fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128850580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128850ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128851570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128851ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128852010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128852560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128852ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128853000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128853550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128853aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128853ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128854540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128854a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128855530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128855a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128855fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128856520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128856a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128856fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128857510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128857a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128857fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128858500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128858a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128858fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1288594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128859a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128859f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12885a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12885aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12885af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12885b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12885ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12885bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12885c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12885ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12885cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12885d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12885da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12885df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12885e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12885e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12885ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12885f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12885f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12885ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128860480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1288609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128860f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128861470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1288619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128861f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128862460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1288629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128862f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1288633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128863840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128863ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128864180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128864620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128864ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128864f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128865400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1288658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128865d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1288661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128866680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128866b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128866fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128867460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1288679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1288680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1288687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128868f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128869630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1288698f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12886a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12886a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12886a9b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127e05310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127e05780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127e05bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127e06060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127e064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127e06940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127e06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127e07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127e07690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127e07b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127e07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127e08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127e09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127e098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127e0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127e0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127e0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127e0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127e0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127e0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127e0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127e0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127e0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127e0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127e0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127e0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127e10010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127e10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127e109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127e10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127e110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f08630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f09000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f09500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f0bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f0d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f0e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f0f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f0ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f11670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f12780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f15ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f16bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f18c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f21380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f23180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f25480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f2cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f2d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f30380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f30880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f33b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f3dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f3e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f3f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f40920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f48350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f48df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f49890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f4a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f4b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f4cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f4d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f4da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f4dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f4efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f50fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f51890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12884c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12884dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12886a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12884bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12884c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12881fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12881f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128821a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12884e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128816dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12881e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12881e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12881d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12881cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128820030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12881ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128815dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1288106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128822040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12882e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128869bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128819270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12884eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12884cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1288173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1288176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128817960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12886ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12886b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12886b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12886b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12886b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12886bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12886be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12886c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12886c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12886c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12886c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12886cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12886cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12886d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12886d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12886d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12886da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12886dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12886df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12886e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12886e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12886e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12886ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12886ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12886f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12886f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12886f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12886f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12886fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12886fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128870090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128870350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128870610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1288708d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128870b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128870e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128871110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1288713d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128871690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128871950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128871c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128871ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128872190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128872450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128872710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1288729d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128872c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128872f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128873210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1288734d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128873790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128873a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128873d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128873fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128874290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128874550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128874810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128874ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128874d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128875050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128875310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1288755d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128875890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.243s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
