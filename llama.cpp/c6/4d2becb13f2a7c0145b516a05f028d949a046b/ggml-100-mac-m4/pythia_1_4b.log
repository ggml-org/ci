Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.584s
user	0m0.878s
sys	0m1.232s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-bpe
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-barrier
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-quantize-fns
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-eval-callback
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gguf-split
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-bench
[ 79%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-lookup-stats
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-cli
[ 83%] Built target llama-passkey
[ 83%] Built target llama-parallel
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-perplexity
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-quantize
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.058s
user	0m6.202s
sys	0m9.577s

main: quantize time =  2941.49 ms
main:    total time =  2941.49 ms

main: quantize time =  1769.82 ms
main:    total time =  1769.82 ms

main: quantize time =  2887.26 ms
main:    total time =  2887.26 ms

main: quantize time =  2624.93 ms
main:    total time =  2624.93 ms

main: quantize time =  1942.86 ms
main:    total time =  1942.86 ms

main: quantize time =  5044.25 ms
main:    total time =  5044.25 ms

main: quantize time =  5707.25 ms
main:    total time =  5707.25 ms

main: quantize time =  6908.96 ms
main:    total time =  6908.96 ms

main: quantize time =  6307.42 ms
main:    total time =  6307.42 ms

main: quantize time =  4592.23 ms
main:    total time =  4592.23 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.179 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.387 I main: llama backend init
0.00.000.398 I main: load the model and apply lora adapter, if any
0.00.059.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.071.711 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.071.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.071.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.071.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.071.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.071.748 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.071.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.078.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.080.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.089.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.847 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.848 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.849 I llama_model_loader: - type  f32:  194 tensors
0.00.089.849 I llama_model_loader: - type  f16:   98 tensors
0.00.089.851 I print_info: file format = GGUF V3 (latest)
0.00.089.852 I print_info: file type   = all F32 (guessed)
0.00.089.854 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.122.702 I load: special tokens cache size = 25
0.00.130.264 I load: token to piece cache size = 0.2984 MB
0.00.130.268 I print_info: arch             = gptneox
0.00.130.268 I print_info: vocab_only       = 0
0.00.130.268 I print_info: n_ctx_train      = 2048
0.00.130.268 I print_info: n_embd           = 2048
0.00.130.268 I print_info: n_layer          = 24
0.00.130.273 I print_info: n_head           = 16
0.00.130.273 I print_info: n_head_kv        = 16
0.00.130.273 I print_info: n_rot            = 32
0.00.130.274 I print_info: n_swa            = 0
0.00.130.274 I print_info: n_embd_head_k    = 128
0.00.130.274 I print_info: n_embd_head_v    = 128
0.00.130.275 I print_info: n_gqa            = 1
0.00.130.275 I print_info: n_embd_k_gqa     = 2048
0.00.130.276 I print_info: n_embd_v_gqa     = 2048
0.00.130.277 I print_info: f_norm_eps       = 1.0e-05
0.00.130.277 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.130.277 I print_info: f_clamp_kqv      = 0.0e+00
0.00.130.277 I print_info: f_max_alibi_bias = 0.0e+00
0.00.130.277 I print_info: f_logit_scale    = 0.0e+00
0.00.130.278 I print_info: n_ff             = 8192
0.00.130.278 I print_info: n_expert         = 0
0.00.130.278 I print_info: n_expert_used    = 0
0.00.130.279 I print_info: causal attn      = 1
0.00.130.279 I print_info: pooling type     = 0
0.00.130.279 I print_info: rope type        = 2
0.00.130.279 I print_info: rope scaling     = linear
0.00.130.279 I print_info: freq_base_train  = 10000.0
0.00.130.280 I print_info: freq_scale_train = 1
0.00.130.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.130.280 I print_info: rope_finetuned   = unknown
0.00.130.282 I print_info: ssm_d_conv       = 0
0.00.130.282 I print_info: ssm_d_inner      = 0
0.00.130.282 I print_info: ssm_d_state      = 0
0.00.130.282 I print_info: ssm_dt_rank      = 0
0.00.130.282 I print_info: ssm_dt_b_c_rms   = 0
0.00.130.283 I print_info: model type       = 1.4B
0.00.130.283 I print_info: model params     = 1.41 B
0.00.130.283 I print_info: general.name     = 1.4B
0.00.130.284 I print_info: vocab type       = BPE
0.00.130.284 I print_info: n_vocab          = 50304
0.00.130.284 I print_info: n_merges         = 50009
0.00.130.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.130.285 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.130.285 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.130.285 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.130.285 I print_info: LF token         = 128 'Ä'
0.00.130.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.130.286 I print_info: max token length = 1024
0.00.132.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.132.278 I load_tensors: offloading output layer to GPU
0.00.132.278 I load_tensors: offloaded 25/25 layers to GPU
0.00.132.297 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.132.298 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.132.616 I llama_init_from_model: n_seq_max     = 1
0.00.132.617 I llama_init_from_model: n_ctx         = 2048
0.00.132.617 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.132.617 I llama_init_from_model: n_batch       = 2048
0.00.132.618 I llama_init_from_model: n_ubatch      = 512
0.00.132.618 I llama_init_from_model: flash_attn    = 0
0.00.132.618 I llama_init_from_model: freq_base     = 10000.0
0.00.132.618 I llama_init_from_model: freq_scale    = 1
0.00.132.619 I ggml_metal_init: allocating
0.00.132.622 I ggml_metal_init: found device: Apple M4
0.00.132.624 I ggml_metal_init: picking default device: Apple M4
0.00.133.362 I ggml_metal_init: using embedded metal library
0.00.149.830 I ggml_metal_init: GPU name:   Apple M4
0.00.149.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.149.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.149.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.149.833 I ggml_metal_init: simdgroup reduction   = true
0.00.149.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.149.833 I ggml_metal_init: has bfloat            = true
0.00.149.833 I ggml_metal_init: use bfloat            = true
0.00.149.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.149.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.212.597 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.236.798 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.236.807 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.236.839 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.237.875 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.237.877 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.237.877 I llama_init_from_model: graph nodes  = 967
0.00.237.877 I llama_init_from_model: graph splits = 2
0.00.237.882 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.238.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.238.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.132 I main: llama threadpool init, n_threads = 4
0.00.336.174 I 
0.00.336.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.208 I 
0.00.336.387 I sampler seed: 1234
0.00.336.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.336.415 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.336.417 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.336.417 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.275.605 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.02.275.606 I llama_perf_context_print:        load time =     275.99 ms
0.02.275.608 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.65 tokens per second)
0.02.275.609 I llama_perf_context_print:        eval time =    1892.37 ms /    63 runs   (   30.04 ms per token,    33.29 tokens per second)
0.02.275.609 I llama_perf_context_print:       total time =    1940.51 ms /    70 tokens
0.02.275.838 I ggml_metal_free: deallocating

real	0m2.572s
user	0m0.153s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.037.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.243 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.244 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.245 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.245 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.609 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.047.592 I llama_model_loader: - type  f32:  194 tensors
0.00.047.593 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.594 I print_info: file format = GGUF V3 (latest)
0.00.047.595 I print_info: file type   = Q8_0
0.00.047.596 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.072.682 I load: special tokens cache size = 25
0.00.081.590 I load: token to piece cache size = 0.2984 MB
0.00.081.595 I print_info: arch             = gptneox
0.00.081.595 I print_info: vocab_only       = 0
0.00.081.596 I print_info: n_ctx_train      = 2048
0.00.081.596 I print_info: n_embd           = 2048
0.00.081.596 I print_info: n_layer          = 24
0.00.081.602 I print_info: n_head           = 16
0.00.081.603 I print_info: n_head_kv        = 16
0.00.081.603 I print_info: n_rot            = 32
0.00.081.603 I print_info: n_swa            = 0
0.00.081.603 I print_info: n_embd_head_k    = 128
0.00.081.604 I print_info: n_embd_head_v    = 128
0.00.081.605 I print_info: n_gqa            = 1
0.00.081.606 I print_info: n_embd_k_gqa     = 2048
0.00.081.606 I print_info: n_embd_v_gqa     = 2048
0.00.081.607 I print_info: f_norm_eps       = 1.0e-05
0.00.081.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.608 I print_info: f_logit_scale    = 0.0e+00
0.00.081.609 I print_info: n_ff             = 8192
0.00.081.609 I print_info: n_expert         = 0
0.00.081.610 I print_info: n_expert_used    = 0
0.00.081.610 I print_info: causal attn      = 1
0.00.081.610 I print_info: pooling type     = 0
0.00.081.610 I print_info: rope type        = 2
0.00.081.610 I print_info: rope scaling     = linear
0.00.081.613 I print_info: freq_base_train  = 10000.0
0.00.081.613 I print_info: freq_scale_train = 1
0.00.081.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.614 I print_info: rope_finetuned   = unknown
0.00.081.614 I print_info: ssm_d_conv       = 0
0.00.081.614 I print_info: ssm_d_inner      = 0
0.00.081.615 I print_info: ssm_d_state      = 0
0.00.081.616 I print_info: ssm_dt_rank      = 0
0.00.081.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.617 I print_info: model type       = 1.4B
0.00.081.617 I print_info: model params     = 1.41 B
0.00.081.617 I print_info: general.name     = 1.4B
0.00.081.618 I print_info: vocab type       = BPE
0.00.081.618 I print_info: n_vocab          = 50304
0.00.081.618 I print_info: n_merges         = 50009
0.00.081.619 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.619 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.620 I print_info: LF token         = 128 'Ä'
0.00.081.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.621 I print_info: max token length = 1024
0.00.084.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.084.236 I load_tensors: offloading output layer to GPU
0.00.084.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.084.248 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.084.250 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.084.673 I llama_init_from_model: n_seq_max     = 1
0.00.084.675 I llama_init_from_model: n_ctx         = 2048
0.00.084.675 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.084.675 I llama_init_from_model: n_batch       = 2048
0.00.084.675 I llama_init_from_model: n_ubatch      = 512
0.00.084.676 I llama_init_from_model: flash_attn    = 0
0.00.084.676 I llama_init_from_model: freq_base     = 10000.0
0.00.084.676 I llama_init_from_model: freq_scale    = 1
0.00.084.677 I ggml_metal_init: allocating
0.00.084.681 I ggml_metal_init: found device: Apple M4
0.00.084.684 I ggml_metal_init: picking default device: Apple M4
0.00.085.658 I ggml_metal_init: using embedded metal library
0.00.089.662 I ggml_metal_init: GPU name:   Apple M4
0.00.089.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.665 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.666 I ggml_metal_init: simdgroup reduction   = true
0.00.089.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.667 I ggml_metal_init: has bfloat            = true
0.00.089.667 I ggml_metal_init: use bfloat            = true
0.00.089.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.436 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.129.678 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.687 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.715 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.130.843 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.130.845 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.130.845 I llama_init_from_model: graph nodes  = 967
0.00.130.845 I llama_init_from_model: graph splits = 2
0.00.130.850 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.346.064 I main: llama threadpool init, n_threads = 4
0.01.346.102 I 
0.01.346.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.346.130 I 
0.01.346.383 I sampler seed: 1234
0.01.346.387 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.346.427 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.346.428 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.346.428 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.431.900 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63392.86 tokens per second)
0.02.431.901 I llama_perf_context_print:        load time =    1334.81 ms
0.02.431.903 I llama_perf_context_print: prompt eval time =      47.43 ms /     7 tokens (    6.78 ms per token,   147.60 tokens per second)
0.02.431.904 I llama_perf_context_print:        eval time =    1035.14 ms /    63 runs   (   16.43 ms per token,    60.86 tokens per second)
0.02.431.904 I llama_perf_context_print:       total time =    1086.89 ms /    70 tokens
0.02.432.126 I ggml_metal_free: deallocating

real	0m2.452s
user	0m0.131s
sys	0m0.260s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.016.476 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.979 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.980 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.980 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.980 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.981 I llama_model_loader: - type  f32:  194 tensors
0.00.047.981 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.982 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.983 I print_info: file format = GGUF V3 (latest)
0.00.047.983 I print_info: file type   = Q4_0
0.00.047.984 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.076.456 I load: special tokens cache size = 25
0.00.088.044 I load: token to piece cache size = 0.2984 MB
0.00.088.049 I print_info: arch             = gptneox
0.00.088.050 I print_info: vocab_only       = 0
0.00.088.050 I print_info: n_ctx_train      = 2048
0.00.088.050 I print_info: n_embd           = 2048
0.00.088.050 I print_info: n_layer          = 24
0.00.088.056 I print_info: n_head           = 16
0.00.088.057 I print_info: n_head_kv        = 16
0.00.088.057 I print_info: n_rot            = 32
0.00.088.057 I print_info: n_swa            = 0
0.00.088.058 I print_info: n_embd_head_k    = 128
0.00.088.058 I print_info: n_embd_head_v    = 128
0.00.088.059 I print_info: n_gqa            = 1
0.00.088.060 I print_info: n_embd_k_gqa     = 2048
0.00.088.061 I print_info: n_embd_v_gqa     = 2048
0.00.088.062 I print_info: f_norm_eps       = 1.0e-05
0.00.088.062 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.063 I print_info: f_logit_scale    = 0.0e+00
0.00.088.064 I print_info: n_ff             = 8192
0.00.088.064 I print_info: n_expert         = 0
0.00.088.064 I print_info: n_expert_used    = 0
0.00.088.064 I print_info: causal attn      = 1
0.00.088.064 I print_info: pooling type     = 0
0.00.088.067 I print_info: rope type        = 2
0.00.088.069 I print_info: rope scaling     = linear
0.00.088.070 I print_info: freq_base_train  = 10000.0
0.00.088.070 I print_info: freq_scale_train = 1
0.00.088.071 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.071 I print_info: rope_finetuned   = unknown
0.00.088.071 I print_info: ssm_d_conv       = 0
0.00.088.071 I print_info: ssm_d_inner      = 0
0.00.088.071 I print_info: ssm_d_state      = 0
0.00.088.072 I print_info: ssm_dt_rank      = 0
0.00.088.072 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.072 I print_info: model type       = 1.4B
0.00.088.073 I print_info: model params     = 1.41 B
0.00.088.079 I print_info: general.name     = 1.4B
0.00.088.080 I print_info: vocab type       = BPE
0.00.088.080 I print_info: n_vocab          = 50304
0.00.088.083 I print_info: n_merges         = 50009
0.00.088.083 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.083 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.084 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.084 I print_info: LF token         = 128 'Ä'
0.00.088.085 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.085 I print_info: max token length = 1024
0.00.091.093 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.094 I load_tensors: offloading output layer to GPU
0.00.091.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.107 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.091.108 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.091.535 I llama_init_from_model: n_seq_max     = 1
0.00.091.536 I llama_init_from_model: n_ctx         = 2048
0.00.091.536 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.091.536 I llama_init_from_model: n_batch       = 2048
0.00.091.537 I llama_init_from_model: n_ubatch      = 512
0.00.091.537 I llama_init_from_model: flash_attn    = 0
0.00.091.538 I llama_init_from_model: freq_base     = 10000.0
0.00.091.538 I llama_init_from_model: freq_scale    = 1
0.00.091.539 I ggml_metal_init: allocating
0.00.091.543 I ggml_metal_init: found device: Apple M4
0.00.091.545 I ggml_metal_init: picking default device: Apple M4
0.00.092.510 I ggml_metal_init: using embedded metal library
0.00.096.171 I ggml_metal_init: GPU name:   Apple M4
0.00.096.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.175 I ggml_metal_init: simdgroup reduction   = true
0.00.096.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.175 I ggml_metal_init: has bfloat            = true
0.00.096.175 I ggml_metal_init: use bfloat            = true
0.00.096.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.177 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.358 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.133.011 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.024 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.052 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.134.158 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.134.159 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.134.159 I llama_init_from_model: graph nodes  = 967
0.00.134.159 I llama_init_from_model: graph splits = 2
0.00.134.163 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.134.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.529 I main: llama threadpool init, n_threads = 4
0.00.772.581 I 
0.00.772.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.620 I 
0.00.772.942 I sampler seed: 1234
0.00.772.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.004 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.009 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.009 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.454.692 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.454.693 I llama_perf_context_print:        load time =     754.85 ms
0.01.454.694 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.70 tokens per second)
0.01.454.694 I llama_perf_context_print:        eval time =     631.62 ms /    63 runs   (   10.03 ms per token,    99.74 tokens per second)
0.01.454.698 I llama_perf_context_print:       total time =     683.36 ms /    70 tokens
0.01.454.899 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.141s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.206 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.774 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.013 I llama_model_loader: - type  f32:  194 tensors
0.00.032.013 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.014 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.014 I print_info: file format = GGUF V3 (latest)
0.00.032.015 I print_info: file type   = Q4_1
0.00.032.016 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.051.724 I load: special tokens cache size = 25
0.00.057.791 I load: token to piece cache size = 0.2984 MB
0.00.057.794 I print_info: arch             = gptneox
0.00.057.795 I print_info: vocab_only       = 0
0.00.057.795 I print_info: n_ctx_train      = 2048
0.00.057.795 I print_info: n_embd           = 2048
0.00.057.795 I print_info: n_layer          = 24
0.00.057.798 I print_info: n_head           = 16
0.00.057.799 I print_info: n_head_kv        = 16
0.00.057.799 I print_info: n_rot            = 32
0.00.057.800 I print_info: n_swa            = 0
0.00.057.800 I print_info: n_embd_head_k    = 128
0.00.057.802 I print_info: n_embd_head_v    = 128
0.00.057.803 I print_info: n_gqa            = 1
0.00.057.804 I print_info: n_embd_k_gqa     = 2048
0.00.057.805 I print_info: n_embd_v_gqa     = 2048
0.00.057.806 I print_info: f_norm_eps       = 1.0e-05
0.00.057.806 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.806 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.808 I print_info: f_logit_scale    = 0.0e+00
0.00.057.808 I print_info: n_ff             = 8192
0.00.057.809 I print_info: n_expert         = 0
0.00.057.809 I print_info: n_expert_used    = 0
0.00.057.809 I print_info: causal attn      = 1
0.00.057.810 I print_info: pooling type     = 0
0.00.057.811 I print_info: rope type        = 2
0.00.057.812 I print_info: rope scaling     = linear
0.00.057.813 I print_info: freq_base_train  = 10000.0
0.00.057.813 I print_info: freq_scale_train = 1
0.00.057.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.814 I print_info: rope_finetuned   = unknown
0.00.057.814 I print_info: ssm_d_conv       = 0
0.00.057.814 I print_info: ssm_d_inner      = 0
0.00.057.815 I print_info: ssm_d_state      = 0
0.00.057.815 I print_info: ssm_dt_rank      = 0
0.00.057.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.815 I print_info: model type       = 1.4B
0.00.057.816 I print_info: model params     = 1.41 B
0.00.057.816 I print_info: general.name     = 1.4B
0.00.057.817 I print_info: vocab type       = BPE
0.00.057.817 I print_info: n_vocab          = 50304
0.00.057.817 I print_info: n_merges         = 50009
0.00.057.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.818 I print_info: LF token         = 128 'Ä'
0.00.057.819 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.819 I print_info: max token length = 1024
0.00.059.813 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.814 I load_tensors: offloading output layer to GPU
0.00.059.814 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.825 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.826 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.060.171 I llama_init_from_model: n_seq_max     = 1
0.00.060.172 I llama_init_from_model: n_ctx         = 2048
0.00.060.172 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.172 I llama_init_from_model: n_batch       = 2048
0.00.060.173 I llama_init_from_model: n_ubatch      = 512
0.00.060.173 I llama_init_from_model: flash_attn    = 0
0.00.060.173 I llama_init_from_model: freq_base     = 10000.0
0.00.060.173 I llama_init_from_model: freq_scale    = 1
0.00.060.174 I ggml_metal_init: allocating
0.00.060.176 I ggml_metal_init: found device: Apple M4
0.00.060.178 I ggml_metal_init: picking default device: Apple M4
0.00.060.777 I ggml_metal_init: using embedded metal library
0.00.063.126 I ggml_metal_init: GPU name:   Apple M4
0.00.063.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.128 I ggml_metal_init: simdgroup reduction   = true
0.00.063.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.128 I ggml_metal_init: has bfloat            = true
0.00.063.128 I ggml_metal_init: use bfloat            = true
0.00.063.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.998 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.251 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.261 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.290 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.094.374 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.094.376 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.094.376 I llama_init_from_model: graph nodes  = 967
0.00.094.376 I llama_init_from_model: graph splits = 2
0.00.094.379 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.651 I main: llama threadpool init, n_threads = 4
0.00.836.686 I 
0.00.836.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.711 I 
0.00.836.933 I sampler seed: 1234
0.00.836.937 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.956 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.957 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.957 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.566.015 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64840.18 tokens per second)
0.01.566.015 I llama_perf_context_print:        load time =     826.55 ms
0.01.566.016 I llama_perf_context_print: prompt eval time =      45.67 ms /     7 tokens (    6.52 ms per token,   153.28 tokens per second)
0.01.566.017 I llama_perf_context_print:        eval time =     680.51 ms /    63 runs   (   10.80 ms per token,    92.58 tokens per second)
0.01.566.018 I llama_perf_context_print:       total time =     730.26 ms /    70 tokens
0.01.566.218 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.111s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.036.024 I llama_model_loader: - type  f32:  194 tensors
0.00.036.024 I llama_model_loader: - type q5_0:   97 tensors
0.00.036.024 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.025 I print_info: file format = GGUF V3 (latest)
0.00.036.025 I print_info: file type   = Q5_0
0.00.036.026 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.058.491 I load: special tokens cache size = 25
0.00.065.106 I load: token to piece cache size = 0.2984 MB
0.00.065.109 I print_info: arch             = gptneox
0.00.065.109 I print_info: vocab_only       = 0
0.00.065.110 I print_info: n_ctx_train      = 2048
0.00.065.110 I print_info: n_embd           = 2048
0.00.065.110 I print_info: n_layer          = 24
0.00.065.113 I print_info: n_head           = 16
0.00.065.114 I print_info: n_head_kv        = 16
0.00.065.114 I print_info: n_rot            = 32
0.00.065.114 I print_info: n_swa            = 0
0.00.065.115 I print_info: n_embd_head_k    = 128
0.00.065.115 I print_info: n_embd_head_v    = 128
0.00.065.115 I print_info: n_gqa            = 1
0.00.065.116 I print_info: n_embd_k_gqa     = 2048
0.00.065.117 I print_info: n_embd_v_gqa     = 2048
0.00.065.118 I print_info: f_norm_eps       = 1.0e-05
0.00.065.118 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.118 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.118 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.118 I print_info: f_logit_scale    = 0.0e+00
0.00.065.119 I print_info: n_ff             = 8192
0.00.065.119 I print_info: n_expert         = 0
0.00.065.119 I print_info: n_expert_used    = 0
0.00.065.121 I print_info: causal attn      = 1
0.00.065.121 I print_info: pooling type     = 0
0.00.065.123 I print_info: rope type        = 2
0.00.065.124 I print_info: rope scaling     = linear
0.00.065.124 I print_info: freq_base_train  = 10000.0
0.00.065.124 I print_info: freq_scale_train = 1
0.00.065.124 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.125 I print_info: rope_finetuned   = unknown
0.00.065.125 I print_info: ssm_d_conv       = 0
0.00.065.125 I print_info: ssm_d_inner      = 0
0.00.065.125 I print_info: ssm_d_state      = 0
0.00.065.125 I print_info: ssm_dt_rank      = 0
0.00.065.125 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.125 I print_info: model type       = 1.4B
0.00.065.126 I print_info: model params     = 1.41 B
0.00.065.126 I print_info: general.name     = 1.4B
0.00.065.126 I print_info: vocab type       = BPE
0.00.065.126 I print_info: n_vocab          = 50304
0.00.065.126 I print_info: n_merges         = 50009
0.00.065.130 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.131 I print_info: LF token         = 128 'Ä'
0.00.065.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.131 I print_info: max token length = 1024
0.00.067.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.281 I load_tensors: offloading output layer to GPU
0.00.067.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.292 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.067.293 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.067.668 I llama_init_from_model: n_seq_max     = 1
0.00.067.668 I llama_init_from_model: n_ctx         = 2048
0.00.067.669 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.669 I llama_init_from_model: n_batch       = 2048
0.00.067.669 I llama_init_from_model: n_ubatch      = 512
0.00.067.669 I llama_init_from_model: flash_attn    = 0
0.00.067.670 I llama_init_from_model: freq_base     = 10000.0
0.00.067.670 I llama_init_from_model: freq_scale    = 1
0.00.067.670 I ggml_metal_init: allocating
0.00.067.673 I ggml_metal_init: found device: Apple M4
0.00.067.676 I ggml_metal_init: picking default device: Apple M4
0.00.068.356 I ggml_metal_init: using embedded metal library
0.00.071.033 I ggml_metal_init: GPU name:   Apple M4
0.00.071.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.035 I ggml_metal_init: simdgroup reduction   = true
0.00.071.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.036 I ggml_metal_init: has bfloat            = true
0.00.071.036 I ggml_metal_init: use bfloat            = true
0.00.071.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.317 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.139 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.168 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.213 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.215 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.216 I llama_init_from_model: graph nodes  = 967
0.00.104.216 I llama_init_from_model: graph splits = 2
0.00.104.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.449 I main: llama threadpool init, n_threads = 4
0.00.963.484 I 
0.00.963.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.506 I 
0.00.963.719 I sampler seed: 1234
0.00.963.723 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.963.761 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.963.765 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.963.765 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.763.535 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.763.535 I llama_perf_context_print:        load time =     951.58 ms
0.01.763.537 I llama_perf_context_print: prompt eval time =      47.02 ms /     7 tokens (    6.72 ms per token,   148.87 tokens per second)
0.01.763.538 I llama_perf_context_print:        eval time =     749.74 ms /    63 runs   (   11.90 ms per token,    84.03 tokens per second)
0.01.763.538 I llama_perf_context_print:       total time =     801.03 ms /    70 tokens
0.01.763.764 I ggml_metal_free: deallocating

real	0m1.782s
user	0m0.117s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.333 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.891 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.951 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.952 I llama_model_loader: - type  f32:  194 tensors
0.00.025.952 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.953 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q5_1
0.00.025.955 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.100 I load: special tokens cache size = 25
0.00.051.305 I load: token to piece cache size = 0.2984 MB
0.00.051.307 I print_info: arch             = gptneox
0.00.051.308 I print_info: vocab_only       = 0
0.00.051.308 I print_info: n_ctx_train      = 2048
0.00.051.308 I print_info: n_embd           = 2048
0.00.051.308 I print_info: n_layer          = 24
0.00.051.312 I print_info: n_head           = 16
0.00.051.313 I print_info: n_head_kv        = 16
0.00.051.313 I print_info: n_rot            = 32
0.00.051.313 I print_info: n_swa            = 0
0.00.051.313 I print_info: n_embd_head_k    = 128
0.00.051.315 I print_info: n_embd_head_v    = 128
0.00.051.316 I print_info: n_gqa            = 1
0.00.051.316 I print_info: n_embd_k_gqa     = 2048
0.00.051.317 I print_info: n_embd_v_gqa     = 2048
0.00.051.318 I print_info: f_norm_eps       = 1.0e-05
0.00.051.318 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.318 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.319 I print_info: f_logit_scale    = 0.0e+00
0.00.051.319 I print_info: n_ff             = 8192
0.00.051.319 I print_info: n_expert         = 0
0.00.051.320 I print_info: n_expert_used    = 0
0.00.051.320 I print_info: causal attn      = 1
0.00.051.320 I print_info: pooling type     = 0
0.00.051.320 I print_info: rope type        = 2
0.00.051.320 I print_info: rope scaling     = linear
0.00.051.320 I print_info: freq_base_train  = 10000.0
0.00.051.321 I print_info: freq_scale_train = 1
0.00.051.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.321 I print_info: rope_finetuned   = unknown
0.00.051.321 I print_info: ssm_d_conv       = 0
0.00.051.321 I print_info: ssm_d_inner      = 0
0.00.051.323 I print_info: ssm_d_state      = 0
0.00.051.323 I print_info: ssm_dt_rank      = 0
0.00.051.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.323 I print_info: model type       = 1.4B
0.00.051.324 I print_info: model params     = 1.41 B
0.00.051.324 I print_info: general.name     = 1.4B
0.00.051.324 I print_info: vocab type       = BPE
0.00.051.324 I print_info: n_vocab          = 50304
0.00.051.325 I print_info: n_merges         = 50009
0.00.051.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.327 I print_info: LF token         = 128 'Ä'
0.00.051.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.327 I print_info: max token length = 1024
0.00.053.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.281 I load_tensors: offloading output layer to GPU
0.00.053.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.292 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.293 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.567 I llama_init_from_model: n_seq_max     = 1
0.00.053.567 I llama_init_from_model: n_ctx         = 2048
0.00.053.568 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.568 I llama_init_from_model: n_batch       = 2048
0.00.053.568 I llama_init_from_model: n_ubatch      = 512
0.00.053.568 I llama_init_from_model: flash_attn    = 0
0.00.053.569 I llama_init_from_model: freq_base     = 10000.0
0.00.053.569 I llama_init_from_model: freq_scale    = 1
0.00.053.569 I ggml_metal_init: allocating
0.00.053.573 I ggml_metal_init: found device: Apple M4
0.00.053.575 I ggml_metal_init: picking default device: Apple M4
0.00.054.206 I ggml_metal_init: using embedded metal library
0.00.056.570 I ggml_metal_init: GPU name:   Apple M4
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.573 I ggml_metal_init: simdgroup reduction   = true
0.00.056.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.573 I ggml_metal_init: has bfloat            = true
0.00.056.573 I ggml_metal_init: use bfloat            = true
0.00.056.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.874 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.042 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.099 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.100 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.101 I llama_init_from_model: graph nodes  = 967
0.00.087.101 I llama_init_from_model: graph splits = 2
0.00.087.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.746 I main: llama threadpool init, n_threads = 4
0.00.650.786 I 
0.00.650.808 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.808 I 
0.00.651.025 I sampler seed: 1234
0.00.651.030 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.651.076 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.651.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.651.077 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.494.908 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47555.26 tokens per second)
0.01.494.909 I llama_perf_context_print:        load time =     640.56 ms
0.01.494.910 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.74 tokens per second)
0.01.494.911 I llama_perf_context_print:        eval time =     799.00 ms /    63 runs   (   12.68 ms per token,    78.85 tokens per second)
0.01.494.911 I llama_perf_context_print:       total time =     845.01 ms /    70 tokens
0.01.495.175 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.111s
sys	0m0.126s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.392 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.424 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.424 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.425 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.426 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.427 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.427 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.430 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.430 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.593 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.595 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.595 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.596 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.596 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.597 I llama_model_loader: - type  f32:  194 tensors
0.00.027.597 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.597 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.598 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.598 I print_info: file format = GGUF V3 (latest)
0.00.027.599 I print_info: file type   = Q2_K - Medium
0.00.027.600 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.285 I load: special tokens cache size = 25
0.00.053.310 I load: token to piece cache size = 0.2984 MB
0.00.053.312 I print_info: arch             = gptneox
0.00.053.313 I print_info: vocab_only       = 0
0.00.053.313 I print_info: n_ctx_train      = 2048
0.00.053.313 I print_info: n_embd           = 2048
0.00.053.313 I print_info: n_layer          = 24
0.00.053.317 I print_info: n_head           = 16
0.00.053.318 I print_info: n_head_kv        = 16
0.00.053.319 I print_info: n_rot            = 32
0.00.053.320 I print_info: n_swa            = 0
0.00.053.320 I print_info: n_embd_head_k    = 128
0.00.053.320 I print_info: n_embd_head_v    = 128
0.00.053.321 I print_info: n_gqa            = 1
0.00.053.321 I print_info: n_embd_k_gqa     = 2048
0.00.053.322 I print_info: n_embd_v_gqa     = 2048
0.00.053.323 I print_info: f_norm_eps       = 1.0e-05
0.00.053.323 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.324 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.324 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.324 I print_info: f_logit_scale    = 0.0e+00
0.00.053.325 I print_info: n_ff             = 8192
0.00.053.325 I print_info: n_expert         = 0
0.00.053.325 I print_info: n_expert_used    = 0
0.00.053.327 I print_info: causal attn      = 1
0.00.053.327 I print_info: pooling type     = 0
0.00.053.327 I print_info: rope type        = 2
0.00.053.328 I print_info: rope scaling     = linear
0.00.053.328 I print_info: freq_base_train  = 10000.0
0.00.053.328 I print_info: freq_scale_train = 1
0.00.053.328 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.329 I print_info: rope_finetuned   = unknown
0.00.053.329 I print_info: ssm_d_conv       = 0
0.00.053.329 I print_info: ssm_d_inner      = 0
0.00.053.329 I print_info: ssm_d_state      = 0
0.00.053.329 I print_info: ssm_dt_rank      = 0
0.00.053.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.329 I print_info: model type       = 1.4B
0.00.053.330 I print_info: model params     = 1.41 B
0.00.053.330 I print_info: general.name     = 1.4B
0.00.053.330 I print_info: vocab type       = BPE
0.00.053.331 I print_info: n_vocab          = 50304
0.00.053.331 I print_info: n_merges         = 50009
0.00.053.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.332 I print_info: LF token         = 128 'Ä'
0.00.053.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.332 I print_info: max token length = 1024
0.00.055.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.253 I load_tensors: offloading output layer to GPU
0.00.055.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.264 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.266 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.055.545 I llama_init_from_model: n_seq_max     = 1
0.00.055.546 I llama_init_from_model: n_ctx         = 2048
0.00.055.546 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.546 I llama_init_from_model: n_batch       = 2048
0.00.055.546 I llama_init_from_model: n_ubatch      = 512
0.00.055.546 I llama_init_from_model: flash_attn    = 0
0.00.055.547 I llama_init_from_model: freq_base     = 10000.0
0.00.055.547 I llama_init_from_model: freq_scale    = 1
0.00.055.548 I ggml_metal_init: allocating
0.00.055.551 I ggml_metal_init: found device: Apple M4
0.00.055.553 I ggml_metal_init: picking default device: Apple M4
0.00.056.174 I ggml_metal_init: using embedded metal library
0.00.058.583 I ggml_metal_init: GPU name:   Apple M4
0.00.058.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.586 I ggml_metal_init: simdgroup reduction   = true
0.00.058.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.586 I ggml_metal_init: has bfloat            = true
0.00.058.587 I ggml_metal_init: use bfloat            = true
0.00.058.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.016 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.313 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.320 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.343 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.331 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.333 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.333 I llama_init_from_model: graph nodes  = 967
0.00.089.334 I llama_init_from_model: graph splits = 2
0.00.089.337 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.694 I main: llama threadpool init, n_threads = 4
0.00.514.735 I 
0.00.514.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.514.763 I 
0.00.514.989 I sampler seed: 1234
0.00.514.994 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.515.004 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.515.005 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.515.005 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.193.655 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63848.92 tokens per second)
0.01.193.656 I llama_perf_context_print:        load time =     502.43 ms
0.01.193.657 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.34 tokens per second)
0.01.193.658 I llama_perf_context_print:        eval time =     639.92 ms /    63 runs   (   10.16 ms per token,    98.45 tokens per second)
0.01.193.658 I llama_perf_context_print:       total time =     679.83 ms /    70 tokens
0.01.193.909 I ggml_metal_free: deallocating

real	0m1.214s
user	0m0.113s
sys	0m0.111s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.924 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.034.280 I llama_model_loader: - type  f32:  194 tensors
0.00.034.280 I llama_model_loader: - type q3_K:   25 tensors
0.00.034.280 I llama_model_loader: - type q4_K:   71 tensors
0.00.034.280 I llama_model_loader: - type q5_K:    1 tensors
0.00.034.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.281 I print_info: file format = GGUF V3 (latest)
0.00.034.282 I print_info: file type   = Q3_K - Medium
0.00.034.283 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.055.155 I load: special tokens cache size = 25
0.00.061.175 I load: token to piece cache size = 0.2984 MB
0.00.061.178 I print_info: arch             = gptneox
0.00.061.178 I print_info: vocab_only       = 0
0.00.061.179 I print_info: n_ctx_train      = 2048
0.00.061.179 I print_info: n_embd           = 2048
0.00.061.179 I print_info: n_layer          = 24
0.00.061.182 I print_info: n_head           = 16
0.00.061.183 I print_info: n_head_kv        = 16
0.00.061.183 I print_info: n_rot            = 32
0.00.061.183 I print_info: n_swa            = 0
0.00.061.183 I print_info: n_embd_head_k    = 128
0.00.061.184 I print_info: n_embd_head_v    = 128
0.00.061.187 I print_info: n_gqa            = 1
0.00.061.187 I print_info: n_embd_k_gqa     = 2048
0.00.061.188 I print_info: n_embd_v_gqa     = 2048
0.00.061.189 I print_info: f_norm_eps       = 1.0e-05
0.00.061.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.189 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.190 I print_info: f_logit_scale    = 0.0e+00
0.00.061.190 I print_info: n_ff             = 8192
0.00.061.196 I print_info: n_expert         = 0
0.00.061.198 I print_info: n_expert_used    = 0
0.00.061.199 I print_info: causal attn      = 1
0.00.061.199 I print_info: pooling type     = 0
0.00.061.199 I print_info: rope type        = 2
0.00.061.199 I print_info: rope scaling     = linear
0.00.061.200 I print_info: freq_base_train  = 10000.0
0.00.061.200 I print_info: freq_scale_train = 1
0.00.061.200 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.201 I print_info: rope_finetuned   = unknown
0.00.061.201 I print_info: ssm_d_conv       = 0
0.00.061.202 I print_info: ssm_d_inner      = 0
0.00.061.202 I print_info: ssm_d_state      = 0
0.00.061.202 I print_info: ssm_dt_rank      = 0
0.00.061.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.202 I print_info: model type       = 1.4B
0.00.061.202 I print_info: model params     = 1.41 B
0.00.061.203 I print_info: general.name     = 1.4B
0.00.061.203 I print_info: vocab type       = BPE
0.00.061.205 I print_info: n_vocab          = 50304
0.00.061.205 I print_info: n_merges         = 50009
0.00.061.205 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.205 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.206 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.206 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.206 I print_info: LF token         = 128 'Ä'
0.00.061.206 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.206 I print_info: max token length = 1024
0.00.063.132 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.132 I load_tensors: offloading output layer to GPU
0.00.063.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.143 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.063.144 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.063.409 I llama_init_from_model: n_seq_max     = 1
0.00.063.410 I llama_init_from_model: n_ctx         = 2048
0.00.063.410 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.063.410 I llama_init_from_model: n_batch       = 2048
0.00.063.410 I llama_init_from_model: n_ubatch      = 512
0.00.063.410 I llama_init_from_model: flash_attn    = 0
0.00.063.410 I llama_init_from_model: freq_base     = 10000.0
0.00.063.411 I llama_init_from_model: freq_scale    = 1
0.00.063.411 I ggml_metal_init: allocating
0.00.063.414 I ggml_metal_init: found device: Apple M4
0.00.063.416 I ggml_metal_init: picking default device: Apple M4
0.00.063.999 I ggml_metal_init: using embedded metal library
0.00.066.306 I ggml_metal_init: GPU name:   Apple M4
0.00.066.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.309 I ggml_metal_init: simdgroup reduction   = true
0.00.066.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.309 I ggml_metal_init: has bfloat            = true
0.00.066.309 I ggml_metal_init: use bfloat            = true
0.00.066.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.645 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.652 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.757 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.759 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.759 I llama_init_from_model: graph nodes  = 967
0.00.097.759 I llama_init_from_model: graph splits = 2
0.00.097.764 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.899 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.152 I main: llama threadpool init, n_threads = 4
0.00.600.193 I 
0.00.600.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.225 I 
0.00.600.434 I sampler seed: 1234
0.00.600.439 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.600.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.600.482 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.600.482 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.344.962 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.344.964 I llama_perf_context_print:        load time =     590.39 ms
0.01.344.964 I llama_perf_context_print: prompt eval time =      43.45 ms /     7 tokens (    6.21 ms per token,   161.12 tokens per second)
0.01.344.965 I llama_perf_context_print:        eval time =     697.95 ms /    63 runs   (   11.08 ms per token,    90.26 tokens per second)
0.01.344.965 I llama_perf_context_print:       total time =     745.72 ms /    70 tokens
0.01.345.221 I ggml_metal_free: deallocating

real	0m1.362s
user	0m0.112s
sys	0m0.129s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.685 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.136 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.143 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.188 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.189 I llama_model_loader: - type  f32:  194 tensors
0.00.027.190 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.190 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.190 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.191 I print_info: file format = GGUF V3 (latest)
0.00.027.191 I print_info: file type   = Q4_K - Medium
0.00.027.192 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.889 I load: special tokens cache size = 25
0.00.051.903 I load: token to piece cache size = 0.2984 MB
0.00.051.906 I print_info: arch             = gptneox
0.00.051.906 I print_info: vocab_only       = 0
0.00.051.906 I print_info: n_ctx_train      = 2048
0.00.051.907 I print_info: n_embd           = 2048
0.00.051.907 I print_info: n_layer          = 24
0.00.051.909 I print_info: n_head           = 16
0.00.051.910 I print_info: n_head_kv        = 16
0.00.051.910 I print_info: n_rot            = 32
0.00.051.910 I print_info: n_swa            = 0
0.00.051.910 I print_info: n_embd_head_k    = 128
0.00.051.911 I print_info: n_embd_head_v    = 128
0.00.051.911 I print_info: n_gqa            = 1
0.00.051.912 I print_info: n_embd_k_gqa     = 2048
0.00.051.913 I print_info: n_embd_v_gqa     = 2048
0.00.051.913 I print_info: f_norm_eps       = 1.0e-05
0.00.051.914 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.914 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.914 I print_info: f_logit_scale    = 0.0e+00
0.00.051.915 I print_info: n_ff             = 8192
0.00.051.915 I print_info: n_expert         = 0
0.00.051.915 I print_info: n_expert_used    = 0
0.00.051.915 I print_info: causal attn      = 1
0.00.051.916 I print_info: pooling type     = 0
0.00.051.916 I print_info: rope type        = 2
0.00.051.917 I print_info: rope scaling     = linear
0.00.051.917 I print_info: freq_base_train  = 10000.0
0.00.051.917 I print_info: freq_scale_train = 1
0.00.051.918 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.918 I print_info: rope_finetuned   = unknown
0.00.051.918 I print_info: ssm_d_conv       = 0
0.00.051.918 I print_info: ssm_d_inner      = 0
0.00.051.918 I print_info: ssm_d_state      = 0
0.00.051.919 I print_info: ssm_dt_rank      = 0
0.00.051.919 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.919 I print_info: model type       = 1.4B
0.00.051.919 I print_info: model params     = 1.41 B
0.00.051.921 I print_info: general.name     = 1.4B
0.00.051.922 I print_info: vocab type       = BPE
0.00.051.922 I print_info: n_vocab          = 50304
0.00.051.922 I print_info: n_merges         = 50009
0.00.051.922 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.923 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.923 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.923 I print_info: LF token         = 128 'Ä'
0.00.051.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.924 I print_info: max token length = 1024
0.00.053.805 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.805 I load_tensors: offloading output layer to GPU
0.00.053.805 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.816 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.817 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.098 I llama_init_from_model: n_seq_max     = 1
0.00.054.099 I llama_init_from_model: n_ctx         = 2048
0.00.054.099 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.100 I llama_init_from_model: n_batch       = 2048
0.00.054.100 I llama_init_from_model: n_ubatch      = 512
0.00.054.100 I llama_init_from_model: flash_attn    = 0
0.00.054.100 I llama_init_from_model: freq_base     = 10000.0
0.00.054.101 I llama_init_from_model: freq_scale    = 1
0.00.054.101 I ggml_metal_init: allocating
0.00.054.104 I ggml_metal_init: found device: Apple M4
0.00.054.106 I ggml_metal_init: picking default device: Apple M4
0.00.054.715 I ggml_metal_init: using embedded metal library
0.00.057.052 I ggml_metal_init: GPU name:   Apple M4
0.00.057.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.054 I ggml_metal_init: simdgroup reduction   = true
0.00.057.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.054 I ggml_metal_init: has bfloat            = true
0.00.057.054 I ggml_metal_init: use bfloat            = true
0.00.057.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.611 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.098 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.105 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.133 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.313 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.314 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.315 I llama_init_from_model: graph nodes  = 967
0.00.086.315 I llama_init_from_model: graph splits = 2
0.00.086.317 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.495 I main: llama threadpool init, n_threads = 4
0.00.626.528 I 
0.00.626.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.551 I 
0.00.626.775 I sampler seed: 1234
0.00.626.779 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.626.789 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.626.792 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.626.792 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.784 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.378.784 I llama_perf_context_print:        load time =     616.92 ms
0.01.378.785 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.81 tokens per second)
0.01.378.786 I llama_perf_context_print:        eval time =     701.81 ms /    63 runs   (   11.14 ms per token,    89.77 tokens per second)
0.01.378.786 I llama_perf_context_print:       total time =     753.18 ms /    70 tokens
0.01.379.022 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.458 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.940 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.908 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.920 I llama_model_loader: - type  f32:  194 tensors
0.00.026.920 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.920 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.921 I print_info: file format = GGUF V3 (latest)
0.00.026.921 I print_info: file type   = Q5_K - Medium
0.00.026.922 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.637 I load: special tokens cache size = 25
0.00.051.690 I load: token to piece cache size = 0.2984 MB
0.00.051.693 I print_info: arch             = gptneox
0.00.051.693 I print_info: vocab_only       = 0
0.00.051.694 I print_info: n_ctx_train      = 2048
0.00.051.694 I print_info: n_embd           = 2048
0.00.051.694 I print_info: n_layer          = 24
0.00.051.697 I print_info: n_head           = 16
0.00.051.699 I print_info: n_head_kv        = 16
0.00.051.699 I print_info: n_rot            = 32
0.00.051.699 I print_info: n_swa            = 0
0.00.051.699 I print_info: n_embd_head_k    = 128
0.00.051.700 I print_info: n_embd_head_v    = 128
0.00.051.700 I print_info: n_gqa            = 1
0.00.051.701 I print_info: n_embd_k_gqa     = 2048
0.00.051.702 I print_info: n_embd_v_gqa     = 2048
0.00.051.702 I print_info: f_norm_eps       = 1.0e-05
0.00.051.703 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.703 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.703 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.703 I print_info: f_logit_scale    = 0.0e+00
0.00.051.704 I print_info: n_ff             = 8192
0.00.051.704 I print_info: n_expert         = 0
0.00.051.704 I print_info: n_expert_used    = 0
0.00.051.704 I print_info: causal attn      = 1
0.00.051.704 I print_info: pooling type     = 0
0.00.051.706 I print_info: rope type        = 2
0.00.051.708 I print_info: rope scaling     = linear
0.00.051.708 I print_info: freq_base_train  = 10000.0
0.00.051.708 I print_info: freq_scale_train = 1
0.00.051.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.709 I print_info: rope_finetuned   = unknown
0.00.051.709 I print_info: ssm_d_conv       = 0
0.00.051.709 I print_info: ssm_d_inner      = 0
0.00.051.709 I print_info: ssm_d_state      = 0
0.00.051.709 I print_info: ssm_dt_rank      = 0
0.00.051.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.710 I print_info: model type       = 1.4B
0.00.051.710 I print_info: model params     = 1.41 B
0.00.051.710 I print_info: general.name     = 1.4B
0.00.051.711 I print_info: vocab type       = BPE
0.00.051.711 I print_info: n_vocab          = 50304
0.00.051.711 I print_info: n_merges         = 50009
0.00.051.711 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.712 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.712 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.712 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.712 I print_info: LF token         = 128 'Ä'
0.00.051.713 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.716 I print_info: max token length = 1024
0.00.053.673 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.673 I load_tensors: offloading output layer to GPU
0.00.053.673 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.684 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.685 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.954 I llama_init_from_model: n_seq_max     = 1
0.00.053.955 I llama_init_from_model: n_ctx         = 2048
0.00.053.955 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.955 I llama_init_from_model: n_batch       = 2048
0.00.053.955 I llama_init_from_model: n_ubatch      = 512
0.00.053.955 I llama_init_from_model: flash_attn    = 0
0.00.053.956 I llama_init_from_model: freq_base     = 10000.0
0.00.053.956 I llama_init_from_model: freq_scale    = 1
0.00.053.956 I ggml_metal_init: allocating
0.00.053.960 I ggml_metal_init: found device: Apple M4
0.00.053.961 I ggml_metal_init: picking default device: Apple M4
0.00.054.562 I ggml_metal_init: using embedded metal library
0.00.056.894 I ggml_metal_init: GPU name:   Apple M4
0.00.056.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.897 I ggml_metal_init: simdgroup reduction   = true
0.00.056.897 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.897 I ggml_metal_init: has bfloat            = true
0.00.056.897 I ggml_metal_init: use bfloat            = true
0.00.056.898 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.493 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.101 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.131 I llama_init_from_model: graph nodes  = 967
0.00.087.131 I llama_init_from_model: graph splits = 2
0.00.087.134 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.444 I main: llama threadpool init, n_threads = 4
0.00.721.483 I 
0.00.721.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.507 I 
0.00.721.730 I sampler seed: 1234
0.00.721.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.774 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.774 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.566.885 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.566.885 I llama_perf_context_print:        load time =     710.12 ms
0.01.566.886 I llama_perf_context_print: prompt eval time =      55.49 ms /     7 tokens (    7.93 ms per token,   126.16 tokens per second)
0.01.566.887 I llama_perf_context_print:        eval time =     786.68 ms /    63 runs   (   12.49 ms per token,    80.08 tokens per second)
0.01.566.888 I llama_perf_context_print:       total time =     846.30 ms /    70 tokens
0.01.567.067 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.740 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.720 I llama_model_loader: - type  f32:  194 tensors
0.00.025.721 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.721 I print_info: file format = GGUF V3 (latest)
0.00.025.722 I print_info: file type   = Q6_K
0.00.025.722 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.457 I load: special tokens cache size = 25
0.00.050.401 I load: token to piece cache size = 0.2984 MB
0.00.050.404 I print_info: arch             = gptneox
0.00.050.404 I print_info: vocab_only       = 0
0.00.050.404 I print_info: n_ctx_train      = 2048
0.00.050.404 I print_info: n_embd           = 2048
0.00.050.405 I print_info: n_layer          = 24
0.00.050.407 I print_info: n_head           = 16
0.00.050.408 I print_info: n_head_kv        = 16
0.00.050.408 I print_info: n_rot            = 32
0.00.050.408 I print_info: n_swa            = 0
0.00.050.411 I print_info: n_embd_head_k    = 128
0.00.050.411 I print_info: n_embd_head_v    = 128
0.00.050.412 I print_info: n_gqa            = 1
0.00.050.412 I print_info: n_embd_k_gqa     = 2048
0.00.050.413 I print_info: n_embd_v_gqa     = 2048
0.00.050.414 I print_info: f_norm_eps       = 1.0e-05
0.00.050.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.415 I print_info: f_logit_scale    = 0.0e+00
0.00.050.416 I print_info: n_ff             = 8192
0.00.050.416 I print_info: n_expert         = 0
0.00.050.416 I print_info: n_expert_used    = 0
0.00.050.416 I print_info: causal attn      = 1
0.00.050.416 I print_info: pooling type     = 0
0.00.050.417 I print_info: rope type        = 2
0.00.050.417 I print_info: rope scaling     = linear
0.00.050.417 I print_info: freq_base_train  = 10000.0
0.00.050.417 I print_info: freq_scale_train = 1
0.00.050.418 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.418 I print_info: rope_finetuned   = unknown
0.00.050.418 I print_info: ssm_d_conv       = 0
0.00.050.418 I print_info: ssm_d_inner      = 0
0.00.050.418 I print_info: ssm_d_state      = 0
0.00.050.418 I print_info: ssm_dt_rank      = 0
0.00.050.419 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.419 I print_info: model type       = 1.4B
0.00.050.419 I print_info: model params     = 1.41 B
0.00.050.419 I print_info: general.name     = 1.4B
0.00.050.420 I print_info: vocab type       = BPE
0.00.050.420 I print_info: n_vocab          = 50304
0.00.050.421 I print_info: n_merges         = 50009
0.00.050.421 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.422 I print_info: LF token         = 128 'Ä'
0.00.050.423 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.423 I print_info: max token length = 1024
0.00.052.190 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.190 I load_tensors: offloading output layer to GPU
0.00.052.190 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.196 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.198 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.472 I llama_init_from_model: n_seq_max     = 1
0.00.052.472 I llama_init_from_model: n_ctx         = 2048
0.00.052.473 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.473 I llama_init_from_model: n_batch       = 2048
0.00.052.473 I llama_init_from_model: n_ubatch      = 512
0.00.052.473 I llama_init_from_model: flash_attn    = 0
0.00.052.474 I llama_init_from_model: freq_base     = 10000.0
0.00.052.474 I llama_init_from_model: freq_scale    = 1
0.00.052.474 I ggml_metal_init: allocating
0.00.052.478 I ggml_metal_init: found device: Apple M4
0.00.052.480 I ggml_metal_init: picking default device: Apple M4
0.00.053.058 I ggml_metal_init: using embedded metal library
0.00.055.408 I ggml_metal_init: GPU name:   Apple M4
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.411 I ggml_metal_init: simdgroup reduction   = true
0.00.055.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.411 I ggml_metal_init: has bfloat            = true
0.00.055.411 I ggml_metal_init: use bfloat            = true
0.00.055.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.118 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.242 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.261 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.290 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.259 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.260 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.260 I llama_init_from_model: graph nodes  = 967
0.00.087.260 I llama_init_from_model: graph splits = 2
0.00.087.266 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.395 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.395 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.060 I main: llama threadpool init, n_threads = 4
0.00.782.097 I 
0.00.782.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.149 I 
0.00.782.385 I sampler seed: 1234
0.00.782.390 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.434 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.438 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.661.591 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.661.591 I llama_perf_context_print:        load time =     772.45 ms
0.01.661.592 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.661.593 I llama_perf_context_print:        eval time =     821.59 ms /    63 runs   (   13.04 ms per token,    76.68 tokens per second)
0.01.661.594 I llama_perf_context_print:       total time =     880.39 ms /    70 tokens
0.01.661.798 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.109s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.637 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.015 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.026 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.028 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.033 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.083 I llama_model_loader: - type  f32:  194 tensors
0.00.060.084 I llama_model_loader: - type  f16:   98 tensors
0.00.060.084 I print_info: file format = GGUF V3 (latest)
0.00.060.085 I print_info: file type   = all F32 (guessed)
0.00.060.086 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.995 I load: special tokens cache size = 25
0.00.091.343 I load: token to piece cache size = 0.2984 MB
0.00.091.346 I print_info: arch             = gptneox
0.00.091.346 I print_info: vocab_only       = 0
0.00.091.347 I print_info: n_ctx_train      = 2048
0.00.091.347 I print_info: n_embd           = 2048
0.00.091.347 I print_info: n_layer          = 24
0.00.091.350 I print_info: n_head           = 16
0.00.091.351 I print_info: n_head_kv        = 16
0.00.091.351 I print_info: n_rot            = 32
0.00.091.351 I print_info: n_swa            = 0
0.00.091.351 I print_info: n_embd_head_k    = 128
0.00.091.351 I print_info: n_embd_head_v    = 128
0.00.091.352 I print_info: n_gqa            = 1
0.00.091.353 I print_info: n_embd_k_gqa     = 2048
0.00.091.353 I print_info: n_embd_v_gqa     = 2048
0.00.091.354 I print_info: f_norm_eps       = 1.0e-05
0.00.091.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.354 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.354 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.355 I print_info: f_logit_scale    = 0.0e+00
0.00.091.355 I print_info: n_ff             = 8192
0.00.091.356 I print_info: n_expert         = 0
0.00.091.356 I print_info: n_expert_used    = 0
0.00.091.358 I print_info: causal attn      = 1
0.00.091.358 I print_info: pooling type     = 0
0.00.091.358 I print_info: rope type        = 2
0.00.091.358 I print_info: rope scaling     = linear
0.00.091.359 I print_info: freq_base_train  = 10000.0
0.00.091.359 I print_info: freq_scale_train = 1
0.00.091.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.359 I print_info: rope_finetuned   = unknown
0.00.091.360 I print_info: ssm_d_conv       = 0
0.00.091.360 I print_info: ssm_d_inner      = 0
0.00.091.360 I print_info: ssm_d_state      = 0
0.00.091.360 I print_info: ssm_dt_rank      = 0
0.00.091.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.364 I print_info: model type       = 1.4B
0.00.091.364 I print_info: model params     = 1.41 B
0.00.091.364 I print_info: general.name     = 1.4B
0.00.091.365 I print_info: vocab type       = BPE
0.00.091.365 I print_info: n_vocab          = 50304
0.00.091.365 I print_info: n_merges         = 50009
0.00.091.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.366 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.366 I print_info: LF token         = 128 'Ä'
0.00.091.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.366 I print_info: max token length = 1024
0.00.094.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.133 I load_tensors: offloading output layer to GPU
0.00.094.134 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.145 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.146 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.421 I llama_init_from_model: n_seq_max     = 1
0.00.094.421 I llama_init_from_model: n_ctx         = 128
0.00.094.422 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.422 I llama_init_from_model: n_batch       = 128
0.00.094.422 I llama_init_from_model: n_ubatch      = 128
0.00.094.422 I llama_init_from_model: flash_attn    = 0
0.00.094.422 I llama_init_from_model: freq_base     = 10000.0
0.00.094.423 I llama_init_from_model: freq_scale    = 1
0.00.094.423 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.424 I ggml_metal_init: allocating
0.00.094.427 I ggml_metal_init: found device: Apple M4
0.00.094.429 I ggml_metal_init: picking default device: Apple M4
0.00.095.056 I ggml_metal_init: using embedded metal library
0.00.097.660 I ggml_metal_init: GPU name:   Apple M4
0.00.097.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.662 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.663 I ggml_metal_init: simdgroup reduction   = true
0.00.097.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.663 I ggml_metal_init: has bfloat            = true
0.00.097.663 I ggml_metal_init: use bfloat            = true
0.00.097.663 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.926 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.202 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.206 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.229 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.073 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.074 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.074 I llama_init_from_model: graph nodes  = 967
0.00.109.074 I llama_init_from_model: graph splits = 2
0.00.109.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.365.951 I 
0.01.366.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.366.019 I perplexity: tokenizing the input ..
0.01.375.444 I perplexity: tokenization took 9.421 ms
0.01.375.463 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.494.201 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.495.672 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.495.687 I llama_perf_context_print:        load time =    1341.14 ms
0.01.495.689 I llama_perf_context_print: prompt eval time =     118.43 ms /   128 tokens (    0.93 ms per token,  1080.78 tokens per second)
0.01.495.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.495.690 I llama_perf_context_print:       total time =     129.74 ms /   129 tokens
0.01.496.067 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.113s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.540 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.550 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.610 I llama_model_loader: - type  f32:  194 tensors
0.00.027.610 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.611 I print_info: file format = GGUF V3 (latest)
0.00.027.611 I print_info: file type   = Q8_0
0.00.027.613 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.928 I load: special tokens cache size = 25
0.00.054.110 I load: token to piece cache size = 0.2984 MB
0.00.054.115 I print_info: arch             = gptneox
0.00.054.115 I print_info: vocab_only       = 0
0.00.054.115 I print_info: n_ctx_train      = 2048
0.00.054.115 I print_info: n_embd           = 2048
0.00.054.115 I print_info: n_layer          = 24
0.00.054.120 I print_info: n_head           = 16
0.00.054.121 I print_info: n_head_kv        = 16
0.00.054.121 I print_info: n_rot            = 32
0.00.054.121 I print_info: n_swa            = 0
0.00.054.121 I print_info: n_embd_head_k    = 128
0.00.054.121 I print_info: n_embd_head_v    = 128
0.00.054.122 I print_info: n_gqa            = 1
0.00.054.123 I print_info: n_embd_k_gqa     = 2048
0.00.054.123 I print_info: n_embd_v_gqa     = 2048
0.00.054.124 I print_info: f_norm_eps       = 1.0e-05
0.00.054.124 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.124 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.125 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.126 I print_info: f_logit_scale    = 0.0e+00
0.00.054.127 I print_info: n_ff             = 8192
0.00.054.127 I print_info: n_expert         = 0
0.00.054.127 I print_info: n_expert_used    = 0
0.00.054.127 I print_info: causal attn      = 1
0.00.054.127 I print_info: pooling type     = 0
0.00.054.127 I print_info: rope type        = 2
0.00.054.127 I print_info: rope scaling     = linear
0.00.054.128 I print_info: freq_base_train  = 10000.0
0.00.054.128 I print_info: freq_scale_train = 1
0.00.054.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.128 I print_info: rope_finetuned   = unknown
0.00.054.129 I print_info: ssm_d_conv       = 0
0.00.054.129 I print_info: ssm_d_inner      = 0
0.00.054.129 I print_info: ssm_d_state      = 0
0.00.054.129 I print_info: ssm_dt_rank      = 0
0.00.054.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.129 I print_info: model type       = 1.4B
0.00.054.130 I print_info: model params     = 1.41 B
0.00.054.130 I print_info: general.name     = 1.4B
0.00.054.130 I print_info: vocab type       = BPE
0.00.054.132 I print_info: n_vocab          = 50304
0.00.054.132 I print_info: n_merges         = 50009
0.00.054.132 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.133 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.133 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.133 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.133 I print_info: LF token         = 128 'Ä'
0.00.054.133 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.134 I print_info: max token length = 1024
0.00.056.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.452 I load_tensors: offloading output layer to GPU
0.00.056.452 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.463 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.464 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.056.775 I llama_init_from_model: n_seq_max     = 1
0.00.056.776 I llama_init_from_model: n_ctx         = 128
0.00.056.777 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.777 I llama_init_from_model: n_batch       = 128
0.00.056.777 I llama_init_from_model: n_ubatch      = 128
0.00.056.777 I llama_init_from_model: flash_attn    = 0
0.00.056.777 I llama_init_from_model: freq_base     = 10000.0
0.00.056.778 I llama_init_from_model: freq_scale    = 1
0.00.056.778 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.778 I ggml_metal_init: allocating
0.00.056.785 I ggml_metal_init: found device: Apple M4
0.00.056.810 I ggml_metal_init: picking default device: Apple M4
0.00.057.493 I ggml_metal_init: using embedded metal library
0.00.059.917 I ggml_metal_init: GPU name:   Apple M4
0.00.059.918 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.919 I ggml_metal_init: simdgroup reduction   = true
0.00.059.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.920 I ggml_metal_init: has bfloat            = true
0.00.059.920 I ggml_metal_init: use bfloat            = true
0.00.059.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.111 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.388 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.391 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.417 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.368 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.369 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.370 I llama_init_from_model: graph nodes  = 967
0.00.071.370 I llama_init_from_model: graph splits = 2
0.00.071.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.852.516 I 
0.00.852.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.852.596 I perplexity: tokenizing the input ..
0.00.860.369 I perplexity: tokenization took 7.77 ms
0.00.860.381 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.984.354 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.985.520 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.985.534 I llama_perf_context_print:        load time =     842.56 ms
0.00.985.535 I llama_perf_context_print: prompt eval time =     123.75 ms /   128 tokens (    0.97 ms per token,  1034.38 tokens per second)
0.00.985.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.985.536 I llama_perf_context_print:       total time =     133.02 ms /   129 tokens
0.00.985.893 I ggml_metal_free: deallocating

real	0m1.000s
user	0m0.080s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.551 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.005 I llama_model_loader: - type  f32:  194 tensors
0.00.026.005 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.006 I print_info: file format = GGUF V3 (latest)
0.00.026.007 I print_info: file type   = Q4_0
0.00.026.007 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.264 I load: special tokens cache size = 25
0.00.051.348 I load: token to piece cache size = 0.2984 MB
0.00.051.351 I print_info: arch             = gptneox
0.00.051.351 I print_info: vocab_only       = 0
0.00.051.351 I print_info: n_ctx_train      = 2048
0.00.051.351 I print_info: n_embd           = 2048
0.00.051.352 I print_info: n_layer          = 24
0.00.051.355 I print_info: n_head           = 16
0.00.051.358 I print_info: n_head_kv        = 16
0.00.051.358 I print_info: n_rot            = 32
0.00.051.358 I print_info: n_swa            = 0
0.00.051.358 I print_info: n_embd_head_k    = 128
0.00.051.358 I print_info: n_embd_head_v    = 128
0.00.051.359 I print_info: n_gqa            = 1
0.00.051.360 I print_info: n_embd_k_gqa     = 2048
0.00.051.361 I print_info: n_embd_v_gqa     = 2048
0.00.051.361 I print_info: f_norm_eps       = 1.0e-05
0.00.051.362 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.362 I print_info: f_logit_scale    = 0.0e+00
0.00.051.363 I print_info: n_ff             = 8192
0.00.051.363 I print_info: n_expert         = 0
0.00.051.363 I print_info: n_expert_used    = 0
0.00.051.363 I print_info: causal attn      = 1
0.00.051.363 I print_info: pooling type     = 0
0.00.051.364 I print_info: rope type        = 2
0.00.051.364 I print_info: rope scaling     = linear
0.00.051.364 I print_info: freq_base_train  = 10000.0
0.00.051.365 I print_info: freq_scale_train = 1
0.00.051.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.365 I print_info: rope_finetuned   = unknown
0.00.051.365 I print_info: ssm_d_conv       = 0
0.00.051.366 I print_info: ssm_d_inner      = 0
0.00.051.366 I print_info: ssm_d_state      = 0
0.00.051.366 I print_info: ssm_dt_rank      = 0
0.00.051.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.368 I print_info: model type       = 1.4B
0.00.051.368 I print_info: model params     = 1.41 B
0.00.051.369 I print_info: general.name     = 1.4B
0.00.051.369 I print_info: vocab type       = BPE
0.00.051.369 I print_info: n_vocab          = 50304
0.00.051.369 I print_info: n_merges         = 50009
0.00.051.370 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.370 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.370 I print_info: LF token         = 128 'Ä'
0.00.051.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.371 I print_info: max token length = 1024
0.00.053.275 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.275 I load_tensors: offloading output layer to GPU
0.00.053.275 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.289 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.289 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.591 I llama_init_from_model: n_seq_max     = 1
0.00.053.591 I llama_init_from_model: n_ctx         = 128
0.00.053.591 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.592 I llama_init_from_model: n_batch       = 128
0.00.053.592 I llama_init_from_model: n_ubatch      = 128
0.00.053.592 I llama_init_from_model: flash_attn    = 0
0.00.053.592 I llama_init_from_model: freq_base     = 10000.0
0.00.053.593 I llama_init_from_model: freq_scale    = 1
0.00.053.593 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.593 I ggml_metal_init: allocating
0.00.053.596 I ggml_metal_init: found device: Apple M4
0.00.053.598 I ggml_metal_init: picking default device: Apple M4
0.00.054.168 I ggml_metal_init: using embedded metal library
0.00.056.533 I ggml_metal_init: GPU name:   Apple M4
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.536 I ggml_metal_init: simdgroup reduction   = true
0.00.056.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.536 I ggml_metal_init: has bfloat            = true
0.00.056.536 I ggml_metal_init: use bfloat            = true
0.00.056.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.510 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.806 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.811 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.827 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.665 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.666 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.666 I llama_init_from_model: graph nodes  = 967
0.00.068.666 I llama_init_from_model: graph splits = 2
0.00.068.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.852 I 
0.00.608.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.901 I perplexity: tokenizing the input ..
0.00.617.278 I perplexity: tokenization took 8.375 ms
0.00.617.290 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.410 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.741.654 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.667 I llama_perf_context_print:        load time =     599.30 ms
0.00.741.668 I llama_perf_context_print: prompt eval time =     122.89 ms /   128 tokens (    0.96 ms per token,  1041.57 tokens per second)
0.00.741.669 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.669 I llama_perf_context_print:       total time =     132.82 ms /   129 tokens
0.00.742.160 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.078s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.001 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.047 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.029 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.030 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.030 I llama_model_loader: - type  f32:  194 tensors
0.00.025.031 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.031 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.031 I print_info: file format = GGUF V3 (latest)
0.00.025.032 I print_info: file type   = Q4_1
0.00.025.037 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.715 I load: special tokens cache size = 25
0.00.049.775 I load: token to piece cache size = 0.2984 MB
0.00.049.778 I print_info: arch             = gptneox
0.00.049.778 I print_info: vocab_only       = 0
0.00.049.778 I print_info: n_ctx_train      = 2048
0.00.049.779 I print_info: n_embd           = 2048
0.00.049.779 I print_info: n_layer          = 24
0.00.049.781 I print_info: n_head           = 16
0.00.049.782 I print_info: n_head_kv        = 16
0.00.049.782 I print_info: n_rot            = 32
0.00.049.782 I print_info: n_swa            = 0
0.00.049.782 I print_info: n_embd_head_k    = 128
0.00.049.784 I print_info: n_embd_head_v    = 128
0.00.049.785 I print_info: n_gqa            = 1
0.00.049.786 I print_info: n_embd_k_gqa     = 2048
0.00.049.786 I print_info: n_embd_v_gqa     = 2048
0.00.049.792 I print_info: f_norm_eps       = 1.0e-05
0.00.049.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.792 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.793 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.793 I print_info: f_logit_scale    = 0.0e+00
0.00.049.794 I print_info: n_ff             = 8192
0.00.049.794 I print_info: n_expert         = 0
0.00.049.794 I print_info: n_expert_used    = 0
0.00.049.794 I print_info: causal attn      = 1
0.00.049.794 I print_info: pooling type     = 0
0.00.049.795 I print_info: rope type        = 2
0.00.049.795 I print_info: rope scaling     = linear
0.00.049.797 I print_info: freq_base_train  = 10000.0
0.00.049.798 I print_info: freq_scale_train = 1
0.00.049.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.799 I print_info: rope_finetuned   = unknown
0.00.049.799 I print_info: ssm_d_conv       = 0
0.00.049.800 I print_info: ssm_d_inner      = 0
0.00.049.800 I print_info: ssm_d_state      = 0
0.00.049.800 I print_info: ssm_dt_rank      = 0
0.00.049.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.801 I print_info: model type       = 1.4B
0.00.049.801 I print_info: model params     = 1.41 B
0.00.049.801 I print_info: general.name     = 1.4B
0.00.049.802 I print_info: vocab type       = BPE
0.00.049.802 I print_info: n_vocab          = 50304
0.00.049.802 I print_info: n_merges         = 50009
0.00.049.802 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.802 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.803 I print_info: LF token         = 128 'Ä'
0.00.049.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.803 I print_info: max token length = 1024
0.00.051.802 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.802 I load_tensors: offloading output layer to GPU
0.00.051.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.813 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.814 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.099 I llama_init_from_model: n_seq_max     = 1
0.00.052.100 I llama_init_from_model: n_ctx         = 128
0.00.052.100 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.101 I llama_init_from_model: n_batch       = 128
0.00.052.101 I llama_init_from_model: n_ubatch      = 128
0.00.052.101 I llama_init_from_model: flash_attn    = 0
0.00.052.101 I llama_init_from_model: freq_base     = 10000.0
0.00.052.101 I llama_init_from_model: freq_scale    = 1
0.00.052.102 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.102 I ggml_metal_init: allocating
0.00.052.105 I ggml_metal_init: found device: Apple M4
0.00.052.107 I ggml_metal_init: picking default device: Apple M4
0.00.052.674 I ggml_metal_init: using embedded metal library
0.00.055.074 I ggml_metal_init: GPU name:   Apple M4
0.00.055.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.076 I ggml_metal_init: simdgroup reduction   = true
0.00.055.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.077 I ggml_metal_init: has bfloat            = true
0.00.055.077 I ggml_metal_init: use bfloat            = true
0.00.055.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.733 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.145 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.160 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.102 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.103 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.103 I llama_init_from_model: graph nodes  = 967
0.00.067.103 I llama_init_from_model: graph splits = 2
0.00.067.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.798 I 
0.00.660.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.844 I perplexity: tokenizing the input ..
0.00.668.532 I perplexity: tokenization took 7.687 ms
0.00.668.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.893 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.792.137 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.792.156 I llama_perf_context_print:        load time =     652.09 ms
0.00.792.158 I llama_perf_context_print: prompt eval time =     122.11 ms /   128 tokens (    0.95 ms per token,  1048.24 tokens per second)
0.00.792.159 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.159 I llama_perf_context_print:       total time =     131.36 ms /   129 tokens
0.00.792.522 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.601 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.020 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.043 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.043 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.046 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.046 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.004 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.005 I llama_model_loader: - type  f32:  194 tensors
0.00.027.005 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.006 I print_info: file format = GGUF V3 (latest)
0.00.027.007 I print_info: file type   = Q5_0
0.00.027.008 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.347 I load: special tokens cache size = 25
0.00.052.386 I load: token to piece cache size = 0.2984 MB
0.00.052.389 I print_info: arch             = gptneox
0.00.052.389 I print_info: vocab_only       = 0
0.00.052.390 I print_info: n_ctx_train      = 2048
0.00.052.390 I print_info: n_embd           = 2048
0.00.052.390 I print_info: n_layer          = 24
0.00.052.392 I print_info: n_head           = 16
0.00.052.393 I print_info: n_head_kv        = 16
0.00.052.393 I print_info: n_rot            = 32
0.00.052.394 I print_info: n_swa            = 0
0.00.052.394 I print_info: n_embd_head_k    = 128
0.00.052.395 I print_info: n_embd_head_v    = 128
0.00.052.397 I print_info: n_gqa            = 1
0.00.052.398 I print_info: n_embd_k_gqa     = 2048
0.00.052.398 I print_info: n_embd_v_gqa     = 2048
0.00.052.399 I print_info: f_norm_eps       = 1.0e-05
0.00.052.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.400 I print_info: f_logit_scale    = 0.0e+00
0.00.052.400 I print_info: n_ff             = 8192
0.00.052.401 I print_info: n_expert         = 0
0.00.052.401 I print_info: n_expert_used    = 0
0.00.052.401 I print_info: causal attn      = 1
0.00.052.401 I print_info: pooling type     = 0
0.00.052.401 I print_info: rope type        = 2
0.00.052.401 I print_info: rope scaling     = linear
0.00.052.402 I print_info: freq_base_train  = 10000.0
0.00.052.406 I print_info: freq_scale_train = 1
0.00.052.406 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.406 I print_info: rope_finetuned   = unknown
0.00.052.406 I print_info: ssm_d_conv       = 0
0.00.052.406 I print_info: ssm_d_inner      = 0
0.00.052.407 I print_info: ssm_d_state      = 0
0.00.052.407 I print_info: ssm_dt_rank      = 0
0.00.052.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.410 I print_info: model type       = 1.4B
0.00.052.411 I print_info: model params     = 1.41 B
0.00.052.411 I print_info: general.name     = 1.4B
0.00.052.411 I print_info: vocab type       = BPE
0.00.052.412 I print_info: n_vocab          = 50304
0.00.052.412 I print_info: n_merges         = 50009
0.00.052.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.413 I print_info: LF token         = 128 'Ä'
0.00.052.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.413 I print_info: max token length = 1024
0.00.054.427 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.427 I load_tensors: offloading output layer to GPU
0.00.054.427 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.438 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.439 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.728 I llama_init_from_model: n_seq_max     = 1
0.00.054.729 I llama_init_from_model: n_ctx         = 128
0.00.054.729 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.729 I llama_init_from_model: n_batch       = 128
0.00.054.729 I llama_init_from_model: n_ubatch      = 128
0.00.054.729 I llama_init_from_model: flash_attn    = 0
0.00.054.730 I llama_init_from_model: freq_base     = 10000.0
0.00.054.730 I llama_init_from_model: freq_scale    = 1
0.00.054.730 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.731 I ggml_metal_init: allocating
0.00.054.734 I ggml_metal_init: found device: Apple M4
0.00.054.735 I ggml_metal_init: picking default device: Apple M4
0.00.055.326 I ggml_metal_init: using embedded metal library
0.00.057.719 I ggml_metal_init: GPU name:   Apple M4
0.00.057.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.722 I ggml_metal_init: simdgroup reduction   = true
0.00.057.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.722 I ggml_metal_init: has bfloat            = true
0.00.057.722 I ggml_metal_init: use bfloat            = true
0.00.057.723 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.931 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.165 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.071 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.072 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.073 I llama_init_from_model: graph nodes  = 967
0.00.070.073 I llama_init_from_model: graph splits = 2
0.00.070.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.555 I 
0.00.712.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.614 I perplexity: tokenizing the input ..
0.00.720.620 I perplexity: tokenization took 8.005 ms
0.00.720.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.911 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.857.073 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.857.097 I llama_perf_context_print:        load time =     701.95 ms
0.00.857.097 I llama_perf_context_print: prompt eval time =     135.03 ms /   128 tokens (    1.05 ms per token,   947.95 tokens per second)
0.00.857.098 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.099 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.857.651 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.761 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.659 I llama_model_loader: - type  f32:  194 tensors
0.00.024.659 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.660 I print_info: file format = GGUF V3 (latest)
0.00.024.660 I print_info: file type   = Q5_1
0.00.024.661 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.045 I load: special tokens cache size = 25
0.00.049.035 I load: token to piece cache size = 0.2984 MB
0.00.049.038 I print_info: arch             = gptneox
0.00.049.038 I print_info: vocab_only       = 0
0.00.049.038 I print_info: n_ctx_train      = 2048
0.00.049.038 I print_info: n_embd           = 2048
0.00.049.039 I print_info: n_layer          = 24
0.00.049.042 I print_info: n_head           = 16
0.00.049.042 I print_info: n_head_kv        = 16
0.00.049.042 I print_info: n_rot            = 32
0.00.049.043 I print_info: n_swa            = 0
0.00.049.043 I print_info: n_embd_head_k    = 128
0.00.049.043 I print_info: n_embd_head_v    = 128
0.00.049.044 I print_info: n_gqa            = 1
0.00.049.044 I print_info: n_embd_k_gqa     = 2048
0.00.049.045 I print_info: n_embd_v_gqa     = 2048
0.00.049.046 I print_info: f_norm_eps       = 1.0e-05
0.00.049.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.046 I print_info: f_logit_scale    = 0.0e+00
0.00.049.047 I print_info: n_ff             = 8192
0.00.049.047 I print_info: n_expert         = 0
0.00.049.048 I print_info: n_expert_used    = 0
0.00.049.048 I print_info: causal attn      = 1
0.00.049.048 I print_info: pooling type     = 0
0.00.049.048 I print_info: rope type        = 2
0.00.049.048 I print_info: rope scaling     = linear
0.00.049.049 I print_info: freq_base_train  = 10000.0
0.00.049.050 I print_info: freq_scale_train = 1
0.00.049.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.050 I print_info: rope_finetuned   = unknown
0.00.049.050 I print_info: ssm_d_conv       = 0
0.00.049.050 I print_info: ssm_d_inner      = 0
0.00.049.051 I print_info: ssm_d_state      = 0
0.00.049.051 I print_info: ssm_dt_rank      = 0
0.00.049.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.051 I print_info: model type       = 1.4B
0.00.049.052 I print_info: model params     = 1.41 B
0.00.049.052 I print_info: general.name     = 1.4B
0.00.049.052 I print_info: vocab type       = BPE
0.00.049.052 I print_info: n_vocab          = 50304
0.00.049.053 I print_info: n_merges         = 50009
0.00.049.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.054 I print_info: LF token         = 128 'Ä'
0.00.049.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.054 I print_info: max token length = 1024
0.00.051.077 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.077 I load_tensors: offloading output layer to GPU
0.00.051.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.088 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.089 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.355 I llama_init_from_model: n_seq_max     = 1
0.00.051.356 I llama_init_from_model: n_ctx         = 128
0.00.051.356 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.356 I llama_init_from_model: n_batch       = 128
0.00.051.356 I llama_init_from_model: n_ubatch      = 128
0.00.051.357 I llama_init_from_model: flash_attn    = 0
0.00.051.357 I llama_init_from_model: freq_base     = 10000.0
0.00.051.357 I llama_init_from_model: freq_scale    = 1
0.00.051.357 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.358 I ggml_metal_init: allocating
0.00.051.361 I ggml_metal_init: found device: Apple M4
0.00.051.362 I ggml_metal_init: picking default device: Apple M4
0.00.051.928 I ggml_metal_init: using embedded metal library
0.00.054.229 I ggml_metal_init: GPU name:   Apple M4
0.00.054.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.231 I ggml_metal_init: simdgroup reduction   = true
0.00.054.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.231 I ggml_metal_init: has bfloat            = true
0.00.054.232 I ggml_metal_init: use bfloat            = true
0.00.054.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.767 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.077 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.081 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.097 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.944 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.945 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.945 I llama_init_from_model: graph nodes  = 967
0.00.065.945 I llama_init_from_model: graph splits = 2
0.00.065.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.901 I 
0.00.662.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.951 I perplexity: tokenizing the input ..
0.00.671.019 I perplexity: tokenization took 8.067 ms
0.00.671.030 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.081 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.807.243 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.807.261 I llama_perf_context_print:        load time =     654.11 ms
0.00.807.263 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.38 tokens per second)
0.00.807.264 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.264 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.807.755 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.077s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.723 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.738 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.738 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.707 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.838 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.838 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.839 I llama_model_loader: - type  f32:  194 tensors
0.00.025.839 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.839 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.840 I print_info: file format = GGUF V3 (latest)
0.00.025.841 I print_info: file type   = Q2_K - Medium
0.00.025.842 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.216 I load: special tokens cache size = 25
0.00.051.121 I load: token to piece cache size = 0.2984 MB
0.00.051.124 I print_info: arch             = gptneox
0.00.051.124 I print_info: vocab_only       = 0
0.00.051.124 I print_info: n_ctx_train      = 2048
0.00.051.125 I print_info: n_embd           = 2048
0.00.051.125 I print_info: n_layer          = 24
0.00.051.128 I print_info: n_head           = 16
0.00.051.128 I print_info: n_head_kv        = 16
0.00.051.129 I print_info: n_rot            = 32
0.00.051.129 I print_info: n_swa            = 0
0.00.051.129 I print_info: n_embd_head_k    = 128
0.00.051.129 I print_info: n_embd_head_v    = 128
0.00.051.130 I print_info: n_gqa            = 1
0.00.051.131 I print_info: n_embd_k_gqa     = 2048
0.00.051.131 I print_info: n_embd_v_gqa     = 2048
0.00.051.132 I print_info: f_norm_eps       = 1.0e-05
0.00.051.132 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.133 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.133 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.133 I print_info: f_logit_scale    = 0.0e+00
0.00.051.134 I print_info: n_ff             = 8192
0.00.051.136 I print_info: n_expert         = 0
0.00.051.136 I print_info: n_expert_used    = 0
0.00.051.137 I print_info: causal attn      = 1
0.00.051.137 I print_info: pooling type     = 0
0.00.051.137 I print_info: rope type        = 2
0.00.051.137 I print_info: rope scaling     = linear
0.00.051.138 I print_info: freq_base_train  = 10000.0
0.00.051.138 I print_info: freq_scale_train = 1
0.00.051.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.138 I print_info: rope_finetuned   = unknown
0.00.051.138 I print_info: ssm_d_conv       = 0
0.00.051.138 I print_info: ssm_d_inner      = 0
0.00.051.139 I print_info: ssm_d_state      = 0
0.00.051.139 I print_info: ssm_dt_rank      = 0
0.00.051.139 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.139 I print_info: model type       = 1.4B
0.00.051.139 I print_info: model params     = 1.41 B
0.00.051.143 I print_info: general.name     = 1.4B
0.00.051.144 I print_info: vocab type       = BPE
0.00.051.144 I print_info: n_vocab          = 50304
0.00.051.144 I print_info: n_merges         = 50009
0.00.051.145 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.145 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.145 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.145 I print_info: LF token         = 128 'Ä'
0.00.051.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.146 I print_info: max token length = 1024
0.00.053.023 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.023 I load_tensors: offloading output layer to GPU
0.00.053.023 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.034 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.035 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.311 I llama_init_from_model: n_seq_max     = 1
0.00.053.311 I llama_init_from_model: n_ctx         = 128
0.00.053.311 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.312 I llama_init_from_model: n_batch       = 128
0.00.053.312 I llama_init_from_model: n_ubatch      = 128
0.00.053.312 I llama_init_from_model: flash_attn    = 0
0.00.053.312 I llama_init_from_model: freq_base     = 10000.0
0.00.053.313 I llama_init_from_model: freq_scale    = 1
0.00.053.313 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.313 I ggml_metal_init: allocating
0.00.053.317 I ggml_metal_init: found device: Apple M4
0.00.053.319 I ggml_metal_init: picking default device: Apple M4
0.00.053.893 I ggml_metal_init: using embedded metal library
0.00.056.274 I ggml_metal_init: GPU name:   Apple M4
0.00.056.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.275 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.276 I ggml_metal_init: simdgroup reduction   = true
0.00.056.276 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.276 I ggml_metal_init: has bfloat            = true
0.00.056.276 I ggml_metal_init: use bfloat            = true
0.00.056.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.334 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.336 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.350 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.306 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.307 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.307 I llama_init_from_model: graph nodes  = 967
0.00.068.307 I llama_init_from_model: graph splits = 2
0.00.068.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.396.950 I 
0.00.396.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.396.993 I perplexity: tokenizing the input ..
0.00.404.627 I perplexity: tokenization took 7.632 ms
0.00.404.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.537.337 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.538.610 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.538.629 I llama_perf_context_print:        load time =     387.02 ms
0.00.538.630 I llama_perf_context_print: prompt eval time =     132.47 ms /   128 tokens (    1.03 ms per token,   966.28 tokens per second)
0.00.538.630 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.538.631 I llama_perf_context_print:       total time =     141.68 ms /   129 tokens
0.00.539.053 I ggml_metal_free: deallocating

real	0m0.554s
user	0m0.078s
sys	0m0.070s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.084 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.084 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.188 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.163 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.166 I llama_model_loader: - type  f32:  194 tensors
0.00.025.166 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.167 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.167 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.167 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.168 I print_info: file format = GGUF V3 (latest)
0.00.025.168 I print_info: file type   = Q3_K - Medium
0.00.025.169 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.449 I load: special tokens cache size = 25
0.00.050.568 I load: token to piece cache size = 0.2984 MB
0.00.050.571 I print_info: arch             = gptneox
0.00.050.571 I print_info: vocab_only       = 0
0.00.050.572 I print_info: n_ctx_train      = 2048
0.00.050.572 I print_info: n_embd           = 2048
0.00.050.572 I print_info: n_layer          = 24
0.00.050.575 I print_info: n_head           = 16
0.00.050.576 I print_info: n_head_kv        = 16
0.00.050.576 I print_info: n_rot            = 32
0.00.050.576 I print_info: n_swa            = 0
0.00.050.576 I print_info: n_embd_head_k    = 128
0.00.050.576 I print_info: n_embd_head_v    = 128
0.00.050.577 I print_info: n_gqa            = 1
0.00.050.578 I print_info: n_embd_k_gqa     = 2048
0.00.050.578 I print_info: n_embd_v_gqa     = 2048
0.00.050.579 I print_info: f_norm_eps       = 1.0e-05
0.00.050.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.580 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.580 I print_info: f_logit_scale    = 0.0e+00
0.00.050.581 I print_info: n_ff             = 8192
0.00.050.581 I print_info: n_expert         = 0
0.00.050.581 I print_info: n_expert_used    = 0
0.00.050.581 I print_info: causal attn      = 1
0.00.050.581 I print_info: pooling type     = 0
0.00.050.582 I print_info: rope type        = 2
0.00.050.582 I print_info: rope scaling     = linear
0.00.050.582 I print_info: freq_base_train  = 10000.0
0.00.050.582 I print_info: freq_scale_train = 1
0.00.050.583 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.583 I print_info: rope_finetuned   = unknown
0.00.050.583 I print_info: ssm_d_conv       = 0
0.00.050.583 I print_info: ssm_d_inner      = 0
0.00.050.583 I print_info: ssm_d_state      = 0
0.00.050.583 I print_info: ssm_dt_rank      = 0
0.00.050.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.587 I print_info: model type       = 1.4B
0.00.050.587 I print_info: model params     = 1.41 B
0.00.050.587 I print_info: general.name     = 1.4B
0.00.050.588 I print_info: vocab type       = BPE
0.00.050.588 I print_info: n_vocab          = 50304
0.00.050.588 I print_info: n_merges         = 50009
0.00.050.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.589 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.589 I print_info: LF token         = 128 'Ä'
0.00.050.593 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.593 I print_info: max token length = 1024
0.00.052.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.479 I load_tensors: offloading output layer to GPU
0.00.052.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.489 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.490 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.762 I llama_init_from_model: n_seq_max     = 1
0.00.052.762 I llama_init_from_model: n_ctx         = 128
0.00.052.762 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.763 I llama_init_from_model: n_batch       = 128
0.00.052.763 I llama_init_from_model: n_ubatch      = 128
0.00.052.763 I llama_init_from_model: flash_attn    = 0
0.00.052.763 I llama_init_from_model: freq_base     = 10000.0
0.00.052.764 I llama_init_from_model: freq_scale    = 1
0.00.052.764 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.764 I ggml_metal_init: allocating
0.00.052.769 I ggml_metal_init: found device: Apple M4
0.00.052.771 I ggml_metal_init: picking default device: Apple M4
0.00.053.374 I ggml_metal_init: using embedded metal library
0.00.055.714 I ggml_metal_init: GPU name:   Apple M4
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.716 I ggml_metal_init: simdgroup reduction   = true
0.00.055.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.716 I ggml_metal_init: has bfloat            = true
0.00.055.717 I ggml_metal_init: use bfloat            = true
0.00.055.717 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.078 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.388 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.401 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.274 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.275 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.276 I llama_init_from_model: graph nodes  = 967
0.00.067.276 I llama_init_from_model: graph splits = 2
0.00.067.277 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.422 I 
0.00.475.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.482 I perplexity: tokenizing the input ..
0.00.483.401 I perplexity: tokenization took 7.917 ms
0.00.483.410 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.661 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.809 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.823 I llama_perf_context_print:        load time =     466.55 ms
0.00.616.824 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.54 tokens per second)
0.00.616.825 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.826 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.617.151 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.078s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.435 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.436 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.438 I llama_model_loader: - type  f32:  194 tensors
0.00.024.439 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.439 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.439 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.440 I print_info: file format = GGUF V3 (latest)
0.00.024.440 I print_info: file type   = Q4_K - Medium
0.00.024.441 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.950 I load: special tokens cache size = 25
0.00.049.032 I load: token to piece cache size = 0.2984 MB
0.00.049.035 I print_info: arch             = gptneox
0.00.049.035 I print_info: vocab_only       = 0
0.00.049.035 I print_info: n_ctx_train      = 2048
0.00.049.035 I print_info: n_embd           = 2048
0.00.049.035 I print_info: n_layer          = 24
0.00.049.038 I print_info: n_head           = 16
0.00.049.039 I print_info: n_head_kv        = 16
0.00.049.039 I print_info: n_rot            = 32
0.00.049.039 I print_info: n_swa            = 0
0.00.049.040 I print_info: n_embd_head_k    = 128
0.00.049.040 I print_info: n_embd_head_v    = 128
0.00.049.040 I print_info: n_gqa            = 1
0.00.049.041 I print_info: n_embd_k_gqa     = 2048
0.00.049.042 I print_info: n_embd_v_gqa     = 2048
0.00.049.042 I print_info: f_norm_eps       = 1.0e-05
0.00.049.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.048 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.048 I print_info: f_logit_scale    = 0.0e+00
0.00.049.048 I print_info: n_ff             = 8192
0.00.049.049 I print_info: n_expert         = 0
0.00.049.049 I print_info: n_expert_used    = 0
0.00.049.049 I print_info: causal attn      = 1
0.00.049.051 I print_info: pooling type     = 0
0.00.049.051 I print_info: rope type        = 2
0.00.049.051 I print_info: rope scaling     = linear
0.00.049.052 I print_info: freq_base_train  = 10000.0
0.00.049.052 I print_info: freq_scale_train = 1
0.00.049.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.052 I print_info: rope_finetuned   = unknown
0.00.049.053 I print_info: ssm_d_conv       = 0
0.00.049.053 I print_info: ssm_d_inner      = 0
0.00.049.053 I print_info: ssm_d_state      = 0
0.00.049.053 I print_info: ssm_dt_rank      = 0
0.00.049.053 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.056 I print_info: model type       = 1.4B
0.00.049.056 I print_info: model params     = 1.41 B
0.00.049.056 I print_info: general.name     = 1.4B
0.00.049.057 I print_info: vocab type       = BPE
0.00.049.057 I print_info: n_vocab          = 50304
0.00.049.058 I print_info: n_merges         = 50009
0.00.049.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: LF token         = 128 'Ä'
0.00.049.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.060 I print_info: max token length = 1024
0.00.051.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.039 I load_tensors: offloading output layer to GPU
0.00.051.039 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.049 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.050 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.323 I llama_init_from_model: n_seq_max     = 1
0.00.051.324 I llama_init_from_model: n_ctx         = 128
0.00.051.324 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.324 I llama_init_from_model: n_batch       = 128
0.00.051.325 I llama_init_from_model: n_ubatch      = 128
0.00.051.325 I llama_init_from_model: flash_attn    = 0
0.00.051.325 I llama_init_from_model: freq_base     = 10000.0
0.00.051.325 I llama_init_from_model: freq_scale    = 1
0.00.051.326 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.326 I ggml_metal_init: allocating
0.00.051.329 I ggml_metal_init: found device: Apple M4
0.00.051.331 I ggml_metal_init: picking default device: Apple M4
0.00.051.913 I ggml_metal_init: using embedded metal library
0.00.054.262 I ggml_metal_init: GPU name:   Apple M4
0.00.054.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.265 I ggml_metal_init: simdgroup reduction   = true
0.00.054.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.265 I ggml_metal_init: has bfloat            = true
0.00.054.265 I ggml_metal_init: use bfloat            = true
0.00.054.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.266 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.123 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.079 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.080 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.081 I llama_init_from_model: graph nodes  = 967
0.00.066.081 I llama_init_from_model: graph splits = 2
0.00.066.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.522 I 
0.00.604.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.656 I perplexity: tokenizing the input ..
0.00.612.684 I perplexity: tokenization took 8.027 ms
0.00.612.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.118 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.748.288 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.748.307 I llama_perf_context_print:        load time =     595.77 ms
0.00.748.308 I llama_perf_context_print: prompt eval time =     134.19 ms /   128 tokens (    1.05 ms per token,   953.91 tokens per second)
0.00.748.314 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.314 I llama_perf_context_print:       total time =     143.79 ms /   129 tokens
0.00.748.874 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.982 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.527 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.528 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.528 I print_info: file format = GGUF V3 (latest)
0.00.025.529 I print_info: file type   = Q5_K - Medium
0.00.025.529 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.816 I load: special tokens cache size = 25
0.00.051.066 I load: token to piece cache size = 0.2984 MB
0.00.051.069 I print_info: arch             = gptneox
0.00.051.069 I print_info: vocab_only       = 0
0.00.051.070 I print_info: n_ctx_train      = 2048
0.00.051.070 I print_info: n_embd           = 2048
0.00.051.070 I print_info: n_layer          = 24
0.00.051.073 I print_info: n_head           = 16
0.00.051.073 I print_info: n_head_kv        = 16
0.00.051.074 I print_info: n_rot            = 32
0.00.051.074 I print_info: n_swa            = 0
0.00.051.074 I print_info: n_embd_head_k    = 128
0.00.051.074 I print_info: n_embd_head_v    = 128
0.00.051.075 I print_info: n_gqa            = 1
0.00.051.076 I print_info: n_embd_k_gqa     = 2048
0.00.051.076 I print_info: n_embd_v_gqa     = 2048
0.00.051.077 I print_info: f_norm_eps       = 1.0e-05
0.00.051.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.077 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.078 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.078 I print_info: f_logit_scale    = 0.0e+00
0.00.051.079 I print_info: n_ff             = 8192
0.00.051.079 I print_info: n_expert         = 0
0.00.051.079 I print_info: n_expert_used    = 0
0.00.051.079 I print_info: causal attn      = 1
0.00.051.079 I print_info: pooling type     = 0
0.00.051.079 I print_info: rope type        = 2
0.00.051.080 I print_info: rope scaling     = linear
0.00.051.080 I print_info: freq_base_train  = 10000.0
0.00.051.080 I print_info: freq_scale_train = 1
0.00.051.081 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.081 I print_info: rope_finetuned   = unknown
0.00.051.081 I print_info: ssm_d_conv       = 0
0.00.051.081 I print_info: ssm_d_inner      = 0
0.00.051.081 I print_info: ssm_d_state      = 0
0.00.051.081 I print_info: ssm_dt_rank      = 0
0.00.051.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.082 I print_info: model type       = 1.4B
0.00.051.084 I print_info: model params     = 1.41 B
0.00.051.084 I print_info: general.name     = 1.4B
0.00.051.085 I print_info: vocab type       = BPE
0.00.051.085 I print_info: n_vocab          = 50304
0.00.051.085 I print_info: n_merges         = 50009
0.00.051.085 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.086 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.086 I print_info: LF token         = 128 'Ä'
0.00.051.086 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.090 I print_info: max token length = 1024
0.00.053.048 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.048 I load_tensors: offloading output layer to GPU
0.00.053.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.059 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.060 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.360 I llama_init_from_model: n_seq_max     = 1
0.00.053.361 I llama_init_from_model: n_ctx         = 128
0.00.053.361 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.361 I llama_init_from_model: n_batch       = 128
0.00.053.361 I llama_init_from_model: n_ubatch      = 128
0.00.053.361 I llama_init_from_model: flash_attn    = 0
0.00.053.362 I llama_init_from_model: freq_base     = 10000.0
0.00.053.362 I llama_init_from_model: freq_scale    = 1
0.00.053.362 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.363 I ggml_metal_init: allocating
0.00.053.365 I ggml_metal_init: found device: Apple M4
0.00.053.367 I ggml_metal_init: picking default device: Apple M4
0.00.053.926 I ggml_metal_init: using embedded metal library
0.00.056.225 I ggml_metal_init: GPU name:   Apple M4
0.00.056.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.227 I ggml_metal_init: simdgroup reduction   = true
0.00.056.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.227 I ggml_metal_init: has bfloat            = true
0.00.056.227 I ggml_metal_init: use bfloat            = true
0.00.056.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.522 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.861 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.864 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.711 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.712 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.713 I llama_init_from_model: graph nodes  = 967
0.00.067.713 I llama_init_from_model: graph splits = 2
0.00.067.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.399 I 
0.00.685.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.604 I perplexity: tokenizing the input ..
0.00.701.987 I perplexity: tokenization took 16.376 ms
0.00.702.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.568 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.851.361 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.851.409 I llama_perf_context_print:        load time =     675.40 ms
0.00.851.412 I llama_perf_context_print: prompt eval time =     144.64 ms /   128 tokens (    1.13 ms per token,   884.98 tokens per second)
0.00.851.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.417 I llama_perf_context_print:       total time =     166.02 ms /   129 tokens
0.00.852.809 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.111s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.175 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.026.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.039.017 I llama_model_loader: - type  f32:  194 tensors
0.00.039.017 I llama_model_loader: - type q6_K:   98 tensors
0.00.039.018 I print_info: file format = GGUF V3 (latest)
0.00.039.018 I print_info: file type   = Q6_K
0.00.039.019 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.060.342 I load: special tokens cache size = 25
0.00.066.232 I load: token to piece cache size = 0.2984 MB
0.00.066.235 I print_info: arch             = gptneox
0.00.066.236 I print_info: vocab_only       = 0
0.00.066.236 I print_info: n_ctx_train      = 2048
0.00.066.236 I print_info: n_embd           = 2048
0.00.066.236 I print_info: n_layer          = 24
0.00.066.239 I print_info: n_head           = 16
0.00.066.239 I print_info: n_head_kv        = 16
0.00.066.239 I print_info: n_rot            = 32
0.00.066.240 I print_info: n_swa            = 0
0.00.066.240 I print_info: n_embd_head_k    = 128
0.00.066.242 I print_info: n_embd_head_v    = 128
0.00.066.243 I print_info: n_gqa            = 1
0.00.066.244 I print_info: n_embd_k_gqa     = 2048
0.00.066.245 I print_info: n_embd_v_gqa     = 2048
0.00.066.245 I print_info: f_norm_eps       = 1.0e-05
0.00.066.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.253 I print_info: f_logit_scale    = 0.0e+00
0.00.066.255 I print_info: n_ff             = 8192
0.00.066.256 I print_info: n_expert         = 0
0.00.066.256 I print_info: n_expert_used    = 0
0.00.066.256 I print_info: causal attn      = 1
0.00.066.256 I print_info: pooling type     = 0
0.00.066.256 I print_info: rope type        = 2
0.00.066.257 I print_info: rope scaling     = linear
0.00.066.257 I print_info: freq_base_train  = 10000.0
0.00.066.257 I print_info: freq_scale_train = 1
0.00.066.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.258 I print_info: rope_finetuned   = unknown
0.00.066.258 I print_info: ssm_d_conv       = 0
0.00.066.258 I print_info: ssm_d_inner      = 0
0.00.066.258 I print_info: ssm_d_state      = 0
0.00.066.258 I print_info: ssm_dt_rank      = 0
0.00.066.258 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.259 I print_info: model type       = 1.4B
0.00.066.259 I print_info: model params     = 1.41 B
0.00.066.260 I print_info: general.name     = 1.4B
0.00.066.260 I print_info: vocab type       = BPE
0.00.066.260 I print_info: n_vocab          = 50304
0.00.066.260 I print_info: n_merges         = 50009
0.00.066.261 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.261 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.261 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.261 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.261 I print_info: LF token         = 128 'Ä'
0.00.066.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.262 I print_info: max token length = 1024
0.00.068.240 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.240 I load_tensors: offloading output layer to GPU
0.00.068.240 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.251 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.068.252 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.068.520 I llama_init_from_model: n_seq_max     = 1
0.00.068.521 I llama_init_from_model: n_ctx         = 128
0.00.068.521 I llama_init_from_model: n_ctx_per_seq = 128
0.00.068.521 I llama_init_from_model: n_batch       = 128
0.00.068.522 I llama_init_from_model: n_ubatch      = 128
0.00.068.522 I llama_init_from_model: flash_attn    = 0
0.00.068.522 I llama_init_from_model: freq_base     = 10000.0
0.00.068.522 I llama_init_from_model: freq_scale    = 1
0.00.068.523 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.523 I ggml_metal_init: allocating
0.00.068.526 I ggml_metal_init: found device: Apple M4
0.00.068.528 I ggml_metal_init: picking default device: Apple M4
0.00.069.085 I ggml_metal_init: using embedded metal library
0.00.071.408 I ggml_metal_init: GPU name:   Apple M4
0.00.071.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.410 I ggml_metal_init: simdgroup reduction   = true
0.00.071.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.411 I ggml_metal_init: has bfloat            = true
0.00.071.411 I ggml_metal_init: use bfloat            = true
0.00.071.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.974 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.206 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.209 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.222 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.119 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.120 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.120 I llama_init_from_model: graph nodes  = 967
0.00.083.120 I llama_init_from_model: graph splits = 2
0.00.083.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.005 I 
0.00.571.057 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.081 I perplexity: tokenizing the input ..
0.00.581.614 I perplexity: tokenization took 10.531 ms
0.00.581.625 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.959 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.723.198 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.723.213 I llama_perf_context_print:        load time =     555.84 ms
0.00.723.214 I llama_perf_context_print: prompt eval time =     140.11 ms /   128 tokens (    1.09 ms per token,   913.57 tokens per second)
0.00.723.215 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.216 I llama_perf_context_print:       total time =     152.22 ms /   129 tokens
0.00.723.785 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.096s
sys	0m0.116s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4528 (c64d2bec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.022 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.599 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.611 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.612 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.623 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.604 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.555 I llama_model_loader: - type  f32:  194 tensors
0.00.052.556 I llama_model_loader: - type  f16:   98 tensors
0.00.052.556 I print_info: file format = GGUF V3 (latest)
0.00.052.557 I print_info: file type   = all F32 (guessed)
0.00.052.558 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.995 I load: special tokens cache size = 25
0.00.085.384 I load: token to piece cache size = 0.2984 MB
0.00.085.387 I print_info: arch             = gptneox
0.00.085.387 I print_info: vocab_only       = 0
0.00.085.387 I print_info: n_ctx_train      = 2048
0.00.085.387 I print_info: n_embd           = 2048
0.00.085.388 I print_info: n_layer          = 24
0.00.085.390 I print_info: n_head           = 16
0.00.085.391 I print_info: n_head_kv        = 16
0.00.085.393 I print_info: n_rot            = 32
0.00.085.393 I print_info: n_swa            = 0
0.00.085.394 I print_info: n_embd_head_k    = 128
0.00.085.394 I print_info: n_embd_head_v    = 128
0.00.085.394 I print_info: n_gqa            = 1
0.00.085.395 I print_info: n_embd_k_gqa     = 2048
0.00.085.396 I print_info: n_embd_v_gqa     = 2048
0.00.085.396 I print_info: f_norm_eps       = 1.0e-05
0.00.085.397 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.397 I print_info: f_logit_scale    = 0.0e+00
0.00.085.398 I print_info: n_ff             = 8192
0.00.085.398 I print_info: n_expert         = 0
0.00.085.398 I print_info: n_expert_used    = 0
0.00.085.398 I print_info: causal attn      = 1
0.00.085.398 I print_info: pooling type     = 0
0.00.085.398 I print_info: rope type        = 2
0.00.085.399 I print_info: rope scaling     = linear
0.00.085.399 I print_info: freq_base_train  = 10000.0
0.00.085.399 I print_info: freq_scale_train = 1
0.00.085.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.400 I print_info: rope_finetuned   = unknown
0.00.085.400 I print_info: ssm_d_conv       = 0
0.00.085.400 I print_info: ssm_d_inner      = 0
0.00.085.400 I print_info: ssm_d_state      = 0
0.00.085.400 I print_info: ssm_dt_rank      = 0
0.00.085.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.405 I print_info: model type       = 1.4B
0.00.085.406 I print_info: model params     = 1.41 B
0.00.085.406 I print_info: general.name     = 1.4B
0.00.085.407 I print_info: vocab type       = BPE
0.00.085.407 I print_info: n_vocab          = 50304
0.00.085.407 I print_info: n_merges         = 50009
0.00.085.407 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.408 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.408 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.408 I print_info: LF token         = 128 'Ä'
0.00.085.410 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.410 I print_info: max token length = 1024
0.00.087.853 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.853 I load_tensors: offloading output layer to GPU
0.00.087.853 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.864 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.865 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.133 I llama_init_from_model: n_seq_max     = 1
0.00.088.133 I llama_init_from_model: n_ctx         = 128
0.00.088.133 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.134 I llama_init_from_model: n_batch       = 128
0.00.088.134 I llama_init_from_model: n_ubatch      = 128
0.00.088.134 I llama_init_from_model: flash_attn    = 0
0.00.088.134 I llama_init_from_model: freq_base     = 10000.0
0.00.088.135 I llama_init_from_model: freq_scale    = 1
0.00.088.135 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.135 I ggml_metal_init: allocating
0.00.088.138 I ggml_metal_init: found device: Apple M4
0.00.088.140 I ggml_metal_init: picking default device: Apple M4
0.00.088.717 I ggml_metal_init: using embedded metal library
0.00.091.234 I ggml_metal_init: GPU name:   Apple M4
0.00.091.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.237 I ggml_metal_init: simdgroup reduction   = true
0.00.091.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.237 I ggml_metal_init: has bfloat            = true
0.00.091.237 I ggml_metal_init: use bfloat            = true
0.00.091.238 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.509 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.752 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.758 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.626 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.627 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.627 I llama_init_from_model: graph nodes  = 967
0.00.102.628 I llama_init_from_model: graph splits = 2
0.00.102.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.629 I 
0.00.102.663 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.664 I compute_imatrix: tokenizing the input ..
0.00.109.349 I compute_imatrix: tokenization took 6.684 ms
0.00.109.351 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.623.142 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.628.701 I llama_perf_context_print:        load time =    1602.11 ms
0.01.628.703 I llama_perf_context_print: prompt eval time =    1513.17 ms /   128 tokens (   11.82 ms per token,    84.59 tokens per second)
0.01.628.704 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.628.705 I llama_perf_context_print:       total time =    1607.67 ms /   129 tokens
0.01.629.322 I ggml_metal_free: deallocating

real	0m1.826s
user	0m0.165s
sys	0m0.228s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4528 (c64d2bec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105e04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105e04a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105e04e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105e052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105e05750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105e05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105e06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105e064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105e06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105e06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105e071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105e07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105e083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105e08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105e09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105e09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105e0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105e0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105e0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105e0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105e0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105e0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105e0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105e0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105e0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105e0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105e0e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105e0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105e0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105e0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105e10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105e10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105e10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105e11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105e11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105e11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105e11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105e12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105e124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105e12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105e12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105e13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105e13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105e13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105e14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105e14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105e150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105e15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105e159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105e15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105e16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105e16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105e17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105e17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105e17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105e18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105e18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105e19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105e19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105e1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105e1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105e1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105e1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105e1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105e1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105e1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105e1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105e1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105e1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105e1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105e1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105e1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105e1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105e1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105e1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105e20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105e20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105e212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105e21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105e21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105e223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105e22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105e234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105e23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105e14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105e24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105e25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105e25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105e26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105e26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105e26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105e27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105e27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105e28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105e28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105e28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105e294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105e29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105e2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105e2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105e2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105e2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105e2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105e2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105e2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105e2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105e2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105e2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105e2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105e2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105e2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105e2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105e30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105e30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105e30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105e31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105e31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105e31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105e32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105e32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105e32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105e33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105e34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105e34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105e34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105e35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105e35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105e35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105e36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105e36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105e36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105e37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105e37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105e37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105e38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105e38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105e39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105e39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105e39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105e3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105e3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105e3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105e3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105e3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105e3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105e3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105e3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105e3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105e3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105e3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105e3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105e3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105e3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105e40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105e40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105e40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105e41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105e41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105e41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105e42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105e42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105e42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105e43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105e435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105e43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105e44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105e446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105e44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105e452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105e458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105e460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105e46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105e47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105e47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105e480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105e48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105e48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105e491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105e49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105e49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105e4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105e4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105e4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105e4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105e4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105e4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105e4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105e4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105e4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105e4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105e4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105e4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105e4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105e4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105e4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105e4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105e50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105e506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105e50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105e51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105e516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105e51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105e52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105e526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105e52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105e53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105e53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105e53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105e54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105e54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105e55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105e55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105e55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105e56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105e56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105e56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105e57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105e57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105e57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105e580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105e58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105e58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105e590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105e59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105e59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105e5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105e5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105e5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105e5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105e5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105e5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105e5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105e5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105e5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105e5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105e5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105e5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105e5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105e5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105e5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105e5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105e5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105e5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105e5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105e5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105e600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105e60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105e60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105e61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105e61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105e62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105e62550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105e62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105e63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105e63610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.117.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115e0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115e0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115e0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115e0b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115e0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115e0bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115e0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115e0c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115e0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115e0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115e0d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115e0e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115e0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115e0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115e10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115e10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115e11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115e11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115e12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115e12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115e135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115e13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115e13f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115e14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115e146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115e14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115e14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115e15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115e15930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115e15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115e16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115e164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115e16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115e16db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115e17220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115e17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115e17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115e17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115e183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115e18850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115e18cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115e19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115e195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115e19a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115e19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115e1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115e1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115e1abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115e1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115e1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115e1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115e1bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115e1c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115e1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115e1d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115e1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115e1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115e1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115e1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115e1e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115e1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115e1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115e1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115e1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115e1fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115e201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115e20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115e20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115e20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x115e21370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115e217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115e21c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115e220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115e22530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115e229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115e22e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115e23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115e236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115e23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115e23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115e24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115e248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115e24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x115e25190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115e25600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115e25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115e25ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115e26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115e267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115e270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115e27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115e27980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115e27df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115e28260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115e286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x115e28b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115e28fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115e29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115e29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115e29d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115e2a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115e2aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x115e2aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115e2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115e2b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115e2bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x115e2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115e2c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115e2c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115e2cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115e2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115e2d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115e2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115e2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115e2e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115e2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115e2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115e2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115e2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115e2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115e2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115e30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115e30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115e30bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115e31060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115e314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115e31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115e31db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115e32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115e32690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115e32b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115e32f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115e333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115e33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115e33cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115e34130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115e345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115e34a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115e34e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115e352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115e35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115e35bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115e36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115e364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115e36920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115e36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115e37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115e37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115e37ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115e37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115e383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115e38830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115e38ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115e39110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115e39580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115e399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115e39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115e3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115e3a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115e3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115e3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115e3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115e3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115e3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115e3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115e3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115e3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115e3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115e3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115e3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115e3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115e3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115e3e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115e3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115e3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115e3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115e3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115e3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115e40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115e408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115e40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115e411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115e41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115e41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115e42370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115e427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115e42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115e430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115e43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115e439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115e43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115e44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115e446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115e44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115e44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115e45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115e45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115e45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115e46320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115e46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115e46c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115e47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115e47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115e481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115e48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115e48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115e48fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115e495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115e49b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115e4a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115e4a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115e4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115e4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115e4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115e4c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115e4c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115e4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115e4daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115e4e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115e4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115e4ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115e4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115e4f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115e502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115e508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115e50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115e51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115e519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115e51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115e52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115e52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115e530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115e536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115e53c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115e54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115e547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115e54da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115e55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115e55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115e564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115e56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115e57020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115e575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115e57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115e58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115e58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115e58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115e592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115e59860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115e59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115e5a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115e5a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115e5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115e5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115e5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115e5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115e5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115e5cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115e5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115e5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115e5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115e5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115e5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115e5e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115e5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115e5f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115e5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115e5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115e60260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115e60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115e60c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115e61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115e61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115e62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115e629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115e630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115e63390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115e63b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115e63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115e64450 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115f045f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115f04a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115f04ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115f05340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115f057b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115f05c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115f06090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115f06500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115f06970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115f06de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115f07250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115f078b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115f083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115f08b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115f09390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115f09ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115f0a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115f0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115f0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115f0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115f0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115f0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115f0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115f0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115f0db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115f0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115f0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115f0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115f0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115f0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115f0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115f0f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115f0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115f0ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115f10390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115f10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115f10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115f110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115f11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115f119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115f11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115f122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115f12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115f12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115f12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115f13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115f138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115f13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115f141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115f14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115f14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115f14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115f15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115f157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115f15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115f160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115f16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115f16b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115f16fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115f17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115f17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115f17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115f18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115f185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115f18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115f18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115f19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115f19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115f19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115f1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115f1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115f1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115f1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x115f1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115f1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115f1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115f1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115f1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115f1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115f1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115f1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115f1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115f1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115f1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115f1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115f1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x115f1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115f1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115f1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115f1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115f20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115f20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115f20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115f20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115f213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115f21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115f21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115f22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115f22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x115f22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115f22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115f232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115f23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115f23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115f242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115f24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115f24b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x115f24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115f25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115f258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115f25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x115f261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115f26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115f26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115f26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115f27370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115f277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115f27c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115f280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115f28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115f289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115f28e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115f29280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115f296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115f29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115f29fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115f2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115f2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115f2ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115f2b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115f2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115f2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115f2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115f2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115f2c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115f2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115f2d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115f2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115f2d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115f2ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115f2e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115f2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115f2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115f2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115f2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115f2f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115f2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115f30170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115f305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115f30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115f30ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115f31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115f317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115f31c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115f32080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115f324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115f32960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115f32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115f33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115f336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115f33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115f33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115f34400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115f34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115f34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115f35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115f355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115f35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115f35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115f36310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115f36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115f36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115f37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115f374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115f37940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115f37db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115f38220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115f38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115f38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115f38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115f393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115f39850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115f39cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115f3a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115f3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115f3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115f3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115f3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115f3b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115f3bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115f3c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115f3c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115f3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115f3cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115f3d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115f3d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115f3dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115f3df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115f3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115f3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115f3eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115f3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115f3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115f3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115f3fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115f402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115f40740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115f40bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115f41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115f41ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115f41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115f42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115f42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115f42a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115f42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115f432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115f43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115f43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115f44030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115f444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115f44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115f451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115f45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115f45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115f45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115f463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115f46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115f46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115f47100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115f47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115f479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115f47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115f482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115f48730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115f48ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115f49010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115f49480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115f498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115f49d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115f4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115f4a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115f4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115f4af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115f4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115f4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115f4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115f4c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115f4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115f4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115f4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115f4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115f4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115f4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115f4e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115f4ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115f4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115f4f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115f4fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115f4ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115f50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115f507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115f50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115f510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115f51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115f519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115f51e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115f52280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115f526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115f52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115f52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115f53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115f538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115f53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115f54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115f54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115f54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115f54ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115f55350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115f557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115f56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115f56950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115f57070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115f57790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115f57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115f57ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115f584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115f58ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.807s
user	0m0.271s
sys	0m0.250s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4528 (c64d2bec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13cf0c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13cf0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13cf0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13cf0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13cf0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13cf0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13cf0ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13cf0f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13cf0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13cf0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13cf101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13cf106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13cf111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13cf11990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13cf121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13cf128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13cf12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13cf13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13cf13e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13cf145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13cf14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13cf15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13cf15b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13cf163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13cf16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13cf16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13cf173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13cf18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13cf18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13cf18850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13cf18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13cf18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13cf19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13cf19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13cf1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13cf1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13cf1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13cf1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13cf1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13cf1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13cf1bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13cf1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13cf1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13cf1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13cf1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13cf1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13cf1d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13cf1e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13cf1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13cf1ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13cf1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13cf1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13cf20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13cf20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13cf20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13cf212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13cf21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13cf21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13cf22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13cf22830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13cf22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13cf22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13cf23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13cf238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13cf23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13cf24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13cf246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13cf24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13cf24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13cf25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13cf25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13cf25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13cf26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13cf267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13cf26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13cf27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13cf277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13cf27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13cf28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13cf287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13cf28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13cf29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13cf29790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13cf29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13cf2a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13cf2a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13cf2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13cf2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13cf2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13cf2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13cf2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13cf2c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13cf2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13cf2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13cf2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13cf2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13cf2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13cf1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13cf2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13cf2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13cf2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13cf2f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13cf2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13cf30350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13cf308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13cf30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13cf31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13cf31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13cf31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13cf32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13cf32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13cf32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13cf33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13cf337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13cf33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13cf34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13cf345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13cf34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13cf34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13cf35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13cf35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13cf35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13cf36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13cf36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13cf36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13cf36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13cf373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13cf37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13cf37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13cf381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13cf38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13cf38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13cf38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13cf39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13cf398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13cf39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13cf3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13cf3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13cf3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13cf3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13cf3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13cf3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13cf3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13cf3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13cf3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13cf3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13cf3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13cf3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13cf3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13cf3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13cf3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13cf3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13cf3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13cf3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13cf3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13cf3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13cf3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13cf40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13cf407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13cf40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13cf41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13cf415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13cf41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13cf41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13cf423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13cf42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13cf42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13cf43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13cf43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13cf43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13cf43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13cf44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13cf448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13cf44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13cf451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13cf45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13cf45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13cf45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13cf46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13cf46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13cf46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13cf47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13cf476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13cf47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13cf48020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13cf484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13cf48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13cf48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13cf492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13cf49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13cf49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13cf4a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13cf4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13cf4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13cf4afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13cf4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13cf4ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13cf4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13cf4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13cf4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13cf4cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13cf4d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13cf4dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13cf4dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13cf4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13cf4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13cf4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13cf4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13cf4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13cf50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13cf50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13cf50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13cf512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13cf51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13cf51d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13cf522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13cf52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13cf52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13cf532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13cf53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13cf53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13cf542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13cf54800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13cf54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13cf552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13cf557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13cf55d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13cf56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13cf567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13cf56d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13cf57280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13cf577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13cf57d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13cf58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13cf587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13cf58d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13cf59260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13cf597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13cf59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13cf5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13cf5a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13cf5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13cf5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13cf5b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13cf5bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13cf5c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13cf5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13cf5ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13cf5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13cf5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13cf5dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13cf5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13cf5e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13cf5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13cf5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13cf5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13cf5fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13cf601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13cf60740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13cf60c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13cf611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13cf61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13cf61c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13cf621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13cf62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13cf62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13cf631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13cf63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13cf63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13cf63fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13cf64440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13cf648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13cf64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13cf65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13cf656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13cf65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13cf66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13cf664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13cf66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13cf66de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13cf67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13cf67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13cf67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13cf68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13cf68ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13cf691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13cf698f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13cf69bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13cf6a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13cf6a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13cf6ac70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ce09660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ce09ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ce09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ce0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ce0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ce0ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ce0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ce0b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ce0b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ce0bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ce0c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ce0ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ce0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ce0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ce0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ce0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ce0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ce0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ce101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ce10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ce110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ce117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ce11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ce12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ce12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ce12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ce132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ce13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ce13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ce13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ce144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ce14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ce14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ce15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ce155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ce15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ce15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ce16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ce16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ce16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ce17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ce17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ce17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ce18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ce18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ce18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ce19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ce194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ce19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ce19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ce1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ce1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ce1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ce1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ce1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ce1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ce1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ce1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ce1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ce1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ce1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ce1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ce1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ce1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ce1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ce1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ce1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ce1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ce1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ce1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ce203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ce20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ce20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ce21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ce217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ce21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ce22260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ce227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ce22d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ce23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ce237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ce23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ce24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ce24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ce24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ce25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ce25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ce25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ce26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ce26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ce26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ce27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ce27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ce27cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ce28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ce28750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ce28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ce291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ce29740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ce29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ce2a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ce2a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ce2ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ce2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ce2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ce2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ce2c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ce2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ce2cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ce2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ce2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ce2dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ce2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ce2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ce2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ce2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ce2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ce2f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ce2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ce30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ce306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ce30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ce30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ce31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ce31920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ce31dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ce32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ce32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ce32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ce33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ce334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ce33980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ce33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ce342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ce34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ce34c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ce350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ce35540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ce359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ce35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ce36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ce367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ce36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ce37100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ce375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ce37a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ce37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ce38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ce38820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ce38cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ce39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ce39600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ce39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ce39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ce3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ce3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ce3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ce3b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ce3b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ce3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ce3bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ce3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ce3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ce3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ce3d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ce3d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ce3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ce3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ce3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ce3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ce3ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ce3f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ce3f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ce3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ce40060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ce40500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ce409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ce40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ce412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ce41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ce41c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ce420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ce42560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ce42a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ce42ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ce43340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ce437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ce43c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ce44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ce445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ce44a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ce44f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ce453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ce458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ce45e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ce46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ce468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ce46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ce471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ce477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ce47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ce485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ce48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ce48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ce49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ce49940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ce4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ce4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ce4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ce4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ce4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ce4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ce4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ce4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ce4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ce4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ce4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ce4dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ce4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ce4e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ce4ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ce4f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ce4f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ce4fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ce50120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ce50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ce50bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ce51110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ce51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ce51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ce52100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ce52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ce52ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ce530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ce53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ce53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ce540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ce54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ce54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ce550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ce55620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ce55b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ce560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ce56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ce56b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ce570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ce57600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ce57b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ce580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ce585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ce58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ce59090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ce595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ce59b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ce5a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ce5a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ce5ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ce5b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ce5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ce5bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ce5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ce5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ce5cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ce5d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ce5d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ce5daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ce5e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ce5e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ce5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ce5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ce5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ce5f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ce5fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ce600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ce60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ce609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ce60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ce61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ce617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ce61c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ce62100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ce625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ce62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ce63210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ce63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ce64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ce64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ce64a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ce65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ce654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ce65af0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e0224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e0253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e0269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e0272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e0291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e02a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e02a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e02ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e02b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e02b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e02c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e02c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e02cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e02d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e02d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e02dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e02e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e02e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e02eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e02ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e02f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e02f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e02fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e0300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e0309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e0328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e0331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e0350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e0366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e0568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e0576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e0579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e058a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.244s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
