Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.874s
sys	0m1.206s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-log
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-chat-template
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-gguf
[ 64%] Built target test-autorelease
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Built target test-model-load-cancel
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-bench
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-quantize
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 93%] Built target llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.122s
user	0m6.111s
sys	0m9.783s

main: quantize time =  4416.96 ms
main:    total time =  4416.96 ms

main: quantize time =  3767.31 ms
main:    total time =  3767.31 ms

main: quantize time =  3119.05 ms
main:    total time =  3119.05 ms

main: quantize time =  2359.22 ms
main:    total time =  2359.22 ms

main: quantize time =  2905.54 ms
main:    total time =  2905.54 ms

main: quantize time =  6154.08 ms
main:    total time =  6154.08 ms

main: quantize time =  5870.97 ms
main:    total time =  5870.97 ms

main: quantize time =  6762.64 ms
main:    total time =  6762.64 ms

main: quantize time =  6028.13 ms
main:    total time =  6028.13 ms

main: quantize time =  4632.50 ms
main:    total time =  4632.50 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.315 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.043.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.056.517 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.546 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.694 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.695 I llama_model_loader: - type  f32:  194 tensors
0.00.073.696 I llama_model_loader: - type  f16:   98 tensors
0.00.073.707 I print_info: file format = GGUF V3 (latest)
0.00.073.709 I print_info: file type   = all F32 (guessed)
0.00.073.711 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.108.965 I load: special tokens cache size = 25
0.00.116.606 I load: token to piece cache size = 0.2984 MB
0.00.116.609 I print_info: arch             = gptneox
0.00.116.609 I print_info: vocab_only       = 0
0.00.116.609 I print_info: n_ctx_train      = 2048
0.00.116.610 I print_info: n_embd           = 2048
0.00.116.610 I print_info: n_layer          = 24
0.00.116.613 I print_info: n_head           = 16
0.00.116.614 I print_info: n_head_kv        = 16
0.00.116.614 I print_info: n_rot            = 32
0.00.116.614 I print_info: n_swa            = 0
0.00.116.615 I print_info: n_embd_head_k    = 128
0.00.116.615 I print_info: n_embd_head_v    = 128
0.00.116.616 I print_info: n_gqa            = 1
0.00.116.616 I print_info: n_embd_k_gqa     = 2048
0.00.116.617 I print_info: n_embd_v_gqa     = 2048
0.00.116.618 I print_info: f_norm_eps       = 1.0e-05
0.00.116.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.619 I print_info: f_logit_scale    = 0.0e+00
0.00.116.619 I print_info: n_ff             = 8192
0.00.116.620 I print_info: n_expert         = 0
0.00.116.620 I print_info: n_expert_used    = 0
0.00.116.620 I print_info: causal attn      = 1
0.00.116.620 I print_info: pooling type     = 0
0.00.116.620 I print_info: rope type        = 2
0.00.116.620 I print_info: rope scaling     = linear
0.00.116.621 I print_info: freq_base_train  = 10000.0
0.00.116.623 I print_info: freq_scale_train = 1
0.00.116.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.623 I print_info: rope_finetuned   = unknown
0.00.116.624 I print_info: ssm_d_conv       = 0
0.00.116.624 I print_info: ssm_d_inner      = 0
0.00.116.624 I print_info: ssm_d_state      = 0
0.00.116.624 I print_info: ssm_dt_rank      = 0
0.00.116.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.624 I print_info: model type       = 1.4B
0.00.116.625 I print_info: model params     = 1.41 B
0.00.116.625 I print_info: general.name     = 1.4B
0.00.116.625 I print_info: vocab type       = BPE
0.00.116.625 I print_info: n_vocab          = 50304
0.00.116.626 I print_info: n_merges         = 50009
0.00.116.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.626 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.626 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.627 I print_info: LF token         = 128 'Ä'
0.00.116.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.627 I print_info: max token length = 1024
0.00.119.398 I load_tensors: offloading 24 repeating layers to GPU
0.00.119.398 I load_tensors: offloading output layer to GPU
0.00.119.398 I load_tensors: offloaded 25/25 layers to GPU
0.00.119.418 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.119.419 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.119.737 I llama_init_from_model: n_seq_max     = 1
0.00.119.738 I llama_init_from_model: n_ctx         = 2048
0.00.119.738 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.119.739 I llama_init_from_model: n_batch       = 2048
0.00.119.739 I llama_init_from_model: n_ubatch      = 512
0.00.119.739 I llama_init_from_model: flash_attn    = 0
0.00.119.739 I llama_init_from_model: freq_base     = 10000.0
0.00.119.740 I llama_init_from_model: freq_scale    = 1
0.00.119.740 I ggml_metal_init: allocating
0.00.119.743 I ggml_metal_init: found device: Apple M4
0.00.119.746 I ggml_metal_init: picking default device: Apple M4
0.00.120.489 I ggml_metal_init: using embedded metal library
0.00.132.182 I ggml_metal_init: GPU name:   Apple M4
0.00.132.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.132.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.132.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.132.186 I ggml_metal_init: simdgroup reduction   = true
0.00.132.186 I ggml_metal_init: simdgroup matrix mul. = true
0.00.132.186 I ggml_metal_init: has bfloat            = true
0.00.132.186 I ggml_metal_init: use bfloat            = true
0.00.132.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.132.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.159.478 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.182.881 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.182.887 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.182.909 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.183.920 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.183.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.183.923 I llama_init_from_model: graph nodes  = 967
0.00.183.923 I llama_init_from_model: graph splits = 2
0.00.183.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.184.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.184.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.468 I main: llama threadpool init, n_threads = 4
0.00.268.510 I 
0.00.268.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.540 I 
0.00.268.602 I sampler seed: 1234
0.00.268.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.631 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.633 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.633 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.112.368 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.02.112.369 I llama_perf_context_print:        load time =     223.58 ms
0.02.112.370 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.57 tokens per second)
0.02.112.371 I llama_perf_context_print:        eval time =    1797.29 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.112.372 I llama_perf_context_print:       total time =    1844.89 ms /    70 tokens
0.02.112.581 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.151s
sys	0m0.110s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.115 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.116 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.116 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.117 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.117 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.118 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.118 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.121 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.394 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.396 I llama_model_loader: - type  f32:  194 tensors
0.00.035.396 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.397 I print_info: file format = GGUF V3 (latest)
0.00.035.398 I print_info: file type   = Q8_0
0.00.035.399 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.681 I load: special tokens cache size = 25
0.00.060.780 I load: token to piece cache size = 0.2984 MB
0.00.060.785 I print_info: arch             = gptneox
0.00.060.785 I print_info: vocab_only       = 0
0.00.060.785 I print_info: n_ctx_train      = 2048
0.00.060.785 I print_info: n_embd           = 2048
0.00.060.786 I print_info: n_layer          = 24
0.00.060.793 I print_info: n_head           = 16
0.00.060.794 I print_info: n_head_kv        = 16
0.00.060.794 I print_info: n_rot            = 32
0.00.060.795 I print_info: n_swa            = 0
0.00.060.795 I print_info: n_embd_head_k    = 128
0.00.060.795 I print_info: n_embd_head_v    = 128
0.00.060.796 I print_info: n_gqa            = 1
0.00.060.796 I print_info: n_embd_k_gqa     = 2048
0.00.060.797 I print_info: n_embd_v_gqa     = 2048
0.00.060.798 I print_info: f_norm_eps       = 1.0e-05
0.00.060.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.799 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.799 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.800 I print_info: f_logit_scale    = 0.0e+00
0.00.060.800 I print_info: n_ff             = 8192
0.00.060.801 I print_info: n_expert         = 0
0.00.060.801 I print_info: n_expert_used    = 0
0.00.060.801 I print_info: causal attn      = 1
0.00.060.801 I print_info: pooling type     = 0
0.00.060.801 I print_info: rope type        = 2
0.00.060.802 I print_info: rope scaling     = linear
0.00.060.802 I print_info: freq_base_train  = 10000.0
0.00.060.802 I print_info: freq_scale_train = 1
0.00.060.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.803 I print_info: rope_finetuned   = unknown
0.00.060.803 I print_info: ssm_d_conv       = 0
0.00.060.803 I print_info: ssm_d_inner      = 0
0.00.060.804 I print_info: ssm_d_state      = 0
0.00.060.804 I print_info: ssm_dt_rank      = 0
0.00.060.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.804 I print_info: model type       = 1.4B
0.00.060.804 I print_info: model params     = 1.41 B
0.00.060.805 I print_info: general.name     = 1.4B
0.00.060.806 I print_info: vocab type       = BPE
0.00.060.806 I print_info: n_vocab          = 50304
0.00.060.807 I print_info: n_merges         = 50009
0.00.060.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.808 I print_info: LF token         = 128 'Ä'
0.00.060.808 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.808 I print_info: max token length = 1024
0.00.062.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.866 I load_tensors: offloading output layer to GPU
0.00.062.866 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.873 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.874 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.182 I llama_init_from_model: n_seq_max     = 1
0.00.063.183 I llama_init_from_model: n_ctx         = 2048
0.00.063.183 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.063.183 I llama_init_from_model: n_batch       = 2048
0.00.063.184 I llama_init_from_model: n_ubatch      = 512
0.00.063.184 I llama_init_from_model: flash_attn    = 0
0.00.063.184 I llama_init_from_model: freq_base     = 10000.0
0.00.063.184 I llama_init_from_model: freq_scale    = 1
0.00.063.185 I ggml_metal_init: allocating
0.00.063.187 I ggml_metal_init: found device: Apple M4
0.00.063.189 I ggml_metal_init: picking default device: Apple M4
0.00.063.928 I ggml_metal_init: using embedded metal library
0.00.066.505 I ggml_metal_init: GPU name:   Apple M4
0.00.066.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.508 I ggml_metal_init: simdgroup reduction   = true
0.00.066.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.508 I ggml_metal_init: has bfloat            = true
0.00.066.508 I ggml_metal_init: use bfloat            = true
0.00.066.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.886 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.861 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.895 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.186 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.188 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.189 I llama_init_from_model: graph nodes  = 967
0.00.102.189 I llama_init_from_model: graph splits = 2
0.00.102.193 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.322 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.144.134 I main: llama threadpool init, n_threads = 4
0.01.144.167 I 
0.01.144.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.144.188 I 
0.01.144.409 I sampler seed: 1234
0.01.144.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.144.425 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.144.425 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.144.425 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.234.803 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61846.69 tokens per second)
0.02.234.804 I llama_perf_context_print:        load time =    1133.56 ms
0.02.234.805 I llama_perf_context_print: prompt eval time =      43.44 ms /     7 tokens (    6.21 ms per token,   161.16 tokens per second)
0.02.234.805 I llama_perf_context_print:        eval time =    1044.09 ms /    63 runs   (   16.57 ms per token,    60.34 tokens per second)
0.02.234.806 I llama_perf_context_print:       total time =    1091.52 ms /    70 tokens
0.02.235.031 I ggml_metal_free: deallocating

real	0m2.253s
user	0m0.112s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.025.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.041.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.292 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.832 I llama_model_loader: - type  f32:  194 tensors
0.00.051.832 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.832 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.833 I print_info: file format = GGUF V3 (latest)
0.00.051.834 I print_info: file type   = Q4_0
0.00.051.835 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.078.722 I load: special tokens cache size = 25
0.00.089.439 I load: token to piece cache size = 0.2984 MB
0.00.089.444 I print_info: arch             = gptneox
0.00.089.444 I print_info: vocab_only       = 0
0.00.089.444 I print_info: n_ctx_train      = 2048
0.00.089.444 I print_info: n_embd           = 2048
0.00.089.445 I print_info: n_layer          = 24
0.00.089.449 I print_info: n_head           = 16
0.00.089.450 I print_info: n_head_kv        = 16
0.00.089.450 I print_info: n_rot            = 32
0.00.089.451 I print_info: n_swa            = 0
0.00.089.451 I print_info: n_embd_head_k    = 128
0.00.089.451 I print_info: n_embd_head_v    = 128
0.00.089.452 I print_info: n_gqa            = 1
0.00.089.453 I print_info: n_embd_k_gqa     = 2048
0.00.089.454 I print_info: n_embd_v_gqa     = 2048
0.00.089.455 I print_info: f_norm_eps       = 1.0e-05
0.00.089.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.456 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.456 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.456 I print_info: f_logit_scale    = 0.0e+00
0.00.089.457 I print_info: n_ff             = 8192
0.00.089.457 I print_info: n_expert         = 0
0.00.089.458 I print_info: n_expert_used    = 0
0.00.089.458 I print_info: causal attn      = 1
0.00.089.458 I print_info: pooling type     = 0
0.00.089.458 I print_info: rope type        = 2
0.00.089.460 I print_info: rope scaling     = linear
0.00.089.461 I print_info: freq_base_train  = 10000.0
0.00.089.461 I print_info: freq_scale_train = 1
0.00.089.461 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.461 I print_info: rope_finetuned   = unknown
0.00.089.462 I print_info: ssm_d_conv       = 0
0.00.089.462 I print_info: ssm_d_inner      = 0
0.00.089.462 I print_info: ssm_d_state      = 0
0.00.089.462 I print_info: ssm_dt_rank      = 0
0.00.089.463 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.463 I print_info: model type       = 1.4B
0.00.089.463 I print_info: model params     = 1.41 B
0.00.089.464 I print_info: general.name     = 1.4B
0.00.089.464 I print_info: vocab type       = BPE
0.00.089.465 I print_info: n_vocab          = 50304
0.00.089.465 I print_info: n_merges         = 50009
0.00.089.465 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.466 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.466 I print_info: LF token         = 128 'Ä'
0.00.089.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.467 I print_info: max token length = 1024
0.00.092.481 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.481 I load_tensors: offloading output layer to GPU
0.00.092.482 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.495 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.495 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.092.986 I llama_init_from_model: n_seq_max     = 1
0.00.092.988 I llama_init_from_model: n_ctx         = 2048
0.00.092.988 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.092.988 I llama_init_from_model: n_batch       = 2048
0.00.092.989 I llama_init_from_model: n_ubatch      = 512
0.00.092.989 I llama_init_from_model: flash_attn    = 0
0.00.092.990 I llama_init_from_model: freq_base     = 10000.0
0.00.092.990 I llama_init_from_model: freq_scale    = 1
0.00.092.991 I ggml_metal_init: allocating
0.00.092.996 I ggml_metal_init: found device: Apple M4
0.00.092.999 I ggml_metal_init: picking default device: Apple M4
0.00.093.961 I ggml_metal_init: using embedded metal library
0.00.097.868 I ggml_metal_init: GPU name:   Apple M4
0.00.097.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.873 I ggml_metal_init: simdgroup reduction   = true
0.00.097.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.873 I ggml_metal_init: has bfloat            = true
0.00.097.873 I ggml_metal_init: use bfloat            = true
0.00.097.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.405 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.137.635 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.137.643 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.137.669 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.138.704 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.138.706 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.138.706 I llama_init_from_model: graph nodes  = 967
0.00.138.706 I llama_init_from_model: graph splits = 2
0.00.138.710 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.138.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.003 I main: llama threadpool init, n_threads = 4
0.00.851.042 I 
0.00.851.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.073 I 
0.00.851.328 I sampler seed: 1234
0.00.851.334 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.851.377 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.851.380 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.851.380 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.531.274 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62500.00 tokens per second)
0.01.531.275 I llama_perf_context_print:        load time =     825.05 ms
0.01.531.276 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.43 tokens per second)
0.01.531.277 I llama_perf_context_print:        eval time =     632.55 ms /    63 runs   (   10.04 ms per token,    99.60 tokens per second)
0.01.531.277 I llama_perf_context_print:       total time =     681.20 ms /    70 tokens
0.01.531.503 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.133s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.354 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.356 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.357 I llama_model_loader: - type  f32:  194 tensors
0.00.030.358 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.358 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.359 I print_info: file format = GGUF V3 (latest)
0.00.030.359 I print_info: file type   = Q4_1
0.00.030.360 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.705 I load: special tokens cache size = 25
0.00.055.675 I load: token to piece cache size = 0.2984 MB
0.00.055.678 I print_info: arch             = gptneox
0.00.055.678 I print_info: vocab_only       = 0
0.00.055.678 I print_info: n_ctx_train      = 2048
0.00.055.678 I print_info: n_embd           = 2048
0.00.055.678 I print_info: n_layer          = 24
0.00.055.682 I print_info: n_head           = 16
0.00.055.683 I print_info: n_head_kv        = 16
0.00.055.683 I print_info: n_rot            = 32
0.00.055.683 I print_info: n_swa            = 0
0.00.055.683 I print_info: n_embd_head_k    = 128
0.00.055.683 I print_info: n_embd_head_v    = 128
0.00.055.684 I print_info: n_gqa            = 1
0.00.055.685 I print_info: n_embd_k_gqa     = 2048
0.00.055.688 I print_info: n_embd_v_gqa     = 2048
0.00.055.689 I print_info: f_norm_eps       = 1.0e-05
0.00.055.689 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.689 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.689 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.689 I print_info: f_logit_scale    = 0.0e+00
0.00.055.690 I print_info: n_ff             = 8192
0.00.055.690 I print_info: n_expert         = 0
0.00.055.691 I print_info: n_expert_used    = 0
0.00.055.691 I print_info: causal attn      = 1
0.00.055.691 I print_info: pooling type     = 0
0.00.055.693 I print_info: rope type        = 2
0.00.055.694 I print_info: rope scaling     = linear
0.00.055.695 I print_info: freq_base_train  = 10000.0
0.00.055.695 I print_info: freq_scale_train = 1
0.00.055.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.695 I print_info: rope_finetuned   = unknown
0.00.055.696 I print_info: ssm_d_conv       = 0
0.00.055.696 I print_info: ssm_d_inner      = 0
0.00.055.696 I print_info: ssm_d_state      = 0
0.00.055.696 I print_info: ssm_dt_rank      = 0
0.00.055.696 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.697 I print_info: model type       = 1.4B
0.00.055.699 I print_info: model params     = 1.41 B
0.00.055.699 I print_info: general.name     = 1.4B
0.00.055.700 I print_info: vocab type       = BPE
0.00.055.700 I print_info: n_vocab          = 50304
0.00.055.700 I print_info: n_merges         = 50009
0.00.055.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.704 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.705 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.705 I print_info: LF token         = 128 'Ä'
0.00.055.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.705 I print_info: max token length = 1024
0.00.057.727 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.727 I load_tensors: offloading output layer to GPU
0.00.057.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.738 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.057.739 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.058.017 I llama_init_from_model: n_seq_max     = 1
0.00.058.018 I llama_init_from_model: n_ctx         = 2048
0.00.058.018 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.018 I llama_init_from_model: n_batch       = 2048
0.00.058.019 I llama_init_from_model: n_ubatch      = 512
0.00.058.019 I llama_init_from_model: flash_attn    = 0
0.00.058.019 I llama_init_from_model: freq_base     = 10000.0
0.00.058.019 I llama_init_from_model: freq_scale    = 1
0.00.058.020 I ggml_metal_init: allocating
0.00.058.023 I ggml_metal_init: found device: Apple M4
0.00.058.025 I ggml_metal_init: picking default device: Apple M4
0.00.058.622 I ggml_metal_init: using embedded metal library
0.00.060.977 I ggml_metal_init: GPU name:   Apple M4
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.979 I ggml_metal_init: simdgroup reduction   = true
0.00.060.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.979 I ggml_metal_init: has bfloat            = true
0.00.060.980 I ggml_metal_init: use bfloat            = true
0.00.060.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.817 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.434 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.443 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.482 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.567 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.569 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.569 I llama_init_from_model: graph nodes  = 967
0.00.092.569 I llama_init_from_model: graph splits = 2
0.00.092.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.591 I main: llama threadpool init, n_threads = 4
0.00.721.627 I 
0.00.721.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.651 I 
0.00.721.884 I sampler seed: 1234
0.00.721.889 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.928 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.929 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.438.350 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67490.49 tokens per second)
0.01.438.350 I llama_perf_context_print:        load time =     710.77 ms
0.01.438.351 I llama_perf_context_print: prompt eval time =      39.49 ms /     7 tokens (    5.64 ms per token,   177.25 tokens per second)
0.01.438.352 I llama_perf_context_print:        eval time =     674.20 ms /    63 runs   (   10.70 ms per token,    93.44 tokens per second)
0.01.438.356 I llama_perf_context_print:       total time =     717.63 ms /    70 tokens
0.01.438.602 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.111s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.999 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.999 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.000 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.001 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.002 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.911 I llama_model_loader: - type  f32:  194 tensors
0.00.026.911 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.911 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.912 I print_info: file format = GGUF V3 (latest)
0.00.026.912 I print_info: file type   = Q5_0
0.00.026.913 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.260 I load: special tokens cache size = 25
0.00.052.184 I load: token to piece cache size = 0.2984 MB
0.00.052.187 I print_info: arch             = gptneox
0.00.052.188 I print_info: vocab_only       = 0
0.00.052.188 I print_info: n_ctx_train      = 2048
0.00.052.188 I print_info: n_embd           = 2048
0.00.052.188 I print_info: n_layer          = 24
0.00.052.191 I print_info: n_head           = 16
0.00.052.192 I print_info: n_head_kv        = 16
0.00.052.192 I print_info: n_rot            = 32
0.00.052.194 I print_info: n_swa            = 0
0.00.052.194 I print_info: n_embd_head_k    = 128
0.00.052.194 I print_info: n_embd_head_v    = 128
0.00.052.195 I print_info: n_gqa            = 1
0.00.052.196 I print_info: n_embd_k_gqa     = 2048
0.00.052.197 I print_info: n_embd_v_gqa     = 2048
0.00.052.197 I print_info: f_norm_eps       = 1.0e-05
0.00.052.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.198 I print_info: f_logit_scale    = 0.0e+00
0.00.052.199 I print_info: n_ff             = 8192
0.00.052.199 I print_info: n_expert         = 0
0.00.052.199 I print_info: n_expert_used    = 0
0.00.052.199 I print_info: causal attn      = 1
0.00.052.205 I print_info: pooling type     = 0
0.00.052.207 I print_info: rope type        = 2
0.00.052.209 I print_info: rope scaling     = linear
0.00.052.209 I print_info: freq_base_train  = 10000.0
0.00.052.210 I print_info: freq_scale_train = 1
0.00.052.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.210 I print_info: rope_finetuned   = unknown
0.00.052.210 I print_info: ssm_d_conv       = 0
0.00.052.210 I print_info: ssm_d_inner      = 0
0.00.052.210 I print_info: ssm_d_state      = 0
0.00.052.211 I print_info: ssm_dt_rank      = 0
0.00.052.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.211 I print_info: model type       = 1.4B
0.00.052.212 I print_info: model params     = 1.41 B
0.00.052.213 I print_info: general.name     = 1.4B
0.00.052.213 I print_info: vocab type       = BPE
0.00.052.213 I print_info: n_vocab          = 50304
0.00.052.213 I print_info: n_merges         = 50009
0.00.052.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.214 I print_info: LF token         = 128 'Ä'
0.00.052.215 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.215 I print_info: max token length = 1024
0.00.054.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.103 I load_tensors: offloading output layer to GPU
0.00.054.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.113 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.114 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.381 I llama_init_from_model: n_seq_max     = 1
0.00.054.381 I llama_init_from_model: n_ctx         = 2048
0.00.054.382 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.382 I llama_init_from_model: n_batch       = 2048
0.00.054.382 I llama_init_from_model: n_ubatch      = 512
0.00.054.382 I llama_init_from_model: flash_attn    = 0
0.00.054.382 I llama_init_from_model: freq_base     = 10000.0
0.00.054.383 I llama_init_from_model: freq_scale    = 1
0.00.054.383 I ggml_metal_init: allocating
0.00.054.386 I ggml_metal_init: found device: Apple M4
0.00.054.388 I ggml_metal_init: picking default device: Apple M4
0.00.054.968 I ggml_metal_init: using embedded metal library
0.00.057.317 I ggml_metal_init: GPU name:   Apple M4
0.00.057.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.320 I ggml_metal_init: simdgroup reduction   = true
0.00.057.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.320 I ggml_metal_init: has bfloat            = true
0.00.057.320 I ggml_metal_init: use bfloat            = true
0.00.057.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.767 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.987 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.994 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.021 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.088 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.089 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.090 I llama_init_from_model: graph nodes  = 967
0.00.087.090 I llama_init_from_model: graph splits = 2
0.00.087.093 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.306 I main: llama threadpool init, n_threads = 4
0.00.738.343 I 
0.00.738.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.375 I 
0.00.738.604 I sampler seed: 1234
0.00.738.608 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.674 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.678 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.678 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.518.225 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.518.226 I llama_perf_context_print:        load time =     727.36 ms
0.01.518.227 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.31 tokens per second)
0.01.518.227 I llama_perf_context_print:        eval time =     733.48 ms /    63 runs   (   11.64 ms per token,    85.89 tokens per second)
0.01.518.228 I llama_perf_context_print:       total time =     780.77 ms /    70 tokens
0.01.518.497 I ggml_metal_free: deallocating

real	0m1.538s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.342 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.958 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.965 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.974 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.773 I llama_model_loader: - type  f32:  194 tensors
0.00.025.774 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.775 I print_info: file format = GGUF V3 (latest)
0.00.025.775 I print_info: file type   = Q5_1
0.00.025.776 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.983 I load: special tokens cache size = 25
0.00.051.085 I load: token to piece cache size = 0.2984 MB
0.00.051.088 I print_info: arch             = gptneox
0.00.051.088 I print_info: vocab_only       = 0
0.00.051.088 I print_info: n_ctx_train      = 2048
0.00.051.089 I print_info: n_embd           = 2048
0.00.051.089 I print_info: n_layer          = 24
0.00.051.092 I print_info: n_head           = 16
0.00.051.092 I print_info: n_head_kv        = 16
0.00.051.092 I print_info: n_rot            = 32
0.00.051.093 I print_info: n_swa            = 0
0.00.051.093 I print_info: n_embd_head_k    = 128
0.00.051.093 I print_info: n_embd_head_v    = 128
0.00.051.094 I print_info: n_gqa            = 1
0.00.051.094 I print_info: n_embd_k_gqa     = 2048
0.00.051.095 I print_info: n_embd_v_gqa     = 2048
0.00.051.096 I print_info: f_norm_eps       = 1.0e-05
0.00.051.096 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.098 I print_info: f_logit_scale    = 0.0e+00
0.00.051.099 I print_info: n_ff             = 8192
0.00.051.099 I print_info: n_expert         = 0
0.00.051.099 I print_info: n_expert_used    = 0
0.00.051.099 I print_info: causal attn      = 1
0.00.051.099 I print_info: pooling type     = 0
0.00.051.101 I print_info: rope type        = 2
0.00.051.103 I print_info: rope scaling     = linear
0.00.051.103 I print_info: freq_base_train  = 10000.0
0.00.051.104 I print_info: freq_scale_train = 1
0.00.051.104 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.104 I print_info: rope_finetuned   = unknown
0.00.051.104 I print_info: ssm_d_conv       = 0
0.00.051.104 I print_info: ssm_d_inner      = 0
0.00.051.104 I print_info: ssm_d_state      = 0
0.00.051.104 I print_info: ssm_dt_rank      = 0
0.00.051.105 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.105 I print_info: model type       = 1.4B
0.00.051.109 I print_info: model params     = 1.41 B
0.00.051.109 I print_info: general.name     = 1.4B
0.00.051.110 I print_info: vocab type       = BPE
0.00.051.110 I print_info: n_vocab          = 50304
0.00.051.110 I print_info: n_merges         = 50009
0.00.051.111 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.111 I print_info: LF token         = 128 'Ä'
0.00.051.112 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.112 I print_info: max token length = 1024
0.00.053.139 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.140 I load_tensors: offloading output layer to GPU
0.00.053.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.151 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.152 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.423 I llama_init_from_model: n_seq_max     = 1
0.00.053.424 I llama_init_from_model: n_ctx         = 2048
0.00.053.424 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.424 I llama_init_from_model: n_batch       = 2048
0.00.053.425 I llama_init_from_model: n_ubatch      = 512
0.00.053.425 I llama_init_from_model: flash_attn    = 0
0.00.053.425 I llama_init_from_model: freq_base     = 10000.0
0.00.053.425 I llama_init_from_model: freq_scale    = 1
0.00.053.426 I ggml_metal_init: allocating
0.00.053.429 I ggml_metal_init: found device: Apple M4
0.00.053.431 I ggml_metal_init: picking default device: Apple M4
0.00.054.031 I ggml_metal_init: using embedded metal library
0.00.056.349 I ggml_metal_init: GPU name:   Apple M4
0.00.056.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.352 I ggml_metal_init: simdgroup reduction   = true
0.00.056.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.352 I ggml_metal_init: has bfloat            = true
0.00.056.352 I ggml_metal_init: use bfloat            = true
0.00.056.352 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.216 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.232 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.256 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.229 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.230 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.230 I llama_init_from_model: graph nodes  = 967
0.00.087.231 I llama_init_from_model: graph splits = 2
0.00.087.233 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.822 I main: llama threadpool init, n_threads = 4
0.00.815.884 I 
0.00.815.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.905 I 
0.00.816.134 I sampler seed: 1234
0.00.816.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.179 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.185 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.649.906 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.649.906 I llama_perf_context_print:        load time =     805.52 ms
0.01.649.907 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.80 tokens per second)
0.01.649.908 I llama_perf_context_print:        eval time =     788.52 ms /    63 runs   (   12.52 ms per token,    79.90 tokens per second)
0.01.649.908 I llama_perf_context_print:       total time =     835.04 ms /    70 tokens
0.01.650.102 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.110s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.776 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.457 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.459 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.462 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.381 I llama_model_loader: - type  f32:  194 tensors
0.00.025.381 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.382 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.382 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.382 I print_info: file format = GGUF V3 (latest)
0.00.025.383 I print_info: file type   = Q2_K - Medium
0.00.025.388 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.948 I load: special tokens cache size = 25
0.00.050.000 I load: token to piece cache size = 0.2984 MB
0.00.050.002 I print_info: arch             = gptneox
0.00.050.003 I print_info: vocab_only       = 0
0.00.050.003 I print_info: n_ctx_train      = 2048
0.00.050.003 I print_info: n_embd           = 2048
0.00.050.003 I print_info: n_layer          = 24
0.00.050.006 I print_info: n_head           = 16
0.00.050.007 I print_info: n_head_kv        = 16
0.00.050.008 I print_info: n_rot            = 32
0.00.050.008 I print_info: n_swa            = 0
0.00.050.009 I print_info: n_embd_head_k    = 128
0.00.050.009 I print_info: n_embd_head_v    = 128
0.00.050.010 I print_info: n_gqa            = 1
0.00.050.011 I print_info: n_embd_k_gqa     = 2048
0.00.050.011 I print_info: n_embd_v_gqa     = 2048
0.00.050.012 I print_info: f_norm_eps       = 1.0e-05
0.00.050.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.013 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.013 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.014 I print_info: f_logit_scale    = 0.0e+00
0.00.050.014 I print_info: n_ff             = 8192
0.00.050.014 I print_info: n_expert         = 0
0.00.050.015 I print_info: n_expert_used    = 0
0.00.050.015 I print_info: causal attn      = 1
0.00.050.015 I print_info: pooling type     = 0
0.00.050.015 I print_info: rope type        = 2
0.00.050.015 I print_info: rope scaling     = linear
0.00.050.016 I print_info: freq_base_train  = 10000.0
0.00.050.016 I print_info: freq_scale_train = 1
0.00.050.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.016 I print_info: rope_finetuned   = unknown
0.00.050.017 I print_info: ssm_d_conv       = 0
0.00.050.017 I print_info: ssm_d_inner      = 0
0.00.050.017 I print_info: ssm_d_state      = 0
0.00.050.019 I print_info: ssm_dt_rank      = 0
0.00.050.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.019 I print_info: model type       = 1.4B
0.00.050.020 I print_info: model params     = 1.41 B
0.00.050.020 I print_info: general.name     = 1.4B
0.00.050.020 I print_info: vocab type       = BPE
0.00.050.020 I print_info: n_vocab          = 50304
0.00.050.021 I print_info: n_merges         = 50009
0.00.050.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.025 I print_info: LF token         = 128 'Ä'
0.00.050.025 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.027 I print_info: max token length = 1024
0.00.051.620 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.620 I load_tensors: offloading output layer to GPU
0.00.051.620 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.630 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.631 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.911 I llama_init_from_model: n_seq_max     = 1
0.00.051.912 I llama_init_from_model: n_ctx         = 2048
0.00.051.912 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.912 I llama_init_from_model: n_batch       = 2048
0.00.051.913 I llama_init_from_model: n_ubatch      = 512
0.00.051.913 I llama_init_from_model: flash_attn    = 0
0.00.051.913 I llama_init_from_model: freq_base     = 10000.0
0.00.051.913 I llama_init_from_model: freq_scale    = 1
0.00.051.914 I ggml_metal_init: allocating
0.00.051.917 I ggml_metal_init: found device: Apple M4
0.00.051.919 I ggml_metal_init: picking default device: Apple M4
0.00.052.511 I ggml_metal_init: using embedded metal library
0.00.054.848 I ggml_metal_init: GPU name:   Apple M4
0.00.054.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.850 I ggml_metal_init: simdgroup reduction   = true
0.00.054.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.851 I ggml_metal_init: has bfloat            = true
0.00.054.851 I ggml_metal_init: use bfloat            = true
0.00.054.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.557 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.522 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.544 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.619 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.620 I llama_init_from_model: graph nodes  = 967
0.00.086.620 I llama_init_from_model: graph splits = 2
0.00.086.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.703 I main: llama threadpool init, n_threads = 4
0.00.502.736 I 
0.00.502.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.757 I 
0.00.502.913 I sampler seed: 1234
0.00.502.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.502.960 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.502.960 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.502.960 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.188.537 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.188.537 I llama_perf_context_print:        load time =     491.99 ms
0.01.188.538 I llama_perf_context_print: prompt eval time =      35.91 ms /     7 tokens (    5.13 ms per token,   194.94 tokens per second)
0.01.188.539 I llama_perf_context_print:        eval time =     646.64 ms /    63 runs   (   10.26 ms per token,    97.43 tokens per second)
0.01.188.539 I llama_perf_context_print:       total time =     686.77 ms /    70 tokens
0.01.188.775 I ggml_metal_free: deallocating

real	0m1.207s
user	0m0.109s
sys	0m0.105s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.282 I llama_model_loader: - type  f32:  194 tensors
0.00.026.282 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.282 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.282 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.282 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.283 I print_info: file format = GGUF V3 (latest)
0.00.026.283 I print_info: file type   = Q3_K - Medium
0.00.026.284 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.841 I load: special tokens cache size = 25
0.00.050.864 I load: token to piece cache size = 0.2984 MB
0.00.050.868 I print_info: arch             = gptneox
0.00.050.868 I print_info: vocab_only       = 0
0.00.050.868 I print_info: n_ctx_train      = 2048
0.00.050.868 I print_info: n_embd           = 2048
0.00.050.868 I print_info: n_layer          = 24
0.00.050.872 I print_info: n_head           = 16
0.00.050.873 I print_info: n_head_kv        = 16
0.00.050.873 I print_info: n_rot            = 32
0.00.050.873 I print_info: n_swa            = 0
0.00.050.874 I print_info: n_embd_head_k    = 128
0.00.050.874 I print_info: n_embd_head_v    = 128
0.00.050.876 I print_info: n_gqa            = 1
0.00.050.877 I print_info: n_embd_k_gqa     = 2048
0.00.050.877 I print_info: n_embd_v_gqa     = 2048
0.00.050.878 I print_info: f_norm_eps       = 1.0e-05
0.00.050.878 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.878 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.879 I print_info: f_logit_scale    = 0.0e+00
0.00.050.880 I print_info: n_ff             = 8192
0.00.050.880 I print_info: n_expert         = 0
0.00.050.880 I print_info: n_expert_used    = 0
0.00.050.880 I print_info: causal attn      = 1
0.00.050.880 I print_info: pooling type     = 0
0.00.050.880 I print_info: rope type        = 2
0.00.050.881 I print_info: rope scaling     = linear
0.00.050.881 I print_info: freq_base_train  = 10000.0
0.00.050.881 I print_info: freq_scale_train = 1
0.00.050.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.882 I print_info: rope_finetuned   = unknown
0.00.050.882 I print_info: ssm_d_conv       = 0
0.00.050.884 I print_info: ssm_d_inner      = 0
0.00.050.884 I print_info: ssm_d_state      = 0
0.00.050.884 I print_info: ssm_dt_rank      = 0
0.00.050.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.884 I print_info: model type       = 1.4B
0.00.050.885 I print_info: model params     = 1.41 B
0.00.050.885 I print_info: general.name     = 1.4B
0.00.050.885 I print_info: vocab type       = BPE
0.00.050.886 I print_info: n_vocab          = 50304
0.00.050.886 I print_info: n_merges         = 50009
0.00.050.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.886 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.886 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.887 I print_info: LF token         = 128 'Ä'
0.00.050.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.887 I print_info: max token length = 1024
0.00.052.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.479 I load_tensors: offloading output layer to GPU
0.00.052.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.489 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.490 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.773 I llama_init_from_model: n_seq_max     = 1
0.00.052.774 I llama_init_from_model: n_ctx         = 2048
0.00.052.774 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.774 I llama_init_from_model: n_batch       = 2048
0.00.052.774 I llama_init_from_model: n_ubatch      = 512
0.00.052.775 I llama_init_from_model: flash_attn    = 0
0.00.052.775 I llama_init_from_model: freq_base     = 10000.0
0.00.052.775 I llama_init_from_model: freq_scale    = 1
0.00.052.776 I ggml_metal_init: allocating
0.00.052.779 I ggml_metal_init: found device: Apple M4
0.00.052.781 I ggml_metal_init: picking default device: Apple M4
0.00.053.379 I ggml_metal_init: using embedded metal library
0.00.055.681 I ggml_metal_init: GPU name:   Apple M4
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.683 I ggml_metal_init: simdgroup reduction   = true
0.00.055.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.683 I ggml_metal_init: has bfloat            = true
0.00.055.683 I ggml_metal_init: use bfloat            = true
0.00.055.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.422 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.963 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.969 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.989 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.038 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.039 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.040 I llama_init_from_model: graph nodes  = 967
0.00.086.040 I llama_init_from_model: graph splits = 2
0.00.086.043 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.151 I main: llama threadpool init, n_threads = 4
0.00.540.190 I 
0.00.540.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.225 I 
0.00.540.379 I sampler seed: 1234
0.00.540.384 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.395 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.396 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.396 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.304.873 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.304.874 I llama_perf_context_print:        load time =     529.47 ms
0.01.304.875 I llama_perf_context_print: prompt eval time =      40.57 ms /     7 tokens (    5.80 ms per token,   172.56 tokens per second)
0.01.304.875 I llama_perf_context_print:        eval time =     720.90 ms /    63 runs   (   11.44 ms per token,    87.39 tokens per second)
0.01.304.876 I llama_perf_context_print:       total time =     765.58 ms /    70 tokens
0.01.305.071 I ggml_metal_free: deallocating

real	0m1.322s
user	0m0.110s
sys	0m0.122s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.014.090 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.021.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.326 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.030.172 I llama_model_loader: - type  f32:  194 tensors
0.00.030.172 I llama_model_loader: - type q4_K:   61 tensors
0.00.030.172 I llama_model_loader: - type q5_K:   24 tensors
0.00.030.172 I llama_model_loader: - type q6_K:   13 tensors
0.00.030.173 I print_info: file format = GGUF V3 (latest)
0.00.030.173 I print_info: file type   = Q4_K - Medium
0.00.030.174 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.049.497 I load: special tokens cache size = 25
0.00.055.452 I load: token to piece cache size = 0.2984 MB
0.00.055.454 I print_info: arch             = gptneox
0.00.055.455 I print_info: vocab_only       = 0
0.00.055.455 I print_info: n_ctx_train      = 2048
0.00.055.455 I print_info: n_embd           = 2048
0.00.055.455 I print_info: n_layer          = 24
0.00.055.458 I print_info: n_head           = 16
0.00.055.459 I print_info: n_head_kv        = 16
0.00.055.459 I print_info: n_rot            = 32
0.00.055.459 I print_info: n_swa            = 0
0.00.055.460 I print_info: n_embd_head_k    = 128
0.00.055.460 I print_info: n_embd_head_v    = 128
0.00.055.461 I print_info: n_gqa            = 1
0.00.055.462 I print_info: n_embd_k_gqa     = 2048
0.00.055.462 I print_info: n_embd_v_gqa     = 2048
0.00.055.463 I print_info: f_norm_eps       = 1.0e-05
0.00.055.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.464 I print_info: f_logit_scale    = 0.0e+00
0.00.055.464 I print_info: n_ff             = 8192
0.00.055.464 I print_info: n_expert         = 0
0.00.055.465 I print_info: n_expert_used    = 0
0.00.055.465 I print_info: causal attn      = 1
0.00.055.465 I print_info: pooling type     = 0
0.00.055.465 I print_info: rope type        = 2
0.00.055.465 I print_info: rope scaling     = linear
0.00.055.466 I print_info: freq_base_train  = 10000.0
0.00.055.466 I print_info: freq_scale_train = 1
0.00.055.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.466 I print_info: rope_finetuned   = unknown
0.00.055.467 I print_info: ssm_d_conv       = 0
0.00.055.467 I print_info: ssm_d_inner      = 0
0.00.055.467 I print_info: ssm_d_state      = 0
0.00.055.467 I print_info: ssm_dt_rank      = 0
0.00.055.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.467 I print_info: model type       = 1.4B
0.00.055.468 I print_info: model params     = 1.41 B
0.00.055.468 I print_info: general.name     = 1.4B
0.00.055.469 I print_info: vocab type       = BPE
0.00.055.469 I print_info: n_vocab          = 50304
0.00.055.469 I print_info: n_merges         = 50009
0.00.055.469 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.469 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.472 I print_info: LF token         = 128 'Ä'
0.00.055.472 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.473 I print_info: max token length = 1024
0.00.057.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.115 I load_tensors: offloading output layer to GPU
0.00.057.115 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.125 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.126 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.057.387 I llama_init_from_model: n_seq_max     = 1
0.00.057.388 I llama_init_from_model: n_ctx         = 2048
0.00.057.388 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.388 I llama_init_from_model: n_batch       = 2048
0.00.057.388 I llama_init_from_model: n_ubatch      = 512
0.00.057.389 I llama_init_from_model: flash_attn    = 0
0.00.057.389 I llama_init_from_model: freq_base     = 10000.0
0.00.057.389 I llama_init_from_model: freq_scale    = 1
0.00.057.390 I ggml_metal_init: allocating
0.00.057.393 I ggml_metal_init: found device: Apple M4
0.00.057.395 I ggml_metal_init: picking default device: Apple M4
0.00.058.003 I ggml_metal_init: using embedded metal library
0.00.060.364 I ggml_metal_init: GPU name:   Apple M4
0.00.060.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.367 I ggml_metal_init: simdgroup reduction   = true
0.00.060.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.367 I ggml_metal_init: has bfloat            = true
0.00.060.367 I ggml_metal_init: use bfloat            = true
0.00.060.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.664 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.675 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.694 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.732 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.734 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.734 I llama_init_from_model: graph nodes  = 967
0.00.090.734 I llama_init_from_model: graph splits = 2
0.00.090.737 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.433 I main: llama threadpool init, n_threads = 4
0.00.618.471 I 
0.00.618.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.495 I 
0.00.618.637 I sampler seed: 1234
0.00.618.643 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.618.653 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.618.654 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.618.654 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.414.176 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.414.176 I llama_perf_context_print:        load time =     603.49 ms
0.01.414.177 I llama_perf_context_print: prompt eval time =      51.08 ms /     7 tokens (    7.30 ms per token,   137.05 tokens per second)
0.01.414.178 I llama_perf_context_print:        eval time =     741.25 ms /    63 runs   (   11.77 ms per token,    84.99 tokens per second)
0.01.414.178 I llama_perf_context_print:       total time =     796.60 ms /    70 tokens
0.01.414.390 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.111s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.693 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.898 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.905 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.906 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.908 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.945 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.931 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.932 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.933 I llama_model_loader: - type  f32:  194 tensors
0.00.026.933 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.934 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.934 I print_info: file format = GGUF V3 (latest)
0.00.026.934 I print_info: file type   = Q5_K - Medium
0.00.026.938 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.349 I load: special tokens cache size = 25
0.00.052.407 I load: token to piece cache size = 0.2984 MB
0.00.052.410 I print_info: arch             = gptneox
0.00.052.410 I print_info: vocab_only       = 0
0.00.052.411 I print_info: n_ctx_train      = 2048
0.00.052.411 I print_info: n_embd           = 2048
0.00.052.411 I print_info: n_layer          = 24
0.00.052.414 I print_info: n_head           = 16
0.00.052.415 I print_info: n_head_kv        = 16
0.00.052.415 I print_info: n_rot            = 32
0.00.052.415 I print_info: n_swa            = 0
0.00.052.415 I print_info: n_embd_head_k    = 128
0.00.052.416 I print_info: n_embd_head_v    = 128
0.00.052.416 I print_info: n_gqa            = 1
0.00.052.417 I print_info: n_embd_k_gqa     = 2048
0.00.052.418 I print_info: n_embd_v_gqa     = 2048
0.00.052.418 I print_info: f_norm_eps       = 1.0e-05
0.00.052.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.419 I print_info: f_logit_scale    = 0.0e+00
0.00.052.420 I print_info: n_ff             = 8192
0.00.052.420 I print_info: n_expert         = 0
0.00.052.420 I print_info: n_expert_used    = 0
0.00.052.422 I print_info: causal attn      = 1
0.00.052.422 I print_info: pooling type     = 0
0.00.052.422 I print_info: rope type        = 2
0.00.052.423 I print_info: rope scaling     = linear
0.00.052.423 I print_info: freq_base_train  = 10000.0
0.00.052.424 I print_info: freq_scale_train = 1
0.00.052.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.424 I print_info: rope_finetuned   = unknown
0.00.052.424 I print_info: ssm_d_conv       = 0
0.00.052.424 I print_info: ssm_d_inner      = 0
0.00.052.424 I print_info: ssm_d_state      = 0
0.00.052.425 I print_info: ssm_dt_rank      = 0
0.00.052.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.425 I print_info: model type       = 1.4B
0.00.052.425 I print_info: model params     = 1.41 B
0.00.052.425 I print_info: general.name     = 1.4B
0.00.052.426 I print_info: vocab type       = BPE
0.00.052.426 I print_info: n_vocab          = 50304
0.00.052.426 I print_info: n_merges         = 50009
0.00.052.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.429 I print_info: LF token         = 128 'Ä'
0.00.052.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.429 I print_info: max token length = 1024
0.00.054.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.082 I load_tensors: offloading output layer to GPU
0.00.054.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.092 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.093 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.403 I llama_init_from_model: n_seq_max     = 1
0.00.054.403 I llama_init_from_model: n_ctx         = 2048
0.00.054.404 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.404 I llama_init_from_model: n_batch       = 2048
0.00.054.404 I llama_init_from_model: n_ubatch      = 512
0.00.054.404 I llama_init_from_model: flash_attn    = 0
0.00.054.405 I llama_init_from_model: freq_base     = 10000.0
0.00.054.405 I llama_init_from_model: freq_scale    = 1
0.00.054.405 I ggml_metal_init: allocating
0.00.054.408 I ggml_metal_init: found device: Apple M4
0.00.054.411 I ggml_metal_init: picking default device: Apple M4
0.00.055.017 I ggml_metal_init: using embedded metal library
0.00.057.392 I ggml_metal_init: GPU name:   Apple M4
0.00.057.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.394 I ggml_metal_init: simdgroup reduction   = true
0.00.057.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.394 I ggml_metal_init: has bfloat            = true
0.00.057.395 I ggml_metal_init: use bfloat            = true
0.00.057.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.413 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.423 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.485 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.487 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.487 I llama_init_from_model: graph nodes  = 967
0.00.088.487 I llama_init_from_model: graph splits = 2
0.00.088.490 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.315 I main: llama threadpool init, n_threads = 4
0.00.724.350 I 
0.00.724.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.384 I 
0.00.724.600 I sampler seed: 1234
0.00.724.604 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.724.645 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.724.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.724.664 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.566.943 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.566.943 I llama_perf_context_print:        load time =     713.74 ms
0.01.566.944 I llama_perf_context_print: prompt eval time =      51.72 ms /     7 tokens (    7.39 ms per token,   135.34 tokens per second)
0.01.566.945 I llama_perf_context_print:        eval time =     787.46 ms /    63 runs   (   12.50 ms per token,    80.00 tokens per second)
0.01.566.945 I llama_perf_context_print:       total time =     843.51 ms /    70 tokens
0.01.567.203 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.111s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.287 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.317 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.271 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.272 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.274 I llama_model_loader: - type  f32:  194 tensors
0.00.026.275 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.275 I print_info: file format = GGUF V3 (latest)
0.00.026.276 I print_info: file type   = Q6_K
0.00.026.276 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.740 I load: special tokens cache size = 25
0.00.051.810 I load: token to piece cache size = 0.2984 MB
0.00.051.813 I print_info: arch             = gptneox
0.00.051.813 I print_info: vocab_only       = 0
0.00.051.814 I print_info: n_ctx_train      = 2048
0.00.051.814 I print_info: n_embd           = 2048
0.00.051.814 I print_info: n_layer          = 24
0.00.051.817 I print_info: n_head           = 16
0.00.051.818 I print_info: n_head_kv        = 16
0.00.051.818 I print_info: n_rot            = 32
0.00.051.818 I print_info: n_swa            = 0
0.00.051.818 I print_info: n_embd_head_k    = 128
0.00.051.818 I print_info: n_embd_head_v    = 128
0.00.051.819 I print_info: n_gqa            = 1
0.00.051.820 I print_info: n_embd_k_gqa     = 2048
0.00.051.821 I print_info: n_embd_v_gqa     = 2048
0.00.051.821 I print_info: f_norm_eps       = 1.0e-05
0.00.051.822 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.822 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.822 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.822 I print_info: f_logit_scale    = 0.0e+00
0.00.051.823 I print_info: n_ff             = 8192
0.00.051.825 I print_info: n_expert         = 0
0.00.051.825 I print_info: n_expert_used    = 0
0.00.051.825 I print_info: causal attn      = 1
0.00.051.825 I print_info: pooling type     = 0
0.00.051.826 I print_info: rope type        = 2
0.00.051.827 I print_info: rope scaling     = linear
0.00.051.829 I print_info: freq_base_train  = 10000.0
0.00.051.829 I print_info: freq_scale_train = 1
0.00.051.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.830 I print_info: rope_finetuned   = unknown
0.00.051.830 I print_info: ssm_d_conv       = 0
0.00.051.830 I print_info: ssm_d_inner      = 0
0.00.051.830 I print_info: ssm_d_state      = 0
0.00.051.830 I print_info: ssm_dt_rank      = 0
0.00.051.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.831 I print_info: model type       = 1.4B
0.00.051.831 I print_info: model params     = 1.41 B
0.00.051.832 I print_info: general.name     = 1.4B
0.00.051.833 I print_info: vocab type       = BPE
0.00.051.833 I print_info: n_vocab          = 50304
0.00.051.833 I print_info: n_merges         = 50009
0.00.051.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.834 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.834 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.834 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.834 I print_info: LF token         = 128 'Ä'
0.00.051.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.835 I print_info: max token length = 1024
0.00.053.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.898 I load_tensors: offloading output layer to GPU
0.00.053.898 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.908 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.910 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.191 I llama_init_from_model: n_seq_max     = 1
0.00.054.191 I llama_init_from_model: n_ctx         = 2048
0.00.054.191 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.192 I llama_init_from_model: n_batch       = 2048
0.00.054.192 I llama_init_from_model: n_ubatch      = 512
0.00.054.192 I llama_init_from_model: flash_attn    = 0
0.00.054.192 I llama_init_from_model: freq_base     = 10000.0
0.00.054.193 I llama_init_from_model: freq_scale    = 1
0.00.054.193 I ggml_metal_init: allocating
0.00.054.196 I ggml_metal_init: found device: Apple M4
0.00.054.198 I ggml_metal_init: picking default device: Apple M4
0.00.054.802 I ggml_metal_init: using embedded metal library
0.00.057.148 I ggml_metal_init: GPU name:   Apple M4
0.00.057.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.150 I ggml_metal_init: simdgroup reduction   = true
0.00.057.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.151 I ggml_metal_init: has bfloat            = true
0.00.057.151 I ggml_metal_init: use bfloat            = true
0.00.057.151 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.174 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.504 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.619 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.620 I llama_init_from_model: graph nodes  = 967
0.00.088.620 I llama_init_from_model: graph splits = 2
0.00.088.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.061 I main: llama threadpool init, n_threads = 4
0.00.753.106 I 
0.00.753.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.131 I 
0.00.753.445 I sampler seed: 1234
0.00.753.455 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.487 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.490 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.628.032 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.628.033 I llama_perf_context_print:        load time =     742.88 ms
0.01.628.034 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.628.034 I llama_perf_context_print:        eval time =     817.63 ms /    63 runs   (   12.98 ms per token,    77.05 tokens per second)
0.01.628.035 I llama_perf_context_print:       total time =     875.86 ms /    70 tokens
0.01.628.333 I ggml_metal_free: deallocating

real	0m1.647s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.545 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.545 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.284 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.322 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.735 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.736 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.736 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.737 I llama_model_loader: - type  f32:  194 tensors
0.00.056.738 I llama_model_loader: - type  f16:   98 tensors
0.00.056.739 I print_info: file format = GGUF V3 (latest)
0.00.056.740 I print_info: file type   = all F32 (guessed)
0.00.056.742 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.000 I load: special tokens cache size = 25
0.00.092.053 I load: token to piece cache size = 0.2984 MB
0.00.092.057 I print_info: arch             = gptneox
0.00.092.057 I print_info: vocab_only       = 0
0.00.092.057 I print_info: n_ctx_train      = 2048
0.00.092.057 I print_info: n_embd           = 2048
0.00.092.057 I print_info: n_layer          = 24
0.00.092.060 I print_info: n_head           = 16
0.00.092.061 I print_info: n_head_kv        = 16
0.00.092.061 I print_info: n_rot            = 32
0.00.092.062 I print_info: n_swa            = 0
0.00.092.062 I print_info: n_embd_head_k    = 128
0.00.092.062 I print_info: n_embd_head_v    = 128
0.00.092.065 I print_info: n_gqa            = 1
0.00.092.066 I print_info: n_embd_k_gqa     = 2048
0.00.092.066 I print_info: n_embd_v_gqa     = 2048
0.00.092.067 I print_info: f_norm_eps       = 1.0e-05
0.00.092.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.067 I print_info: f_logit_scale    = 0.0e+00
0.00.092.074 I print_info: n_ff             = 8192
0.00.092.076 I print_info: n_expert         = 0
0.00.092.076 I print_info: n_expert_used    = 0
0.00.092.078 I print_info: causal attn      = 1
0.00.092.078 I print_info: pooling type     = 0
0.00.092.078 I print_info: rope type        = 2
0.00.092.078 I print_info: rope scaling     = linear
0.00.092.079 I print_info: freq_base_train  = 10000.0
0.00.092.079 I print_info: freq_scale_train = 1
0.00.092.079 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.080 I print_info: rope_finetuned   = unknown
0.00.092.080 I print_info: ssm_d_conv       = 0
0.00.092.080 I print_info: ssm_d_inner      = 0
0.00.092.080 I print_info: ssm_d_state      = 0
0.00.092.080 I print_info: ssm_dt_rank      = 0
0.00.092.080 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.081 I print_info: model type       = 1.4B
0.00.092.081 I print_info: model params     = 1.41 B
0.00.092.081 I print_info: general.name     = 1.4B
0.00.092.082 I print_info: vocab type       = BPE
0.00.092.082 I print_info: n_vocab          = 50304
0.00.092.082 I print_info: n_merges         = 50009
0.00.092.087 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.087 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.088 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.089 I print_info: LF token         = 128 'Ä'
0.00.092.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.091 I print_info: max token length = 1024
0.00.094.693 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.693 I load_tensors: offloading output layer to GPU
0.00.094.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.704 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.705 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.981 I llama_init_from_model: n_seq_max     = 1
0.00.094.982 I llama_init_from_model: n_ctx         = 128
0.00.094.982 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.982 I llama_init_from_model: n_batch       = 128
0.00.094.982 I llama_init_from_model: n_ubatch      = 128
0.00.094.982 I llama_init_from_model: flash_attn    = 0
0.00.094.983 I llama_init_from_model: freq_base     = 10000.0
0.00.094.983 I llama_init_from_model: freq_scale    = 1
0.00.094.983 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.984 I ggml_metal_init: allocating
0.00.094.986 I ggml_metal_init: found device: Apple M4
0.00.094.988 I ggml_metal_init: picking default device: Apple M4
0.00.095.636 I ggml_metal_init: using embedded metal library
0.00.098.340 I ggml_metal_init: GPU name:   Apple M4
0.00.098.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.342 I ggml_metal_init: simdgroup reduction   = true
0.00.098.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.343 I ggml_metal_init: has bfloat            = true
0.00.098.343 I ggml_metal_init: use bfloat            = true
0.00.098.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.443 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.734 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.736 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.751 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.725 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.726 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.726 I llama_init_from_model: graph nodes  = 967
0.00.110.727 I llama_init_from_model: graph splits = 2
0.00.110.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.801 I 
0.00.912.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.865 I perplexity: tokenizing the input ..
0.00.926.028 I perplexity: tokenization took 13.162 ms
0.00.926.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.046.393 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.048.020 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.048.044 I llama_perf_context_print:        load time =     888.24 ms
0.01.048.048 I llama_perf_context_print: prompt eval time =     119.48 ms /   128 tokens (    0.93 ms per token,  1071.35 tokens per second)
0.01.048.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.048.052 I llama_perf_context_print:       total time =     135.24 ms /   129 tokens
0.01.048.751 I ggml_metal_free: deallocating

real	0m1.242s
user	0m0.125s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.680 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.791 I llama_model_loader: - type  f32:  194 tensors
0.00.034.791 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.792 I print_info: file format = GGUF V3 (latest)
0.00.034.793 I print_info: file type   = Q8_0
0.00.034.794 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.827 I load: special tokens cache size = 25
0.00.065.406 I load: token to piece cache size = 0.2984 MB
0.00.065.410 I print_info: arch             = gptneox
0.00.065.410 I print_info: vocab_only       = 0
0.00.065.410 I print_info: n_ctx_train      = 2048
0.00.065.410 I print_info: n_embd           = 2048
0.00.065.410 I print_info: n_layer          = 24
0.00.065.414 I print_info: n_head           = 16
0.00.065.414 I print_info: n_head_kv        = 16
0.00.065.417 I print_info: n_rot            = 32
0.00.065.417 I print_info: n_swa            = 0
0.00.065.417 I print_info: n_embd_head_k    = 128
0.00.065.417 I print_info: n_embd_head_v    = 128
0.00.065.418 I print_info: n_gqa            = 1
0.00.065.419 I print_info: n_embd_k_gqa     = 2048
0.00.065.419 I print_info: n_embd_v_gqa     = 2048
0.00.065.420 I print_info: f_norm_eps       = 1.0e-05
0.00.065.420 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.426 I print_info: f_logit_scale    = 0.0e+00
0.00.065.435 I print_info: n_ff             = 8192
0.00.065.435 I print_info: n_expert         = 0
0.00.065.436 I print_info: n_expert_used    = 0
0.00.065.436 I print_info: causal attn      = 1
0.00.065.436 I print_info: pooling type     = 0
0.00.065.436 I print_info: rope type        = 2
0.00.065.436 I print_info: rope scaling     = linear
0.00.065.437 I print_info: freq_base_train  = 10000.0
0.00.065.437 I print_info: freq_scale_train = 1
0.00.065.437 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.437 I print_info: rope_finetuned   = unknown
0.00.065.438 I print_info: ssm_d_conv       = 0
0.00.065.438 I print_info: ssm_d_inner      = 0
0.00.065.439 I print_info: ssm_d_state      = 0
0.00.065.439 I print_info: ssm_dt_rank      = 0
0.00.065.439 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.439 I print_info: model type       = 1.4B
0.00.065.439 I print_info: model params     = 1.41 B
0.00.065.440 I print_info: general.name     = 1.4B
0.00.065.440 I print_info: vocab type       = BPE
0.00.065.440 I print_info: n_vocab          = 50304
0.00.065.440 I print_info: n_merges         = 50009
0.00.065.441 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.441 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.441 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.441 I print_info: LF token         = 128 'Ä'
0.00.065.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.442 I print_info: max token length = 1024
0.00.067.747 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.747 I load_tensors: offloading output layer to GPU
0.00.067.748 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.758 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.759 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.054 I llama_init_from_model: n_seq_max     = 1
0.00.068.055 I llama_init_from_model: n_ctx         = 128
0.00.068.055 I llama_init_from_model: n_ctx_per_seq = 128
0.00.068.055 I llama_init_from_model: n_batch       = 128
0.00.068.056 I llama_init_from_model: n_ubatch      = 128
0.00.068.056 I llama_init_from_model: flash_attn    = 0
0.00.068.056 I llama_init_from_model: freq_base     = 10000.0
0.00.068.056 I llama_init_from_model: freq_scale    = 1
0.00.068.057 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.057 I ggml_metal_init: allocating
0.00.068.061 I ggml_metal_init: found device: Apple M4
0.00.068.063 I ggml_metal_init: picking default device: Apple M4
0.00.068.675 I ggml_metal_init: using embedded metal library
0.00.071.617 I ggml_metal_init: GPU name:   Apple M4
0.00.071.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.620 I ggml_metal_init: simdgroup reduction   = true
0.00.071.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.620 I ggml_metal_init: has bfloat            = true
0.00.071.621 I ggml_metal_init: use bfloat            = true
0.00.071.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.121 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.407 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.357 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.358 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.358 I llama_init_from_model: graph nodes  = 967
0.00.083.358 I llama_init_from_model: graph splits = 2
0.00.083.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.187 I 
0.00.875.217 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.225 I perplexity: tokenizing the input ..
0.00.883.104 I perplexity: tokenization took 7.877 ms
0.00.883.119 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.007.405 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.008.611 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.008.622 I llama_perf_context_print:        load time =     862.50 ms
0.01.008.623 I llama_perf_context_print: prompt eval time =     124.06 ms /   128 tokens (    0.97 ms per token,  1031.75 tokens per second)
0.01.008.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.008.624 I llama_perf_context_print:       total time =     133.44 ms /   129 tokens
0.01.008.945 I ggml_metal_free: deallocating

real	0m1.027s
user	0m0.092s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.208 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.257 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.264 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.266 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.267 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.270 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.270 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.233 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.234 I llama_model_loader: - type  f32:  194 tensors
0.00.026.234 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.234 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.235 I print_info: file format = GGUF V3 (latest)
0.00.026.235 I print_info: file type   = Q4_0
0.00.026.236 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.662 I load: special tokens cache size = 25
0.00.050.616 I load: token to piece cache size = 0.2984 MB
0.00.050.619 I print_info: arch             = gptneox
0.00.050.619 I print_info: vocab_only       = 0
0.00.050.620 I print_info: n_ctx_train      = 2048
0.00.050.620 I print_info: n_embd           = 2048
0.00.050.620 I print_info: n_layer          = 24
0.00.050.623 I print_info: n_head           = 16
0.00.050.624 I print_info: n_head_kv        = 16
0.00.050.624 I print_info: n_rot            = 32
0.00.050.624 I print_info: n_swa            = 0
0.00.050.624 I print_info: n_embd_head_k    = 128
0.00.050.625 I print_info: n_embd_head_v    = 128
0.00.050.625 I print_info: n_gqa            = 1
0.00.050.626 I print_info: n_embd_k_gqa     = 2048
0.00.050.627 I print_info: n_embd_v_gqa     = 2048
0.00.050.627 I print_info: f_norm_eps       = 1.0e-05
0.00.050.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.628 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.628 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.628 I print_info: f_logit_scale    = 0.0e+00
0.00.050.629 I print_info: n_ff             = 8192
0.00.050.629 I print_info: n_expert         = 0
0.00.050.629 I print_info: n_expert_used    = 0
0.00.050.630 I print_info: causal attn      = 1
0.00.050.630 I print_info: pooling type     = 0
0.00.050.630 I print_info: rope type        = 2
0.00.050.630 I print_info: rope scaling     = linear
0.00.050.630 I print_info: freq_base_train  = 10000.0
0.00.050.631 I print_info: freq_scale_train = 1
0.00.050.631 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.631 I print_info: rope_finetuned   = unknown
0.00.050.631 I print_info: ssm_d_conv       = 0
0.00.050.631 I print_info: ssm_d_inner      = 0
0.00.050.634 I print_info: ssm_d_state      = 0
0.00.050.634 I print_info: ssm_dt_rank      = 0
0.00.050.634 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.635 I print_info: model type       = 1.4B
0.00.050.635 I print_info: model params     = 1.41 B
0.00.050.635 I print_info: general.name     = 1.4B
0.00.050.636 I print_info: vocab type       = BPE
0.00.050.636 I print_info: n_vocab          = 50304
0.00.050.636 I print_info: n_merges         = 50009
0.00.050.636 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.636 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.637 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.637 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.637 I print_info: LF token         = 128 'Ä'
0.00.050.641 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.641 I print_info: max token length = 1024
0.00.052.519 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.520 I load_tensors: offloading output layer to GPU
0.00.052.520 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.530 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.532 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.804 I llama_init_from_model: n_seq_max     = 1
0.00.052.805 I llama_init_from_model: n_ctx         = 128
0.00.052.805 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.805 I llama_init_from_model: n_batch       = 128
0.00.052.806 I llama_init_from_model: n_ubatch      = 128
0.00.052.806 I llama_init_from_model: flash_attn    = 0
0.00.052.806 I llama_init_from_model: freq_base     = 10000.0
0.00.052.806 I llama_init_from_model: freq_scale    = 1
0.00.052.807 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.807 I ggml_metal_init: allocating
0.00.052.810 I ggml_metal_init: found device: Apple M4
0.00.052.812 I ggml_metal_init: picking default device: Apple M4
0.00.053.407 I ggml_metal_init: using embedded metal library
0.00.055.808 I ggml_metal_init: GPU name:   Apple M4
0.00.055.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.810 I ggml_metal_init: simdgroup reduction   = true
0.00.055.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.811 I ggml_metal_init: has bfloat            = true
0.00.055.811 I ggml_metal_init: use bfloat            = true
0.00.055.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.574 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.886 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.860 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.861 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.861 I llama_init_from_model: graph nodes  = 967
0.00.067.861 I llama_init_from_model: graph splits = 2
0.00.067.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.418 I 
0.00.650.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.474 I perplexity: tokenizing the input ..
0.00.658.392 I perplexity: tokenization took 7.917 ms
0.00.658.409 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.994 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.782.124 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.782.140 I llama_perf_context_print:        load time =     640.21 ms
0.00.782.141 I llama_perf_context_print: prompt eval time =     122.36 ms /   128 tokens (    0.96 ms per token,  1046.11 tokens per second)
0.00.782.142 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.142 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.00.782.440 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.077s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.093 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.101 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.103 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.104 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.091 I llama_model_loader: - type  f32:  194 tensors
0.00.025.091 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.091 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.092 I print_info: file format = GGUF V3 (latest)
0.00.025.092 I print_info: file type   = Q4_1
0.00.025.093 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.342 I load: special tokens cache size = 25
0.00.050.195 I load: token to piece cache size = 0.2984 MB
0.00.050.198 I print_info: arch             = gptneox
0.00.050.199 I print_info: vocab_only       = 0
0.00.050.199 I print_info: n_ctx_train      = 2048
0.00.050.199 I print_info: n_embd           = 2048
0.00.050.199 I print_info: n_layer          = 24
0.00.050.203 I print_info: n_head           = 16
0.00.050.203 I print_info: n_head_kv        = 16
0.00.050.204 I print_info: n_rot            = 32
0.00.050.204 I print_info: n_swa            = 0
0.00.050.204 I print_info: n_embd_head_k    = 128
0.00.050.205 I print_info: n_embd_head_v    = 128
0.00.050.206 I print_info: n_gqa            = 1
0.00.050.207 I print_info: n_embd_k_gqa     = 2048
0.00.050.207 I print_info: n_embd_v_gqa     = 2048
0.00.050.208 I print_info: f_norm_eps       = 1.0e-05
0.00.050.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.209 I print_info: f_logit_scale    = 0.0e+00
0.00.050.209 I print_info: n_ff             = 8192
0.00.050.209 I print_info: n_expert         = 0
0.00.050.210 I print_info: n_expert_used    = 0
0.00.050.210 I print_info: causal attn      = 1
0.00.050.210 I print_info: pooling type     = 0
0.00.050.210 I print_info: rope type        = 2
0.00.050.210 I print_info: rope scaling     = linear
0.00.050.211 I print_info: freq_base_train  = 10000.0
0.00.050.211 I print_info: freq_scale_train = 1
0.00.050.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.211 I print_info: rope_finetuned   = unknown
0.00.050.212 I print_info: ssm_d_conv       = 0
0.00.050.212 I print_info: ssm_d_inner      = 0
0.00.050.212 I print_info: ssm_d_state      = 0
0.00.050.212 I print_info: ssm_dt_rank      = 0
0.00.050.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.213 I print_info: model type       = 1.4B
0.00.050.213 I print_info: model params     = 1.41 B
0.00.050.213 I print_info: general.name     = 1.4B
0.00.050.214 I print_info: vocab type       = BPE
0.00.050.214 I print_info: n_vocab          = 50304
0.00.050.214 I print_info: n_merges         = 50009
0.00.050.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: LF token         = 128 'Ä'
0.00.050.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: max token length = 1024
0.00.051.831 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.831 I load_tensors: offloading output layer to GPU
0.00.051.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.841 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.842 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.117 I llama_init_from_model: n_seq_max     = 1
0.00.052.118 I llama_init_from_model: n_ctx         = 128
0.00.052.119 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.119 I llama_init_from_model: n_batch       = 128
0.00.052.119 I llama_init_from_model: n_ubatch      = 128
0.00.052.119 I llama_init_from_model: flash_attn    = 0
0.00.052.120 I llama_init_from_model: freq_base     = 10000.0
0.00.052.120 I llama_init_from_model: freq_scale    = 1
0.00.052.120 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.121 I ggml_metal_init: allocating
0.00.052.124 I ggml_metal_init: found device: Apple M4
0.00.052.126 I ggml_metal_init: picking default device: Apple M4
0.00.052.735 I ggml_metal_init: using embedded metal library
0.00.055.132 I ggml_metal_init: GPU name:   Apple M4
0.00.055.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.135 I ggml_metal_init: simdgroup reduction   = true
0.00.055.135 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.135 I ggml_metal_init: has bfloat            = true
0.00.055.135 I ggml_metal_init: use bfloat            = true
0.00.055.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.958 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.299 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.313 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.286 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.287 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.287 I llama_init_from_model: graph nodes  = 967
0.00.067.287 I llama_init_from_model: graph splits = 2
0.00.067.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.413 I 
0.00.681.450 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.459 I perplexity: tokenizing the input ..
0.00.689.203 I perplexity: tokenization took 7.742 ms
0.00.689.221 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.905 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.813.055 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.813.072 I llama_perf_context_print:        load time =     672.50 ms
0.00.813.073 I llama_perf_context_print: prompt eval time =     122.46 ms /   128 tokens (    0.96 ms per token,  1045.26 tokens per second)
0.00.813.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.074 I llama_perf_context_print:       total time =     131.66 ms /   129 tokens
0.00.813.422 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.108 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.191 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.262 I llama_model_loader: - type  f32:  194 tensors
0.00.026.262 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.263 I print_info: file format = GGUF V3 (latest)
0.00.026.263 I print_info: file type   = Q5_0
0.00.026.264 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.472 I load: special tokens cache size = 25
0.00.051.561 I load: token to piece cache size = 0.2984 MB
0.00.051.564 I print_info: arch             = gptneox
0.00.051.564 I print_info: vocab_only       = 0
0.00.051.564 I print_info: n_ctx_train      = 2048
0.00.051.564 I print_info: n_embd           = 2048
0.00.051.564 I print_info: n_layer          = 24
0.00.051.567 I print_info: n_head           = 16
0.00.051.568 I print_info: n_head_kv        = 16
0.00.051.568 I print_info: n_rot            = 32
0.00.051.568 I print_info: n_swa            = 0
0.00.051.569 I print_info: n_embd_head_k    = 128
0.00.051.569 I print_info: n_embd_head_v    = 128
0.00.051.570 I print_info: n_gqa            = 1
0.00.051.570 I print_info: n_embd_k_gqa     = 2048
0.00.051.571 I print_info: n_embd_v_gqa     = 2048
0.00.051.571 I print_info: f_norm_eps       = 1.0e-05
0.00.051.572 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.572 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.572 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.572 I print_info: f_logit_scale    = 0.0e+00
0.00.051.573 I print_info: n_ff             = 8192
0.00.051.573 I print_info: n_expert         = 0
0.00.051.573 I print_info: n_expert_used    = 0
0.00.051.573 I print_info: causal attn      = 1
0.00.051.574 I print_info: pooling type     = 0
0.00.051.574 I print_info: rope type        = 2
0.00.051.574 I print_info: rope scaling     = linear
0.00.051.574 I print_info: freq_base_train  = 10000.0
0.00.051.575 I print_info: freq_scale_train = 1
0.00.051.575 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.578 I print_info: rope_finetuned   = unknown
0.00.051.578 I print_info: ssm_d_conv       = 0
0.00.051.578 I print_info: ssm_d_inner      = 0
0.00.051.578 I print_info: ssm_d_state      = 0
0.00.051.578 I print_info: ssm_dt_rank      = 0
0.00.051.578 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.578 I print_info: model type       = 1.4B
0.00.051.579 I print_info: model params     = 1.41 B
0.00.051.579 I print_info: general.name     = 1.4B
0.00.051.579 I print_info: vocab type       = BPE
0.00.051.580 I print_info: n_vocab          = 50304
0.00.051.580 I print_info: n_merges         = 50009
0.00.051.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.585 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.585 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.585 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.585 I print_info: LF token         = 128 'Ä'
0.00.051.586 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.586 I print_info: max token length = 1024
0.00.053.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.579 I load_tensors: offloading output layer to GPU
0.00.053.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.590 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.591 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.865 I llama_init_from_model: n_seq_max     = 1
0.00.053.865 I llama_init_from_model: n_ctx         = 128
0.00.053.865 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.866 I llama_init_from_model: n_batch       = 128
0.00.053.866 I llama_init_from_model: n_ubatch      = 128
0.00.053.866 I llama_init_from_model: flash_attn    = 0
0.00.053.866 I llama_init_from_model: freq_base     = 10000.0
0.00.053.867 I llama_init_from_model: freq_scale    = 1
0.00.053.867 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.867 I ggml_metal_init: allocating
0.00.053.870 I ggml_metal_init: found device: Apple M4
0.00.053.872 I ggml_metal_init: picking default device: Apple M4
0.00.054.453 I ggml_metal_init: using embedded metal library
0.00.056.796 I ggml_metal_init: GPU name:   Apple M4
0.00.056.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.798 I ggml_metal_init: simdgroup reduction   = true
0.00.056.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.799 I ggml_metal_init: has bfloat            = true
0.00.056.799 I ggml_metal_init: use bfloat            = true
0.00.056.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.436 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.678 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.679 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.660 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.661 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.661 I llama_init_from_model: graph nodes  = 967
0.00.068.661 I llama_init_from_model: graph splits = 2
0.00.068.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.521 I 
0.00.769.574 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.594 I perplexity: tokenizing the input ..
0.00.777.538 I perplexity: tokenization took 7.942 ms
0.00.777.549 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.912.871 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.914.104 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.914.133 I llama_perf_context_print:        load time =     759.41 ms
0.00.914.134 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.50 tokens per second)
0.00.914.135 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.914.135 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.914.652 I ggml_metal_free: deallocating

real	0m0.930s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.605 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.606 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.607 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.547 I llama_model_loader: - type  f32:  194 tensors
0.00.024.547 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.548 I print_info: file format = GGUF V3 (latest)
0.00.024.548 I print_info: file type   = Q5_1
0.00.024.549 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.157 I load: special tokens cache size = 25
0.00.049.223 I load: token to piece cache size = 0.2984 MB
0.00.049.226 I print_info: arch             = gptneox
0.00.049.227 I print_info: vocab_only       = 0
0.00.049.227 I print_info: n_ctx_train      = 2048
0.00.049.227 I print_info: n_embd           = 2048
0.00.049.227 I print_info: n_layer          = 24
0.00.049.230 I print_info: n_head           = 16
0.00.049.231 I print_info: n_head_kv        = 16
0.00.049.231 I print_info: n_rot            = 32
0.00.049.231 I print_info: n_swa            = 0
0.00.049.232 I print_info: n_embd_head_k    = 128
0.00.049.232 I print_info: n_embd_head_v    = 128
0.00.049.232 I print_info: n_gqa            = 1
0.00.049.233 I print_info: n_embd_k_gqa     = 2048
0.00.049.234 I print_info: n_embd_v_gqa     = 2048
0.00.049.234 I print_info: f_norm_eps       = 1.0e-05
0.00.049.235 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.235 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.235 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.236 I print_info: f_logit_scale    = 0.0e+00
0.00.049.237 I print_info: n_ff             = 8192
0.00.049.237 I print_info: n_expert         = 0
0.00.049.237 I print_info: n_expert_used    = 0
0.00.049.238 I print_info: causal attn      = 1
0.00.049.238 I print_info: pooling type     = 0
0.00.049.238 I print_info: rope type        = 2
0.00.049.238 I print_info: rope scaling     = linear
0.00.049.239 I print_info: freq_base_train  = 10000.0
0.00.049.239 I print_info: freq_scale_train = 1
0.00.049.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.239 I print_info: rope_finetuned   = unknown
0.00.049.240 I print_info: ssm_d_conv       = 0
0.00.049.240 I print_info: ssm_d_inner      = 0
0.00.049.240 I print_info: ssm_d_state      = 0
0.00.049.240 I print_info: ssm_dt_rank      = 0
0.00.049.240 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.240 I print_info: model type       = 1.4B
0.00.049.243 I print_info: model params     = 1.41 B
0.00.049.243 I print_info: general.name     = 1.4B
0.00.049.244 I print_info: vocab type       = BPE
0.00.049.244 I print_info: n_vocab          = 50304
0.00.049.244 I print_info: n_merges         = 50009
0.00.049.245 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.245 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.245 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.245 I print_info: LF token         = 128 'Ä'
0.00.049.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.246 I print_info: max token length = 1024
0.00.051.228 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.228 I load_tensors: offloading output layer to GPU
0.00.051.228 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.239 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.240 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.604 I llama_init_from_model: n_seq_max     = 1
0.00.051.605 I llama_init_from_model: n_ctx         = 128
0.00.051.605 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.606 I llama_init_from_model: n_batch       = 128
0.00.051.606 I llama_init_from_model: n_ubatch      = 128
0.00.051.606 I llama_init_from_model: flash_attn    = 0
0.00.051.606 I llama_init_from_model: freq_base     = 10000.0
0.00.051.606 I llama_init_from_model: freq_scale    = 1
0.00.051.607 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.607 I ggml_metal_init: allocating
0.00.051.612 I ggml_metal_init: found device: Apple M4
0.00.051.614 I ggml_metal_init: picking default device: Apple M4
0.00.052.187 I ggml_metal_init: using embedded metal library
0.00.054.556 I ggml_metal_init: GPU name:   Apple M4
0.00.054.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.558 I ggml_metal_init: simdgroup reduction   = true
0.00.054.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.559 I ggml_metal_init: has bfloat            = true
0.00.054.559 I ggml_metal_init: use bfloat            = true
0.00.054.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.197 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.462 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.465 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.479 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.416 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.417 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.418 I llama_init_from_model: graph nodes  = 967
0.00.066.418 I llama_init_from_model: graph splits = 2
0.00.066.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.647 I 
0.00.745.702 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.717 I perplexity: tokenizing the input ..
0.00.753.552 I perplexity: tokenization took 7.834 ms
0.00.753.565 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.548 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.710 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.724 I llama_perf_context_print:        load time =     736.82 ms
0.00.889.725 I llama_perf_context_print: prompt eval time =     134.76 ms /   128 tokens (    1.05 ms per token,   949.86 tokens per second)
0.00.889.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.726 I llama_perf_context_print:       total time =     144.08 ms /   129 tokens
0.00.890.143 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.077s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.323 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.037 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.044 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.045 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.049 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.054 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.054 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.836 I llama_model_loader: - type  f32:  194 tensors
0.00.025.836 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.837 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.837 I print_info: file format = GGUF V3 (latest)
0.00.025.838 I print_info: file type   = Q2_K - Medium
0.00.025.843 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.038 I load: special tokens cache size = 25
0.00.051.067 I load: token to piece cache size = 0.2984 MB
0.00.051.071 I print_info: arch             = gptneox
0.00.051.071 I print_info: vocab_only       = 0
0.00.051.071 I print_info: n_ctx_train      = 2048
0.00.051.071 I print_info: n_embd           = 2048
0.00.051.072 I print_info: n_layer          = 24
0.00.051.074 I print_info: n_head           = 16
0.00.051.075 I print_info: n_head_kv        = 16
0.00.051.075 I print_info: n_rot            = 32
0.00.051.075 I print_info: n_swa            = 0
0.00.051.076 I print_info: n_embd_head_k    = 128
0.00.051.076 I print_info: n_embd_head_v    = 128
0.00.051.076 I print_info: n_gqa            = 1
0.00.051.077 I print_info: n_embd_k_gqa     = 2048
0.00.051.078 I print_info: n_embd_v_gqa     = 2048
0.00.051.078 I print_info: f_norm_eps       = 1.0e-05
0.00.051.079 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.079 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.079 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.079 I print_info: f_logit_scale    = 0.0e+00
0.00.051.080 I print_info: n_ff             = 8192
0.00.051.080 I print_info: n_expert         = 0
0.00.051.080 I print_info: n_expert_used    = 0
0.00.051.081 I print_info: causal attn      = 1
0.00.051.081 I print_info: pooling type     = 0
0.00.051.081 I print_info: rope type        = 2
0.00.051.081 I print_info: rope scaling     = linear
0.00.051.083 I print_info: freq_base_train  = 10000.0
0.00.051.084 I print_info: freq_scale_train = 1
0.00.051.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.084 I print_info: rope_finetuned   = unknown
0.00.051.094 I print_info: ssm_d_conv       = 0
0.00.051.094 I print_info: ssm_d_inner      = 0
0.00.051.094 I print_info: ssm_d_state      = 0
0.00.051.095 I print_info: ssm_dt_rank      = 0
0.00.051.095 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.095 I print_info: model type       = 1.4B
0.00.051.096 I print_info: model params     = 1.41 B
0.00.051.096 I print_info: general.name     = 1.4B
0.00.051.096 I print_info: vocab type       = BPE
0.00.051.096 I print_info: n_vocab          = 50304
0.00.051.097 I print_info: n_merges         = 50009
0.00.051.097 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.097 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.097 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.097 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.098 I print_info: LF token         = 128 'Ä'
0.00.051.098 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.098 I print_info: max token length = 1024
0.00.052.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.954 I load_tensors: offloading output layer to GPU
0.00.052.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.965 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.966 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.233 I llama_init_from_model: n_seq_max     = 1
0.00.053.234 I llama_init_from_model: n_ctx         = 128
0.00.053.234 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.234 I llama_init_from_model: n_batch       = 128
0.00.053.235 I llama_init_from_model: n_ubatch      = 128
0.00.053.235 I llama_init_from_model: flash_attn    = 0
0.00.053.235 I llama_init_from_model: freq_base     = 10000.0
0.00.053.235 I llama_init_from_model: freq_scale    = 1
0.00.053.236 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.236 I ggml_metal_init: allocating
0.00.053.239 I ggml_metal_init: found device: Apple M4
0.00.053.241 I ggml_metal_init: picking default device: Apple M4
0.00.053.812 I ggml_metal_init: using embedded metal library
0.00.056.136 I ggml_metal_init: GPU name:   Apple M4
0.00.056.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.138 I ggml_metal_init: simdgroup reduction   = true
0.00.056.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.139 I ggml_metal_init: has bfloat            = true
0.00.056.139 I ggml_metal_init: use bfloat            = true
0.00.056.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.412 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.678 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.680 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.693 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.502 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.503 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.504 I llama_init_from_model: graph nodes  = 967
0.00.067.504 I llama_init_from_model: graph splits = 2
0.00.067.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.782 I 
0.00.476.824 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.837 I perplexity: tokenizing the input ..
0.00.484.664 I perplexity: tokenization took 7.825 ms
0.00.484.675 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.271 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.618.533 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.618.549 I llama_perf_context_print:        load time =     466.45 ms
0.00.618.550 I llama_perf_context_print: prompt eval time =     132.37 ms /   128 tokens (    1.03 ms per token,   966.99 tokens per second)
0.00.618.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.551 I llama_perf_context_print:       total time =     141.77 ms /   129 tokens
0.00.619.076 I ggml_metal_free: deallocating

real	0m0.635s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.188 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.189 I llama_model_loader: - type  f32:  194 tensors
0.00.025.190 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.190 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.190 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.190 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.191 I print_info: file format = GGUF V3 (latest)
0.00.025.191 I print_info: file type   = Q3_K - Medium
0.00.025.192 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.593 I load: special tokens cache size = 25
0.00.050.732 I load: token to piece cache size = 0.2984 MB
0.00.050.736 I print_info: arch             = gptneox
0.00.050.736 I print_info: vocab_only       = 0
0.00.050.736 I print_info: n_ctx_train      = 2048
0.00.050.736 I print_info: n_embd           = 2048
0.00.050.737 I print_info: n_layer          = 24
0.00.050.739 I print_info: n_head           = 16
0.00.050.740 I print_info: n_head_kv        = 16
0.00.050.740 I print_info: n_rot            = 32
0.00.050.741 I print_info: n_swa            = 0
0.00.050.741 I print_info: n_embd_head_k    = 128
0.00.050.742 I print_info: n_embd_head_v    = 128
0.00.050.742 I print_info: n_gqa            = 1
0.00.050.743 I print_info: n_embd_k_gqa     = 2048
0.00.050.744 I print_info: n_embd_v_gqa     = 2048
0.00.050.745 I print_info: f_norm_eps       = 1.0e-05
0.00.050.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.745 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.746 I print_info: f_logit_scale    = 0.0e+00
0.00.050.746 I print_info: n_ff             = 8192
0.00.050.746 I print_info: n_expert         = 0
0.00.050.747 I print_info: n_expert_used    = 0
0.00.050.747 I print_info: causal attn      = 1
0.00.050.747 I print_info: pooling type     = 0
0.00.050.747 I print_info: rope type        = 2
0.00.050.747 I print_info: rope scaling     = linear
0.00.050.748 I print_info: freq_base_train  = 10000.0
0.00.050.748 I print_info: freq_scale_train = 1
0.00.050.748 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.748 I print_info: rope_finetuned   = unknown
0.00.050.749 I print_info: ssm_d_conv       = 0
0.00.050.749 I print_info: ssm_d_inner      = 0
0.00.050.749 I print_info: ssm_d_state      = 0
0.00.050.749 I print_info: ssm_dt_rank      = 0
0.00.050.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.749 I print_info: model type       = 1.4B
0.00.050.750 I print_info: model params     = 1.41 B
0.00.050.750 I print_info: general.name     = 1.4B
0.00.050.751 I print_info: vocab type       = BPE
0.00.050.751 I print_info: n_vocab          = 50304
0.00.050.751 I print_info: n_merges         = 50009
0.00.050.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.752 I print_info: LF token         = 128 'Ä'
0.00.050.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.752 I print_info: max token length = 1024
0.00.052.703 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.703 I load_tensors: offloading output layer to GPU
0.00.052.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.714 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.715 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.009 I llama_init_from_model: n_seq_max     = 1
0.00.053.009 I llama_init_from_model: n_ctx         = 128
0.00.053.009 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.010 I llama_init_from_model: n_batch       = 128
0.00.053.010 I llama_init_from_model: n_ubatch      = 128
0.00.053.010 I llama_init_from_model: flash_attn    = 0
0.00.053.010 I llama_init_from_model: freq_base     = 10000.0
0.00.053.011 I llama_init_from_model: freq_scale    = 1
0.00.053.011 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.011 I ggml_metal_init: allocating
0.00.053.015 I ggml_metal_init: found device: Apple M4
0.00.053.016 I ggml_metal_init: picking default device: Apple M4
0.00.053.596 I ggml_metal_init: using embedded metal library
0.00.055.984 I ggml_metal_init: GPU name:   Apple M4
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.986 I ggml_metal_init: simdgroup reduction   = true
0.00.055.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.986 I ggml_metal_init: has bfloat            = true
0.00.055.987 I ggml_metal_init: use bfloat            = true
0.00.055.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.780 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.043 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.069 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.005 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.007 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.007 I llama_init_from_model: graph nodes  = 967
0.00.068.007 I llama_init_from_model: graph splits = 2
0.00.068.008 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.426 I 
0.00.522.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.476 I perplexity: tokenizing the input ..
0.00.530.672 I perplexity: tokenization took 8.194 ms
0.00.530.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.662.656 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.663.816 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.663.833 I llama_perf_context_print:        load time =     513.44 ms
0.00.663.834 I llama_perf_context_print: prompt eval time =     131.74 ms /   128 tokens (    1.03 ms per token,   971.64 tokens per second)
0.00.663.835 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.663.835 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.664.249 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.049 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.152 I llama_model_loader: - type  f32:  194 tensors
0.00.026.153 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.153 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.153 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.154 I print_info: file format = GGUF V3 (latest)
0.00.026.154 I print_info: file type   = Q4_K - Medium
0.00.026.155 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.595 I load: special tokens cache size = 25
0.00.050.467 I load: token to piece cache size = 0.2984 MB
0.00.050.470 I print_info: arch             = gptneox
0.00.050.470 I print_info: vocab_only       = 0
0.00.050.470 I print_info: n_ctx_train      = 2048
0.00.050.471 I print_info: n_embd           = 2048
0.00.050.471 I print_info: n_layer          = 24
0.00.050.474 I print_info: n_head           = 16
0.00.050.474 I print_info: n_head_kv        = 16
0.00.050.475 I print_info: n_rot            = 32
0.00.050.475 I print_info: n_swa            = 0
0.00.050.475 I print_info: n_embd_head_k    = 128
0.00.050.475 I print_info: n_embd_head_v    = 128
0.00.050.476 I print_info: n_gqa            = 1
0.00.050.477 I print_info: n_embd_k_gqa     = 2048
0.00.050.477 I print_info: n_embd_v_gqa     = 2048
0.00.050.478 I print_info: f_norm_eps       = 1.0e-05
0.00.050.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.479 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.479 I print_info: f_logit_scale    = 0.0e+00
0.00.050.480 I print_info: n_ff             = 8192
0.00.050.480 I print_info: n_expert         = 0
0.00.050.480 I print_info: n_expert_used    = 0
0.00.050.480 I print_info: causal attn      = 1
0.00.050.480 I print_info: pooling type     = 0
0.00.050.480 I print_info: rope type        = 2
0.00.050.480 I print_info: rope scaling     = linear
0.00.050.481 I print_info: freq_base_train  = 10000.0
0.00.050.482 I print_info: freq_scale_train = 1
0.00.050.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.483 I print_info: rope_finetuned   = unknown
0.00.050.483 I print_info: ssm_d_conv       = 0
0.00.050.483 I print_info: ssm_d_inner      = 0
0.00.050.483 I print_info: ssm_d_state      = 0
0.00.050.483 I print_info: ssm_dt_rank      = 0
0.00.050.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.483 I print_info: model type       = 1.4B
0.00.050.484 I print_info: model params     = 1.41 B
0.00.050.484 I print_info: general.name     = 1.4B
0.00.050.485 I print_info: vocab type       = BPE
0.00.050.485 I print_info: n_vocab          = 50304
0.00.050.485 I print_info: n_merges         = 50009
0.00.050.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.486 I print_info: LF token         = 128 'Ä'
0.00.050.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.487 I print_info: max token length = 1024
0.00.052.454 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.454 I load_tensors: offloading output layer to GPU
0.00.052.454 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.465 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.466 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.736 I llama_init_from_model: n_seq_max     = 1
0.00.052.737 I llama_init_from_model: n_ctx         = 128
0.00.052.737 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.737 I llama_init_from_model: n_batch       = 128
0.00.052.737 I llama_init_from_model: n_ubatch      = 128
0.00.052.737 I llama_init_from_model: flash_attn    = 0
0.00.052.737 I llama_init_from_model: freq_base     = 10000.0
0.00.052.738 I llama_init_from_model: freq_scale    = 1
0.00.052.738 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.738 I ggml_metal_init: allocating
0.00.052.741 I ggml_metal_init: found device: Apple M4
0.00.052.743 I ggml_metal_init: picking default device: Apple M4
0.00.053.304 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.626 I ggml_metal_init: simdgroup reduction   = true
0.00.055.626 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.626 I ggml_metal_init: has bfloat            = true
0.00.055.626 I ggml_metal_init: use bfloat            = true
0.00.055.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.079 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.336 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.340 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.364 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.238 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.239 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.239 I llama_init_from_model: graph nodes  = 967
0.00.067.239 I llama_init_from_model: graph splits = 2
0.00.067.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.587 I 
0.00.541.623 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.636 I perplexity: tokenizing the input ..
0.00.549.797 I perplexity: tokenization took 8.159 ms
0.00.549.807 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.683.977 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.685.247 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.685.265 I llama_perf_context_print:        load time =     531.53 ms
0.00.685.266 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.64 tokens per second)
0.00.685.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.685.267 I llama_perf_context_print:       total time =     143.68 ms /   129 tokens
0.00.685.755 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.759 I llama_model_loader: - type  f32:  194 tensors
0.00.024.759 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.759 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.760 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q5_K - Medium
0.00.024.761 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.363 I load: special tokens cache size = 25
0.00.049.381 I load: token to piece cache size = 0.2984 MB
0.00.049.384 I print_info: arch             = gptneox
0.00.049.384 I print_info: vocab_only       = 0
0.00.049.384 I print_info: n_ctx_train      = 2048
0.00.049.385 I print_info: n_embd           = 2048
0.00.049.385 I print_info: n_layer          = 24
0.00.049.388 I print_info: n_head           = 16
0.00.049.389 I print_info: n_head_kv        = 16
0.00.049.389 I print_info: n_rot            = 32
0.00.049.389 I print_info: n_swa            = 0
0.00.049.389 I print_info: n_embd_head_k    = 128
0.00.049.389 I print_info: n_embd_head_v    = 128
0.00.049.390 I print_info: n_gqa            = 1
0.00.049.391 I print_info: n_embd_k_gqa     = 2048
0.00.049.391 I print_info: n_embd_v_gqa     = 2048
0.00.049.393 I print_info: f_norm_eps       = 1.0e-05
0.00.049.393 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.393 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.394 I print_info: f_logit_scale    = 0.0e+00
0.00.049.394 I print_info: n_ff             = 8192
0.00.049.394 I print_info: n_expert         = 0
0.00.049.395 I print_info: n_expert_used    = 0
0.00.049.395 I print_info: causal attn      = 1
0.00.049.395 I print_info: pooling type     = 0
0.00.049.395 I print_info: rope type        = 2
0.00.049.395 I print_info: rope scaling     = linear
0.00.049.398 I print_info: freq_base_train  = 10000.0
0.00.049.399 I print_info: freq_scale_train = 1
0.00.049.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.399 I print_info: rope_finetuned   = unknown
0.00.049.399 I print_info: ssm_d_conv       = 0
0.00.049.400 I print_info: ssm_d_inner      = 0
0.00.049.400 I print_info: ssm_d_state      = 0
0.00.049.400 I print_info: ssm_dt_rank      = 0
0.00.049.401 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.405 I print_info: model type       = 1.4B
0.00.049.405 I print_info: model params     = 1.41 B
0.00.049.405 I print_info: general.name     = 1.4B
0.00.049.407 I print_info: vocab type       = BPE
0.00.049.407 I print_info: n_vocab          = 50304
0.00.049.407 I print_info: n_merges         = 50009
0.00.049.408 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.408 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.408 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.408 I print_info: LF token         = 128 'Ä'
0.00.049.409 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.409 I print_info: max token length = 1024
0.00.051.194 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.194 I load_tensors: offloading output layer to GPU
0.00.051.194 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.200 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.201 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.574 I llama_init_from_model: n_seq_max     = 1
0.00.051.575 I llama_init_from_model: n_ctx         = 128
0.00.051.575 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.575 I llama_init_from_model: n_batch       = 128
0.00.051.575 I llama_init_from_model: n_ubatch      = 128
0.00.051.575 I llama_init_from_model: flash_attn    = 0
0.00.051.576 I llama_init_from_model: freq_base     = 10000.0
0.00.051.576 I llama_init_from_model: freq_scale    = 1
0.00.051.576 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.577 I ggml_metal_init: allocating
0.00.051.579 I ggml_metal_init: found device: Apple M4
0.00.051.581 I ggml_metal_init: picking default device: Apple M4
0.00.052.158 I ggml_metal_init: using embedded metal library
0.00.054.607 I ggml_metal_init: GPU name:   Apple M4
0.00.054.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.609 I ggml_metal_init: simdgroup reduction   = true
0.00.054.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.609 I ggml_metal_init: has bfloat            = true
0.00.054.609 I ggml_metal_init: use bfloat            = true
0.00.054.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.611 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.155 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.391 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.389 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.391 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.391 I llama_init_from_model: graph nodes  = 967
0.00.066.391 I llama_init_from_model: graph splits = 2
0.00.066.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.702 I 
0.00.700.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.795 I perplexity: tokenizing the input ..
0.00.708.889 I perplexity: tokenization took 8.093 ms
0.00.708.902 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.850 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.851.028 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.851.044 I llama_perf_context_print:        load time =     691.85 ms
0.00.851.045 I llama_perf_context_print: prompt eval time =     140.72 ms /   128 tokens (    1.10 ms per token,   909.60 tokens per second)
0.00.851.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.046 I llama_perf_context_print:       total time =     150.35 ms /   129 tokens
0.00.851.507 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.077s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.168 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.952 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.958 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.959 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.888 I llama_model_loader: - type  f32:  194 tensors
0.00.025.888 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.888 I print_info: file format = GGUF V3 (latest)
0.00.025.889 I print_info: file type   = Q6_K
0.00.025.890 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.131 I load: special tokens cache size = 25
0.00.051.252 I load: token to piece cache size = 0.2984 MB
0.00.051.255 I print_info: arch             = gptneox
0.00.051.256 I print_info: vocab_only       = 0
0.00.051.256 I print_info: n_ctx_train      = 2048
0.00.051.256 I print_info: n_embd           = 2048
0.00.051.256 I print_info: n_layer          = 24
0.00.051.259 I print_info: n_head           = 16
0.00.051.260 I print_info: n_head_kv        = 16
0.00.051.260 I print_info: n_rot            = 32
0.00.051.260 I print_info: n_swa            = 0
0.00.051.260 I print_info: n_embd_head_k    = 128
0.00.051.261 I print_info: n_embd_head_v    = 128
0.00.051.261 I print_info: n_gqa            = 1
0.00.051.262 I print_info: n_embd_k_gqa     = 2048
0.00.051.263 I print_info: n_embd_v_gqa     = 2048
0.00.051.263 I print_info: f_norm_eps       = 1.0e-05
0.00.051.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.264 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.264 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.264 I print_info: f_logit_scale    = 0.0e+00
0.00.051.265 I print_info: n_ff             = 8192
0.00.051.265 I print_info: n_expert         = 0
0.00.051.265 I print_info: n_expert_used    = 0
0.00.051.265 I print_info: causal attn      = 1
0.00.051.265 I print_info: pooling type     = 0
0.00.051.265 I print_info: rope type        = 2
0.00.051.266 I print_info: rope scaling     = linear
0.00.051.266 I print_info: freq_base_train  = 10000.0
0.00.051.266 I print_info: freq_scale_train = 1
0.00.051.267 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.267 I print_info: rope_finetuned   = unknown
0.00.051.267 I print_info: ssm_d_conv       = 0
0.00.051.267 I print_info: ssm_d_inner      = 0
0.00.051.267 I print_info: ssm_d_state      = 0
0.00.051.267 I print_info: ssm_dt_rank      = 0
0.00.051.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.268 I print_info: model type       = 1.4B
0.00.051.268 I print_info: model params     = 1.41 B
0.00.051.268 I print_info: general.name     = 1.4B
0.00.051.269 I print_info: vocab type       = BPE
0.00.051.269 I print_info: n_vocab          = 50304
0.00.051.269 I print_info: n_merges         = 50009
0.00.051.270 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.270 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.273 I print_info: LF token         = 128 'Ä'
0.00.051.273 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.273 I print_info: max token length = 1024
0.00.053.258 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.259 I load_tensors: offloading output layer to GPU
0.00.053.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.269 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.271 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.577 I llama_init_from_model: n_seq_max     = 1
0.00.053.578 I llama_init_from_model: n_ctx         = 128
0.00.053.578 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.579 I llama_init_from_model: n_batch       = 128
0.00.053.579 I llama_init_from_model: n_ubatch      = 128
0.00.053.579 I llama_init_from_model: flash_attn    = 0
0.00.053.579 I llama_init_from_model: freq_base     = 10000.0
0.00.053.580 I llama_init_from_model: freq_scale    = 1
0.00.053.580 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.580 I ggml_metal_init: allocating
0.00.053.584 I ggml_metal_init: found device: Apple M4
0.00.053.586 I ggml_metal_init: picking default device: Apple M4
0.00.054.163 I ggml_metal_init: using embedded metal library
0.00.056.504 I ggml_metal_init: GPU name:   Apple M4
0.00.056.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.506 I ggml_metal_init: simdgroup reduction   = true
0.00.056.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.506 I ggml_metal_init: has bfloat            = true
0.00.056.506 I ggml_metal_init: use bfloat            = true
0.00.056.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.479 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.463 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.464 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.464 I llama_init_from_model: graph nodes  = 967
0.00.068.464 I llama_init_from_model: graph splits = 2
0.00.068.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.198.764 I 
0.00.198.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.198.821 I perplexity: tokenizing the input ..
0.00.206.290 I perplexity: tokenization took 7.467 ms
0.00.206.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.345.693 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.347.088 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.347.102 I llama_perf_context_print:        load time =     188.59 ms
0.00.347.103 I llama_perf_context_print: prompt eval time =     139.12 ms /   128 tokens (    1.09 ms per token,   920.04 tokens per second)
0.00.347.104 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.347.104 I llama_perf_context_print:       total time =     148.34 ms /   129 tokens
0.00.347.457 I ggml_metal_free: deallocating

real	0m0.362s
user	0m0.078s
sys	0m0.043s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4531 (6152129d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.578 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.496 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.533 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.534 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.535 I llama_model_loader: - type  f32:  194 tensors
0.00.049.535 I llama_model_loader: - type  f16:   98 tensors
0.00.049.536 I print_info: file format = GGUF V3 (latest)
0.00.049.536 I print_info: file type   = all F32 (guessed)
0.00.049.537 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.372 I load: special tokens cache size = 25
0.00.083.091 I load: token to piece cache size = 0.2984 MB
0.00.083.094 I print_info: arch             = gptneox
0.00.083.094 I print_info: vocab_only       = 0
0.00.083.094 I print_info: n_ctx_train      = 2048
0.00.083.094 I print_info: n_embd           = 2048
0.00.083.095 I print_info: n_layer          = 24
0.00.083.098 I print_info: n_head           = 16
0.00.083.099 I print_info: n_head_kv        = 16
0.00.083.099 I print_info: n_rot            = 32
0.00.083.099 I print_info: n_swa            = 0
0.00.083.099 I print_info: n_embd_head_k    = 128
0.00.083.099 I print_info: n_embd_head_v    = 128
0.00.083.100 I print_info: n_gqa            = 1
0.00.083.101 I print_info: n_embd_k_gqa     = 2048
0.00.083.101 I print_info: n_embd_v_gqa     = 2048
0.00.083.102 I print_info: f_norm_eps       = 1.0e-05
0.00.083.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.083.102 I print_info: f_clamp_kqv      = 0.0e+00
0.00.083.102 I print_info: f_max_alibi_bias = 0.0e+00
0.00.083.102 I print_info: f_logit_scale    = 0.0e+00
0.00.083.103 I print_info: n_ff             = 8192
0.00.083.103 I print_info: n_expert         = 0
0.00.083.103 I print_info: n_expert_used    = 0
0.00.083.103 I print_info: causal attn      = 1
0.00.083.103 I print_info: pooling type     = 0
0.00.083.104 I print_info: rope type        = 2
0.00.083.104 I print_info: rope scaling     = linear
0.00.083.106 I print_info: freq_base_train  = 10000.0
0.00.083.106 I print_info: freq_scale_train = 1
0.00.083.106 I print_info: n_ctx_orig_yarn  = 2048
0.00.083.107 I print_info: rope_finetuned   = unknown
0.00.083.107 I print_info: ssm_d_conv       = 0
0.00.083.107 I print_info: ssm_d_inner      = 0
0.00.083.107 I print_info: ssm_d_state      = 0
0.00.083.107 I print_info: ssm_dt_rank      = 0
0.00.083.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.083.108 I print_info: model type       = 1.4B
0.00.083.109 I print_info: model params     = 1.41 B
0.00.083.109 I print_info: general.name     = 1.4B
0.00.083.109 I print_info: vocab type       = BPE
0.00.083.109 I print_info: n_vocab          = 50304
0.00.083.110 I print_info: n_merges         = 50009
0.00.083.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.083.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.083.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.083.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.083.111 I print_info: LF token         = 128 'Ä'
0.00.083.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.083.111 I print_info: max token length = 1024
0.00.085.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.085.710 I load_tensors: offloading output layer to GPU
0.00.085.710 I load_tensors: offloaded 25/25 layers to GPU
0.00.085.721 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.722 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.086.009 I llama_init_from_model: n_seq_max     = 1
0.00.086.010 I llama_init_from_model: n_ctx         = 128
0.00.086.010 I llama_init_from_model: n_ctx_per_seq = 128
0.00.086.010 I llama_init_from_model: n_batch       = 128
0.00.086.010 I llama_init_from_model: n_ubatch      = 128
0.00.086.011 I llama_init_from_model: flash_attn    = 0
0.00.086.011 I llama_init_from_model: freq_base     = 10000.0
0.00.086.011 I llama_init_from_model: freq_scale    = 1
0.00.086.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.012 I ggml_metal_init: allocating
0.00.086.015 I ggml_metal_init: found device: Apple M4
0.00.086.017 I ggml_metal_init: picking default device: Apple M4
0.00.086.643 I ggml_metal_init: using embedded metal library
0.00.089.235 I ggml_metal_init: GPU name:   Apple M4
0.00.089.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.238 I ggml_metal_init: simdgroup reduction   = true
0.00.089.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.238 I ggml_metal_init: has bfloat            = true
0.00.089.238 I ggml_metal_init: use bfloat            = true
0.00.089.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.538 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.802 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.804 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.677 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.101.678 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.101.679 I llama_init_from_model: graph nodes  = 967
0.00.101.679 I llama_init_from_model: graph splits = 2
0.00.101.680 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.101.681 I 
0.00.101.719 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.720 I compute_imatrix: tokenizing the input ..
0.00.109.031 I compute_imatrix: tokenization took 7.309 ms
0.00.109.033 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.572.244 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.574.756 I llama_perf_context_print:        load time =    1551.66 ms
0.01.574.757 I llama_perf_context_print: prompt eval time =    1462.49 ms /   128 tokens (   11.43 ms per token,    87.52 tokens per second)
0.01.574.757 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.574.758 I llama_perf_context_print:       total time =    1554.17 ms /   129 tokens
0.01.575.353 I ggml_metal_free: deallocating

real	0m1.769s
user	0m0.165s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4531 (6152129d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f0d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f33710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f45f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f61550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f63a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f63ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f5f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f60d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f61880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f62900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f62e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f63400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f64a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f64cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f65500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f6c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f6c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f6cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f6ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f6d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f6d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f6dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f6e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f6e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f6e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f6e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f6ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f6f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f6f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f6fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f6fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f6ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f70240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f70500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f70a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f70d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f71000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f71580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f71840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f71b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f72080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f72600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f72b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f73680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f73940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f73c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f73ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f74180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f74700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f74c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f74f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f75200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f75a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f75d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f75fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f76280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f76540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f76800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f76ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f76d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f77040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f77b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f77e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f78380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f78640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f78900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f78bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f78e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f79140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f79400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f79980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f79c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f79f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f7bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f7c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f7c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f7cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f7d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f7d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f7dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f7e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f7e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f7ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f7f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f7fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f80270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f80d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f81260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f81d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f82250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f82cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f83240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f83790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f83ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f84cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f85220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f85770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f85cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f86210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f86760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f86cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f87200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f87750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f87ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f88740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f88c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f89730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f89c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f8a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f8a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f8ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f8b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f8b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f8bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f8c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f8c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f8c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f8cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f8d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f8d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f8dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f8df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f8e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f8e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f8ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f8f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f8f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f8f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f8fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f90fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f91df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f920b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f92520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f92b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f93130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.831s
user	0m0.298s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4531 (6152129d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f60b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f60bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f60c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f60c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f60cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f60d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f60d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f60de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f60e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f60e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f60ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f60f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f60fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f6105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f611500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f6171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f6189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f6195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f619f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f61a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f61ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f61b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f61b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f61b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f61bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f61ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f61d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f61da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f61e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f61ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f61f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f61fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f61ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f6203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f622070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f622510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f6229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f6232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f623c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f6240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f6263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f6273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f6283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f6293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f62a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f62a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f62ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f62b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f62b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f62be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f62c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f62c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f62ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f61cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f62d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f62da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f62dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f62e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f62ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f62ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f62f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f62fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f62ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f6304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f6314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f6328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f6331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f6356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f6364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f6372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f6389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f6397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f63a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f63a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f63aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f63aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f63b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f63b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f63bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f63c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f63c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f63ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f63d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f63d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f63dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f63e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f63e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f63eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f63ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f63f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f63f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f63fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f6406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f6434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f6442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f6450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f6459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f6467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f647100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f6475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f6496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f649c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f64a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f64a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f64a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f64af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f64b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f64bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f64c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f64c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f64cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f64d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f64d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f64def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f64e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f64e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f64ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f64f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f64f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f64ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f6509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f6519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f6529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f6583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f6593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f65a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f65a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f65ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f65b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f65b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f65be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f65c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f65c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f65ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f65d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f65d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f65de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f65e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f65e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f65ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f65f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f65f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f65fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f6608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f6618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f6622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f6639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f6647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f6650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f6668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f6676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f667e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f6687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f6692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f6698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.090.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1310055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1310075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131007a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1310080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1310093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13100a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13100a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13100b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13100b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13100c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13100c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13100ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13100d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13100dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13100e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13100e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13100e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13100ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13100f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13100f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13100fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131010010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131010480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131010740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131010bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131011020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131011490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131011900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131011d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1310121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131012650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131012ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131012f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1310133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131013810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131013c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1310140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131014560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1310149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131014e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1310152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131015b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131016000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131016470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1310168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131017350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1310177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131017c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1310180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131018510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131018980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131018df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131019260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1310196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131019b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131019fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13101a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13101a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13101ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13101b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13101b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13101ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13101bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13101c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13101c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13101cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13101d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13101d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13101d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13101ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13101e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13101e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13101eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13101ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13101f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13101f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13101fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131020150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1310205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131020a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131020ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131021780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131021bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131022060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1310224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131022940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131022db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131023690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131023b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131023f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1310243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131024850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131024cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1310255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131025a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131025e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1310262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131026760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131026bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131027040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1310274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131027920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131027d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131028200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131028670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131028ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131028f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1310293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131029830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131029ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13102a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13102a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13102a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13102ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13102b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13102b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13102bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13102c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13102c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13102c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13102cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13102d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13102d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13102dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13102df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13102e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13102e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13102ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13102f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13102f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13102f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13102fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1310302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131030720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131030b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131031000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131031470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1310318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131031d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1310321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131032630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131032aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131032f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131033380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1310337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131033c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1310340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131034540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1310349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131035290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131035ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131036180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131036440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1310368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131036d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131037190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131037600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131037a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131037ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131038350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1310387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131038c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1310390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131039510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131039980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131039df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13103a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13103a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13103ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13103afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13103b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13103b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13103bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13103c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13103c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13103ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13103cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13103d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13103d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13103dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13103e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13103e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13103e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13103edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13103f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13103f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13103fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131040120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131040590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131040a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131040e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1310412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131041800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131041d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131042880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131042b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131043100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1310436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131043c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131044800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131044dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131045380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131045940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131045f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1310464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131046a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131047040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131047600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131047bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131048180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131048740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1310492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131049880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131049e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13104a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13104a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13104af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13104b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13104bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13104c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13104c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13104cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13104d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13104d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13104dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13104e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13104e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13104eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13104f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13104fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131050000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1310505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131050b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131051140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131051700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131051cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131052280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131052840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131052e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1310533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131053980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131053f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131054500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131054ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131055080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131055640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131055c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1310561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131056780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131056d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131057240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131057740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131057c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131058140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131058640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131058b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131059040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131059540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131059a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131059f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13105a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13105a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13105ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13105b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13105b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13105c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13105c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13105d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13105d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13105da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13105e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13105e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13105eb30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f70ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f70e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f70e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f70e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f70edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f70f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f70f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f70fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f70ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f7103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f710860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f711aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f712250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f712a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f713180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f7138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f713fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f7146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f715c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f716370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f716a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f7171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f717730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f717ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f718010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f718480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f718e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f719300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f7195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f719a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f71a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f71a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f71ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f71b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f71bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f71c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f71c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f71cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f71d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f71d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f71ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f71e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f71e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f71eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f71ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f71f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f71f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f720030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f7204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f720da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f721590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f721a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f721ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f722370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f722810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f722cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f7235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f723a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f723f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f7243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ef04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ef044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ef04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ef04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ef05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ef056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ef05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ef05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ef06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ef06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ef06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ef07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ef075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ef07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ef07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ef08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ef08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ef08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ef09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ef094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ef09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ef09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ef0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ef0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ef0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ef0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ef0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ef0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ef0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ef0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ef0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ef0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ef0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ef0d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ef0dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ef0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ef0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ef0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ef0f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ef0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ef0fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ef10390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ef10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ef10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ef113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ef118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ef11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ef122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ef127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ef12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ef131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ef136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ef13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ef140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ef145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ef14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ef14ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ef154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ef159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ef15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ef163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ef168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ef16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ef172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ef177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ef17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ef181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ef186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ef18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ef190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ef195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ef19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ef19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ef1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ef1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ef1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ef1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ef1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ef1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ef1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ef1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ef1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ef1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ef1d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ef1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ef1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ef1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ef1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ef1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ef1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ef1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ef1fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ef203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ef208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ef20df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ef212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ef217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ef21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ef221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ef226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ef22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ef230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ef235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ef23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ef23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ef244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ef249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ef24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ef253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ef258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ef25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ef262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ef267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ef26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ef271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ef276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ef27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ef280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ef285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ef28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ef28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ef294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ef299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ef29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ef2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ef2aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ef2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ef2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ef2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ef2c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ef2c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ef2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ef2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ef2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ef2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ef2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ef2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ef2efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ef2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ef2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ef300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ef30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ef30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ef310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ef31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ef31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ef320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ef32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ef32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ef330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ef335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ef33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ef34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ef345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ef34b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ef35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ef355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ef35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ef36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ef365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ef36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ef37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ef375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ef37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ef38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ef385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ef38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ef39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ef39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ef39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ef3a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ef3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ef3aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ef3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ef3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ef3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ef3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ef3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ef3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ef3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ef3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ef3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ef3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ef3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ef3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ef3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ef3f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ef3fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ef3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ef40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ef40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ef40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ef41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ef41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ef41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ef42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ef42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ef42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ef43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ef43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ef43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ef44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ef44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ef44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ef44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ef453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ef45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ef45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ef461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ef46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ef46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ef46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ef47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ef47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ef48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ef48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ef49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ef49440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ef49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ef49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ef4a500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.918s
user	0m0.244s
sys	0m0.141s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
