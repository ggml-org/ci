Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.536s
user	0m0.874s
sys	0m1.218s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Built target llava_static
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Built target llama-infill
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup
[ 80%] Built target llama-cli
[ 80%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-parallel
[ 83%] Generating loading.html.hpp
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Generating index.html.gz.hpp
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-perplexity
[ 87%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.112s
user	0m6.023s
sys	0m9.489s

main: quantize time =  5563.05 ms
main:    total time =  5563.05 ms

main: quantize time =  2314.47 ms
main:    total time =  2314.47 ms

main: quantize time =  1926.20 ms
main:    total time =  1926.20 ms

main: quantize time =  1691.42 ms
main:    total time =  1691.42 ms

main: quantize time =  2049.02 ms
main:    total time =  2049.02 ms

main: quantize time =  5310.28 ms
main:    total time =  5310.28 ms

main: quantize time =  5705.15 ms
main:    total time =  5705.15 ms

main: quantize time =  6745.93 ms
main:    total time =  6745.93 ms

main: quantize time =  6037.25 ms
main:    total time =  6037.25 ms

main: quantize time =  4388.73 ms
main:    total time =  4388.73 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.115 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.231 I main: llama backend init
0.00.000.240 I main: load the model and apply lora adapter, if any
0.00.026.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.803 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.819 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.826 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.826 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.827 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.827 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.257 I llama_model_loader: - type  f32:  194 tensors
0.00.054.257 I llama_model_loader: - type  f16:   98 tensors
0.00.054.258 I print_info: file format = GGUF V3 (latest)
0.00.054.258 I print_info: file type   = all F32 (guessed)
0.00.054.260 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.074.061 I load: special tokens cache size = 25
0.00.079.926 I load: token to piece cache size = 0.2984 MB
0.00.079.932 I print_info: arch             = gptneox
0.00.079.933 I print_info: vocab_only       = 0
0.00.079.934 I print_info: n_ctx_train      = 2048
0.00.079.934 I print_info: n_embd           = 2048
0.00.079.935 I print_info: n_layer          = 24
0.00.079.939 I print_info: n_head           = 16
0.00.079.940 I print_info: n_head_kv        = 16
0.00.079.940 I print_info: n_rot            = 32
0.00.079.941 I print_info: n_swa            = 0
0.00.079.941 I print_info: n_embd_head_k    = 128
0.00.079.941 I print_info: n_embd_head_v    = 128
0.00.079.942 I print_info: n_gqa            = 1
0.00.079.942 I print_info: n_embd_k_gqa     = 2048
0.00.079.944 I print_info: n_embd_v_gqa     = 2048
0.00.079.945 I print_info: f_norm_eps       = 1.0e-05
0.00.079.945 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.945 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.945 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.946 I print_info: f_logit_scale    = 0.0e+00
0.00.079.946 I print_info: n_ff             = 8192
0.00.079.946 I print_info: n_expert         = 0
0.00.079.947 I print_info: n_expert_used    = 0
0.00.079.947 I print_info: causal attn      = 1
0.00.079.947 I print_info: pooling type     = 0
0.00.079.947 I print_info: rope type        = 2
0.00.079.948 I print_info: rope scaling     = linear
0.00.079.948 I print_info: freq_base_train  = 10000.0
0.00.079.948 I print_info: freq_scale_train = 1
0.00.079.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.949 I print_info: rope_finetuned   = unknown
0.00.079.949 I print_info: ssm_d_conv       = 0
0.00.079.949 I print_info: ssm_d_inner      = 0
0.00.079.949 I print_info: ssm_d_state      = 0
0.00.079.949 I print_info: ssm_dt_rank      = 0
0.00.079.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.950 I print_info: model type       = 1.4B
0.00.079.950 I print_info: model params     = 1.41 B
0.00.079.950 I print_info: general.name     = 1.4B
0.00.079.951 I print_info: vocab type       = BPE
0.00.079.951 I print_info: n_vocab          = 50304
0.00.079.952 I print_info: n_merges         = 50009
0.00.079.953 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.953 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.953 I print_info: LF token         = 128 'Ä'
0.00.079.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.954 I print_info: max token length = 1024
0.00.082.279 I load_tensors: offloading 24 repeating layers to GPU
0.00.082.279 I load_tensors: offloading output layer to GPU
0.00.082.279 I load_tensors: offloaded 25/25 layers to GPU
0.00.082.299 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.082.301 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.082.609 I llama_init_from_model: n_seq_max     = 1
0.00.082.610 I llama_init_from_model: n_ctx         = 2048
0.00.082.610 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.082.610 I llama_init_from_model: n_batch       = 2048
0.00.082.611 I llama_init_from_model: n_ubatch      = 512
0.00.082.611 I llama_init_from_model: flash_attn    = 0
0.00.082.611 I llama_init_from_model: freq_base     = 10000.0
0.00.082.613 I llama_init_from_model: freq_scale    = 1
0.00.082.613 I ggml_metal_init: allocating
0.00.082.617 I ggml_metal_init: found device: Apple M4
0.00.082.619 I ggml_metal_init: picking default device: Apple M4
0.00.083.304 I ggml_metal_init: using embedded metal library
0.00.129.903 I ggml_metal_init: GPU name:   Apple M4
0.00.129.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.129.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.129.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.129.909 I ggml_metal_init: simdgroup reduction   = true
0.00.129.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.129.909 I ggml_metal_init: has bfloat            = true
0.00.129.909 I ggml_metal_init: use bfloat            = true
0.00.129.910 I ggml_metal_init: hasUnifiedMemory      = true
0.00.129.911 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.173.584 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.192.968 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.192.973 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.192.995 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.193.974 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.193.976 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.193.976 I llama_init_from_model: graph nodes  = 967
0.00.193.976 I llama_init_from_model: graph splits = 2
0.00.193.979 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.194.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.194.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.276.513 I main: llama threadpool init, n_threads = 4
0.00.276.549 I 
0.00.276.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.276.588 I 
0.00.276.657 I sampler seed: 1234
0.00.276.663 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.276.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.276.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.276.691 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.148.256 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47811.45 tokens per second)
0.02.148.257 I llama_perf_context_print:        load time =     249.77 ms
0.02.148.259 I llama_perf_context_print: prompt eval time =      44.04 ms /     7 tokens (    6.29 ms per token,   158.94 tokens per second)
0.02.148.260 I llama_perf_context_print:        eval time =    1824.68 ms /    63 runs   (   28.96 ms per token,    34.53 tokens per second)
0.02.148.260 I llama_perf_context_print:       total time =    1871.75 ms /    70 tokens
0.02.148.477 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.131s
sys	0m0.094s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.836 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.482 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.483 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.483 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.484 I llama_model_loader: - type  f32:  194 tensors
0.00.032.484 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.485 I print_info: file format = GGUF V3 (latest)
0.00.032.485 I print_info: file type   = Q8_0
0.00.032.487 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.476 I load: special tokens cache size = 25
0.00.058.723 I load: token to piece cache size = 0.2984 MB
0.00.058.728 I print_info: arch             = gptneox
0.00.058.729 I print_info: vocab_only       = 0
0.00.058.729 I print_info: n_ctx_train      = 2048
0.00.058.729 I print_info: n_embd           = 2048
0.00.058.733 I print_info: n_layer          = 24
0.00.058.739 I print_info: n_head           = 16
0.00.058.741 I print_info: n_head_kv        = 16
0.00.058.741 I print_info: n_rot            = 32
0.00.058.741 I print_info: n_swa            = 0
0.00.058.741 I print_info: n_embd_head_k    = 128
0.00.058.741 I print_info: n_embd_head_v    = 128
0.00.058.742 I print_info: n_gqa            = 1
0.00.058.743 I print_info: n_embd_k_gqa     = 2048
0.00.058.744 I print_info: n_embd_v_gqa     = 2048
0.00.058.744 I print_info: f_norm_eps       = 1.0e-05
0.00.058.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.745 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.745 I print_info: f_logit_scale    = 0.0e+00
0.00.058.746 I print_info: n_ff             = 8192
0.00.058.746 I print_info: n_expert         = 0
0.00.058.746 I print_info: n_expert_used    = 0
0.00.058.747 I print_info: causal attn      = 1
0.00.058.748 I print_info: pooling type     = 0
0.00.058.748 I print_info: rope type        = 2
0.00.058.748 I print_info: rope scaling     = linear
0.00.058.749 I print_info: freq_base_train  = 10000.0
0.00.058.749 I print_info: freq_scale_train = 1
0.00.058.749 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.749 I print_info: rope_finetuned   = unknown
0.00.058.750 I print_info: ssm_d_conv       = 0
0.00.058.750 I print_info: ssm_d_inner      = 0
0.00.058.750 I print_info: ssm_d_state      = 0
0.00.058.750 I print_info: ssm_dt_rank      = 0
0.00.058.750 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.751 I print_info: model type       = 1.4B
0.00.058.751 I print_info: model params     = 1.41 B
0.00.058.751 I print_info: general.name     = 1.4B
0.00.058.758 I print_info: vocab type       = BPE
0.00.058.760 I print_info: n_vocab          = 50304
0.00.058.760 I print_info: n_merges         = 50009
0.00.058.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.761 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.761 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.761 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.761 I print_info: LF token         = 128 'Ä'
0.00.058.761 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.763 I print_info: max token length = 1024
0.00.061.173 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.173 I load_tensors: offloading output layer to GPU
0.00.061.174 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.186 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.187 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.061.543 I llama_init_from_model: n_seq_max     = 1
0.00.061.544 I llama_init_from_model: n_ctx         = 2048
0.00.061.544 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.545 I llama_init_from_model: n_batch       = 2048
0.00.061.545 I llama_init_from_model: n_ubatch      = 512
0.00.061.545 I llama_init_from_model: flash_attn    = 0
0.00.061.546 I llama_init_from_model: freq_base     = 10000.0
0.00.061.546 I llama_init_from_model: freq_scale    = 1
0.00.061.547 I ggml_metal_init: allocating
0.00.061.551 I ggml_metal_init: found device: Apple M4
0.00.061.552 I ggml_metal_init: picking default device: Apple M4
0.00.062.300 I ggml_metal_init: using embedded metal library
0.00.064.844 I ggml_metal_init: GPU name:   Apple M4
0.00.064.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.846 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.847 I ggml_metal_init: simdgroup reduction   = true
0.00.064.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.847 I ggml_metal_init: has bfloat            = true
0.00.064.847 I ggml_metal_init: use bfloat            = true
0.00.064.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.342 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.046 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.053 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.249 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.101.251 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.101.252 I llama_init_from_model: graph nodes  = 967
0.00.101.252 I llama_init_from_model: graph splits = 2
0.00.101.256 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.285.972 I main: llama threadpool init, n_threads = 4
0.01.286.059 I 
0.01.286.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.286.142 I 
0.01.286.602 I sampler seed: 1234
0.01.286.608 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.286.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.286.692 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.286.692 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.385.679 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.02.385.680 I llama_perf_context_print:        load time =    1276.12 ms
0.02.385.681 I llama_perf_context_print: prompt eval time =      50.54 ms /     7 tokens (    7.22 ms per token,   138.50 tokens per second)
0.02.385.682 I llama_perf_context_print:        eval time =    1045.52 ms /    63 runs   (   16.60 ms per token,    60.26 tokens per second)
0.02.385.682 I llama_perf_context_print:       total time =    1099.71 ms /    70 tokens
0.02.385.907 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.121s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.012.149 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.003 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.009 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.777 I llama_model_loader: - type  f32:  194 tensors
0.00.030.778 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.779 I print_info: file format = GGUF V3 (latest)
0.00.030.780 I print_info: file type   = Q4_0
0.00.030.781 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.435 I load: special tokens cache size = 25
0.00.056.380 I load: token to piece cache size = 0.2984 MB
0.00.056.383 I print_info: arch             = gptneox
0.00.056.383 I print_info: vocab_only       = 0
0.00.056.383 I print_info: n_ctx_train      = 2048
0.00.056.383 I print_info: n_embd           = 2048
0.00.056.383 I print_info: n_layer          = 24
0.00.056.388 I print_info: n_head           = 16
0.00.056.388 I print_info: n_head_kv        = 16
0.00.056.389 I print_info: n_rot            = 32
0.00.056.389 I print_info: n_swa            = 0
0.00.056.389 I print_info: n_embd_head_k    = 128
0.00.056.389 I print_info: n_embd_head_v    = 128
0.00.056.390 I print_info: n_gqa            = 1
0.00.056.391 I print_info: n_embd_k_gqa     = 2048
0.00.056.391 I print_info: n_embd_v_gqa     = 2048
0.00.056.392 I print_info: f_norm_eps       = 1.0e-05
0.00.056.392 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.393 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.393 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.393 I print_info: f_logit_scale    = 0.0e+00
0.00.056.394 I print_info: n_ff             = 8192
0.00.056.394 I print_info: n_expert         = 0
0.00.056.394 I print_info: n_expert_used    = 0
0.00.056.394 I print_info: causal attn      = 1
0.00.056.394 I print_info: pooling type     = 0
0.00.056.395 I print_info: rope type        = 2
0.00.056.395 I print_info: rope scaling     = linear
0.00.056.397 I print_info: freq_base_train  = 10000.0
0.00.056.397 I print_info: freq_scale_train = 1
0.00.056.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.397 I print_info: rope_finetuned   = unknown
0.00.056.398 I print_info: ssm_d_conv       = 0
0.00.056.398 I print_info: ssm_d_inner      = 0
0.00.056.398 I print_info: ssm_d_state      = 0
0.00.056.398 I print_info: ssm_dt_rank      = 0
0.00.056.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.399 I print_info: model type       = 1.4B
0.00.056.399 I print_info: model params     = 1.41 B
0.00.056.399 I print_info: general.name     = 1.4B
0.00.056.402 I print_info: vocab type       = BPE
0.00.056.402 I print_info: n_vocab          = 50304
0.00.056.402 I print_info: n_merges         = 50009
0.00.056.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.403 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.404 I print_info: LF token         = 128 'Ä'
0.00.056.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.405 I print_info: max token length = 1024
0.00.058.647 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.647 I load_tensors: offloading output layer to GPU
0.00.058.648 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.659 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.660 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.058.992 I llama_init_from_model: n_seq_max     = 1
0.00.058.993 I llama_init_from_model: n_ctx         = 2048
0.00.058.993 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.993 I llama_init_from_model: n_batch       = 2048
0.00.058.993 I llama_init_from_model: n_ubatch      = 512
0.00.058.993 I llama_init_from_model: flash_attn    = 0
0.00.058.994 I llama_init_from_model: freq_base     = 10000.0
0.00.058.994 I llama_init_from_model: freq_scale    = 1
0.00.058.995 I ggml_metal_init: allocating
0.00.058.998 I ggml_metal_init: found device: Apple M4
0.00.059.000 I ggml_metal_init: picking default device: Apple M4
0.00.059.766 I ggml_metal_init: using embedded metal library
0.00.062.434 I ggml_metal_init: GPU name:   Apple M4
0.00.062.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.436 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.436 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.437 I ggml_metal_init: simdgroup reduction   = true
0.00.062.437 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.437 I ggml_metal_init: has bfloat            = true
0.00.062.437 I ggml_metal_init: use bfloat            = true
0.00.062.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.966 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.558 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.569 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.596 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.793 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.795 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.796 I llama_init_from_model: graph nodes  = 967
0.00.098.796 I llama_init_from_model: graph splits = 2
0.00.098.802 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.942 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.942 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.206 I main: llama threadpool init, n_threads = 4
0.00.730.261 I 
0.00.730.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.296 I 
0.00.730.529 I sampler seed: 1234
0.00.730.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.584 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.584 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.412.509 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.412.510 I llama_perf_context_print:        load time =     718.05 ms
0.01.412.511 I llama_perf_context_print: prompt eval time =      46.58 ms /     7 tokens (    6.65 ms per token,   150.27 tokens per second)
0.01.412.511 I llama_perf_context_print:        eval time =     632.45 ms /    63 runs   (   10.04 ms per token,    99.61 tokens per second)
0.01.412.512 I llama_perf_context_print:       total time =     682.31 ms /    70 tokens
0.01.412.739 I ggml_metal_free: deallocating

real	0m1.432s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.840 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.847 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.787 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.640 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.641 I print_info: file format = GGUF V3 (latest)
0.00.025.641 I print_info: file type   = Q4_1
0.00.025.642 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.211 I load: special tokens cache size = 25
0.00.051.179 I load: token to piece cache size = 0.2984 MB
0.00.051.182 I print_info: arch             = gptneox
0.00.051.183 I print_info: vocab_only       = 0
0.00.051.183 I print_info: n_ctx_train      = 2048
0.00.051.183 I print_info: n_embd           = 2048
0.00.051.183 I print_info: n_layer          = 24
0.00.051.186 I print_info: n_head           = 16
0.00.051.187 I print_info: n_head_kv        = 16
0.00.051.188 I print_info: n_rot            = 32
0.00.051.189 I print_info: n_swa            = 0
0.00.051.189 I print_info: n_embd_head_k    = 128
0.00.051.189 I print_info: n_embd_head_v    = 128
0.00.051.190 I print_info: n_gqa            = 1
0.00.051.191 I print_info: n_embd_k_gqa     = 2048
0.00.051.191 I print_info: n_embd_v_gqa     = 2048
0.00.051.192 I print_info: f_norm_eps       = 1.0e-05
0.00.051.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.192 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.193 I print_info: f_logit_scale    = 0.0e+00
0.00.051.193 I print_info: n_ff             = 8192
0.00.051.193 I print_info: n_expert         = 0
0.00.051.194 I print_info: n_expert_used    = 0
0.00.051.195 I print_info: causal attn      = 1
0.00.051.195 I print_info: pooling type     = 0
0.00.051.197 I print_info: rope type        = 2
0.00.051.198 I print_info: rope scaling     = linear
0.00.051.199 I print_info: freq_base_train  = 10000.0
0.00.051.199 I print_info: freq_scale_train = 1
0.00.051.199 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.199 I print_info: rope_finetuned   = unknown
0.00.051.200 I print_info: ssm_d_conv       = 0
0.00.051.200 I print_info: ssm_d_inner      = 0
0.00.051.200 I print_info: ssm_d_state      = 0
0.00.051.200 I print_info: ssm_dt_rank      = 0
0.00.051.201 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.202 I print_info: model type       = 1.4B
0.00.051.202 I print_info: model params     = 1.41 B
0.00.051.202 I print_info: general.name     = 1.4B
0.00.051.203 I print_info: vocab type       = BPE
0.00.051.203 I print_info: n_vocab          = 50304
0.00.051.203 I print_info: n_merges         = 50009
0.00.051.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: LF token         = 128 'Ä'
0.00.051.204 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: max token length = 1024
0.00.053.155 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.156 I load_tensors: offloading output layer to GPU
0.00.053.156 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.166 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.167 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.450 I llama_init_from_model: n_seq_max     = 1
0.00.053.451 I llama_init_from_model: n_ctx         = 2048
0.00.053.451 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.451 I llama_init_from_model: n_batch       = 2048
0.00.053.451 I llama_init_from_model: n_ubatch      = 512
0.00.053.451 I llama_init_from_model: flash_attn    = 0
0.00.053.452 I llama_init_from_model: freq_base     = 10000.0
0.00.053.452 I llama_init_from_model: freq_scale    = 1
0.00.053.452 I ggml_metal_init: allocating
0.00.053.455 I ggml_metal_init: found device: Apple M4
0.00.053.457 I ggml_metal_init: picking default device: Apple M4
0.00.054.056 I ggml_metal_init: using embedded metal library
0.00.056.402 I ggml_metal_init: GPU name:   Apple M4
0.00.056.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.405 I ggml_metal_init: simdgroup reduction   = true
0.00.056.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.405 I ggml_metal_init: has bfloat            = true
0.00.056.405 I ggml_metal_init: use bfloat            = true
0.00.056.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.970 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.478 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.489 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.536 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.538 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.538 I llama_init_from_model: graph nodes  = 967
0.00.085.538 I llama_init_from_model: graph splits = 2
0.00.085.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.655 I main: llama threadpool init, n_threads = 4
0.00.703.700 I 
0.00.703.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.731 I 
0.00.703.964 I sampler seed: 1234
0.00.703.968 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.009 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.013 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.013 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.427.932 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62831.86 tokens per second)
0.01.427.933 I llama_perf_context_print:        load time =     694.94 ms
0.01.427.934 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.85 tokens per second)
0.01.427.934 I llama_perf_context_print:        eval time =     677.52 ms /    63 runs   (   10.75 ms per token,    92.99 tokens per second)
0.01.427.936 I llama_perf_context_print:       total time =     724.28 ms /    70 tokens
0.01.428.206 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.098 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.690 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.700 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.550 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.342 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.342 I llama_model_loader: - type  f32:  194 tensors
0.00.027.342 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.343 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.343 I print_info: file format = GGUF V3 (latest)
0.00.027.344 I print_info: file type   = Q5_0
0.00.027.345 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.101 I load: special tokens cache size = 25
0.00.053.067 I load: token to piece cache size = 0.2984 MB
0.00.053.070 I print_info: arch             = gptneox
0.00.053.070 I print_info: vocab_only       = 0
0.00.053.071 I print_info: n_ctx_train      = 2048
0.00.053.071 I print_info: n_embd           = 2048
0.00.053.071 I print_info: n_layer          = 24
0.00.053.075 I print_info: n_head           = 16
0.00.053.075 I print_info: n_head_kv        = 16
0.00.053.076 I print_info: n_rot            = 32
0.00.053.076 I print_info: n_swa            = 0
0.00.053.076 I print_info: n_embd_head_k    = 128
0.00.053.076 I print_info: n_embd_head_v    = 128
0.00.053.077 I print_info: n_gqa            = 1
0.00.053.078 I print_info: n_embd_k_gqa     = 2048
0.00.053.078 I print_info: n_embd_v_gqa     = 2048
0.00.053.079 I print_info: f_norm_eps       = 1.0e-05
0.00.053.079 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.079 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.080 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.080 I print_info: f_logit_scale    = 0.0e+00
0.00.053.080 I print_info: n_ff             = 8192
0.00.053.081 I print_info: n_expert         = 0
0.00.053.081 I print_info: n_expert_used    = 0
0.00.053.081 I print_info: causal attn      = 1
0.00.053.081 I print_info: pooling type     = 0
0.00.053.081 I print_info: rope type        = 2
0.00.053.081 I print_info: rope scaling     = linear
0.00.053.082 I print_info: freq_base_train  = 10000.0
0.00.053.082 I print_info: freq_scale_train = 1
0.00.053.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.083 I print_info: rope_finetuned   = unknown
0.00.053.083 I print_info: ssm_d_conv       = 0
0.00.053.083 I print_info: ssm_d_inner      = 0
0.00.053.085 I print_info: ssm_d_state      = 0
0.00.053.086 I print_info: ssm_dt_rank      = 0
0.00.053.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.086 I print_info: model type       = 1.4B
0.00.053.086 I print_info: model params     = 1.41 B
0.00.053.087 I print_info: general.name     = 1.4B
0.00.053.087 I print_info: vocab type       = BPE
0.00.053.087 I print_info: n_vocab          = 50304
0.00.053.088 I print_info: n_merges         = 50009
0.00.053.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.089 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.089 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.091 I print_info: LF token         = 128 'Ä'
0.00.053.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.091 I print_info: max token length = 1024
0.00.055.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.064 I load_tensors: offloading output layer to GPU
0.00.055.064 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.075 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.076 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.388 I llama_init_from_model: n_seq_max     = 1
0.00.055.389 I llama_init_from_model: n_ctx         = 2048
0.00.055.389 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.389 I llama_init_from_model: n_batch       = 2048
0.00.055.389 I llama_init_from_model: n_ubatch      = 512
0.00.055.389 I llama_init_from_model: flash_attn    = 0
0.00.055.390 I llama_init_from_model: freq_base     = 10000.0
0.00.055.390 I llama_init_from_model: freq_scale    = 1
0.00.055.391 I ggml_metal_init: allocating
0.00.055.394 I ggml_metal_init: found device: Apple M4
0.00.055.396 I ggml_metal_init: picking default device: Apple M4
0.00.056.002 I ggml_metal_init: using embedded metal library
0.00.058.394 I ggml_metal_init: GPU name:   Apple M4
0.00.058.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.396 I ggml_metal_init: simdgroup reduction   = true
0.00.058.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.397 I ggml_metal_init: has bfloat            = true
0.00.058.397 I ggml_metal_init: use bfloat            = true
0.00.058.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.486 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.646 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.655 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.686 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.605 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.606 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.606 I llama_init_from_model: graph nodes  = 967
0.00.087.607 I llama_init_from_model: graph splits = 2
0.00.087.610 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.746 I main: llama threadpool init, n_threads = 4
0.00.741.802 I 
0.00.741.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.840 I 
0.00.742.057 I sampler seed: 1234
0.00.742.061 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.115 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.126 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.127 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.529.251 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.529.251 I llama_perf_context_print:        load time =     731.64 ms
0.01.529.252 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.81 tokens per second)
0.01.529.255 I llama_perf_context_print:        eval time =     737.07 ms /    63 runs   (   11.70 ms per token,    85.47 tokens per second)
0.01.529.255 I llama_perf_context_print:       total time =     787.51 ms /    70 tokens
0.01.529.454 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.506 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.417 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.419 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.420 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.422 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.423 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.107 I llama_model_loader: - type  f32:  194 tensors
0.00.025.107 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.107 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.108 I print_info: file format = GGUF V3 (latest)
0.00.025.108 I print_info: file type   = Q5_1
0.00.025.109 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.086 I load: special tokens cache size = 25
0.00.049.960 I load: token to piece cache size = 0.2984 MB
0.00.049.963 I print_info: arch             = gptneox
0.00.049.963 I print_info: vocab_only       = 0
0.00.049.964 I print_info: n_ctx_train      = 2048
0.00.049.964 I print_info: n_embd           = 2048
0.00.049.964 I print_info: n_layer          = 24
0.00.049.967 I print_info: n_head           = 16
0.00.049.967 I print_info: n_head_kv        = 16
0.00.049.968 I print_info: n_rot            = 32
0.00.049.969 I print_info: n_swa            = 0
0.00.049.970 I print_info: n_embd_head_k    = 128
0.00.049.970 I print_info: n_embd_head_v    = 128
0.00.049.971 I print_info: n_gqa            = 1
0.00.049.972 I print_info: n_embd_k_gqa     = 2048
0.00.049.973 I print_info: n_embd_v_gqa     = 2048
0.00.049.973 I print_info: f_norm_eps       = 1.0e-05
0.00.049.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.974 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.974 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.974 I print_info: f_logit_scale    = 0.0e+00
0.00.049.975 I print_info: n_ff             = 8192
0.00.049.975 I print_info: n_expert         = 0
0.00.049.975 I print_info: n_expert_used    = 0
0.00.049.975 I print_info: causal attn      = 1
0.00.049.976 I print_info: pooling type     = 0
0.00.049.976 I print_info: rope type        = 2
0.00.049.976 I print_info: rope scaling     = linear
0.00.049.976 I print_info: freq_base_train  = 10000.0
0.00.049.977 I print_info: freq_scale_train = 1
0.00.049.977 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.977 I print_info: rope_finetuned   = unknown
0.00.049.977 I print_info: ssm_d_conv       = 0
0.00.049.977 I print_info: ssm_d_inner      = 0
0.00.049.978 I print_info: ssm_d_state      = 0
0.00.049.978 I print_info: ssm_dt_rank      = 0
0.00.049.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.978 I print_info: model type       = 1.4B
0.00.049.978 I print_info: model params     = 1.41 B
0.00.049.979 I print_info: general.name     = 1.4B
0.00.049.979 I print_info: vocab type       = BPE
0.00.049.979 I print_info: n_vocab          = 50304
0.00.049.980 I print_info: n_merges         = 50009
0.00.049.984 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.984 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.985 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.985 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.985 I print_info: LF token         = 128 'Ä'
0.00.049.986 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.986 I print_info: max token length = 1024
0.00.052.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.040 I load_tensors: offloading output layer to GPU
0.00.052.041 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.051 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.053 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.345 I llama_init_from_model: n_seq_max     = 1
0.00.052.345 I llama_init_from_model: n_ctx         = 2048
0.00.052.345 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.346 I llama_init_from_model: n_batch       = 2048
0.00.052.346 I llama_init_from_model: n_ubatch      = 512
0.00.052.346 I llama_init_from_model: flash_attn    = 0
0.00.052.346 I llama_init_from_model: freq_base     = 10000.0
0.00.052.347 I llama_init_from_model: freq_scale    = 1
0.00.052.347 I ggml_metal_init: allocating
0.00.052.350 I ggml_metal_init: found device: Apple M4
0.00.052.352 I ggml_metal_init: picking default device: Apple M4
0.00.052.931 I ggml_metal_init: using embedded metal library
0.00.055.278 I ggml_metal_init: GPU name:   Apple M4
0.00.055.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.280 I ggml_metal_init: simdgroup reduction   = true
0.00.055.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.280 I ggml_metal_init: has bfloat            = true
0.00.055.280 I ggml_metal_init: use bfloat            = true
0.00.055.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.043 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.251 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.259 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.300 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.301 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.301 I llama_init_from_model: graph nodes  = 967
0.00.085.302 I llama_init_from_model: graph splits = 2
0.00.085.304 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.036 I main: llama threadpool init, n_threads = 4
0.00.809.081 I 
0.00.809.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.117 I 
0.00.809.360 I sampler seed: 1234
0.00.809.367 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.378 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.380 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.380 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.646.154 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49477.35 tokens per second)
0.01.646.155 I llama_perf_context_print:        load time =     800.52 ms
0.01.646.156 I llama_perf_context_print: prompt eval time =      41.99 ms /     7 tokens (    6.00 ms per token,   166.70 tokens per second)
0.01.646.156 I llama_perf_context_print:        eval time =     792.00 ms /    63 runs   (   12.57 ms per token,    79.55 tokens per second)
0.01.646.157 I llama_perf_context_print:       total time =     837.12 ms /    70 tokens
0.01.646.451 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.650 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.340 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.412 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.165 I llama_model_loader: - type  f32:  194 tensors
0.00.025.165 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.166 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.166 I print_info: file format = GGUF V3 (latest)
0.00.025.167 I print_info: file type   = Q2_K - Medium
0.00.025.168 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.237 I load: special tokens cache size = 25
0.00.050.137 I load: token to piece cache size = 0.2984 MB
0.00.050.140 I print_info: arch             = gptneox
0.00.050.140 I print_info: vocab_only       = 0
0.00.050.141 I print_info: n_ctx_train      = 2048
0.00.050.141 I print_info: n_embd           = 2048
0.00.050.141 I print_info: n_layer          = 24
0.00.050.144 I print_info: n_head           = 16
0.00.050.145 I print_info: n_head_kv        = 16
0.00.050.145 I print_info: n_rot            = 32
0.00.050.145 I print_info: n_swa            = 0
0.00.050.146 I print_info: n_embd_head_k    = 128
0.00.050.146 I print_info: n_embd_head_v    = 128
0.00.050.146 I print_info: n_gqa            = 1
0.00.050.147 I print_info: n_embd_k_gqa     = 2048
0.00.050.148 I print_info: n_embd_v_gqa     = 2048
0.00.050.148 I print_info: f_norm_eps       = 1.0e-05
0.00.050.149 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.149 I print_info: f_logit_scale    = 0.0e+00
0.00.050.150 I print_info: n_ff             = 8192
0.00.050.150 I print_info: n_expert         = 0
0.00.050.150 I print_info: n_expert_used    = 0
0.00.050.151 I print_info: causal attn      = 1
0.00.050.152 I print_info: pooling type     = 0
0.00.050.154 I print_info: rope type        = 2
0.00.050.154 I print_info: rope scaling     = linear
0.00.050.155 I print_info: freq_base_train  = 10000.0
0.00.050.155 I print_info: freq_scale_train = 1
0.00.050.155 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.155 I print_info: rope_finetuned   = unknown
0.00.050.156 I print_info: ssm_d_conv       = 0
0.00.050.156 I print_info: ssm_d_inner      = 0
0.00.050.156 I print_info: ssm_d_state      = 0
0.00.050.156 I print_info: ssm_dt_rank      = 0
0.00.050.156 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.156 I print_info: model type       = 1.4B
0.00.050.157 I print_info: model params     = 1.41 B
0.00.050.157 I print_info: general.name     = 1.4B
0.00.050.158 I print_info: vocab type       = BPE
0.00.050.158 I print_info: n_vocab          = 50304
0.00.050.159 I print_info: n_merges         = 50009
0.00.050.159 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.160 I print_info: LF token         = 128 'Ä'
0.00.050.160 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.161 I print_info: max token length = 1024
0.00.052.087 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.087 I load_tensors: offloading output layer to GPU
0.00.052.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.098 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.099 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.395 I llama_init_from_model: n_seq_max     = 1
0.00.052.396 I llama_init_from_model: n_ctx         = 2048
0.00.052.396 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.396 I llama_init_from_model: n_batch       = 2048
0.00.052.396 I llama_init_from_model: n_ubatch      = 512
0.00.052.396 I llama_init_from_model: flash_attn    = 0
0.00.052.397 I llama_init_from_model: freq_base     = 10000.0
0.00.052.397 I llama_init_from_model: freq_scale    = 1
0.00.052.397 I ggml_metal_init: allocating
0.00.052.400 I ggml_metal_init: found device: Apple M4
0.00.052.402 I ggml_metal_init: picking default device: Apple M4
0.00.053.032 I ggml_metal_init: using embedded metal library
0.00.055.404 I ggml_metal_init: GPU name:   Apple M4
0.00.055.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.407 I ggml_metal_init: simdgroup reduction   = true
0.00.055.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.407 I ggml_metal_init: has bfloat            = true
0.00.055.407 I ggml_metal_init: use bfloat            = true
0.00.055.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.280 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.446 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.451 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.652 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.654 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.654 I llama_init_from_model: graph nodes  = 967
0.00.085.655 I llama_init_from_model: graph splits = 2
0.00.085.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.905 I main: llama threadpool init, n_threads = 4
0.00.497.946 I 
0.00.497.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.977 I 
0.00.498.202 I sampler seed: 1234
0.00.498.206 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.498.230 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.498.231 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.498.232 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.173.078 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.173.079 I llama_perf_context_print:        load time =     488.25 ms
0.01.173.080 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.45 tokens per second)
0.01.173.081 I llama_perf_context_print:        eval time =     636.04 ms /    63 runs   (   10.10 ms per token,    99.05 tokens per second)
0.01.173.081 I llama_perf_context_print:       total time =     675.18 ms /    70 tokens
0.01.173.324 I ggml_metal_free: deallocating

real	0m1.190s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.717 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.382 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.382 I llama_model_loader: - type  f32:  194 tensors
0.00.026.383 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.383 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.383 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.383 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.384 I print_info: file format = GGUF V3 (latest)
0.00.026.384 I print_info: file type   = Q3_K - Medium
0.00.026.385 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.549 I load: special tokens cache size = 25
0.00.051.513 I load: token to piece cache size = 0.2984 MB
0.00.051.516 I print_info: arch             = gptneox
0.00.051.516 I print_info: vocab_only       = 0
0.00.051.516 I print_info: n_ctx_train      = 2048
0.00.051.516 I print_info: n_embd           = 2048
0.00.051.517 I print_info: n_layer          = 24
0.00.051.519 I print_info: n_head           = 16
0.00.051.520 I print_info: n_head_kv        = 16
0.00.051.520 I print_info: n_rot            = 32
0.00.051.520 I print_info: n_swa            = 0
0.00.051.521 I print_info: n_embd_head_k    = 128
0.00.051.521 I print_info: n_embd_head_v    = 128
0.00.051.521 I print_info: n_gqa            = 1
0.00.051.522 I print_info: n_embd_k_gqa     = 2048
0.00.051.523 I print_info: n_embd_v_gqa     = 2048
0.00.051.524 I print_info: f_norm_eps       = 1.0e-05
0.00.051.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.525 I print_info: f_logit_scale    = 0.0e+00
0.00.051.525 I print_info: n_ff             = 8192
0.00.051.526 I print_info: n_expert         = 0
0.00.051.526 I print_info: n_expert_used    = 0
0.00.051.526 I print_info: causal attn      = 1
0.00.051.526 I print_info: pooling type     = 0
0.00.051.528 I print_info: rope type        = 2
0.00.051.528 I print_info: rope scaling     = linear
0.00.051.529 I print_info: freq_base_train  = 10000.0
0.00.051.529 I print_info: freq_scale_train = 1
0.00.051.529 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.529 I print_info: rope_finetuned   = unknown
0.00.051.529 I print_info: ssm_d_conv       = 0
0.00.051.529 I print_info: ssm_d_inner      = 0
0.00.051.530 I print_info: ssm_d_state      = 0
0.00.051.530 I print_info: ssm_dt_rank      = 0
0.00.051.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.530 I print_info: model type       = 1.4B
0.00.051.531 I print_info: model params     = 1.41 B
0.00.051.531 I print_info: general.name     = 1.4B
0.00.051.531 I print_info: vocab type       = BPE
0.00.051.532 I print_info: n_vocab          = 50304
0.00.051.532 I print_info: n_merges         = 50009
0.00.051.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.533 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.533 I print_info: LF token         = 128 'Ä'
0.00.051.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.534 I print_info: max token length = 1024
0.00.053.524 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.525 I load_tensors: offloading output layer to GPU
0.00.053.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.535 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.536 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.834 I llama_init_from_model: n_seq_max     = 1
0.00.053.835 I llama_init_from_model: n_ctx         = 2048
0.00.053.835 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.835 I llama_init_from_model: n_batch       = 2048
0.00.053.835 I llama_init_from_model: n_ubatch      = 512
0.00.053.836 I llama_init_from_model: flash_attn    = 0
0.00.053.836 I llama_init_from_model: freq_base     = 10000.0
0.00.053.836 I llama_init_from_model: freq_scale    = 1
0.00.053.837 I ggml_metal_init: allocating
0.00.053.839 I ggml_metal_init: found device: Apple M4
0.00.053.841 I ggml_metal_init: picking default device: Apple M4
0.00.054.453 I ggml_metal_init: using embedded metal library
0.00.056.798 I ggml_metal_init: GPU name:   Apple M4
0.00.056.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.800 I ggml_metal_init: simdgroup reduction   = true
0.00.056.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.800 I ggml_metal_init: has bfloat            = true
0.00.056.800 I ggml_metal_init: use bfloat            = true
0.00.056.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.622 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.256 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.262 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.420 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.421 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.421 I llama_init_from_model: graph nodes  = 967
0.00.087.422 I llama_init_from_model: graph splits = 2
0.00.087.424 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.136 I main: llama threadpool init, n_threads = 4
0.00.527.177 I 
0.00.527.208 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.208 I 
0.00.527.425 I sampler seed: 1234
0.00.527.430 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.527.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.527.456 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.527.456 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.268.306 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.268.307 I llama_perf_context_print:        load time =     517.13 ms
0.01.268.307 I llama_perf_context_print: prompt eval time =      40.48 ms /     7 tokens (    5.78 ms per token,   172.91 tokens per second)
0.01.268.308 I llama_perf_context_print:        eval time =     697.41 ms /    63 runs   (   11.07 ms per token,    90.33 tokens per second)
0.01.268.308 I llama_perf_context_print:       total time =     741.17 ms /    70 tokens
0.01.268.537 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.713 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.713 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.713 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.714 I print_info: file format = GGUF V3 (latest)
0.00.024.715 I print_info: file type   = Q4_K - Medium
0.00.024.715 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.463 I load: special tokens cache size = 25
0.00.050.468 I load: token to piece cache size = 0.2984 MB
0.00.050.471 I print_info: arch             = gptneox
0.00.050.472 I print_info: vocab_only       = 0
0.00.050.472 I print_info: n_ctx_train      = 2048
0.00.050.472 I print_info: n_embd           = 2048
0.00.050.472 I print_info: n_layer          = 24
0.00.050.475 I print_info: n_head           = 16
0.00.050.476 I print_info: n_head_kv        = 16
0.00.050.476 I print_info: n_rot            = 32
0.00.050.477 I print_info: n_swa            = 0
0.00.050.477 I print_info: n_embd_head_k    = 128
0.00.050.477 I print_info: n_embd_head_v    = 128
0.00.050.478 I print_info: n_gqa            = 1
0.00.050.479 I print_info: n_embd_k_gqa     = 2048
0.00.050.479 I print_info: n_embd_v_gqa     = 2048
0.00.050.480 I print_info: f_norm_eps       = 1.0e-05
0.00.050.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.481 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.481 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.481 I print_info: f_logit_scale    = 0.0e+00
0.00.050.482 I print_info: n_ff             = 8192
0.00.050.482 I print_info: n_expert         = 0
0.00.050.482 I print_info: n_expert_used    = 0
0.00.050.482 I print_info: causal attn      = 1
0.00.050.482 I print_info: pooling type     = 0
0.00.050.484 I print_info: rope type        = 2
0.00.050.485 I print_info: rope scaling     = linear
0.00.050.485 I print_info: freq_base_train  = 10000.0
0.00.050.485 I print_info: freq_scale_train = 1
0.00.050.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.486 I print_info: rope_finetuned   = unknown
0.00.050.486 I print_info: ssm_d_conv       = 0
0.00.050.486 I print_info: ssm_d_inner      = 0
0.00.050.486 I print_info: ssm_d_state      = 0
0.00.050.486 I print_info: ssm_dt_rank      = 0
0.00.050.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.487 I print_info: model type       = 1.4B
0.00.050.487 I print_info: model params     = 1.41 B
0.00.050.488 I print_info: general.name     = 1.4B
0.00.050.488 I print_info: vocab type       = BPE
0.00.050.488 I print_info: n_vocab          = 50304
0.00.050.489 I print_info: n_merges         = 50009
0.00.050.489 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: LF token         = 128 'Ä'
0.00.050.492 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: max token length = 1024
0.00.052.476 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.476 I load_tensors: offloading output layer to GPU
0.00.052.476 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.487 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.488 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.786 I llama_init_from_model: n_seq_max     = 1
0.00.052.787 I llama_init_from_model: n_ctx         = 2048
0.00.052.787 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.787 I llama_init_from_model: n_batch       = 2048
0.00.052.788 I llama_init_from_model: n_ubatch      = 512
0.00.052.788 I llama_init_from_model: flash_attn    = 0
0.00.052.788 I llama_init_from_model: freq_base     = 10000.0
0.00.052.788 I llama_init_from_model: freq_scale    = 1
0.00.052.789 I ggml_metal_init: allocating
0.00.052.792 I ggml_metal_init: found device: Apple M4
0.00.052.794 I ggml_metal_init: picking default device: Apple M4
0.00.053.401 I ggml_metal_init: using embedded metal library
0.00.055.818 I ggml_metal_init: GPU name:   Apple M4
0.00.055.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.821 I ggml_metal_init: simdgroup reduction   = true
0.00.055.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.821 I ggml_metal_init: has bfloat            = true
0.00.055.821 I ggml_metal_init: use bfloat            = true
0.00.055.822 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.880 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.190 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.195 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.325 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.326 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.326 I llama_init_from_model: graph nodes  = 967
0.00.087.327 I llama_init_from_model: graph splits = 2
0.00.087.330 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.661 I main: llama threadpool init, n_threads = 4
0.00.616.706 I 
0.00.616.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.759 I 
0.00.616.985 I sampler seed: 1234
0.00.616.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.041 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.043 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.093 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.376.094 I llama_perf_context_print:        load time =     607.74 ms
0.01.376.095 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.36 tokens per second)
0.01.376.095 I llama_perf_context_print:        eval time =     708.90 ms /    63 runs   (   11.25 ms per token,    88.87 tokens per second)
0.01.376.096 I llama_perf_context_print:       total time =     759.44 ms /    70 tokens
0.01.376.312 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.545 I llama_model_loader: - type  f32:  194 tensors
0.00.025.545 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.545 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.546 I print_info: file format = GGUF V3 (latest)
0.00.025.546 I print_info: file type   = Q5_K - Medium
0.00.025.547 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.704 I load: special tokens cache size = 25
0.00.050.738 I load: token to piece cache size = 0.2984 MB
0.00.050.741 I print_info: arch             = gptneox
0.00.050.741 I print_info: vocab_only       = 0
0.00.050.741 I print_info: n_ctx_train      = 2048
0.00.050.741 I print_info: n_embd           = 2048
0.00.050.741 I print_info: n_layer          = 24
0.00.050.744 I print_info: n_head           = 16
0.00.050.745 I print_info: n_head_kv        = 16
0.00.050.745 I print_info: n_rot            = 32
0.00.050.745 I print_info: n_swa            = 0
0.00.050.747 I print_info: n_embd_head_k    = 128
0.00.050.747 I print_info: n_embd_head_v    = 128
0.00.050.748 I print_info: n_gqa            = 1
0.00.050.749 I print_info: n_embd_k_gqa     = 2048
0.00.050.749 I print_info: n_embd_v_gqa     = 2048
0.00.050.751 I print_info: f_norm_eps       = 1.0e-05
0.00.050.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.751 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.752 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.752 I print_info: f_logit_scale    = 0.0e+00
0.00.050.753 I print_info: n_ff             = 8192
0.00.050.753 I print_info: n_expert         = 0
0.00.050.753 I print_info: n_expert_used    = 0
0.00.050.753 I print_info: causal attn      = 1
0.00.050.753 I print_info: pooling type     = 0
0.00.050.754 I print_info: rope type        = 2
0.00.050.754 I print_info: rope scaling     = linear
0.00.050.759 I print_info: freq_base_train  = 10000.0
0.00.050.759 I print_info: freq_scale_train = 1
0.00.050.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.760 I print_info: rope_finetuned   = unknown
0.00.050.760 I print_info: ssm_d_conv       = 0
0.00.050.760 I print_info: ssm_d_inner      = 0
0.00.050.760 I print_info: ssm_d_state      = 0
0.00.050.760 I print_info: ssm_dt_rank      = 0
0.00.050.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.761 I print_info: model type       = 1.4B
0.00.050.761 I print_info: model params     = 1.41 B
0.00.050.761 I print_info: general.name     = 1.4B
0.00.050.762 I print_info: vocab type       = BPE
0.00.050.762 I print_info: n_vocab          = 50304
0.00.050.762 I print_info: n_merges         = 50009
0.00.050.763 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.764 I print_info: LF token         = 128 'Ä'
0.00.050.764 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I print_info: max token length = 1024
0.00.052.519 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.519 I load_tensors: offloading output layer to GPU
0.00.052.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.525 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.527 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.801 I llama_init_from_model: n_seq_max     = 1
0.00.052.802 I llama_init_from_model: n_ctx         = 2048
0.00.052.802 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.802 I llama_init_from_model: n_batch       = 2048
0.00.052.802 I llama_init_from_model: n_ubatch      = 512
0.00.052.802 I llama_init_from_model: flash_attn    = 0
0.00.052.803 I llama_init_from_model: freq_base     = 10000.0
0.00.052.803 I llama_init_from_model: freq_scale    = 1
0.00.052.804 I ggml_metal_init: allocating
0.00.052.807 I ggml_metal_init: found device: Apple M4
0.00.052.809 I ggml_metal_init: picking default device: Apple M4
0.00.053.418 I ggml_metal_init: using embedded metal library
0.00.055.715 I ggml_metal_init: GPU name:   Apple M4
0.00.055.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.718 I ggml_metal_init: simdgroup reduction   = true
0.00.055.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.718 I ggml_metal_init: has bfloat            = true
0.00.055.718 I ggml_metal_init: use bfloat            = true
0.00.055.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.339 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.433 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.442 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.471 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.467 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.469 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.469 I llama_init_from_model: graph nodes  = 967
0.00.085.469 I llama_init_from_model: graph splits = 2
0.00.085.472 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.263 I main: llama threadpool init, n_threads = 4
0.00.682.307 I 
0.00.682.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.343 I 
0.00.682.564 I sampler seed: 1234
0.00.682.571 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.591 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.533.634 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.533.635 I llama_perf_context_print:        load time =     672.30 ms
0.01.533.635 I llama_perf_context_print: prompt eval time =      51.59 ms /     7 tokens (    7.37 ms per token,   135.69 tokens per second)
0.01.533.636 I llama_perf_context_print:        eval time =     796.36 ms /    63 runs   (   12.64 ms per token,    79.11 tokens per second)
0.01.533.636 I llama_perf_context_print:       total time =     851.38 ms /    70 tokens
0.01.533.840 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.999 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.000 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.001 I llama_model_loader: - type  f32:  194 tensors
0.00.024.001 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.001 I print_info: file format = GGUF V3 (latest)
0.00.024.002 I print_info: file type   = Q6_K
0.00.024.003 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.032 I load: special tokens cache size = 25
0.00.049.056 I load: token to piece cache size = 0.2984 MB
0.00.049.058 I print_info: arch             = gptneox
0.00.049.059 I print_info: vocab_only       = 0
0.00.049.059 I print_info: n_ctx_train      = 2048
0.00.049.059 I print_info: n_embd           = 2048
0.00.049.059 I print_info: n_layer          = 24
0.00.049.062 I print_info: n_head           = 16
0.00.049.063 I print_info: n_head_kv        = 16
0.00.049.063 I print_info: n_rot            = 32
0.00.049.064 I print_info: n_swa            = 0
0.00.049.064 I print_info: n_embd_head_k    = 128
0.00.049.064 I print_info: n_embd_head_v    = 128
0.00.049.065 I print_info: n_gqa            = 1
0.00.049.066 I print_info: n_embd_k_gqa     = 2048
0.00.049.066 I print_info: n_embd_v_gqa     = 2048
0.00.049.067 I print_info: f_norm_eps       = 1.0e-05
0.00.049.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.070 I print_info: f_logit_scale    = 0.0e+00
0.00.049.071 I print_info: n_ff             = 8192
0.00.049.071 I print_info: n_expert         = 0
0.00.049.071 I print_info: n_expert_used    = 0
0.00.049.071 I print_info: causal attn      = 1
0.00.049.071 I print_info: pooling type     = 0
0.00.049.072 I print_info: rope type        = 2
0.00.049.072 I print_info: rope scaling     = linear
0.00.049.072 I print_info: freq_base_train  = 10000.0
0.00.049.073 I print_info: freq_scale_train = 1
0.00.049.073 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.073 I print_info: rope_finetuned   = unknown
0.00.049.073 I print_info: ssm_d_conv       = 0
0.00.049.073 I print_info: ssm_d_inner      = 0
0.00.049.073 I print_info: ssm_d_state      = 0
0.00.049.075 I print_info: ssm_dt_rank      = 0
0.00.049.075 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.075 I print_info: model type       = 1.4B
0.00.049.076 I print_info: model params     = 1.41 B
0.00.049.076 I print_info: general.name     = 1.4B
0.00.049.076 I print_info: vocab type       = BPE
0.00.049.077 I print_info: n_vocab          = 50304
0.00.049.077 I print_info: n_merges         = 50009
0.00.049.077 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.077 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.077 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.078 I print_info: LF token         = 128 'Ä'
0.00.049.078 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.078 I print_info: max token length = 1024
0.00.051.075 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.075 I load_tensors: offloading output layer to GPU
0.00.051.075 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.086 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.087 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.366 I llama_init_from_model: n_seq_max     = 1
0.00.051.367 I llama_init_from_model: n_ctx         = 2048
0.00.051.367 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.368 I llama_init_from_model: n_batch       = 2048
0.00.051.368 I llama_init_from_model: n_ubatch      = 512
0.00.051.368 I llama_init_from_model: flash_attn    = 0
0.00.051.368 I llama_init_from_model: freq_base     = 10000.0
0.00.051.368 I llama_init_from_model: freq_scale    = 1
0.00.051.369 I ggml_metal_init: allocating
0.00.051.372 I ggml_metal_init: found device: Apple M4
0.00.051.374 I ggml_metal_init: picking default device: Apple M4
0.00.051.955 I ggml_metal_init: using embedded metal library
0.00.054.312 I ggml_metal_init: GPU name:   Apple M4
0.00.054.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.314 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.315 I ggml_metal_init: simdgroup reduction   = true
0.00.054.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.315 I ggml_metal_init: has bfloat            = true
0.00.054.315 I ggml_metal_init: use bfloat            = true
0.00.054.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.627 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.617 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.619 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.620 I llama_init_from_model: graph nodes  = 967
0.00.083.620 I llama_init_from_model: graph splits = 2
0.00.083.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.073 I main: llama threadpool init, n_threads = 4
0.00.733.121 I 
0.00.733.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.172 I 
0.00.733.410 I sampler seed: 1234
0.00.733.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.468 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.480 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.480 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.618.766 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.618.767 I llama_perf_context_print:        load time =     724.37 ms
0.01.618.768 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.66 tokens per second)
0.01.618.769 I llama_perf_context_print:        eval time =     827.82 ms /    63 runs   (   13.14 ms per token,    76.10 tokens per second)
0.01.618.770 I llama_perf_context_print:       total time =     885.70 ms /    70 tokens
0.01.618.967 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.567 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.110 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.577 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.597 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.607 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.022 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.658 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.661 I llama_model_loader: - type  f32:  194 tensors
0.00.055.661 I llama_model_loader: - type  f16:   98 tensors
0.00.055.662 I print_info: file format = GGUF V3 (latest)
0.00.055.663 I print_info: file type   = all F32 (guessed)
0.00.055.664 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.110 I load: special tokens cache size = 25
0.00.088.517 I load: token to piece cache size = 0.2984 MB
0.00.088.521 I print_info: arch             = gptneox
0.00.088.521 I print_info: vocab_only       = 0
0.00.088.521 I print_info: n_ctx_train      = 2048
0.00.088.521 I print_info: n_embd           = 2048
0.00.088.521 I print_info: n_layer          = 24
0.00.088.524 I print_info: n_head           = 16
0.00.088.527 I print_info: n_head_kv        = 16
0.00.088.527 I print_info: n_rot            = 32
0.00.088.527 I print_info: n_swa            = 0
0.00.088.527 I print_info: n_embd_head_k    = 128
0.00.088.527 I print_info: n_embd_head_v    = 128
0.00.088.528 I print_info: n_gqa            = 1
0.00.088.532 I print_info: n_embd_k_gqa     = 2048
0.00.088.533 I print_info: n_embd_v_gqa     = 2048
0.00.088.534 I print_info: f_norm_eps       = 1.0e-05
0.00.088.534 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.534 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.535 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.535 I print_info: f_logit_scale    = 0.0e+00
0.00.088.536 I print_info: n_ff             = 8192
0.00.088.536 I print_info: n_expert         = 0
0.00.088.536 I print_info: n_expert_used    = 0
0.00.088.536 I print_info: causal attn      = 1
0.00.088.536 I print_info: pooling type     = 0
0.00.088.536 I print_info: rope type        = 2
0.00.088.538 I print_info: rope scaling     = linear
0.00.088.538 I print_info: freq_base_train  = 10000.0
0.00.088.538 I print_info: freq_scale_train = 1
0.00.088.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.539 I print_info: rope_finetuned   = unknown
0.00.088.539 I print_info: ssm_d_conv       = 0
0.00.088.539 I print_info: ssm_d_inner      = 0
0.00.088.539 I print_info: ssm_d_state      = 0
0.00.088.539 I print_info: ssm_dt_rank      = 0
0.00.088.539 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.540 I print_info: model type       = 1.4B
0.00.088.540 I print_info: model params     = 1.41 B
0.00.088.540 I print_info: general.name     = 1.4B
0.00.088.541 I print_info: vocab type       = BPE
0.00.088.542 I print_info: n_vocab          = 50304
0.00.088.542 I print_info: n_merges         = 50009
0.00.088.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.543 I print_info: LF token         = 128 'Ä'
0.00.088.543 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.543 I print_info: max token length = 1024
0.00.091.021 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.021 I load_tensors: offloading output layer to GPU
0.00.091.021 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.032 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.033 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.328 I llama_init_from_model: n_seq_max     = 1
0.00.091.329 I llama_init_from_model: n_ctx         = 128
0.00.091.329 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.329 I llama_init_from_model: n_batch       = 128
0.00.091.329 I llama_init_from_model: n_ubatch      = 128
0.00.091.330 I llama_init_from_model: flash_attn    = 0
0.00.091.330 I llama_init_from_model: freq_base     = 10000.0
0.00.091.330 I llama_init_from_model: freq_scale    = 1
0.00.091.331 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.331 I ggml_metal_init: allocating
0.00.091.334 I ggml_metal_init: found device: Apple M4
0.00.091.336 I ggml_metal_init: picking default device: Apple M4
0.00.091.942 I ggml_metal_init: using embedded metal library
0.00.094.505 I ggml_metal_init: GPU name:   Apple M4
0.00.094.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.507 I ggml_metal_init: simdgroup reduction   = true
0.00.094.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.508 I ggml_metal_init: has bfloat            = true
0.00.094.508 I ggml_metal_init: use bfloat            = true
0.00.094.508 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.864 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.123 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.040 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.106.041 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.106.041 I llama_init_from_model: graph nodes  = 967
0.00.106.041 I llama_init_from_model: graph splits = 2
0.00.106.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.333.652 I 
0.01.333.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.333.701 I perplexity: tokenizing the input ..
0.01.347.126 I perplexity: tokenization took 13.422 ms
0.01.347.133 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.469.750 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.471.586 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.471.637 I llama_perf_context_print:        load time =    1310.53 ms
0.01.471.638 I llama_perf_context_print: prompt eval time =     121.69 ms /   128 tokens (    0.95 ms per token,  1051.87 tokens per second)
0.01.471.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.471.640 I llama_perf_context_print:       total time =     137.99 ms /   129 tokens
0.01.472.347 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.125s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.127 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.274 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.422 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.534 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.539 I llama_model_loader: - type  f32:  194 tensors
0.00.032.539 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.540 I print_info: file format = GGUF V3 (latest)
0.00.032.542 I print_info: file type   = Q8_0
0.00.032.544 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.554 I load: special tokens cache size = 25
0.00.061.772 I load: token to piece cache size = 0.2984 MB
0.00.061.775 I print_info: arch             = gptneox
0.00.061.776 I print_info: vocab_only       = 0
0.00.061.776 I print_info: n_ctx_train      = 2048
0.00.061.776 I print_info: n_embd           = 2048
0.00.061.776 I print_info: n_layer          = 24
0.00.061.780 I print_info: n_head           = 16
0.00.061.781 I print_info: n_head_kv        = 16
0.00.061.783 I print_info: n_rot            = 32
0.00.061.783 I print_info: n_swa            = 0
0.00.061.783 I print_info: n_embd_head_k    = 128
0.00.061.783 I print_info: n_embd_head_v    = 128
0.00.061.784 I print_info: n_gqa            = 1
0.00.061.785 I print_info: n_embd_k_gqa     = 2048
0.00.061.785 I print_info: n_embd_v_gqa     = 2048
0.00.061.786 I print_info: f_norm_eps       = 1.0e-05
0.00.061.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.788 I print_info: f_logit_scale    = 0.0e+00
0.00.061.789 I print_info: n_ff             = 8192
0.00.061.789 I print_info: n_expert         = 0
0.00.061.789 I print_info: n_expert_used    = 0
0.00.061.789 I print_info: causal attn      = 1
0.00.061.789 I print_info: pooling type     = 0
0.00.061.789 I print_info: rope type        = 2
0.00.061.789 I print_info: rope scaling     = linear
0.00.061.790 I print_info: freq_base_train  = 10000.0
0.00.061.790 I print_info: freq_scale_train = 1
0.00.061.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.794 I print_info: rope_finetuned   = unknown
0.00.061.794 I print_info: ssm_d_conv       = 0
0.00.061.794 I print_info: ssm_d_inner      = 0
0.00.061.794 I print_info: ssm_d_state      = 0
0.00.061.795 I print_info: ssm_dt_rank      = 0
0.00.061.795 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.795 I print_info: model type       = 1.4B
0.00.061.796 I print_info: model params     = 1.41 B
0.00.061.797 I print_info: general.name     = 1.4B
0.00.061.797 I print_info: vocab type       = BPE
0.00.061.797 I print_info: n_vocab          = 50304
0.00.061.797 I print_info: n_merges         = 50009
0.00.061.798 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.798 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.798 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.798 I print_info: LF token         = 128 'Ä'
0.00.061.799 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.803 I print_info: max token length = 1024
0.00.064.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.133 I load_tensors: offloading output layer to GPU
0.00.064.133 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.144 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.146 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.437 I llama_init_from_model: n_seq_max     = 1
0.00.064.438 I llama_init_from_model: n_ctx         = 128
0.00.064.438 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.438 I llama_init_from_model: n_batch       = 128
0.00.064.438 I llama_init_from_model: n_ubatch      = 128
0.00.064.438 I llama_init_from_model: flash_attn    = 0
0.00.064.439 I llama_init_from_model: freq_base     = 10000.0
0.00.064.439 I llama_init_from_model: freq_scale    = 1
0.00.064.439 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.439 I ggml_metal_init: allocating
0.00.064.442 I ggml_metal_init: found device: Apple M4
0.00.064.444 I ggml_metal_init: picking default device: Apple M4
0.00.065.082 I ggml_metal_init: using embedded metal library
0.00.067.546 I ggml_metal_init: GPU name:   Apple M4
0.00.067.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.549 I ggml_metal_init: simdgroup reduction   = true
0.00.067.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.550 I ggml_metal_init: has bfloat            = true
0.00.067.550 I ggml_metal_init: use bfloat            = true
0.00.067.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.215 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.219 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.189 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.190 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.190 I llama_init_from_model: graph nodes  = 967
0.00.080.190 I llama_init_from_model: graph splits = 2
0.00.080.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.856.307 I 
0.00.856.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.856.342 I perplexity: tokenizing the input ..
0.00.864.407 I perplexity: tokenization took 8.063 ms
0.00.864.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.988.610 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.989.844 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.989.883 I llama_perf_context_print:        load time =     845.03 ms
0.00.989.885 I llama_perf_context_print: prompt eval time =     123.97 ms /   128 tokens (    0.97 ms per token,  1032.50 tokens per second)
0.00.989.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.989.886 I llama_perf_context_print:       total time =     133.57 ms /   129 tokens
0.00.990.380 I ggml_metal_free: deallocating

real	0m1.009s
user	0m0.090s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.370 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.505 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.389 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.173 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.173 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.174 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.174 I print_info: file format = GGUF V3 (latest)
0.00.025.174 I print_info: file type   = Q4_0
0.00.025.175 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.266 I load: special tokens cache size = 25
0.00.050.027 I load: token to piece cache size = 0.2984 MB
0.00.050.030 I print_info: arch             = gptneox
0.00.050.030 I print_info: vocab_only       = 0
0.00.050.030 I print_info: n_ctx_train      = 2048
0.00.050.030 I print_info: n_embd           = 2048
0.00.050.031 I print_info: n_layer          = 24
0.00.050.034 I print_info: n_head           = 16
0.00.050.035 I print_info: n_head_kv        = 16
0.00.050.035 I print_info: n_rot            = 32
0.00.050.035 I print_info: n_swa            = 0
0.00.050.035 I print_info: n_embd_head_k    = 128
0.00.050.036 I print_info: n_embd_head_v    = 128
0.00.050.037 I print_info: n_gqa            = 1
0.00.050.038 I print_info: n_embd_k_gqa     = 2048
0.00.050.038 I print_info: n_embd_v_gqa     = 2048
0.00.050.039 I print_info: f_norm_eps       = 1.0e-05
0.00.050.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.040 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.040 I print_info: f_logit_scale    = 0.0e+00
0.00.050.041 I print_info: n_ff             = 8192
0.00.050.041 I print_info: n_expert         = 0
0.00.050.041 I print_info: n_expert_used    = 0
0.00.050.041 I print_info: causal attn      = 1
0.00.050.041 I print_info: pooling type     = 0
0.00.050.042 I print_info: rope type        = 2
0.00.050.042 I print_info: rope scaling     = linear
0.00.050.042 I print_info: freq_base_train  = 10000.0
0.00.050.044 I print_info: freq_scale_train = 1
0.00.050.044 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.045 I print_info: rope_finetuned   = unknown
0.00.050.045 I print_info: ssm_d_conv       = 0
0.00.050.045 I print_info: ssm_d_inner      = 0
0.00.050.045 I print_info: ssm_d_state      = 0
0.00.050.045 I print_info: ssm_dt_rank      = 0
0.00.050.045 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.045 I print_info: model type       = 1.4B
0.00.050.046 I print_info: model params     = 1.41 B
0.00.050.046 I print_info: general.name     = 1.4B
0.00.050.046 I print_info: vocab type       = BPE
0.00.050.047 I print_info: n_vocab          = 50304
0.00.050.047 I print_info: n_merges         = 50009
0.00.050.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.047 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.048 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.048 I print_info: LF token         = 128 'Ä'
0.00.050.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.048 I print_info: max token length = 1024
0.00.051.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.982 I load_tensors: offloading output layer to GPU
0.00.051.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.993 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.995 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.353 I llama_init_from_model: n_seq_max     = 1
0.00.052.354 I llama_init_from_model: n_ctx         = 128
0.00.052.354 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.354 I llama_init_from_model: n_batch       = 128
0.00.052.354 I llama_init_from_model: n_ubatch      = 128
0.00.052.354 I llama_init_from_model: flash_attn    = 0
0.00.052.355 I llama_init_from_model: freq_base     = 10000.0
0.00.052.355 I llama_init_from_model: freq_scale    = 1
0.00.052.355 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.356 I ggml_metal_init: allocating
0.00.052.359 I ggml_metal_init: found device: Apple M4
0.00.052.361 I ggml_metal_init: picking default device: Apple M4
0.00.052.902 I ggml_metal_init: using embedded metal library
0.00.055.283 I ggml_metal_init: GPU name:   Apple M4
0.00.055.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.286 I ggml_metal_init: simdgroup reduction   = true
0.00.055.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.286 I ggml_metal_init: has bfloat            = true
0.00.055.286 I ggml_metal_init: use bfloat            = true
0.00.055.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.026 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.334 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.350 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.242 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.243 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.243 I llama_init_from_model: graph nodes  = 967
0.00.067.243 I llama_init_from_model: graph splits = 2
0.00.067.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.684 I 
0.00.584.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.740 I perplexity: tokenizing the input ..
0.00.592.665 I perplexity: tokenization took 7.923 ms
0.00.592.669 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.420 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.716.563 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.716.595 I llama_perf_context_print:        load time =     575.31 ms
0.00.716.596 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.69 tokens per second)
0.00.716.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.598 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.00.716.952 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.077s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.124 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.124 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.126 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.129 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.873 I llama_model_loader: - type  f32:  194 tensors
0.00.024.873 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.874 I print_info: file format = GGUF V3 (latest)
0.00.024.875 I print_info: file type   = Q4_1
0.00.024.875 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.345 I load: special tokens cache size = 25
0.00.050.477 I load: token to piece cache size = 0.2984 MB
0.00.050.480 I print_info: arch             = gptneox
0.00.050.480 I print_info: vocab_only       = 0
0.00.050.480 I print_info: n_ctx_train      = 2048
0.00.050.480 I print_info: n_embd           = 2048
0.00.050.480 I print_info: n_layer          = 24
0.00.050.484 I print_info: n_head           = 16
0.00.050.485 I print_info: n_head_kv        = 16
0.00.050.485 I print_info: n_rot            = 32
0.00.050.486 I print_info: n_swa            = 0
0.00.050.486 I print_info: n_embd_head_k    = 128
0.00.050.486 I print_info: n_embd_head_v    = 128
0.00.050.487 I print_info: n_gqa            = 1
0.00.050.488 I print_info: n_embd_k_gqa     = 2048
0.00.050.488 I print_info: n_embd_v_gqa     = 2048
0.00.050.489 I print_info: f_norm_eps       = 1.0e-05
0.00.050.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.490 I print_info: f_logit_scale    = 0.0e+00
0.00.050.491 I print_info: n_ff             = 8192
0.00.050.491 I print_info: n_expert         = 0
0.00.050.491 I print_info: n_expert_used    = 0
0.00.050.491 I print_info: causal attn      = 1
0.00.050.492 I print_info: pooling type     = 0
0.00.050.492 I print_info: rope type        = 2
0.00.050.492 I print_info: rope scaling     = linear
0.00.050.493 I print_info: freq_base_train  = 10000.0
0.00.050.494 I print_info: freq_scale_train = 1
0.00.050.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.494 I print_info: rope_finetuned   = unknown
0.00.050.494 I print_info: ssm_d_conv       = 0
0.00.050.494 I print_info: ssm_d_inner      = 0
0.00.050.495 I print_info: ssm_d_state      = 0
0.00.050.495 I print_info: ssm_dt_rank      = 0
0.00.050.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.495 I print_info: model type       = 1.4B
0.00.050.495 I print_info: model params     = 1.41 B
0.00.050.496 I print_info: general.name     = 1.4B
0.00.050.497 I print_info: vocab type       = BPE
0.00.050.501 I print_info: n_vocab          = 50304
0.00.050.501 I print_info: n_merges         = 50009
0.00.050.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.502 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.502 I print_info: LF token         = 128 'Ä'
0.00.050.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.504 I print_info: max token length = 1024
0.00.052.533 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.533 I load_tensors: offloading output layer to GPU
0.00.052.534 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.544 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.545 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.840 I llama_init_from_model: n_seq_max     = 1
0.00.052.841 I llama_init_from_model: n_ctx         = 128
0.00.052.841 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.841 I llama_init_from_model: n_batch       = 128
0.00.052.841 I llama_init_from_model: n_ubatch      = 128
0.00.052.842 I llama_init_from_model: flash_attn    = 0
0.00.052.842 I llama_init_from_model: freq_base     = 10000.0
0.00.052.842 I llama_init_from_model: freq_scale    = 1
0.00.052.843 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.843 I ggml_metal_init: allocating
0.00.052.846 I ggml_metal_init: found device: Apple M4
0.00.052.848 I ggml_metal_init: picking default device: Apple M4
0.00.053.412 I ggml_metal_init: using embedded metal library
0.00.055.745 I ggml_metal_init: GPU name:   Apple M4
0.00.055.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.747 I ggml_metal_init: simdgroup reduction   = true
0.00.055.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.747 I ggml_metal_init: has bfloat            = true
0.00.055.747 I ggml_metal_init: use bfloat            = true
0.00.055.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.863 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.865 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.811 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.812 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.812 I llama_init_from_model: graph nodes  = 967
0.00.067.812 I llama_init_from_model: graph splits = 2
0.00.067.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.814 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.075 I 
0.00.632.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.123 I perplexity: tokenizing the input ..
0.00.640.116 I perplexity: tokenization took 7.991 ms
0.00.640.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.797 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.764.019 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.764.044 I llama_perf_context_print:        load time =     623.15 ms
0.00.764.045 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.34 tokens per second)
0.00.764.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.046 I llama_perf_context_print:       total time =     131.97 ms /   129 tokens
0.00.764.430 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.654 I llama_model_loader: - type  f32:  194 tensors
0.00.024.655 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.656 I print_info: file format = GGUF V3 (latest)
0.00.024.656 I print_info: file type   = Q5_0
0.00.024.657 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.654 I load: special tokens cache size = 25
0.00.049.648 I load: token to piece cache size = 0.2984 MB
0.00.049.652 I print_info: arch             = gptneox
0.00.049.653 I print_info: vocab_only       = 0
0.00.049.653 I print_info: n_ctx_train      = 2048
0.00.049.653 I print_info: n_embd           = 2048
0.00.049.653 I print_info: n_layer          = 24
0.00.049.656 I print_info: n_head           = 16
0.00.049.657 I print_info: n_head_kv        = 16
0.00.049.657 I print_info: n_rot            = 32
0.00.049.658 I print_info: n_swa            = 0
0.00.049.658 I print_info: n_embd_head_k    = 128
0.00.049.658 I print_info: n_embd_head_v    = 128
0.00.049.660 I print_info: n_gqa            = 1
0.00.049.661 I print_info: n_embd_k_gqa     = 2048
0.00.049.662 I print_info: n_embd_v_gqa     = 2048
0.00.049.662 I print_info: f_norm_eps       = 1.0e-05
0.00.049.668 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.669 I print_info: f_logit_scale    = 0.0e+00
0.00.049.678 I print_info: n_ff             = 8192
0.00.049.678 I print_info: n_expert         = 0
0.00.049.678 I print_info: n_expert_used    = 0
0.00.049.679 I print_info: causal attn      = 1
0.00.049.679 I print_info: pooling type     = 0
0.00.049.679 I print_info: rope type        = 2
0.00.049.679 I print_info: rope scaling     = linear
0.00.049.680 I print_info: freq_base_train  = 10000.0
0.00.049.681 I print_info: freq_scale_train = 1
0.00.049.681 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.681 I print_info: rope_finetuned   = unknown
0.00.049.682 I print_info: ssm_d_conv       = 0
0.00.049.682 I print_info: ssm_d_inner      = 0
0.00.049.682 I print_info: ssm_d_state      = 0
0.00.049.682 I print_info: ssm_dt_rank      = 0
0.00.049.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.683 I print_info: model type       = 1.4B
0.00.049.683 I print_info: model params     = 1.41 B
0.00.049.683 I print_info: general.name     = 1.4B
0.00.049.684 I print_info: vocab type       = BPE
0.00.049.684 I print_info: n_vocab          = 50304
0.00.049.684 I print_info: n_merges         = 50009
0.00.049.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.685 I print_info: LF token         = 128 'Ä'
0.00.049.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.687 I print_info: max token length = 1024
0.00.051.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.451 I load_tensors: offloading output layer to GPU
0.00.051.452 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.457 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.458 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.848 I llama_init_from_model: n_seq_max     = 1
0.00.051.849 I llama_init_from_model: n_ctx         = 128
0.00.051.849 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.849 I llama_init_from_model: n_batch       = 128
0.00.051.849 I llama_init_from_model: n_ubatch      = 128
0.00.051.850 I llama_init_from_model: flash_attn    = 0
0.00.051.850 I llama_init_from_model: freq_base     = 10000.0
0.00.051.850 I llama_init_from_model: freq_scale    = 1
0.00.051.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.851 I ggml_metal_init: allocating
0.00.051.854 I ggml_metal_init: found device: Apple M4
0.00.051.856 I ggml_metal_init: picking default device: Apple M4
0.00.052.398 I ggml_metal_init: using embedded metal library
0.00.054.722 I ggml_metal_init: GPU name:   Apple M4
0.00.054.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.724 I ggml_metal_init: simdgroup reduction   = true
0.00.054.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.725 I ggml_metal_init: has bfloat            = true
0.00.054.725 I ggml_metal_init: use bfloat            = true
0.00.054.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.404 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.769 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.774 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.790 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.708 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.709 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.710 I llama_init_from_model: graph nodes  = 967
0.00.066.710 I llama_init_from_model: graph splits = 2
0.00.066.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.737 I 
0.00.675.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.780 I perplexity: tokenizing the input ..
0.00.683.506 I perplexity: tokenization took 7.724 ms
0.00.683.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.733 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.819.898 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.819.923 I llama_perf_context_print:        load time =     666.87 ms
0.00.819.924 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.16 tokens per second)
0.00.819.925 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.925 I llama_perf_context_print:       total time =     144.19 ms /   129 tokens
0.00.820.417 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.227 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.227 I llama_model_loader: - type  f32:  194 tensors
0.00.025.228 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.228 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.228 I print_info: file format = GGUF V3 (latest)
0.00.025.229 I print_info: file type   = Q5_1
0.00.025.231 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.170 I load: special tokens cache size = 25
0.00.050.098 I load: token to piece cache size = 0.2984 MB
0.00.050.101 I print_info: arch             = gptneox
0.00.050.101 I print_info: vocab_only       = 0
0.00.050.101 I print_info: n_ctx_train      = 2048
0.00.050.101 I print_info: n_embd           = 2048
0.00.050.102 I print_info: n_layer          = 24
0.00.050.104 I print_info: n_head           = 16
0.00.050.105 I print_info: n_head_kv        = 16
0.00.050.105 I print_info: n_rot            = 32
0.00.050.106 I print_info: n_swa            = 0
0.00.050.106 I print_info: n_embd_head_k    = 128
0.00.050.106 I print_info: n_embd_head_v    = 128
0.00.050.107 I print_info: n_gqa            = 1
0.00.050.108 I print_info: n_embd_k_gqa     = 2048
0.00.050.108 I print_info: n_embd_v_gqa     = 2048
0.00.050.109 I print_info: f_norm_eps       = 1.0e-05
0.00.050.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.110 I print_info: f_logit_scale    = 0.0e+00
0.00.050.110 I print_info: n_ff             = 8192
0.00.050.110 I print_info: n_expert         = 0
0.00.050.111 I print_info: n_expert_used    = 0
0.00.050.111 I print_info: causal attn      = 1
0.00.050.111 I print_info: pooling type     = 0
0.00.050.111 I print_info: rope type        = 2
0.00.050.111 I print_info: rope scaling     = linear
0.00.050.112 I print_info: freq_base_train  = 10000.0
0.00.050.112 I print_info: freq_scale_train = 1
0.00.050.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.112 I print_info: rope_finetuned   = unknown
0.00.050.112 I print_info: ssm_d_conv       = 0
0.00.050.113 I print_info: ssm_d_inner      = 0
0.00.050.113 I print_info: ssm_d_state      = 0
0.00.050.114 I print_info: ssm_dt_rank      = 0
0.00.050.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.114 I print_info: model type       = 1.4B
0.00.050.115 I print_info: model params     = 1.41 B
0.00.050.115 I print_info: general.name     = 1.4B
0.00.050.115 I print_info: vocab type       = BPE
0.00.050.115 I print_info: n_vocab          = 50304
0.00.050.116 I print_info: n_merges         = 50009
0.00.050.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.118 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.118 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.118 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.118 I print_info: LF token         = 128 'Ä'
0.00.050.118 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.119 I print_info: max token length = 1024
0.00.052.086 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.086 I load_tensors: offloading output layer to GPU
0.00.052.086 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.097 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.098 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.404 I llama_init_from_model: n_seq_max     = 1
0.00.052.404 I llama_init_from_model: n_ctx         = 128
0.00.052.404 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.405 I llama_init_from_model: n_batch       = 128
0.00.052.405 I llama_init_from_model: n_ubatch      = 128
0.00.052.405 I llama_init_from_model: flash_attn    = 0
0.00.052.405 I llama_init_from_model: freq_base     = 10000.0
0.00.052.405 I llama_init_from_model: freq_scale    = 1
0.00.052.406 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.406 I ggml_metal_init: allocating
0.00.052.409 I ggml_metal_init: found device: Apple M4
0.00.052.411 I ggml_metal_init: picking default device: Apple M4
0.00.052.997 I ggml_metal_init: using embedded metal library
0.00.055.344 I ggml_metal_init: GPU name:   Apple M4
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.347 I ggml_metal_init: simdgroup reduction   = true
0.00.055.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.347 I ggml_metal_init: has bfloat            = true
0.00.055.347 I ggml_metal_init: use bfloat            = true
0.00.055.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.304 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.307 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.323 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.208 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.209 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.210 I llama_init_from_model: graph nodes  = 967
0.00.067.210 I llama_init_from_model: graph splits = 2
0.00.067.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.168 I 
0.00.738.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.204 I perplexity: tokenizing the input ..
0.00.745.593 I perplexity: tokenization took 7.388 ms
0.00.745.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.994 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.882.233 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.882.260 I llama_perf_context_print:        load time =     728.40 ms
0.00.882.261 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.91 tokens per second)
0.00.882.262 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.263 I llama_perf_context_print:       total time =     144.09 ms /   129 tokens
0.00.882.803 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.652 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.655 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.655 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.655 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.286 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.286 I llama_model_loader: - type  f32:  194 tensors
0.00.024.286 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.287 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.287 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.288 I print_info: file format = GGUF V3 (latest)
0.00.024.288 I print_info: file type   = Q2_K - Medium
0.00.024.289 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.119 I load: special tokens cache size = 25
0.00.049.127 I load: token to piece cache size = 0.2984 MB
0.00.049.132 I print_info: arch             = gptneox
0.00.049.132 I print_info: vocab_only       = 0
0.00.049.132 I print_info: n_ctx_train      = 2048
0.00.049.132 I print_info: n_embd           = 2048
0.00.049.132 I print_info: n_layer          = 24
0.00.049.136 I print_info: n_head           = 16
0.00.049.137 I print_info: n_head_kv        = 16
0.00.049.137 I print_info: n_rot            = 32
0.00.049.137 I print_info: n_swa            = 0
0.00.049.137 I print_info: n_embd_head_k    = 128
0.00.049.137 I print_info: n_embd_head_v    = 128
0.00.049.139 I print_info: n_gqa            = 1
0.00.049.140 I print_info: n_embd_k_gqa     = 2048
0.00.049.142 I print_info: n_embd_v_gqa     = 2048
0.00.049.143 I print_info: f_norm_eps       = 1.0e-05
0.00.049.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.144 I print_info: f_logit_scale    = 0.0e+00
0.00.049.144 I print_info: n_ff             = 8192
0.00.049.145 I print_info: n_expert         = 0
0.00.049.145 I print_info: n_expert_used    = 0
0.00.049.145 I print_info: causal attn      = 1
0.00.049.145 I print_info: pooling type     = 0
0.00.049.145 I print_info: rope type        = 2
0.00.049.146 I print_info: rope scaling     = linear
0.00.049.146 I print_info: freq_base_train  = 10000.0
0.00.049.146 I print_info: freq_scale_train = 1
0.00.049.146 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.147 I print_info: rope_finetuned   = unknown
0.00.049.147 I print_info: ssm_d_conv       = 0
0.00.049.147 I print_info: ssm_d_inner      = 0
0.00.049.147 I print_info: ssm_d_state      = 0
0.00.049.147 I print_info: ssm_dt_rank      = 0
0.00.049.147 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.148 I print_info: model type       = 1.4B
0.00.049.148 I print_info: model params     = 1.41 B
0.00.049.148 I print_info: general.name     = 1.4B
0.00.049.149 I print_info: vocab type       = BPE
0.00.049.149 I print_info: n_vocab          = 50304
0.00.049.150 I print_info: n_merges         = 50009
0.00.049.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: LF token         = 128 'Ä'
0.00.049.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: max token length = 1024
0.00.051.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.064 I load_tensors: offloading output layer to GPU
0.00.051.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.075 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.077 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.406 I llama_init_from_model: n_seq_max     = 1
0.00.051.407 I llama_init_from_model: n_ctx         = 128
0.00.051.407 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.407 I llama_init_from_model: n_batch       = 128
0.00.051.407 I llama_init_from_model: n_ubatch      = 128
0.00.051.407 I llama_init_from_model: flash_attn    = 0
0.00.051.408 I llama_init_from_model: freq_base     = 10000.0
0.00.051.408 I llama_init_from_model: freq_scale    = 1
0.00.051.408 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.409 I ggml_metal_init: allocating
0.00.051.412 I ggml_metal_init: found device: Apple M4
0.00.051.413 I ggml_metal_init: picking default device: Apple M4
0.00.051.991 I ggml_metal_init: using embedded metal library
0.00.054.360 I ggml_metal_init: GPU name:   Apple M4
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.363 I ggml_metal_init: simdgroup reduction   = true
0.00.054.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.363 I ggml_metal_init: has bfloat            = true
0.00.054.363 I ggml_metal_init: use bfloat            = true
0.00.054.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.088 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.446 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.448 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.462 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.329 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.330 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.331 I llama_init_from_model: graph nodes  = 967
0.00.066.331 I llama_init_from_model: graph splits = 2
0.00.066.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.494 I 
0.00.445.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.537 I perplexity: tokenizing the input ..
0.00.453.374 I perplexity: tokenization took 7.835 ms
0.00.453.378 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.781 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.586.951 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.586.991 I llama_perf_context_print:        load time =     436.49 ms
0.00.586.992 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.39 tokens per second)
0.00.586.993 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.586.993 I llama_perf_context_print:       total time =     141.50 ms /   129 tokens
0.00.587.458 I ggml_metal_free: deallocating

real	0m0.601s
user	0m0.077s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.053 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.878 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.713 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.716 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.716 I llama_model_loader: - type  f32:  194 tensors
0.00.024.716 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.717 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.717 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.717 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.718 I print_info: file format = GGUF V3 (latest)
0.00.024.718 I print_info: file type   = Q3_K - Medium
0.00.024.720 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.207 I load: special tokens cache size = 25
0.00.050.142 I load: token to piece cache size = 0.2984 MB
0.00.050.145 I print_info: arch             = gptneox
0.00.050.146 I print_info: vocab_only       = 0
0.00.050.146 I print_info: n_ctx_train      = 2048
0.00.050.146 I print_info: n_embd           = 2048
0.00.050.146 I print_info: n_layer          = 24
0.00.050.149 I print_info: n_head           = 16
0.00.050.150 I print_info: n_head_kv        = 16
0.00.050.150 I print_info: n_rot            = 32
0.00.050.150 I print_info: n_swa            = 0
0.00.050.152 I print_info: n_embd_head_k    = 128
0.00.050.152 I print_info: n_embd_head_v    = 128
0.00.050.153 I print_info: n_gqa            = 1
0.00.050.154 I print_info: n_embd_k_gqa     = 2048
0.00.050.155 I print_info: n_embd_v_gqa     = 2048
0.00.050.155 I print_info: f_norm_eps       = 1.0e-05
0.00.050.156 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.156 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.156 I print_info: f_logit_scale    = 0.0e+00
0.00.050.157 I print_info: n_ff             = 8192
0.00.050.157 I print_info: n_expert         = 0
0.00.050.157 I print_info: n_expert_used    = 0
0.00.050.157 I print_info: causal attn      = 1
0.00.050.157 I print_info: pooling type     = 0
0.00.050.159 I print_info: rope type        = 2
0.00.050.160 I print_info: rope scaling     = linear
0.00.050.161 I print_info: freq_base_train  = 10000.0
0.00.050.161 I print_info: freq_scale_train = 1
0.00.050.161 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.161 I print_info: rope_finetuned   = unknown
0.00.050.166 I print_info: ssm_d_conv       = 0
0.00.050.166 I print_info: ssm_d_inner      = 0
0.00.050.166 I print_info: ssm_d_state      = 0
0.00.050.166 I print_info: ssm_dt_rank      = 0
0.00.050.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.167 I print_info: model type       = 1.4B
0.00.050.167 I print_info: model params     = 1.41 B
0.00.050.168 I print_info: general.name     = 1.4B
0.00.050.168 I print_info: vocab type       = BPE
0.00.050.168 I print_info: n_vocab          = 50304
0.00.050.168 I print_info: n_merges         = 50009
0.00.050.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.169 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.169 I print_info: LF token         = 128 'Ä'
0.00.050.169 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.169 I print_info: max token length = 1024
0.00.052.041 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.041 I load_tensors: offloading output layer to GPU
0.00.052.042 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.052 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.053 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.331 I llama_init_from_model: n_seq_max     = 1
0.00.052.332 I llama_init_from_model: n_ctx         = 128
0.00.052.332 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.332 I llama_init_from_model: n_batch       = 128
0.00.052.332 I llama_init_from_model: n_ubatch      = 128
0.00.052.332 I llama_init_from_model: flash_attn    = 0
0.00.052.333 I llama_init_from_model: freq_base     = 10000.0
0.00.052.333 I llama_init_from_model: freq_scale    = 1
0.00.052.333 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.334 I ggml_metal_init: allocating
0.00.052.336 I ggml_metal_init: found device: Apple M4
0.00.052.338 I ggml_metal_init: picking default device: Apple M4
0.00.052.914 I ggml_metal_init: using embedded metal library
0.00.055.255 I ggml_metal_init: GPU name:   Apple M4
0.00.055.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.258 I ggml_metal_init: simdgroup reduction   = true
0.00.055.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.258 I ggml_metal_init: has bfloat            = true
0.00.055.258 I ggml_metal_init: use bfloat            = true
0.00.055.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.004 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.301 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.161 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.163 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.163 I llama_init_from_model: graph nodes  = 967
0.00.067.163 I llama_init_from_model: graph splits = 2
0.00.067.164 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.164 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.345 I 
0.00.472.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.396 I perplexity: tokenizing the input ..
0.00.480.290 I perplexity: tokenization took 7.892 ms
0.00.480.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.136 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.613.383 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.613.405 I llama_perf_context_print:        load time =     463.48 ms
0.00.613.406 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.53 tokens per second)
0.00.613.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.613.407 I llama_perf_context_print:       total time =     141.06 ms /   129 tokens
0.00.613.847 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.078s
sys	0m0.078s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.592 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.594 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.596 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.597 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.597 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.598 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.226 I llama_model_loader: - type  f32:  194 tensors
0.00.025.226 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.226 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.226 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.227 I print_info: file format = GGUF V3 (latest)
0.00.025.227 I print_info: file type   = Q4_K - Medium
0.00.025.228 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.093 I load: special tokens cache size = 25
0.00.050.151 I load: token to piece cache size = 0.2984 MB
0.00.050.154 I print_info: arch             = gptneox
0.00.050.154 I print_info: vocab_only       = 0
0.00.050.155 I print_info: n_ctx_train      = 2048
0.00.050.155 I print_info: n_embd           = 2048
0.00.050.155 I print_info: n_layer          = 24
0.00.050.158 I print_info: n_head           = 16
0.00.050.159 I print_info: n_head_kv        = 16
0.00.050.159 I print_info: n_rot            = 32
0.00.050.159 I print_info: n_swa            = 0
0.00.050.160 I print_info: n_embd_head_k    = 128
0.00.050.160 I print_info: n_embd_head_v    = 128
0.00.050.160 I print_info: n_gqa            = 1
0.00.050.161 I print_info: n_embd_k_gqa     = 2048
0.00.050.162 I print_info: n_embd_v_gqa     = 2048
0.00.050.162 I print_info: f_norm_eps       = 1.0e-05
0.00.050.163 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.163 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.164 I print_info: f_logit_scale    = 0.0e+00
0.00.050.164 I print_info: n_ff             = 8192
0.00.050.165 I print_info: n_expert         = 0
0.00.050.165 I print_info: n_expert_used    = 0
0.00.050.165 I print_info: causal attn      = 1
0.00.050.165 I print_info: pooling type     = 0
0.00.050.165 I print_info: rope type        = 2
0.00.050.167 I print_info: rope scaling     = linear
0.00.050.169 I print_info: freq_base_train  = 10000.0
0.00.050.169 I print_info: freq_scale_train = 1
0.00.050.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.170 I print_info: rope_finetuned   = unknown
0.00.050.170 I print_info: ssm_d_conv       = 0
0.00.050.170 I print_info: ssm_d_inner      = 0
0.00.050.170 I print_info: ssm_d_state      = 0
0.00.050.170 I print_info: ssm_dt_rank      = 0
0.00.050.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.172 I print_info: model type       = 1.4B
0.00.050.176 I print_info: model params     = 1.41 B
0.00.050.176 I print_info: general.name     = 1.4B
0.00.050.176 I print_info: vocab type       = BPE
0.00.050.178 I print_info: n_vocab          = 50304
0.00.050.178 I print_info: n_merges         = 50009
0.00.050.178 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: LF token         = 128 'Ä'
0.00.050.179 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.180 I print_info: max token length = 1024
0.00.052.045 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.045 I load_tensors: offloading output layer to GPU
0.00.052.045 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.056 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.057 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.460 I llama_init_from_model: n_seq_max     = 1
0.00.052.460 I llama_init_from_model: n_ctx         = 128
0.00.052.461 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.461 I llama_init_from_model: n_batch       = 128
0.00.052.461 I llama_init_from_model: n_ubatch      = 128
0.00.052.461 I llama_init_from_model: flash_attn    = 0
0.00.052.461 I llama_init_from_model: freq_base     = 10000.0
0.00.052.462 I llama_init_from_model: freq_scale    = 1
0.00.052.462 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.462 I ggml_metal_init: allocating
0.00.052.465 I ggml_metal_init: found device: Apple M4
0.00.052.467 I ggml_metal_init: picking default device: Apple M4
0.00.053.036 I ggml_metal_init: using embedded metal library
0.00.055.402 I ggml_metal_init: GPU name:   Apple M4
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.404 I ggml_metal_init: simdgroup reduction   = true
0.00.055.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.404 I ggml_metal_init: has bfloat            = true
0.00.055.405 I ggml_metal_init: use bfloat            = true
0.00.055.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.093 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.094 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.094 I llama_init_from_model: graph nodes  = 967
0.00.067.094 I llama_init_from_model: graph splits = 2
0.00.067.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.585 I 
0.00.561.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.624 I perplexity: tokenizing the input ..
0.00.569.575 I perplexity: tokenization took 7.95 ms
0.00.569.580 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.704.227 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.705.393 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.705.423 I llama_perf_context_print:        load time =     551.83 ms
0.00.705.424 I llama_perf_context_print: prompt eval time =     134.42 ms /   128 tokens (    1.05 ms per token,   952.23 tokens per second)
0.00.705.424 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.705.425 I llama_perf_context_print:       total time =     143.84 ms /   129 tokens
0.00.705.885 I ggml_metal_free: deallocating

real	0m0.721s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.825 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.726 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.727 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.727 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.730 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.731 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.315 I llama_model_loader: - type  f32:  194 tensors
0.00.024.315 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.315 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.316 I print_info: file format = GGUF V3 (latest)
0.00.024.316 I print_info: file type   = Q5_K - Medium
0.00.024.317 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.893 I load: special tokens cache size = 25
0.00.049.793 I load: token to piece cache size = 0.2984 MB
0.00.049.796 I print_info: arch             = gptneox
0.00.049.797 I print_info: vocab_only       = 0
0.00.049.797 I print_info: n_ctx_train      = 2048
0.00.049.797 I print_info: n_embd           = 2048
0.00.049.797 I print_info: n_layer          = 24
0.00.049.800 I print_info: n_head           = 16
0.00.049.801 I print_info: n_head_kv        = 16
0.00.049.801 I print_info: n_rot            = 32
0.00.049.801 I print_info: n_swa            = 0
0.00.049.803 I print_info: n_embd_head_k    = 128
0.00.049.803 I print_info: n_embd_head_v    = 128
0.00.049.804 I print_info: n_gqa            = 1
0.00.049.805 I print_info: n_embd_k_gqa     = 2048
0.00.049.811 I print_info: n_embd_v_gqa     = 2048
0.00.049.813 I print_info: f_norm_eps       = 1.0e-05
0.00.049.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.814 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.814 I print_info: f_logit_scale    = 0.0e+00
0.00.049.820 I print_info: n_ff             = 8192
0.00.049.821 I print_info: n_expert         = 0
0.00.049.822 I print_info: n_expert_used    = 0
0.00.049.822 I print_info: causal attn      = 1
0.00.049.822 I print_info: pooling type     = 0
0.00.049.822 I print_info: rope type        = 2
0.00.049.823 I print_info: rope scaling     = linear
0.00.049.823 I print_info: freq_base_train  = 10000.0
0.00.049.823 I print_info: freq_scale_train = 1
0.00.049.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.824 I print_info: rope_finetuned   = unknown
0.00.049.824 I print_info: ssm_d_conv       = 0
0.00.049.824 I print_info: ssm_d_inner      = 0
0.00.049.824 I print_info: ssm_d_state      = 0
0.00.049.824 I print_info: ssm_dt_rank      = 0
0.00.049.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.825 I print_info: model type       = 1.4B
0.00.049.825 I print_info: model params     = 1.41 B
0.00.049.825 I print_info: general.name     = 1.4B
0.00.049.826 I print_info: vocab type       = BPE
0.00.049.826 I print_info: n_vocab          = 50304
0.00.049.826 I print_info: n_merges         = 50009
0.00.049.826 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.828 I print_info: LF token         = 128 'Ä'
0.00.049.829 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.830 I print_info: max token length = 1024
0.00.051.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.826 I load_tensors: offloading output layer to GPU
0.00.051.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.836 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.837 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.121 I llama_init_from_model: n_seq_max     = 1
0.00.052.122 I llama_init_from_model: n_ctx         = 128
0.00.052.122 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.122 I llama_init_from_model: n_batch       = 128
0.00.052.122 I llama_init_from_model: n_ubatch      = 128
0.00.052.122 I llama_init_from_model: flash_attn    = 0
0.00.052.123 I llama_init_from_model: freq_base     = 10000.0
0.00.052.123 I llama_init_from_model: freq_scale    = 1
0.00.052.123 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.124 I ggml_metal_init: allocating
0.00.052.126 I ggml_metal_init: found device: Apple M4
0.00.052.128 I ggml_metal_init: picking default device: Apple M4
0.00.052.688 I ggml_metal_init: using embedded metal library
0.00.055.018 I ggml_metal_init: GPU name:   Apple M4
0.00.055.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.020 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.021 I ggml_metal_init: simdgroup reduction   = true
0.00.055.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.021 I ggml_metal_init: has bfloat            = true
0.00.055.021 I ggml_metal_init: use bfloat            = true
0.00.055.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.352 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.555 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.442 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.443 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.443 I llama_init_from_model: graph nodes  = 967
0.00.066.443 I llama_init_from_model: graph splits = 2
0.00.066.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.709 I 
0.00.617.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.759 I perplexity: tokenizing the input ..
0.00.625.618 I perplexity: tokenization took 7.858 ms
0.00.625.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.360 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.767.588 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.767.617 I llama_perf_context_print:        load time =     608.88 ms
0.00.767.618 I llama_perf_context_print: prompt eval time =     140.51 ms /   128 tokens (    1.10 ms per token,   910.95 tokens per second)
0.00.767.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.619 I llama_perf_context_print:       total time =     149.91 ms /   129 tokens
0.00.768.135 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.911 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.424 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.106 I llama_model_loader: - type  f32:  194 tensors
0.00.024.106 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.107 I print_info: file format = GGUF V3 (latest)
0.00.024.107 I print_info: file type   = Q6_K
0.00.024.108 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.986 I load: special tokens cache size = 25
0.00.048.892 I load: token to piece cache size = 0.2984 MB
0.00.048.897 I print_info: arch             = gptneox
0.00.048.897 I print_info: vocab_only       = 0
0.00.048.897 I print_info: n_ctx_train      = 2048
0.00.048.897 I print_info: n_embd           = 2048
0.00.048.898 I print_info: n_layer          = 24
0.00.048.900 I print_info: n_head           = 16
0.00.048.901 I print_info: n_head_kv        = 16
0.00.048.901 I print_info: n_rot            = 32
0.00.048.902 I print_info: n_swa            = 0
0.00.048.902 I print_info: n_embd_head_k    = 128
0.00.048.902 I print_info: n_embd_head_v    = 128
0.00.048.903 I print_info: n_gqa            = 1
0.00.048.904 I print_info: n_embd_k_gqa     = 2048
0.00.048.904 I print_info: n_embd_v_gqa     = 2048
0.00.048.905 I print_info: f_norm_eps       = 1.0e-05
0.00.048.905 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.905 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.905 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.908 I print_info: f_logit_scale    = 0.0e+00
0.00.048.908 I print_info: n_ff             = 8192
0.00.048.908 I print_info: n_expert         = 0
0.00.048.909 I print_info: n_expert_used    = 0
0.00.048.909 I print_info: causal attn      = 1
0.00.048.909 I print_info: pooling type     = 0
0.00.048.909 I print_info: rope type        = 2
0.00.048.909 I print_info: rope scaling     = linear
0.00.048.910 I print_info: freq_base_train  = 10000.0
0.00.048.910 I print_info: freq_scale_train = 1
0.00.048.910 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.910 I print_info: rope_finetuned   = unknown
0.00.048.911 I print_info: ssm_d_conv       = 0
0.00.048.912 I print_info: ssm_d_inner      = 0
0.00.048.912 I print_info: ssm_d_state      = 0
0.00.048.913 I print_info: ssm_dt_rank      = 0
0.00.048.913 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.913 I print_info: model type       = 1.4B
0.00.048.913 I print_info: model params     = 1.41 B
0.00.048.913 I print_info: general.name     = 1.4B
0.00.048.914 I print_info: vocab type       = BPE
0.00.048.914 I print_info: n_vocab          = 50304
0.00.048.914 I print_info: n_merges         = 50009
0.00.048.915 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.915 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.915 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.915 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.915 I print_info: LF token         = 128 'Ä'
0.00.048.916 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.917 I print_info: max token length = 1024
0.00.050.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.954 I load_tensors: offloading output layer to GPU
0.00.050.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.965 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.966 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.321 I llama_init_from_model: n_seq_max     = 1
0.00.051.322 I llama_init_from_model: n_ctx         = 128
0.00.051.322 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.322 I llama_init_from_model: n_batch       = 128
0.00.051.322 I llama_init_from_model: n_ubatch      = 128
0.00.051.322 I llama_init_from_model: flash_attn    = 0
0.00.051.323 I llama_init_from_model: freq_base     = 10000.0
0.00.051.323 I llama_init_from_model: freq_scale    = 1
0.00.051.323 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.324 I ggml_metal_init: allocating
0.00.051.326 I ggml_metal_init: found device: Apple M4
0.00.051.328 I ggml_metal_init: picking default device: Apple M4
0.00.051.899 I ggml_metal_init: using embedded metal library
0.00.054.255 I ggml_metal_init: GPU name:   Apple M4
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.257 I ggml_metal_init: simdgroup reduction   = true
0.00.054.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.257 I ggml_metal_init: has bfloat            = true
0.00.054.257 I ggml_metal_init: use bfloat            = true
0.00.054.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.964 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.261 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.263 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.278 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.145 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.146 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.147 I llama_init_from_model: graph nodes  = 967
0.00.066.147 I llama_init_from_model: graph splits = 2
0.00.066.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.837 I 
0.00.580.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.872 I perplexity: tokenizing the input ..
0.00.588.880 I perplexity: tokenization took 8.007 ms
0.00.588.884 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.156 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.729.535 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.729.562 I llama_perf_context_print:        load time =     571.92 ms
0.00.729.562 I llama_perf_context_print: prompt eval time =     139.04 ms /   128 tokens (    1.09 ms per token,   920.60 tokens per second)
0.00.729.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.564 I llama_perf_context_print:       total time =     148.73 ms /   129 tokens
0.00.729.915 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.204 I build: 4523 (e28245f3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.525 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.147 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.153 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.155 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.859 I llama_model_loader: - type  f32:  194 tensors
0.00.035.860 I llama_model_loader: - type  f16:   98 tensors
0.00.035.860 I print_info: file format = GGUF V3 (latest)
0.00.035.861 I print_info: file type   = all F32 (guessed)
0.00.035.863 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.055.539 I load: special tokens cache size = 25
0.00.061.427 I load: token to piece cache size = 0.2984 MB
0.00.061.432 I print_info: arch             = gptneox
0.00.061.432 I print_info: vocab_only       = 0
0.00.061.432 I print_info: n_ctx_train      = 2048
0.00.061.433 I print_info: n_embd           = 2048
0.00.061.433 I print_info: n_layer          = 24
0.00.061.437 I print_info: n_head           = 16
0.00.061.437 I print_info: n_head_kv        = 16
0.00.061.437 I print_info: n_rot            = 32
0.00.061.437 I print_info: n_swa            = 0
0.00.061.438 I print_info: n_embd_head_k    = 128
0.00.061.438 I print_info: n_embd_head_v    = 128
0.00.061.438 I print_info: n_gqa            = 1
0.00.061.439 I print_info: n_embd_k_gqa     = 2048
0.00.061.439 I print_info: n_embd_v_gqa     = 2048
0.00.061.440 I print_info: f_norm_eps       = 1.0e-05
0.00.061.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.441 I print_info: f_logit_scale    = 0.0e+00
0.00.061.441 I print_info: n_ff             = 8192
0.00.061.442 I print_info: n_expert         = 0
0.00.061.442 I print_info: n_expert_used    = 0
0.00.061.442 I print_info: causal attn      = 1
0.00.061.442 I print_info: pooling type     = 0
0.00.061.442 I print_info: rope type        = 2
0.00.061.442 I print_info: rope scaling     = linear
0.00.061.445 I print_info: freq_base_train  = 10000.0
0.00.061.445 I print_info: freq_scale_train = 1
0.00.061.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.445 I print_info: rope_finetuned   = unknown
0.00.061.445 I print_info: ssm_d_conv       = 0
0.00.061.446 I print_info: ssm_d_inner      = 0
0.00.061.446 I print_info: ssm_d_state      = 0
0.00.061.446 I print_info: ssm_dt_rank      = 0
0.00.061.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.446 I print_info: model type       = 1.4B
0.00.061.446 I print_info: model params     = 1.41 B
0.00.061.446 I print_info: general.name     = 1.4B
0.00.061.447 I print_info: vocab type       = BPE
0.00.061.447 I print_info: n_vocab          = 50304
0.00.061.447 I print_info: n_merges         = 50009
0.00.061.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.448 I print_info: LF token         = 128 'Ä'
0.00.061.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.449 I print_info: max token length = 1024
0.00.063.994 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.995 I load_tensors: offloading output layer to GPU
0.00.063.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.005 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.064.006 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.064.390 I llama_init_from_model: n_seq_max     = 1
0.00.064.391 I llama_init_from_model: n_ctx         = 128
0.00.064.391 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.391 I llama_init_from_model: n_batch       = 128
0.00.064.391 I llama_init_from_model: n_ubatch      = 128
0.00.064.391 I llama_init_from_model: flash_attn    = 0
0.00.064.392 I llama_init_from_model: freq_base     = 10000.0
0.00.064.392 I llama_init_from_model: freq_scale    = 1
0.00.064.392 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.393 I ggml_metal_init: allocating
0.00.064.396 I ggml_metal_init: found device: Apple M4
0.00.064.399 I ggml_metal_init: picking default device: Apple M4
0.00.065.045 I ggml_metal_init: using embedded metal library
0.00.067.829 I ggml_metal_init: GPU name:   Apple M4
0.00.067.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.832 I ggml_metal_init: simdgroup reduction   = true
0.00.067.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.837 I ggml_metal_init: has bfloat            = true
0.00.067.837 I ggml_metal_init: use bfloat            = true
0.00.067.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.042 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.078.339 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.344 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.079.340 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.079.341 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.079.342 I llama_init_from_model: graph nodes  = 967
0.00.079.342 I llama_init_from_model: graph splits = 2
0.00.079.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.344 I 
0.00.079.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.079.383 I compute_imatrix: tokenizing the input ..
0.00.087.049 I compute_imatrix: tokenization took 7.664 ms
0.00.087.052 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.568.026 I compute_imatrix: 1.48 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.574.692 I llama_perf_context_print:        load time =    1550.50 ms
0.01.574.693 I llama_perf_context_print: prompt eval time =    1480.28 ms /   128 tokens (   11.56 ms per token,    86.47 tokens per second)
0.01.574.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.574.694 I llama_perf_context_print:       total time =    1557.16 ms /   129 tokens
0.01.575.272 I ggml_metal_free: deallocating

real	0m1.760s
user	0m0.145s
sys	0m0.200s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4523 (e28245f3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11160a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11160a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11160afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11160b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11160bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11160c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11160c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11160cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11160d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11160d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11160dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11160e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11160ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11160f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11160fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1116102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1116109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111611100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111611ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111612710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111613550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111613df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1116147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111615a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111615f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111616250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1116166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1116169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111617a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111618380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111618820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111619160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111619600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111619f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11161a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11161a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11161acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11161b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11161bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11161c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11161c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11161ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11161d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11161da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11161e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11161e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11161ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11161f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11161f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11161fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111620230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1116204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111620990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111620e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1116212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111621770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111621c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1116220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111622550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1116229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111622e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111623330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1116237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111623c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1116241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111624710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1116251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111625700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1116261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1116266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x111627190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1116276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111627c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x111628180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1116286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x111629170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1116296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111629c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11162a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11162a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11162ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11162b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11162b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11162bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11161b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11162c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11162c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11162cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11162d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11162d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11162dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11162e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11162e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11162ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11162f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11162f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11162fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x111630280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1116307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x111630d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1116311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111631660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111631b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111631fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111632440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1116328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111632d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111633220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1116336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111633b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1116344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111636060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111636500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1116369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111636e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1116372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1116380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111638560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1116397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11163a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11163a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11163aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11163af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11163b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11163b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11163bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11163c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11163c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11163cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11163cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11163d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11163d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11163dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11163e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11163e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11163eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11163efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11163f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11163f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11163fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111640240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1116406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111640b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111641020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1116414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111641960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1116422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111642740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1116439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111644300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1116447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111644c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1116450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111646360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111646ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111647140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1116475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111647a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111647f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111648470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1116489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111649720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111649d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11164a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11164a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11164b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11164b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11164b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11164beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11164c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11164ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11164d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11164d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11164da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11164e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11164e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11164ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11164f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11164f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11164fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111650220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111650770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111650cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111651210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111651760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111651cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111652200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111652750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111652ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1116531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111653740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111653c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1116541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111654730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111654c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1116551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111655720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111655c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1116561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111656710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111656c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1116571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111657700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1116581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1116586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111658c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111659190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1116596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111659c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11165a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11165a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11165ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11165b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11165b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11165bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11165c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11165c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11165cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11165d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11165d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11165dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11165e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11165e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11165ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11165f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11165f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11165fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111660120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111660670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111660bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111661060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111661500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1116619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111661e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1116622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111662780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111662c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1116630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111663560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111663a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111663ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111664340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1116647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111664c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111665120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111665670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111665d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1116664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111666bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1116672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1116675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111668060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111668670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.141.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107504d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1075051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107505630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107505aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107505f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107506380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1075067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107506c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1075070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107507540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1075079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1075080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107508bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107509370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107509b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10750a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10750a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10750b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10750b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10750bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10750c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10750cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10750d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10750dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10750e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10750e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10750e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10750ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10750f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10750f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10750fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10750ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1075103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107510670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107510ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107510f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1075113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107511830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107511ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107512110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107512580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1075129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107512e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1075132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107513740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107513bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107514020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107514490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107514900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107514d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1075151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107515650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107515ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107515f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1075163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107516810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107516d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107517280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1075176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107517b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107517fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107518440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1075188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107518d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107519190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107519600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107519a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107519ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10751a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10751a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10751ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10751b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10751b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10751b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10751bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10751c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10751c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10751cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10751cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10751d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10751d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10751dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10751e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10751e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10751ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10751eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10751f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10751f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10751fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107520080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1075204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107520960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107520dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107521240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1075216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107521b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107521f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107522400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107522870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107522ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107523150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1075235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107523a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107523ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107524310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107524780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107524bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107525060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1075254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107525940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107525db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107526220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107526690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107526b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107526f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1075273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107527850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107527cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107528130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1075285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107528a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107528e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1075292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107529760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107529bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10752a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10752a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10752a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10752ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10752b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10752b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10752bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10752bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10752c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10752c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10752cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10752d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10752d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10752d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10752de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10752e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10752e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10752ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10752f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10752f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10752f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10752fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1075301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107530650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107530ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107530f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1075313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107531810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107531c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1075320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107532560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1075329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107532e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1075332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107533720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107533b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107534000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107534470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1075348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107534d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1075351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107535df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1075360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107536370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1075367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107536c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1075370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107537530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1075379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107537e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107538280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1075386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107538b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107538fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107539440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1075398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107539d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10753a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10753a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10753aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10753aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10753b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10753b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10753bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10753c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10753c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10753c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10753cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10753d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10753d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10753db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10753dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10753e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10753e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10753ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10753f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10753f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10753fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107540050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1075404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107540930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107540da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107541210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107541730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107541c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1075427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107542a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107543030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1075435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107543bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107544170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107544730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107544cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1075452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107545870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107545e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1075463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1075469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107546f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107547530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107547af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1075480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107548670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107548c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1075491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1075497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107549d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10754a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10754a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10754aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10754b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10754ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10754bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10754c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10754cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10754d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10754d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10754dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10754e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10754e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10754edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10754f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10754f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10754ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1075504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107550ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107551070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107551630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107551bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1075521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107552770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107552d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1075532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1075538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107553e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107554430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1075549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107554fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107555570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107555b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1075560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1075566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107556c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107557170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107557670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107557b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107558070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107558570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107558a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107558f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107559470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107559970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107559e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10755a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10755a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10755ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10755b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10755b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10755c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10755c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10755cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10755d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10755d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10755e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10755e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10755ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116d044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116d04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116d04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116d05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116d056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116d05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116d05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116d063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116d06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116d06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116d07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116d078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116d083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116d08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116d09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116d09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116d0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116d0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116d0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116d0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116d0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116d0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116d0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116d0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116d0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116d0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116d0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116d0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116d0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116d0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116d0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116d0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116d0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116d0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116d10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116d107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116d10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116d110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116d11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116d119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116d11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116d12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116d12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116d12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116d12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116d13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116d138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116d13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116d141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116d14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116d14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116d14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116d15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116d157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116d15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116d160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116d16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116d16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116d16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116d17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116d17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116d17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116d18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116d185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116d18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116d18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116d19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116d19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116d19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116d1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116d1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116d1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116d1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116d1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116d1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116d1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116d1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116d1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116d1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116d1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116d1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116d1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116d1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116d1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116d1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116d1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116d1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116d1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116d1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116d1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116d1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116d20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116d20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116d20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116d20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116d213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116d21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116d21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116d22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116d22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116d229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116d22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116d232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116d23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116d23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116d24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116d24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116d24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116d24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116d25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116d258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116d25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116d261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116d26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116d26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116d26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116d27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116d277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116d27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116d280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116d28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116d28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116d28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116d29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116d296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116d29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116d29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116d2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116d2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116d2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116d2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116d2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116d2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116d2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116d2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116d2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116d2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116d2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116d2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116d2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116d2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116d2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116d2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116d2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116d2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116d2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116d2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116d2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116d30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116d305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116d30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116d30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116d31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116d31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116d31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116d32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116d324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116d32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116d32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116d33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116d336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116d33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116d33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116d343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116d34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116d34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116d35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116d355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116d35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116d35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116d36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116d36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116d36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116d37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116d374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116d37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116d37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116d38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116d38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116d38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116d38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116d393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116d39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116d39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116d3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116d3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116d3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116d3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116d3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116d3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116d3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116d3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116d3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116d3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116d3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116d3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116d3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116d3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116d3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116d3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116d3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116d3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116d3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116d3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116d3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116d3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116d402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116d40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116d40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116d41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116d41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116d41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116d42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116d42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116d429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116d42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116d432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116d43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116d43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116d44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116d44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116d44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116d44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116d451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116d45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116d45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116d45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116d463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116d46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116d46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116d470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116d47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116d479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116d47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116d482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116d48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116d48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116d49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116d49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116d498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116d49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116d4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116d4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116d4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116d4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116d4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116d4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116d4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116d4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116d4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116d4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116d4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116d4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116d4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116d4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116d4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116d4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116d4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116d4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116d4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116d4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116d4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116d4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116d50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116d507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116d50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116d510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116d51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116d51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116d51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116d52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116d526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116d52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116d52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116d53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116d538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116d53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116d54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116d545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116d54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116d54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116d55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116d557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116d56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116d56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116d57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116d57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116d57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116d57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116d584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116d58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.789s
user	0m0.296s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4523 (e28245f3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127e0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127e0bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127e0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127e0c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127e0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127e0d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127e0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127e0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127e0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127e0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127e0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127e0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127e0fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127e10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127e10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127e11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127e11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127e123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127e12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127e13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127e139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127e140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127e147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127e15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127e157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127e15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127e16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127e16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127e17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127e174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127e17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127e17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127e184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127e18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127e18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e19620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127e24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127e24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127e259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127e25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127e27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127e27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127e28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127e28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127e28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127e29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127e29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127e2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127e2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127e2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127e2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127e2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127e2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127e2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127e1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127e2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127e2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127e2e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127e2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127e2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127e2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127e2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127e2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127e30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127e30a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127e30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127e31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127e31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127e31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127e32460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127e32900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127e32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127e33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127e336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127e33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127e34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127e344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127e34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127e352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127e35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127e35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127e36080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e36520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127e36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127e37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127e377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127e37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127e380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127e38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127e38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127e38ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127e39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127e39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127e39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127e3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127e3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127e3af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127e3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127e3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127e3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127e3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e3e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e3f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e41040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e43540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127e45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127e46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127e46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127e46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127e47160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127e47600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127e47aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127e47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127e483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127e48880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127e48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127e491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127e49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127e4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127e4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127e4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127e4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127e4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127e4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127e4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127e4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127e4cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127e4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127e4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127e4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127e4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127e4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127e4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127e4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127e4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127e4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127e504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127e50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127e514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127e51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127e51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127e524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127e52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127e52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127e534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127e539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127e53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127e54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127e549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127e54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127e55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127e559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127e55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127e56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127e569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127e56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127e57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127e579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127e57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127e58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127e589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127e58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127e59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127e59990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127e59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127e5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127e5a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127e5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127e5b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127e5b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127e5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127e5c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127e5c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127e5ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127e5d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127e5d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127e5dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127e5e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127e5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127e5ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127e5f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127e5f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127e5fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127e603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127e60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127e60e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127e613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127e61910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127e61e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127e62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127e627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127e62c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127e630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127e63580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127e63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127e63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127e64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127e64800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127e64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127e65140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127e655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127e65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127e65f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127e663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127e66910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127e67030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127e67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127e67e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127e68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127e68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127e69040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127e69300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127e69910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127e695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127e4b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127e4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127e4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127e1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127e1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127e20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127e4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127e15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127e1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127e1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127e1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127e1bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127e1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127e14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127e20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127e2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127e68b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127e17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127e181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127e4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127e4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127e16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127e16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127e168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127e69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127e6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127e6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127e6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127e6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127e6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127e6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127e6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127e6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127e6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127e71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127e71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127e71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127e71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127e72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127e72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127e726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127e729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127e72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127e72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127e731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127e734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127e73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127e73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127e73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127e73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127e74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127e74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127e747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127e74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127e74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127e75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127e752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127e755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127e75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127e75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127e75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127e760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127e76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127e76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127e768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127e76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127e76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127e77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127e773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127e776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127e77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127e77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127e77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127e781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127e78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127e78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127e789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127e78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127e78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127e79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127e794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127e797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127e79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127e79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127e79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127e7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127e7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127e7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127e7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127e7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127e7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127e7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127e7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127e7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127e7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127e7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127e7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127e7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127e7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127e7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127e7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127e7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127e7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127e7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127e7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127e7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127e7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127e7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127e7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127e7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127e7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e7f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e7f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e7f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e7fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e7fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e80030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e80b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e80df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e81370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e81bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e81e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e82130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e82c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e82ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e83cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127e84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127e84d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127e84ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127e852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127e85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127e85830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127e85af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127e85db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127e86070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127e86330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127e865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127e868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127e86b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127e86e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117f058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117f05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117f06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117f06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117f06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117f06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117f07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117f077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117f07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117f08c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117f08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117f091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117f09640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117f09ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117f09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117f0a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117f0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117f0ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117f0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117f0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117f0b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117f0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117f0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117f0c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117f0cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117f0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117f0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117f0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117f0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117f0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117f100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117f10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117f109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117f10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117f11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117f116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117f11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117f11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117f12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117f128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117f12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117f13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117f13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117f14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117f147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117f14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117f150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117f15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117f15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117f15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117f16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117f166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117f16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117f16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117f17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117f17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117f17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117f18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117f185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117f18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117f18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117f19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117f197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117f19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117f1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117f1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117f1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117f1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117f1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117f1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117f1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117f1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117f1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117f1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117f1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117f1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117f1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117f1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117f1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117f1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f1fb80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f0b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f0bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f0cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f0dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f12a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f13130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f14230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f15460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f17d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f1a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f21980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f22760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f23540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f23fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f28a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f29a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f2a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f2ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f2df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f2e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f31c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f32a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f37420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f42880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f45220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f46940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f4ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f4b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f4c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f4cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f4dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f4e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f4eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f4fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f50410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f50960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f50eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f51ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f52940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f54920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f54e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f55910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f55e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f56900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f56e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f58390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f58e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f5a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f5ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f5b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f5b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f5be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f5c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f5cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f5d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f5dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f5e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f5edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f5f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f5fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f60860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f60d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f61640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f61ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f61f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f62420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f62d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f63fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f65310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f66150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f66870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f66f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f67250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f67a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f67d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f68310 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.243s
sys	0m0.137s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
