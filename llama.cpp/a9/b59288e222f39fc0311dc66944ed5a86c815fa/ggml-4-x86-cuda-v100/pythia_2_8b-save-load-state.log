+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.287 I build: 4925 (a9b59288e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.264.562 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.280.387 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.280.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.280.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.280.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.280.426 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.280.427 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.280.428 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.280.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.280.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.280.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.280.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.280.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.280.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.280.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.280.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.280.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.280.463 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.287.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.289.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.295.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.295.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.295.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.295.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.295.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.295.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.295.810 I llama_model_loader: - type  f32:  258 tensors
0.00.295.811 I llama_model_loader: - type q4_0:  129 tensors
0.00.295.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.295.814 I print_info: file format = GGUF V3 (latest)
0.00.295.815 I print_info: file type   = Q4_0
0.00.295.817 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.339.910 I load: special tokens cache size = 25
0.00.362.185 I load: token to piece cache size = 0.2984 MB
0.00.362.210 I print_info: arch             = gptneox
0.00.362.210 I print_info: vocab_only       = 0
0.00.362.211 I print_info: n_ctx_train      = 2048
0.00.362.211 I print_info: n_embd           = 2560
0.00.362.212 I print_info: n_layer          = 32
0.00.362.226 I print_info: n_head           = 32
0.00.362.228 I print_info: n_head_kv        = 32
0.00.362.228 I print_info: n_rot            = 20
0.00.362.229 I print_info: n_swa            = 0
0.00.362.229 I print_info: n_swa_pattern    = 1
0.00.362.230 I print_info: n_embd_head_k    = 80
0.00.362.231 I print_info: n_embd_head_v    = 80
0.00.362.234 I print_info: n_gqa            = 1
0.00.362.236 I print_info: n_embd_k_gqa     = 2560
0.00.362.238 I print_info: n_embd_v_gqa     = 2560
0.00.362.239 I print_info: f_norm_eps       = 1.0e-05
0.00.362.240 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.362.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.362.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.362.243 I print_info: f_logit_scale    = 0.0e+00
0.00.362.243 I print_info: f_attn_scale     = 0.0e+00
0.00.362.245 I print_info: n_ff             = 10240
0.00.362.248 I print_info: n_expert         = 0
0.00.362.249 I print_info: n_expert_used    = 0
0.00.362.249 I print_info: causal attn      = 1
0.00.362.249 I print_info: pooling type     = 0
0.00.362.250 I print_info: rope type        = 2
0.00.362.251 I print_info: rope scaling     = linear
0.00.362.253 I print_info: freq_base_train  = 10000.0
0.00.362.253 I print_info: freq_scale_train = 1
0.00.362.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.362.254 I print_info: rope_finetuned   = unknown
0.00.362.255 I print_info: ssm_d_conv       = 0
0.00.362.255 I print_info: ssm_d_inner      = 0
0.00.362.256 I print_info: ssm_d_state      = 0
0.00.362.256 I print_info: ssm_dt_rank      = 0
0.00.362.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.362.257 I print_info: model type       = 2.8B
0.00.362.259 I print_info: model params     = 2.78 B
0.00.362.259 I print_info: general.name     = 2.8B
0.00.362.262 I print_info: vocab type       = BPE
0.00.362.263 I print_info: n_vocab          = 50304
0.00.362.263 I print_info: n_merges         = 50009
0.00.362.264 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.362.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.362.265 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.362.265 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.362.266 I print_info: LF token         = 187 'Ċ'
0.00.362.267 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.362.268 I print_info: max token length = 1024
0.00.362.269 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.093 I load_tensors: offloading 10 repeating layers to GPU
0.00.455.103 I load_tensors: offloaded 10/33 layers to GPU
0.00.455.110 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.455.112 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.455.113 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.055.235 I llama_context: constructing llama_context
0.01.055.242 I llama_context: n_seq_max     = 1
0.01.055.242 I llama_context: n_ctx         = 2048
0.01.055.243 I llama_context: n_ctx_per_seq = 2048
0.01.055.243 I llama_context: n_batch       = 2048
0.01.055.244 I llama_context: n_ubatch      = 512
0.01.055.244 I llama_context: causal_attn   = 1
0.01.055.245 I llama_context: flash_attn    = 0
0.01.055.250 I llama_context: freq_base     = 10000.0
0.01.055.251 I llama_context: freq_scale    = 1
0.01.055.347 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.055.359 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.056.083 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.192.899 I init:        CPU KV buffer size =   440.00 MiB
0.01.192.929 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.231.790 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.231.805 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.231.806 I llama_context: graph nodes  = 1351
0.01.231.806 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.231.816 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.231.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.381.131 I llama_context: constructing llama_context
0.02.381.155 I llama_context: n_seq_max     = 1
0.02.381.156 I llama_context: n_ctx         = 2048
0.02.381.156 I llama_context: n_ctx_per_seq = 2048
0.02.381.157 I llama_context: n_batch       = 2048
0.02.381.157 I llama_context: n_ubatch      = 512
0.02.381.158 I llama_context: causal_attn   = 1
0.02.381.159 I llama_context: flash_attn    = 0
0.02.381.165 I llama_context: freq_base     = 10000.0
0.02.381.166 I llama_context: freq_scale    = 1
0.02.381.228 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.381.240 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.381.982 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.519.613 I init:        CPU KV buffer size =   440.00 MiB
0.02.519.640 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.548.020 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.548.030 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.548.031 I llama_context: graph nodes  = 1351
0.02.548.032 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.03.465.113 I llama_context: constructing llama_context
0.03.465.143 I llama_context: n_seq_max     = 1
0.03.465.143 I llama_context: n_ctx         = 2048
0.03.465.144 I llama_context: n_ctx_per_seq = 2048
0.03.465.144 I llama_context: n_batch       = 2048
0.03.465.145 I llama_context: n_ubatch      = 512
0.03.465.146 I llama_context: causal_attn   = 1
0.03.465.146 I llama_context: flash_attn    = 0
0.03.465.151 I llama_context: freq_base     = 10000.0
0.03.465.152 I llama_context: freq_scale    = 1
0.03.465.212 I llama_context:        CPU  output buffer size =     0.19 MiB
0.03.465.216 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.03.465.975 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.601.951 I init:        CPU KV buffer size =   440.00 MiB
0.03.601.973 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.630.458 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.630.468 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.630.469 I llama_context: graph nodes  = 1351
0.03.630.470 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


second run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


single seq run: The quick brown fox
            Gigot the wall from the wall,
            Scraped

real	0m5.344s
user	0m12.926s
sys	0m1.372s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.281 I build: 4925 (a9b59288e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.251.300 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.267.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.267.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.267.352 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.267.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.267.358 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.267.359 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.267.360 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.267.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.267.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.267.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.267.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.267.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.267.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.267.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.267.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.267.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.267.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.274.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.276.136 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.283.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.283.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.283.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.283.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.283.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.283.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.283.030 I llama_model_loader: - type  f32:  258 tensors
0.00.283.031 I llama_model_loader: - type q4_0:  129 tensors
0.00.283.032 I llama_model_loader: - type q6_K:    1 tensors
0.00.283.034 I print_info: file format = GGUF V3 (latest)
0.00.283.035 I print_info: file type   = Q4_0
0.00.283.039 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.328.967 I load: special tokens cache size = 25
0.00.351.188 I load: token to piece cache size = 0.2984 MB
0.00.351.206 I print_info: arch             = gptneox
0.00.351.207 I print_info: vocab_only       = 0
0.00.351.208 I print_info: n_ctx_train      = 2048
0.00.351.208 I print_info: n_embd           = 2560
0.00.351.209 I print_info: n_layer          = 32
0.00.351.227 I print_info: n_head           = 32
0.00.351.229 I print_info: n_head_kv        = 32
0.00.351.230 I print_info: n_rot            = 20
0.00.351.230 I print_info: n_swa            = 0
0.00.351.232 I print_info: n_swa_pattern    = 1
0.00.351.233 I print_info: n_embd_head_k    = 80
0.00.351.233 I print_info: n_embd_head_v    = 80
0.00.351.236 I print_info: n_gqa            = 1
0.00.351.238 I print_info: n_embd_k_gqa     = 2560
0.00.351.240 I print_info: n_embd_v_gqa     = 2560
0.00.351.241 I print_info: f_norm_eps       = 1.0e-05
0.00.351.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.351.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.351.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.351.245 I print_info: f_logit_scale    = 0.0e+00
0.00.351.248 I print_info: f_attn_scale     = 0.0e+00
0.00.351.250 I print_info: n_ff             = 10240
0.00.351.250 I print_info: n_expert         = 0
0.00.351.251 I print_info: n_expert_used    = 0
0.00.351.251 I print_info: causal attn      = 1
0.00.351.252 I print_info: pooling type     = 0
0.00.351.252 I print_info: rope type        = 2
0.00.351.253 I print_info: rope scaling     = linear
0.00.351.254 I print_info: freq_base_train  = 10000.0
0.00.351.255 I print_info: freq_scale_train = 1
0.00.351.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.351.256 I print_info: rope_finetuned   = unknown
0.00.351.256 I print_info: ssm_d_conv       = 0
0.00.351.256 I print_info: ssm_d_inner      = 0
0.00.351.257 I print_info: ssm_d_state      = 0
0.00.351.257 I print_info: ssm_dt_rank      = 0
0.00.351.258 I print_info: ssm_dt_b_c_rms   = 0
0.00.351.258 I print_info: model type       = 2.8B
0.00.351.259 I print_info: model params     = 2.78 B
0.00.351.261 I print_info: general.name     = 2.8B
0.00.351.263 I print_info: vocab type       = BPE
0.00.351.265 I print_info: n_vocab          = 50304
0.00.351.265 I print_info: n_merges         = 50009
0.00.351.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.351.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.351.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.351.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.351.269 I print_info: LF token         = 187 'Ċ'
0.00.351.270 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.351.271 I print_info: max token length = 1024
0.00.351.272 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.802 I load_tensors: offloading 10 repeating layers to GPU
0.00.445.812 I load_tensors: offloaded 10/33 layers to GPU
0.00.445.821 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.445.823 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.445.824 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.039.869 I llama_context: constructing llama_context
0.01.039.876 I llama_context: n_seq_max     = 1
0.01.039.876 I llama_context: n_ctx         = 2048
0.01.039.877 I llama_context: n_ctx_per_seq = 2048
0.01.039.877 I llama_context: n_batch       = 2048
0.01.039.878 I llama_context: n_ubatch      = 512
0.01.039.878 I llama_context: causal_attn   = 1
0.01.039.879 I llama_context: flash_attn    = 1
0.01.039.884 I llama_context: freq_base     = 10000.0
0.01.039.885 I llama_context: freq_scale    = 1
0.01.039.977 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.039.988 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.040.765 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.177.613 I init:        CPU KV buffer size =   440.00 MiB
0.01.177.642 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.205.188 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.205.201 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.205.202 I llama_context: graph nodes  = 1224
0.01.205.203 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.205.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.205.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.065.125 I llama_context: constructing llama_context
0.02.065.185 I llama_context: n_seq_max     = 1
0.02.065.196 I llama_context: n_ctx         = 2048
0.02.065.207 I llama_context: n_ctx_per_seq = 2048
0.02.065.218 I llama_context: n_batch       = 2048
0.02.065.229 I llama_context: n_ubatch      = 512
0.02.065.243 I llama_context: causal_attn   = 1
0.02.065.255 I llama_context: flash_attn    = 1
0.02.065.274 I llama_context: freq_base     = 10000.0
0.02.065.290 I llama_context: freq_scale    = 1
0.02.065.368 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.065.468 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.066.497 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.203.204 I init:        CPU KV buffer size =   440.00 MiB
0.02.203.235 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.231.839 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.231.853 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.231.854 I llama_context: graph nodes  = 1224
0.02.231.854 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.02.907.728 I llama_context: constructing llama_context
0.02.907.755 I llama_context: n_seq_max     = 1
0.02.907.756 I llama_context: n_ctx         = 2048
0.02.907.756 I llama_context: n_ctx_per_seq = 2048
0.02.907.757 I llama_context: n_batch       = 2048
0.02.907.758 I llama_context: n_ubatch      = 512
0.02.907.759 I llama_context: causal_attn   = 1
0.02.907.759 I llama_context: flash_attn    = 1
0.02.907.765 I llama_context: freq_base     = 10000.0
0.02.907.766 I llama_context: freq_scale    = 1
0.02.907.826 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.907.835 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.908.596 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.044.772 I init:        CPU KV buffer size =   440.00 MiB
0.03.044.798 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.072.677 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.072.690 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.072.691 I llama_context: graph nodes  = 1224
0.03.072.692 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


second run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


single seq run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the

real	0m4.151s
user	0m11.353s
sys	0m1.334s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.313 I build: 4925 (a9b59288e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.263.236 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.279.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.279.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.279.256 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.279.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.279.260 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.279.261 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.279.261 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.279.265 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.279.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.279.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.279.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.279.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.279.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.279.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.279.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.279.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.279.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.286.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.287.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.294.660 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.294.669 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.294.669 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.294.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.294.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.294.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.294.675 I llama_model_loader: - type  f32:  258 tensors
0.00.294.675 I llama_model_loader: - type q4_0:  129 tensors
0.00.294.676 I llama_model_loader: - type q6_K:    1 tensors
0.00.294.678 I print_info: file format = GGUF V3 (latest)
0.00.294.679 I print_info: file type   = Q4_0
0.00.294.682 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.339.430 I load: special tokens cache size = 25
0.00.362.499 I load: token to piece cache size = 0.2984 MB
0.00.362.518 I print_info: arch             = gptneox
0.00.362.519 I print_info: vocab_only       = 0
0.00.362.519 I print_info: n_ctx_train      = 2048
0.00.362.520 I print_info: n_embd           = 2560
0.00.362.520 I print_info: n_layer          = 32
0.00.362.541 I print_info: n_head           = 32
0.00.362.543 I print_info: n_head_kv        = 32
0.00.362.543 I print_info: n_rot            = 20
0.00.362.544 I print_info: n_swa            = 0
0.00.362.546 I print_info: n_swa_pattern    = 1
0.00.362.546 I print_info: n_embd_head_k    = 80
0.00.362.546 I print_info: n_embd_head_v    = 80
0.00.362.549 I print_info: n_gqa            = 1
0.00.362.551 I print_info: n_embd_k_gqa     = 2560
0.00.362.553 I print_info: n_embd_v_gqa     = 2560
0.00.362.555 I print_info: f_norm_eps       = 1.0e-05
0.00.362.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.362.561 I print_info: f_clamp_kqv      = 0.0e+00
0.00.362.561 I print_info: f_max_alibi_bias = 0.0e+00
0.00.362.563 I print_info: f_logit_scale    = 0.0e+00
0.00.362.564 I print_info: f_attn_scale     = 0.0e+00
0.00.362.566 I print_info: n_ff             = 10240
0.00.362.569 I print_info: n_expert         = 0
0.00.362.570 I print_info: n_expert_used    = 0
0.00.362.570 I print_info: causal attn      = 1
0.00.362.571 I print_info: pooling type     = 0
0.00.362.571 I print_info: rope type        = 2
0.00.362.572 I print_info: rope scaling     = linear
0.00.362.573 I print_info: freq_base_train  = 10000.0
0.00.362.574 I print_info: freq_scale_train = 1
0.00.362.574 I print_info: n_ctx_orig_yarn  = 2048
0.00.362.575 I print_info: rope_finetuned   = unknown
0.00.362.575 I print_info: ssm_d_conv       = 0
0.00.362.576 I print_info: ssm_d_inner      = 0
0.00.362.576 I print_info: ssm_d_state      = 0
0.00.362.577 I print_info: ssm_dt_rank      = 0
0.00.362.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.362.578 I print_info: model type       = 2.8B
0.00.362.579 I print_info: model params     = 2.78 B
0.00.362.580 I print_info: general.name     = 2.8B
0.00.362.583 I print_info: vocab type       = BPE
0.00.362.584 I print_info: n_vocab          = 50304
0.00.362.584 I print_info: n_merges         = 50009
0.00.362.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.362.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.362.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.362.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.362.587 I print_info: LF token         = 187 'Ċ'
0.00.362.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.362.590 I print_info: max token length = 1024
0.00.362.598 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.458.207 I load_tensors: offloading 32 repeating layers to GPU
0.00.458.221 I load_tensors: offloading output layer to GPU
0.00.458.222 I load_tensors: offloaded 33/33 layers to GPU
0.00.458.230 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.458.232 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.719.501 I llama_context: constructing llama_context
0.00.719.508 I llama_context: n_seq_max     = 1
0.00.719.509 I llama_context: n_ctx         = 2048
0.00.719.509 I llama_context: n_ctx_per_seq = 2048
0.00.719.510 I llama_context: n_batch       = 2048
0.00.719.510 I llama_context: n_ubatch      = 512
0.00.719.511 I llama_context: causal_attn   = 1
0.00.719.513 I llama_context: flash_attn    = 0
0.00.719.518 I llama_context: freq_base     = 10000.0
0.00.719.520 I llama_context: freq_scale    = 1
0.00.721.041 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.721.058 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.722.204 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.722.218 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.738.567 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.00.738.577 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.738.578 I llama_context: graph nodes  = 1351
0.00.738.578 I llama_context: graph splits = 2
0.00.738.585 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.01.699.531 I llama_context: constructing llama_context
0.01.699.544 I llama_context: n_seq_max     = 1
0.01.699.544 I llama_context: n_ctx         = 2048
0.01.699.545 I llama_context: n_ctx_per_seq = 2048
0.01.699.545 I llama_context: n_batch       = 2048
0.01.699.546 I llama_context: n_ubatch      = 512
0.01.699.547 I llama_context: causal_attn   = 1
0.01.699.547 I llama_context: flash_attn    = 0
0.01.699.553 I llama_context: freq_base     = 10000.0
0.01.699.554 I llama_context: freq_scale    = 1
0.01.699.629 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.699.638 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.702.763 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.702.773 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.719.894 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.01.719.903 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.719.904 I llama_context: graph nodes  = 1351
0.01.719.905 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.02.402.152 I llama_context: constructing llama_context
0.02.402.165 I llama_context: n_seq_max     = 1
0.02.402.165 I llama_context: n_ctx         = 2048
0.02.402.166 I llama_context: n_ctx_per_seq = 2048
0.02.402.166 I llama_context: n_batch       = 2048
0.02.402.167 I llama_context: n_ubatch      = 512
0.02.402.167 I llama_context: causal_attn   = 1
0.02.402.168 I llama_context: flash_attn    = 0
0.02.402.174 I llama_context: freq_base     = 10000.0
0.02.402.175 I llama_context: freq_scale    = 1
0.02.402.252 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.02.402.260 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.405.444 I init:      CUDA0 KV buffer size =   640.00 MiB
0.02.405.455 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.422.345 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.02.422.357 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.02.422.358 I llama_context: graph nodes  = 1351
0.02.422.358 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


second run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


single seq run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st

real	0m4.583s
user	0m3.881s
sys	0m0.702s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.302 I build: 4925 (a9b59288e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.258.708 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.274.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.274.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.274.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.274.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.274.636 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.274.637 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.274.637 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.274.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.274.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.274.643 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.274.644 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.274.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.274.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.274.648 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.274.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.274.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.274.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.281.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.283.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.289.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.289.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.289.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.289.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.289.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.289.970 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.289.973 I llama_model_loader: - type  f32:  258 tensors
0.00.289.973 I llama_model_loader: - type q4_0:  129 tensors
0.00.289.974 I llama_model_loader: - type q6_K:    1 tensors
0.00.289.977 I print_info: file format = GGUF V3 (latest)
0.00.289.978 I print_info: file type   = Q4_0
0.00.289.981 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.337.446 I load: special tokens cache size = 25
0.00.359.668 I load: token to piece cache size = 0.2984 MB
0.00.359.687 I print_info: arch             = gptneox
0.00.359.689 I print_info: vocab_only       = 0
0.00.359.690 I print_info: n_ctx_train      = 2048
0.00.359.690 I print_info: n_embd           = 2560
0.00.359.691 I print_info: n_layer          = 32
0.00.359.707 I print_info: n_head           = 32
0.00.359.709 I print_info: n_head_kv        = 32
0.00.359.710 I print_info: n_rot            = 20
0.00.359.710 I print_info: n_swa            = 0
0.00.359.711 I print_info: n_swa_pattern    = 1
0.00.359.711 I print_info: n_embd_head_k    = 80
0.00.359.712 I print_info: n_embd_head_v    = 80
0.00.359.714 I print_info: n_gqa            = 1
0.00.359.716 I print_info: n_embd_k_gqa     = 2560
0.00.359.718 I print_info: n_embd_v_gqa     = 2560
0.00.359.720 I print_info: f_norm_eps       = 1.0e-05
0.00.359.721 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.359.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.359.723 I print_info: f_max_alibi_bias = 0.0e+00
0.00.359.724 I print_info: f_logit_scale    = 0.0e+00
0.00.359.724 I print_info: f_attn_scale     = 0.0e+00
0.00.359.726 I print_info: n_ff             = 10240
0.00.359.730 I print_info: n_expert         = 0
0.00.359.730 I print_info: n_expert_used    = 0
0.00.359.731 I print_info: causal attn      = 1
0.00.359.731 I print_info: pooling type     = 0
0.00.359.731 I print_info: rope type        = 2
0.00.359.732 I print_info: rope scaling     = linear
0.00.359.734 I print_info: freq_base_train  = 10000.0
0.00.359.734 I print_info: freq_scale_train = 1
0.00.359.735 I print_info: n_ctx_orig_yarn  = 2048
0.00.359.737 I print_info: rope_finetuned   = unknown
0.00.359.738 I print_info: ssm_d_conv       = 0
0.00.359.738 I print_info: ssm_d_inner      = 0
0.00.359.739 I print_info: ssm_d_state      = 0
0.00.359.739 I print_info: ssm_dt_rank      = 0
0.00.359.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.359.740 I print_info: model type       = 2.8B
0.00.359.741 I print_info: model params     = 2.78 B
0.00.359.742 I print_info: general.name     = 2.8B
0.00.359.745 I print_info: vocab type       = BPE
0.00.359.747 I print_info: n_vocab          = 50304
0.00.359.747 I print_info: n_merges         = 50009
0.00.359.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.359.748 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.359.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.359.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.359.751 I print_info: LF token         = 187 'Ċ'
0.00.359.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.359.752 I print_info: max token length = 1024
0.00.359.753 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.452.356 I load_tensors: offloading 32 repeating layers to GPU
0.00.452.366 I load_tensors: offloading output layer to GPU
0.00.452.367 I load_tensors: offloaded 33/33 layers to GPU
0.00.452.377 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.452.379 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.705.697 I llama_context: constructing llama_context
0.00.705.704 I llama_context: n_seq_max     = 1
0.00.705.705 I llama_context: n_ctx         = 2048
0.00.705.706 I llama_context: n_ctx_per_seq = 2048
0.00.705.706 I llama_context: n_batch       = 2048
0.00.705.707 I llama_context: n_ubatch      = 512
0.00.705.707 I llama_context: causal_attn   = 1
0.00.705.708 I llama_context: flash_attn    = 1
0.00.705.714 I llama_context: freq_base     = 10000.0
0.00.705.715 I llama_context: freq_scale    = 1
0.00.707.050 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.707.063 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.708.271 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.708.284 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.725.613 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.725.624 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.725.625 I llama_context: graph nodes  = 1224
0.00.725.625 I llama_context: graph splits = 2
0.00.725.633 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.00.927.962 I llama_context: constructing llama_context
0.00.927.972 I llama_context: n_seq_max     = 1
0.00.927.973 I llama_context: n_ctx         = 2048
0.00.927.973 I llama_context: n_ctx_per_seq = 2048
0.00.927.974 I llama_context: n_batch       = 2048
0.00.927.974 I llama_context: n_ubatch      = 512
0.00.927.975 I llama_context: causal_attn   = 1
0.00.927.975 I llama_context: flash_attn    = 1
0.00.927.980 I llama_context: freq_base     = 10000.0
0.00.927.981 I llama_context: freq_scale    = 1
0.00.928.061 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.928.070 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.931.630 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.931.641 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.948.004 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.948.013 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.948.014 I llama_context: graph nodes  = 1224
0.00.948.015 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.01.111.743 I llama_context: constructing llama_context
0.01.111.753 I llama_context: n_seq_max     = 1
0.01.111.754 I llama_context: n_ctx         = 2048
0.01.111.754 I llama_context: n_ctx_per_seq = 2048
0.01.111.754 I llama_context: n_batch       = 2048
0.01.111.755 I llama_context: n_ubatch      = 512
0.01.111.755 I llama_context: causal_attn   = 1
0.01.111.756 I llama_context: flash_attn    = 1
0.01.111.760 I llama_context: freq_base     = 10000.0
0.01.111.761 I llama_context: freq_scale    = 1
0.01.111.846 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.111.854 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.115.327 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.115.335 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.131.768 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.01.131.778 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.131.779 I llama_context: graph nodes  = 1224
0.01.131.779 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


second run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


single seq run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see

real	0m1.582s
user	0m0.896s
sys	0m0.684s
